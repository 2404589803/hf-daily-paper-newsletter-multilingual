[
  {
    "paper": {
      "id": "2506.01939",
      "authors": [
        {
          "_id": "683e7a6d97fd742a8edee1ba",
          "user": {
            "_id": "6486dde1f74857df3f1a5828",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
            "isPro": false,
            "fullname": "Shenzhi Wang",
            "user": "shenzhi-wang",
            "type": "user"
          },
          "name": "Shenzhi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:16.481Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1bb",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1bc",
          "name": "Chang Gao",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1bd",
          "user": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "isPro": false,
            "fullname": "Chujie Zheng",
            "user": "chujiezheng",
            "type": "user"
          },
          "name": "Chujie Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:13.605Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1be",
          "name": "Shixuan Liu",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1bf",
          "name": "Rui Lu",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c0",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c1",
          "user": {
            "_id": "63f30b870a16587ea970edfe",
            "avatarUrl": "/avatars/059491b33fecec69032e6d481229ee31.svg",
            "isPro": false,
            "fullname": "Xiong-Hui Chen",
            "user": "xionghuichen",
            "type": "user"
          },
          "name": "Xionghui Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:10.358Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c2",
          "name": "Jianxin Yang",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c3",
          "user": {
            "_id": "64704e973601bb7b06643e98",
            "avatarUrl": "/avatars/52e51f4d1be6769e4397b8be2799cf32.svg",
            "isPro": false,
            "fullname": "Zhenru Zhang",
            "user": "Zhenru",
            "type": "user"
          },
          "name": "Zhenru Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:40:21.027Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c4",
          "user": {
            "_id": "666aacfb918ba11c7c598194",
            "avatarUrl": "/avatars/45bee8f1fdbdd256ee47d25e4bf01a7a.svg",
            "isPro": false,
            "fullname": "Yuqiong Liu",
            "user": "lyq333",
            "type": "user"
          },
          "name": "Yuqiong Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:40:30.240Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c5",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c6",
          "name": "Andrew Zhao",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c7",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c8",
          "name": "Shiji Song",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1c9",
          "user": {
            "_id": "63d9d68c1cae35c27bf7a6a7",
            "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
            "isPro": false,
            "fullname": "Bowen Yu",
            "user": "Tigerph",
            "type": "user"
          },
          "name": "Bowen Yu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T04:30:38.648Z",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1ca",
          "name": "Gao Huang",
          "hidden": false
        },
        {
          "_id": "683e7a6d97fd742a8edee1cb",
          "user": {
            "_id": "620760a26e3b7210c2ff1943",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
            "isPro": false,
            "fullname": "Junyang Lin",
            "user": "JustinLin610",
            "type": "user"
          },
          "name": "Junyang Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:39:54.278Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:54:39.000Z",
      "submittedOnDailyAt": "2025-06-03T03:09:30.655Z",
      "title": "80/20ルールを超えて：高エントロピーの少数トークンが効果的なLLM論理の強化学習を駆動する",
      "submittedOnDailyBy": {
        "_id": "6486dde1f74857df3f1a5828",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
        "isPro": false,
        "fullname": "Shenzhi Wang",
        "user": "shenzhi-wang",
        "type": "user"
      },
      "summary": "RLVR（Verifiable Rewards）は、大語言モデル（LLMs）の理由能力を向上させる強化学習の有力なアプローチとして現れたが、その機構はまだ理解されていません。本研究では、トークンエントロピーパターンの新しい視点からRLVRを先駆的に調査し、異なるトークンが理由性能にどのように影響しているかを詳細に分析します。Chain-of-Thought（CoT）理由のトークンエントロピーパターンを検討し、その中では、高エントロピーを示すトークンが少なくとも一部しかないことが見出され、これらのトークンは理由の多様なパスウェークを引き出す重要なフォークとして機能していることがわかります。また、RLVRトレーニング中のエントロピーパターンの進化を研究すると、RLVRは主に基礎モデルのエントロピーパターンに沿っていることがわかり、特に高エントロピートークンのエントロピーを調整していることがわかります。これらの発見は、RLVRにおける高エントロピートークン（即、フォークトークン）の重要性を明らかにします。最終的に、ポリシーグラデイン更新をフォークトークンに限定し、Qwen3-8B基礎モデルでは性能を比較的または超えるように20%のトークンを利用することができ、Qwen3-32B（AIME'25で+11.04、AIME'24で+7.71）とQwen3-14B（AIME'25で+4.79、AIME'24で+5.21）の基礎モデルでは大幅に性能を超えることが見出されます。これらの発見は、RLVRの効果は理由の方向を決定する高エントロピートークンの最適化に基づくことを示し、トークンエントロピーの視点からRLVRを理解し、高エントロピーの少数トークンを活用してLLMの理由能力を進化させることが可能であることを示します。",
      "upvotes": 66,
      "discussionId": "683e7a6e97fd742a8edee227",
      "projectPage": "https://shenzhi-wang.github.io/high-entropy-minority-tokens-rlvr/",
      "ai_summary": "Token entropy patterns are crucial in RLVR, with high-entropy tokens significantly impacting reasoning performance and model optimization.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "RLVR",
        "Large Language Models",
        "LLMs",
        "token entropy patterns",
        "Chain-of-Thought",
        "CoT reasoning",
        "high-entropy tokens",
        "policy gradient updates",
        "Qwen3-8B",
        "Qwen3-32B",
        "Qwen3-14B",
        "AIME"
      ]
    },
    "publishedAt": "2025-06-02T13:54:39.000Z",
    "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective\n  Reinforcement Learning for LLM Reasoning",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful approach to enhancing the reasoning capabilities of Large Language\nModels (LLMs), while its mechanisms are not yet well understood. In this work,\nwe undertake a pioneering exploration of RLVR through the novel perspective of\ntoken entropy patterns, comprehensively analyzing how different tokens\ninfluence reasoning performance. By examining token entropy patterns in\nChain-of-Thought (CoT) reasoning, we observe that only a small fraction of\ntokens exhibit high entropy, and these tokens act as critical forks that steer\nthe model toward diverse reasoning pathways. Furthermore, studying how entropy\npatterns evolve during RLVR training reveals that RLVR largely adheres to the\nbase model's entropy patterns, primarily adjusting the entropy of high-entropy\ntokens. These findings highlight the significance of high-entropy tokens (i.e.,\nforking tokens) to RLVR. We ultimately improve RLVR by restricting policy\ngradient updates to forking tokens and uncover a finding even beyond the 80/20\nrule: utilizing only 20% of the tokens while maintaining performance comparable\nto full-gradient updates on the Qwen3-8B base model and significantly\nsurpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71\non AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,\nhighlighting a strong scaling trend. In contrast, training exclusively on the\n80% lowest-entropy tokens leads to a marked decline in performance. These\nfindings indicate that the efficacy of RLVR primarily arises from optimizing\nthe high-entropy tokens that decide reasoning directions. Collectively, our\nresults highlight the potential to understand RLVR through a token-entropy\nperspective and optimize RLVR by leveraging high-entropy minority tokens to\nfurther improve LLM reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01939.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6486dde1f74857df3f1a5828",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
      "fullname": "Shenzhi Wang",
      "name": "shenzhi-wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 326
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01049",
      "authors": [
        {
          "_id": "683e5b9a1167d9630159b27f",
          "user": {
            "_id": "640f7083208821a59b74c757",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678735253848-640f7083208821a59b74c757.jpeg",
            "isPro": false,
            "fullname": "Siyuan Li",
            "user": "Lupin1998",
            "type": "user"
          },
          "name": "Siyuan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:39.296Z",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b280",
          "user": {
            "_id": "670880950e79a8b46f7ff9dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
            "isPro": false,
            "fullname": "Juanxi Tian",
            "user": "Juanxi",
            "type": "user"
          },
          "name": "Juanxi Tian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:40:52.165Z",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b281",
          "user": {
            "_id": "6594d390674349122ce6f368",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/KdWz6lZyGYQpjAgBDeiC1.jpeg",
            "isPro": false,
            "fullname": "Zedong Wang (Jacky)",
            "user": "ZedongWangAI",
            "type": "user"
          },
          "name": "Zedong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:41.911Z",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b282",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b283",
          "user": {
            "_id": "67ee7eef2a8e2fd1445407ab",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/TelMtjU8Ki8ulQU4-b0He.jpeg",
            "isPro": false,
            "fullname": "Zicheng Liu",
            "user": "MarcusB3n",
            "type": "user"
          },
          "name": "Zicheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:41:29.426Z",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b284",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "683e5b9a1167d9630159b285",
          "user": {
            "_id": "67cd1d6c96e0a33b99c78b26",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wsUk7e5BPHa6L7GroWtat.png",
            "isPro": false,
            "fullname": "Dan Xu",
            "user": "danxu",
            "type": "user"
          },
          "name": "Dan Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:41:02.065Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T15:30:37.000Z",
      "submittedOnDailyAt": "2025-06-03T00:52:33.852Z",
      "title": "エロティクスLLMの制御を行うための学習レートのスケーリングによる勾配グルーピング",
      "submittedOnDailyBy": {
        "_id": "6594d390674349122ce6f368",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/KdWz6lZyGYQpjAgBDeiC1.jpeg",
        "isPro": false,
        "fullname": "Zedong Wang (Jacky)",
        "user": "ZedongWangAI",
        "type": "user"
      },
      "summary": "大規模言語モデル（LLMs）の訓練は、その巨大なサイズと異なるアーキテクチャによって課題を抱えています。AdamWなどの適応可能な最適化アルゴリズムは勾配の変動を解決することができますが、パラメータごとに学習率を適切に評価するには難しく、訓練の不穩定、収束の遅めさ、パラメータ効率的な微調節（PEFT）手法との不適合性による問題を残しています。本論文では、適応可能な学習率の評価を改善するためのグラフィングワイズヤースタップ（SGG）を導入します。SGGは、動的なグループ分けとグループ特有のスケーリングを用いて、各層の勾配統計をクラスターにまとめ、各パラメータの学習率を調整します。SGGは、グループ全体の制約を加えながら、それぞれのパラメータの適切な応答を維持することを目指しています。多様な（M）LLMベンチマーク上の実験により、SGGは現在の最適化アルゴリズムと統合し、ベースラインよりも一貫した効果と速い収束を示し、モデルサイズの種類によらず実現できます。SGGは、変動するバッチサイズと学習率による不穩定を克服し、LLMの最適化に強固な選択肢として立ち上がっています。",
      "upvotes": 22,
      "discussionId": "683e5b9b1167d9630159b2ef",
      "ai_summary": "SGG, an optimizer wrapper, enhances adaptive learning rates for large language models by grouping gradients and applying cluster-specific scaling, improving convergence and stability.",
      "ai_keywords": [
        "large language models",
        "adaptive optimizers",
        "AdamW",
        "parameter-wise learning rate estimation",
        "training instability",
        "parameter-efficient fine-tuning",
        "Scaling with Gradient Grouping",
        "gradient grouping",
        "cluster-specific scaling",
        "LLM benchmarks",
        "robust choice for LLM optimization"
      ]
    },
    "publishedAt": "2025-06-01T11:30:37.000Z",
    "title": "Taming LLMs by Scaling Learning Rates with Gradient Grouping",
    "summary": "Training large language models (LLMs) poses challenges due to their massive\nscale and heterogeneous architectures. While adaptive optimizers like AdamW\nhelp address gradient variations, they still struggle with efficient and\neffective parameter-wise learning rate estimation, resulting in training\ninstability, slow convergence, and poor compatibility with parameter-efficient\nfine-tuning (PEFT) techniques. This work introduces Scaling with Gradient\nGrouping (SGG), an optimizer wrapper that improves adaptive learning rate\nestimation by dynamic grouping and group-specific scaling. SGG first groups\ngradient statistics in each layer into clusters and then applies\ncluster-specific scaling to calibrate learning rates for each parameter, thus\nimposing collective group-wise constraints while maintaining precise\nper-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that\nSGG integrates seamlessly with existing optimizers, and offers consistent gains\nand faster convergence over baselines, with various model sizes. Its stability\nacross varying batch sizes and learning rates establishes SGG as a robust\nchoice for LLM optimization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01049.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6594d390674349122ce6f368",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/KdWz6lZyGYQpjAgBDeiC1.jpeg",
      "fullname": "Zedong Wang (Jacky)",
      "name": "ZedongWangAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23590",
      "authors": [
        {
          "_id": "683e6b2d97fd742a8edb8a8e",
          "user": {
            "_id": "64f5c7cb65a4b1acb20ffc15",
            "avatarUrl": "/avatars/67c42a6e26bbeddc4bf706eb8f1a75b1.svg",
            "isPro": false,
            "fullname": "Zifu Wang",
            "user": "wangzifu",
            "type": "user"
          },
          "name": "Zifu Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:41:52.137Z",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a8f",
          "user": {
            "_id": "6477b2038ab7e732b6d8a9b5",
            "avatarUrl": "/avatars/c859a35b8965a904f103bbc34f36ab2a.svg",
            "isPro": false,
            "fullname": "Junyi Zhu",
            "user": "RyanZhu",
            "type": "user"
          },
          "name": "Junyi Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:42:00.703Z",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a90",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a91",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a92",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a93",
          "name": "Jiaqian Yu",
          "hidden": false
        },
        {
          "_id": "683e6b2d97fd742a8edb8a94",
          "name": "Matthew B. Blaschko",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:01:22.000Z",
      "submittedOnDailyAt": "2025-06-03T01:56:17.990Z",
      "title": "Jigsaw-R1: ジグサーパズルによるルールベース可視化再強化学習の研究",
      "submittedOnDailyBy": {
        "_id": "64f5c7cb65a4b1acb20ffc15",
        "avatarUrl": "/avatars/67c42a6e26bbeddc4bf706eb8f1a75b1.svg",
        "isPro": false,
        "fullname": "Zifu Wang",
        "user": "wangzifu",
        "type": "user"
      },
      "summary": "ルールベースの強化学習（RL）を多モデルの大語言モデル（MLLMs）に適用することにより、文書だけの領域での見つかり方や特徴を偏りさせる可能性があり、視覚的な仕事に特に重要なチャレンジが生じます。本論文は、ジジーパズルを構造付きの実験フレームワークとして用いてルールベースの視覚的なRLについての詳細な研究を行います。ジジーパズルは内蔵的な正解、難易度の調整、複雑な判断を求めることで、本研究に最適なものとなります。本研究では、以下の数ノウ項が明らかになります。\n\n1. MLLMsは、最も簡単なジジーパズルでは近似した乱数の予測に近い性能を示したが、ファイナルチューニングでは近似した完全な正確性を達成し、複雑な、未見の配置に一般化できます。\n2. ジジーパズルでの学習は、他の視覚的な仕事にも一般化を引き起こすことができ、その効果は特定の仕事の設定に依存します。\n3. MLLMsは、明示的な理由を持って学習して一般化することができ、ほとんどの開放ソースモデルは直接の回答を好ましくなっています。そのため、ステップごとの理由を学ぶように訓練された場合、最終的な回答を求める過程では理由を無視することも可能です。\n4. 複雑な理由のパターンは、既存的であり、その頻度は学習と仕事の難易度に伴い増加します。\n5. 結果として、RLは視覚的な仕事において、サブバイダーフィードワーニング（SFT）よりも効果的な一般化を示し、SFTの初期ステップは後続のRL最適化において妨げることがあります。\n\nこれらの見つかり方は、ジジーパズルに基づいているため、他の視覚的な仕事においては異なるものとなる可能性がありますが、この研究は、ルールベースの視覚的なRLの理解とその多モデル学習の可能性に貢献します。コードは、https://github.com/zifuwanggg/Jigsaw-R1 に公開されています。",
      "upvotes": 20,
      "discussionId": "683e6b2e97fd742a8edb8ac1",
      "githubRepo": "https://github.com/zifuwanggg/Jigsaw-R1",
      "ai_summary": "Rule-based reinforcement learning applied to multimodal large language models demonstrates effective generalization in visual tasks, particularly using jigsaw puzzles, outperforming supervised fine-tuning.",
      "ai_keywords": [
        "rule-based reinforcement learning",
        "multimodal large language models",
        "visual RL",
        "jigsaw puzzles",
        "fine-tuning",
        "supervised fine-tuning",
        "complex decision-making",
        "visual tasks",
        "step-by-step reasoning",
        "generalization"
      ]
    },
    "publishedAt": "2025-05-29T12:01:22.000Z",
    "title": "Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with\n  Jigsaw Puzzles",
    "summary": "The application of rule-based reinforcement learning (RL) to multimodal large\nlanguage models (MLLMs) introduces unique challenges and potential deviations\nfrom findings in text-only domains, particularly for perception-heavy tasks.\nThis paper provides a comprehensive study of rule-based visual RL, using jigsaw\npuzzles as a structured experimental framework. Jigsaw puzzles offer inherent\nground truth, adjustable difficulty, and demand complex decision-making, making\nthem ideal for this study. Our research reveals several key findings:\nFirstly, we find that MLLMs, initially performing near to random\nguessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and\ngeneralize to complex, unseen configurations through fine-tuning.\nSecondly, training on jigsaw puzzles can induce generalization to\nother visual tasks, with effectiveness tied to specific task configurations.\nThirdly, MLLMs can learn and generalize with or without explicit\nreasoning, though open-source models often favor direct answering.\nConsequently, even when trained for step-by-step reasoning, they can ignore the\nthinking process in deriving the final answer. Fourthly, we observe\nthat complex reasoning patterns appear to be pre-existing rather than emergent,\nwith their frequency increasing alongside training and task difficulty.\nFinally, our results demonstrate that RL exhibits more effective\ngeneralization than Supervised Fine-Tuning (SFT), and an initial SFT cold start\nphase can hinder subsequent RL optimization. Although these observations are\nbased on jigsaw puzzles and may vary across other visual tasks, this research\ncontributes a valuable piece of jigsaw to the larger puzzle of collective\nunderstanding rule-based visual RL and its potential in multimodal learning.\nThe code is available at: https://github.com/zifuwanggg/Jigsaw-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f5c7cb65a4b1acb20ffc15",
      "avatarUrl": "/avatars/67c42a6e26bbeddc4bf706eb8f1a75b1.svg",
      "fullname": "Zifu Wang",
      "name": "wangzifu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00539",
      "authors": [
        {
          "_id": "683e76c17a0996f979e72700",
          "user": {
            "_id": "671a4abbef737c0abe21b3f8",
            "avatarUrl": "/avatars/da826af5472a3b9f1969f0c766672731.svg",
            "isPro": false,
            "fullname": "Ruihan Yang",
            "user": "rhyang2021",
            "type": "user"
          },
          "name": "Ruihan Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:32.053Z",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72701",
          "name": "Yikai Zhang",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72702",
          "user": {
            "_id": "63f86b099f87cc3e645b51d9",
            "avatarUrl": "/avatars/27ca5ba425640bf67474cee871e8e53a.svg",
            "isPro": false,
            "fullname": "Ellie Chen",
            "user": "sheep33333",
            "type": "user"
          },
          "name": "Aili Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:41:28.422Z",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72703",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72704",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72705",
          "name": "Jiangjie Chen",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72706",
          "name": "Deqing Yang",
          "hidden": false
        },
        {
          "_id": "683e76c17a0996f979e72707",
          "name": "Yanghua Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T12:54:49.000Z",
      "submittedOnDailyAt": "2025-06-03T02:50:22.439Z",
      "title": "ARIA: 目的駆動の報酬集約を用いた言語アウトローバーの訓練",
      "submittedOnDailyBy": {
        "_id": "671a4abbef737c0abe21b3f8",
        "avatarUrl": "/avatars/da826af5472a3b9f1969f0c766672731.svg",
        "isPro": false,
        "fullname": "Ruihan Yang",
        "user": "rhyang2021",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、単語の組み合わせの共分布を表現する開放的な言語行動環境での行動空間を指数的に大きくすることを可能にし、複雑な理由論と決策を行うことができます。しかし、例えば交渉や質問ゲームのような開放的な言語行動環境では、行動空間は単語の組み合わせの共分布を表現することにより、指数的に大きくなります。このような空間で行動をサンプリングすることは、報酬の極端なスパーシティを招くことにより、報酬の分散が大きくなり、効果的な再励励効訓練（RL）を妨げます。これに対して、我々は、意図空間での報酬の集約を行う方法を提案します。この方法は、効率的かつ効果的な言語アウトプットの訓練を可能にします。ARIAは、高次元の単語の組み合わせの共分布空間からの自然言語行動を低次元の意図空間に投射し、語意的に類似した行動をクラスタリングし、共有報酬を割り当てます。この意図に関する報酬の集約は、報酬の分散を減少させ、報酬の信号を密にすることで、より良いポリシーの最適化を促進します。厳密な実験により、ARIAは、4つの下流タスクの平均9.95%の効果的な性能向上を収得し、オンラインとオフラインRLベースラインを一致して優れていることが示されました。",
      "upvotes": 19,
      "discussionId": "683e76ca7a0996f979e728e0",
      "projectPage": "https://aria-agent.github.io/",
      "githubRepo": "https://github.com/rhyang2021/ARIA",
      "ai_summary": "ARIA aggregates rewards in an intention space to mitigate reward sparsity and improve policy optimization in language-based reinforcement learning tasks.",
      "ai_keywords": [
        "large language models",
        "reinforcement learning",
        "action space",
        "token distribution",
        "extreme reward sparsity",
        "reward variance",
        "policy optimization",
        "intention space",
        "semantically similar actions",
        "shared rewards",
        "policy gradient variance",
        "performance gains",
        "offline RL",
        "online RL"
      ]
    },
    "publishedAt": "2025-05-31T08:54:49.000Z",
    "title": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation",
    "summary": "Large language models (LLMs) have enabled agents to perform complex reasoning\nand decision-making through free-form language interactions. However, in\nopen-ended language action environments (e.g., negotiation or question-asking\ngames), the action space can be formulated as a joint distribution over tokens,\nresulting in an exponentially large action space. Sampling actions in such a\nspace can lead to extreme reward sparsity, which brings large reward variance,\nhindering effective reinforcement learning (RL). To address this, we propose\nARIA, a method that Aggregates Rewards in Intention space to enable efficient\nand effective language Agents training. ARIA aims to project natural language\nactions from the high-dimensional joint token distribution space into a\nlow-dimensional intention space, where semantically similar actions are\nclustered and assigned shared rewards. This intention-aware reward aggregation\nreduces reward variance by densifying reward signals, fostering better policy\noptimization. Extensive experiments demonstrate that ARIA not only\nsignificantly reduces policy gradient variance, but also delivers substantial\nperformance gains of an average of 9.95% across four downstream tasks,\nconsistently outperforming offline and online RL baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00539.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671a4abbef737c0abe21b3f8",
      "avatarUrl": "/avatars/da826af5472a3b9f1969f0c766672731.svg",
      "fullname": "Ruihan Yang",
      "name": "rhyang2021",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00411",
      "authors": [
        {
          "_id": "683e86e31bca54bb6d169fc4",
          "user": {
            "_id": "6707eaaf5f50b17754ff9cbc",
            "avatarUrl": "/avatars/f08ca6228b124a8955787d0662a52bbd.svg",
            "isPro": false,
            "fullname": "Yang",
            "user": "Yysrc",
            "type": "user"
          },
          "name": "Yi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:40:09.713Z",
          "hidden": false
        },
        {
          "_id": "683e86e31bca54bb6d169fc5",
          "name": "Jiaxuan Sun",
          "hidden": false
        },
        {
          "_id": "683e86e31bca54bb6d169fc6",
          "name": "Siqi Kou",
          "hidden": false
        },
        {
          "_id": "683e86e31bca54bb6d169fc7",
          "name": "Yihan Wang",
          "hidden": false
        },
        {
          "_id": "683e86e31bca54bb6d169fc8",
          "name": "Zhijie Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T06:01:03.000Z",
      "submittedOnDailyAt": "2025-06-03T03:54:29.259Z",
      "title": "LoHoVLA: 長期視覚言語行動モデル",
      "submittedOnDailyBy": {
        "_id": "654e330f350abceb30a1390b",
        "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg",
        "isPro": false,
        "fullname": "KouSiqi",
        "user": "karrykkk",
        "type": "user"
      },
      "summary": "実世界的具象化アガントは長期間のタスクを面対する、高レベルの目標を求める多段階的な解決策を実行することが求められる。これらのタスクを成功に歩み込むには、高レベルのタスク計画（つまり、目標をステップごとに分解する）と低レベルの動作制御（つまり、機械人の動作を精確に生成する）が両方必要である。現在のビジョン言語アクション（VLA）モデルと階層構造は具象化タスクに潜在的な可能性を提供するが、前者は計画に違いがあり、後者は協調問題に苦しむ、これらは性能を妨げている。私たちは長期間のタスクに対する新しい統合フレームワークを紹介し、LoHoVLAと呼ばれる。LoHoVLAは、大規模な事前学習ビジョン言語モデル（VLM）をバックボードとして利用し、ステップごとのタスク生成と機械人アクション予測について言語とアクショントークンを共に生成する。この共有表現は、タスク間のより良い一般化を促進する。また、LoHoVLAは、高レベルの計画と低レベルの制御からの誤差を軽減するために階層構造の閉ループ制御機構を採用する。LoHoVLAの訓練には、Ravensシミュレーターに基づくデータセットLoHoSetを導入し、20つの長期間タスクを含む、それぞれ1,000プロフェッショナルのデモンストレーションを構成する可視的観測、言語的目標、ステップごとのタスク、機械人アクションを含むデータセットを構築した。実験結果は、Ravensシミュレーターでの長期間の具象化タスクにおいて、LoHoVLAは階層的および標準的なVLAアプローチを大幅に超えることを示し、これらの発見は、統合フレームワークが広範囲的な具象化知能の進歩についての可能性を強調する。",
      "upvotes": 19,
      "discussionId": "683e86e31bca54bb6d169ff5",
      "ai_summary": "A unified vision language action framework, LoHoVLA, combines a large pretrained vision language model with hierarchical closed-loop control to improve performance on long-horizon embodied tasks.",
      "ai_keywords": [
        "vision language action models",
        "hierarchical architectures",
        "high-level task planning",
        "low-level motion control",
        "LoHoVLA",
        "large pretrained vision language model",
        "shared representation",
        "hierarchical closed-loop control",
        "LoHoSet",
        "Ravens simulator",
        "long-horizon tasks",
        "sub-task generation",
        "robot action prediction",
        "embodied intelligence"
      ]
    },
    "publishedAt": "2025-05-31T02:01:03.000Z",
    "title": "LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon\n  Embodied Tasks",
    "summary": "Real-world embodied agents face long-horizon tasks, characterized by\nhigh-level goals demanding multi-step solutions beyond single actions.\nSuccessfully navigating these requires both high-level task planning (i.e.,\ndecomposing goals into sub-tasks) and low-level motion control (i.e.,\ngenerating precise robot actions). While existing vision language action (VLA)\nmodels and hierarchical architectures offer potential in embodied tasks, the\nformer often falter in planning, and the latter can suffer from coordination\nissues, both hampering performance. We introduce a new unified VLA framework\nfor long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA\nleverages a large pretrained vision language model (VLM) as the backbone to\njointly generate language and action tokens for sub-task generation and robot\naction prediction, respectively. This shared representation promotes better\ngeneralization across tasks. Additionally, LoHoVLA embraces a hierarchical\nclosed-loop control mechanism to mitigate errors originating from both\nhigh-level planning and low-level control. To train LoHoVLA, we introduce\nLoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon\ntasks, each with 1,000 expert demonstrations composed of visual observations,\nlinguistic goals, sub-tasks, and robot actions. Experimental results show that\nLoHoVLA significantly surpasses both hierarchical and standard VLA approaches\non long-horizon embodied tasks in the Ravens simulator. These findings\nunderscore the promise of unified architectures for advancing generalizable\nembodied intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654e330f350abceb30a1390b",
      "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg",
      "fullname": "KouSiqi",
      "name": "karrykkk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01844",
      "authors": [
        {
          "_id": "683eb85825fcc99d2a7fc26d",
          "user": {
            "_id": "62bdeedd01dc22b4d22a371e",
            "avatarUrl": "/avatars/6adc904fb1e08661d293a966270afabb.svg",
            "isPro": false,
            "fullname": "Mustafa Shukor",
            "user": "mshukor",
            "type": "user"
          },
          "name": "Mustafa Shukor",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:09.058Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc26e",
          "user": {
            "_id": "640e21ef3c82bd463ee5a76d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e21ef3c82bd463ee5a76d/nVR1DFPAsiLw6Boys28Rb.jpeg",
            "isPro": false,
            "fullname": "Dana Aubakirova",
            "user": "danaaubakirova",
            "type": "user"
          },
          "name": "Dana Aubakirova",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:18.586Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc26f",
          "user": {
            "_id": "63d67eac6f49aa8230601996",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d67eac6f49aa8230601996/djvtWdy718whUgh7tu1Ko.jpeg",
            "isPro": false,
            "fullname": "Francesco Capuano",
            "user": "fracapuano",
            "type": "user"
          },
          "name": "Francesco Capuano",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:29.639Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc270",
          "user": {
            "_id": "65f9d37113336392bad1e49c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f9d37113336392bad1e49c/B0Fxwconnu7lvtjBz4Ruq.jpeg",
            "isPro": false,
            "fullname": "Pepijn Kooijmans",
            "user": "pepijn223",
            "type": "user"
          },
          "name": "Pepijn Kooijmans",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:38.693Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc271",
          "user": {
            "_id": "67b124b081d4eae18b957606",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/CXvSv2l15uPkMQL_HBRDF.png",
            "isPro": false,
            "fullname": "Steven Palma",
            "user": "imstevenpmwork",
            "type": "user"
          },
          "name": "Steven Palma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:47.379Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc272",
          "user": {
            "_id": "64c255b2254239173af0570a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c255b2254239173af0570a/JvYNX4gpk0hVQJeJif8Mo.jpeg",
            "isPro": false,
            "fullname": "Adil Zouitine",
            "user": "AdilZtn",
            "type": "user"
          },
          "name": "Adil Zouitine",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:11:56.957Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc273",
          "user": {
            "_id": "668bd06dd58b51a628566d80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668bd06dd58b51a628566d80/II7Yr5dT5ItMrpoMkQEy3.jpeg",
            "isPro": false,
            "fullname": "Michel Aractingi",
            "user": "aractingi",
            "type": "user"
          },
          "name": "Michel Aractingi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:05.798Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc274",
          "user": {
            "_id": "67d7dea1786ddcb3af5a44b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d7dea1786ddcb3af5a44b3/gEgXTH4oO91GIzjHR-yrb.png",
            "isPro": false,
            "fullname": "Caroline Pascal",
            "user": "CarolinePascal",
            "type": "user"
          },
          "name": "Caroline Pascal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:15.340Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc275",
          "user": {
            "_id": "631365ad289cf15634c6f600",
            "avatarUrl": "/avatars/a464d228328719274a20121e2a82f703.svg",
            "isPro": false,
            "fullname": "Martino Russi",
            "user": "nepyope",
            "type": "user"
          },
          "name": "Martino Russi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:23.474Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc276",
          "user": {
            "_id": "65d66b494bbd0d92b641cdbb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
            "isPro": false,
            "fullname": "Andres Marafioti",
            "user": "andito",
            "type": "user"
          },
          "name": "Andres Marafioti",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:31.740Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc277",
          "user": {
            "_id": "65fcb7f133a3d6f126772121",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fcb7f133a3d6f126772121/BvVbNqnlQgDr2f_9dm5Es.jpeg",
            "isPro": false,
            "fullname": "Simon  Alibert",
            "user": "aliberts",
            "type": "user"
          },
          "name": "Simon Alibert",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:12:39.999Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc278",
          "name": "Matthieu Cord",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc279",
          "user": {
            "_id": "5df7e9e5da6d0311fd3d53f9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg",
            "isPro": false,
            "fullname": "Thomas Wolf",
            "user": "thomwolf",
            "type": "user"
          },
          "name": "Thomas Wolf",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:13:02.139Z",
          "hidden": false
        },
        {
          "_id": "683eb85825fcc99d2a7fc27a",
          "user": {
            "_id": "62f857fbb9fda55613ce80d9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f857fbb9fda55613ce80d9/d7bRniKLmOt-iFN07k1Su.png",
            "isPro": false,
            "fullname": "Remi Cadene",
            "user": "cadene",
            "type": "user"
          },
          "name": "Remi Cadene",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-03T09:13:11.882Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T16:30:19.000Z",
      "submittedOnDailyAt": "2025-06-03T07:31:41.152Z",
      "title": "SmolVLA: 低コストと効率的なロボット工学向けの視覚・言語・行動モデル",
      "submittedOnDailyBy": {
        "_id": "65d66b494bbd0d92b641cdbb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
        "isPro": false,
        "fullname": "Andres Marafioti",
        "user": "andito",
        "type": "user"
      },
      "summary": "視覚言語モデル（VLMs）は、大規模な多タイプデータセットで事前学習され、豊富な視覚的および言語的知識を記述し、機械人システムの強力な基盤となる。機械人ポリシーをセットアップするのには、これらのモデルをより自然言語に駆動させるように調整し、視覚言語行動（VLA）モデルに変換することが最近のアプローチで行われている。しかし、現在のVLAは通常、数ブィリオンパラメーターを持ち、高い学習コストと実世界での機能性の限りを伴う。また、データセットは学術と工業のものを中心に、価格低い機械人プラットフォームから構築されたコミュニティデータの増加を無視している。本研究では、SmolVLAを紹介し、小さくて効率的で、コミュニティによるVLAであり、学習コストと推論コストを大幅に減少させ、対抗的な性能を維持する。SmolVLAは、単一のGPUで学習し、消費者レベルのGPUまたはCPUで機能するように設計されている。また、より応答性を向上させるために、非同期推論スタックを導入し、視覚と行動予測を行動実行から分離し、チャンク化された行動生成でより高い制御レートを実現する。その小ささにもかかわらず、SmolVLAは10倍大きなVLAと同等の性能を達成する。SmolVLAは、複数のシミュレーションおよび実世界の機械人ベンチマークで評価され、すべてのコード、事前学習モデル、および学習データを公開している。",
      "upvotes": 17,
      "discussionId": "683eb85925fcc99d2a7fc2dc",
      "ai_summary": "SmolVLA is a compact, efficient vision-language-action model that achieves competitive performance at reduced computational costs and can be deployed on consumer-grade hardware.",
      "ai_keywords": [
        "vision-language models",
        "multimodal datasets",
        "robotic policies",
        "vision-language-action models",
        "natural language-driven perception",
        "asynchronous inference",
        "action prediction",
        "action execution",
        "chunked action generation",
        "performance benchmarks"
      ]
    },
    "publishedAt": "2025-06-02T12:30:19.000Z",
    "title": "SmolVLA: A Vision-Language-Action Model for Affordable and Efficient\n  Robotics",
    "summary": "Vision-language models (VLMs) pretrained on large-scale multimodal datasets\nencode rich visual and linguistic knowledge, making them a strong foundation\nfor robotics. Rather than training robotic policies from scratch, recent\napproaches adapt VLMs into vision-language-action (VLA) models that enable\nnatural language-driven perception and control. However, existing VLAs are\ntypically massive--often with billions of parameters--leading to high training\ncosts and limited real-world deployability. Moreover, they rely on academic and\nindustrial datasets, overlooking the growing availability of\ncommunity-collected data from affordable robotic platforms. In this work, we\npresent SmolVLA, a small, efficient, and community-driven VLA that drastically\nreduces both training and inference costs, while retaining competitive\nperformance. SmolVLA is designed to be trained on a single GPU and deployed on\nconsumer-grade GPUs or even CPUs. To further improve responsiveness, we\nintroduce an asynchronous inference stack decoupling perception and action\nprediction from action execution, allowing higher control rates with chunked\naction generation. Despite its compact size, SmolVLA achieves performance\ncomparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both\nsimulated as well as real-world robotic benchmarks and release all code,\npretrained models, and training data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01844.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65d66b494bbd0d92b641cdbb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
      "fullname": "Andres Marafioti",
      "name": "andito",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 185
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01943",
      "authors": [
        {
          "_id": "683e6b6424742a21489ec9f8",
          "name": "Xiao Fu",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9f9",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fa",
          "name": "Xian Liu",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fb",
          "name": "Jianhong Bai",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fc",
          "name": "Runsen Xu",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fd",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9fe",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "683e6b6424742a21489ec9ff",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63aef2cafcca84593e6682db/9mFDJaCOc6KLHlhboYA59.mp4"
      ],
      "publishedAt": "2025-06-02T17:57:06.000Z",
      "submittedOnDailyAt": "2025-06-03T01:57:30.514Z",
      "title": "学習ビデオ生成によるロボット操作のコラボレーション的軌道制御",
      "submittedOnDailyBy": {
        "_id": "63aef2cafcca84593e6682db",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672409763337-noauth.jpeg",
        "isPro": false,
        "fullname": "Xiao Fu",
        "user": "lemonaddie",
        "type": "user"
      },
      "summary": "最近の映像ディフュージョンモデルの進展は、機械人の決策データの生成に強い可能性を示しています。トラジェクト条件は、より細かな制御を可能にします。しかし、現在のトラジェクトに基づく方法は、個々の物体の動きを主に焦点としていて、複雑な機械人操作に必要な多物体相互作用を捉えられません。この制限は、重なりの領域での多様な特徴の結合によって原因です。これを解決するために、我々はRoboMasterを紹介します。RoboMasterは、協議的なトラジェクト形成により物体間の動力学をモデル化します。先行の方法と違って、物体を分解するものではなく、相互作用プロセスを3つのステージに分解します：相互作用前、相互作用中、相互作用後。各ステージは、主導物体の特徴を使用してモデル化されます。相互作用前と相互作用後のステージでは、機械人アーム、相互作用中のステージでは操作対象物体を特徴として使用し、先行の研究で見られる多物体特徴融合の欠点を軽減します。また、ビデオ全体で主題の語意的な一貫性を確保するために、物体に関する外観と形状に関する潜在表現を組み込みます。難しいBridge V2データセットにおける詳細な実験および野生状態の評価により、我々の方法が先行の手法を上回り、機械人操作におけるトラジェクト制御ビデオ生成の新しい最先端性能を奨励します。",
      "upvotes": 15,
      "discussionId": "683e6b6724742a21489eca8d",
      "ai_summary": "A novel framework, RoboMaster, enhances trajectory-controlled video generation for robotic manipulation by modeling inter-object dynamics through a collaborative trajectory formulation, achieving state-of-the-art performance on the Bridge V2 dataset.",
      "ai_keywords": [
        "video diffusion models",
        "trajectory conditions",
        "multi-object interaction",
        "multi-feature entanglement",
        "visual fidelity",
        "collaborative trajectory formulation",
        "pre-interaction",
        "interaction",
        "post-interaction",
        "appearance-aware latent representations",
        "shape-aware latent representations",
        "trajectory-controlled video generation",
        "robotic manipulation",
        "Bridge V2 dataset"
      ]
    },
    "publishedAt": "2025-06-02T13:57:06.000Z",
    "title": "Learning Video Generation for Robotic Manipulation with Collaborative\n  Trajectory Control",
    "summary": "Recent advances in video diffusion models have demonstrated strong potential\nfor generating robotic decision-making data, with trajectory conditions further\nenabling fine-grained control. However, existing trajectory-based methods\nprimarily focus on individual object motion and struggle to capture\nmulti-object interaction crucial in complex robotic manipulation. This\nlimitation arises from multi-feature entanglement in overlapping regions, which\nleads to degraded visual fidelity. To address this, we present RoboMaster, a\nnovel framework that models inter-object dynamics through a collaborative\ntrajectory formulation. Unlike prior methods that decompose objects, our core\nis to decompose the interaction process into three sub-stages: pre-interaction,\ninteraction, and post-interaction. Each stage is modeled using the feature of\nthe dominant object, specifically the robotic arm in the pre- and\npost-interaction phases and the manipulated object during interaction, thereby\nmitigating the drawback of multi-object feature fusion present during\ninteraction in prior work. To further ensure subject semantic consistency\nthroughout the video, we incorporate appearance- and shape-aware latent\nrepresentations for objects. Extensive experiments on the challenging Bridge V2\ndataset, as well as in-the-wild evaluation, demonstrate that our method\noutperforms existing approaches, establishing new state-of-the-art performance\nin trajectory-controlled video generation for robotic manipulation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63aef2cafcca84593e6682db/9mFDJaCOc6KLHlhboYA59.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01943.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63aef2cafcca84593e6682db",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672409763337-noauth.jpeg",
      "fullname": "Xiao Fu",
      "name": "lemonaddie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01853",
      "authors": [
        {
          "_id": "683e671483a130f817c4937a",
          "name": "Junliang Ye",
          "hidden": false
        },
        {
          "_id": "683e671483a130f817c4937b",
          "name": "Zhengyi Wang",
          "hidden": false
        },
        {
          "_id": "683e671483a130f817c4937c",
          "user": {
            "_id": "6522e4fbd89bc7773ddc4b58",
            "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
            "isPro": false,
            "fullname": "Ruowen Zhao",
            "user": "zzzrw",
            "type": "user"
          },
          "name": "Ruowen Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:03.565Z",
          "hidden": false
        },
        {
          "_id": "683e671483a130f817c4937d",
          "name": "Shenghao Xie",
          "hidden": false
        },
        {
          "_id": "683e671483a130f817c4937e",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/84A40qBWJeZaBv2qYO6Pj.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/Xl6IajTG3VravdQABgC6l.mp4"
      ],
      "publishedAt": "2025-06-02T16:40:50.000Z",
      "submittedOnDailyAt": "2025-06-03T03:57:42.122Z",
      "title": "ShapeLLM-Omni: 3D生成と理解のための原生モノモダルLLM",
      "submittedOnDailyBy": {
        "_id": "65a420cd90e65dc39a6abe9e",
        "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
        "isPro": false,
        "fullname": "yejunliang",
        "user": "yejunliang23",
        "type": "user"
      },
      "summary": "最近、ChatGPT-4oの強力な文字から画像への変換能力により、原生の多モデル大語言モデルの楽しみを増やしていますが、その多モデル能力はまだ画像と文字のみで限られています。しかし画像よりも、3Dコンテンツの理解と生成の能力は同等に重要です。この空間を埋めるために、ShapeLLM-Omni、つまり原生の3D大語言モデルを提案します。これは、任意の順序で3Dアセットと文字を理解し、生成することができます。まず、3Dオブジェクトを離散的潜在空間にマップし、効率的かつ正確な形状表現と再構成を実現するために3Dベクトルキャリーフィジカル変分自動エンコーダー（VQVAE）を訓練します。3D-Alpacaという大規模な継続的訓練データセットを構築し、生成、理解、編集の全般を含むもので、将来の研究と訓練に豊富なリソースを提供します。最後に、Qwen-2.5-vl-7B-Instructモデルを3D-Alpacaデータセット上でインストラクションベース訓練を行います。私たちの研究は、基本的な3D能力を持つ多モデルモデルの拡張についての効果的な試みを提供し、将来の3D原生AIの研究に貢献します。プロジェクトページ：https://github.com/JAMESYJL/ShapeLLM-Omni",
      "upvotes": 15,
      "discussionId": "683e671683a130f817c493cd",
      "ai_summary": "A native 3D large language model, ShapeLLM-Omni, is proposed to understand and generate 3D assets and text, trained using a 3D vector-quantized variational autoencoder and a new 3D-Alpaca dataset.",
      "ai_keywords": [
        "3D vector-quantized variational autoencoder (VQVAE)",
        "discrete latent space",
        "instruction-based training",
        "3D-Alpaca dataset",
        "3D-native AI"
      ]
    },
    "publishedAt": "2025-06-02T12:40:50.000Z",
    "title": "ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and\n  Understanding",
    "summary": "Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to\ngrowing appreciation for native multimodal large language models. However, its\nmultimodal capabilities remain confined to images and text. Yet beyond images,\nthe ability to understand and generate 3D content is equally crucial. To\naddress this gap, we propose ShapeLLM-Omni-a native 3D large language model\ncapable of understanding and generating 3D assets and text in any sequence.\nFirst, we train a 3D vector-quantized variational autoencoder (VQVAE), which\nmaps 3D objects into a discrete latent space to achieve efficient and accurate\nshape representation and reconstruction. Building upon the 3D-aware discrete\ntokens, we innovatively construct a large-scale continuous training dataset\nnamed 3D-Alpaca, encompassing generation, comprehension, and editing, thus\nproviding rich resources for future research and training. Finally, by\nperforming instruction-based training of the Qwen-2.5-vl-7B-Instruct model on\nthe 3D-Alpaca dataset. Our work provides an effective attempt at extending\nmultimodal models with basic 3D capabilities, which contributes to future\nresearch in 3D-native AI. Project page:\nhttps://github.com/JAMESYJL/ShapeLLM-Omni",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/84A40qBWJeZaBv2qYO6Pj.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/Xl6IajTG3VravdQABgC6l.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65a420cd90e65dc39a6abe9e",
      "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
      "fullname": "yejunliang",
      "name": "yejunliang23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24760",
      "authors": [
        {
          "_id": "683e6af92139ea008faa74ba",
          "user": {
            "_id": "65144e46004a986ccc9d21d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65144e46004a986ccc9d21d6/oImRY64V-UeYGdrbd_ozU.jpeg",
            "isPro": false,
            "fullname": "Zafir Stojanovski",
            "user": "zafstojano",
            "type": "user"
          },
          "name": "Zafir Stojanovski",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-03T06:28:46.378Z",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74bb",
          "user": {
            "_id": "6303f5f37b50dd9d0a371b28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6303f5f37b50dd9d0a371b28/H25eCzAYVwBtpSpD8tnUV.jpeg",
            "isPro": false,
            "fullname": "Oliver Stanley",
            "user": "OllieStanley",
            "type": "user"
          },
          "name": "Oliver Stanley",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:56.277Z",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74bc",
          "name": "Joe Sharratt",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74bd",
          "name": "Richard Jones",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74be",
          "name": "Abdulhakeem Adefioye",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74bf",
          "user": {
            "_id": "6304061c0547362a22a76a17",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661339692442-6304061c0547362a22a76a17.jpeg",
            "isPro": false,
            "fullname": "Jean Kaddour",
            "user": "JeanKaddour",
            "type": "user"
          },
          "name": "Jean Kaddour",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T09:13:54.739Z",
          "hidden": false
        },
        {
          "_id": "683e6af92139ea008faa74c0",
          "name": "Andreas Köpf",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T16:20:18.000Z",
      "submittedOnDailyAt": "2025-06-03T02:02:10.153Z",
      "title": "レーシングジム：実証可能な報酬を持つ強化学習の理由論環境",
      "submittedOnDailyBy": {
        "_id": "65144e46004a986ccc9d21d6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65144e46004a986ccc9d21d6/oImRY64V-UeYGdrbd_ozU.jpeg",
        "isPro": false,
        "fullname": "Zafir Stojanovski",
        "user": "zafstojano",
        "type": "user"
      },
      "summary": "Reasoning Gym (RG) を紹介します。RG は、実験的な報酬を証明できる強化学習の理由論環境のライブラリです。100以上のデータ生成器と証明器があり、代数、算術、計算、知覚、幾何学、グラフ理論、ロジック、そして多くの一般的なゲームの分野を拡張しています。その主な革新的点は、前の理由論データセットと異なり、可変な複雑さで無限にデータを生成することです。このプロセス生成アプローチにより、変化する難易度レベルでの継続的な評価が可能になります。実験結果から、RG が理由論モデルの評価と強化学習においての効果性を示しています。",
      "upvotes": 15,
      "discussionId": "683e6afa2139ea008faa7531",
      "githubRepo": "https://github.com/open-thought/reasoning-gym",
      "ai_summary": "Reasoning Gym provides a library of reasoning environments with verifiable rewards and procedural data generation for reinforcement learning, enabling the evaluation and training of reasoning models at varying difficulty levels.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "data generators",
        "verifiers",
        "procedural generation",
        "reasoning models"
      ]
    },
    "publishedAt": "2025-05-30T12:20:18.000Z",
    "title": "REASONING GYM: Reasoning Environments for Reinforcement Learning with\n  Verifiable Rewards",
    "summary": "We introduce Reasoning Gym (RG), a library of reasoning environments for\nreinforcement learning with verifiable rewards. It provides over 100 data\ngenerators and verifiers spanning multiple domains including algebra,\narithmetic, computation, cognition, geometry, graph theory, logic, and various\ncommon games. Its key innovation is the ability to generate virtually infinite\ntraining data with adjustable complexity, unlike most previous reasoning\ndatasets, which are typically fixed. This procedural generation approach allows\nfor continuous evaluation across varying difficulty levels. Our experimental\nresults demonstrate the efficacy of RG in both evaluating and reinforcement\nlearning of reasoning models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24760.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65144e46004a986ccc9d21d6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65144e46004a986ccc9d21d6/oImRY64V-UeYGdrbd_ozU.jpeg",
      "fullname": "Zafir Stojanovski",
      "name": "zafstojano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00996",
      "authors": [
        {
          "_id": "683e7cbf402acb186580d5ec",
          "name": "Kinam Kim",
          "hidden": false
        },
        {
          "_id": "683e7cbf402acb186580d5ed",
          "name": "Junha Hyung",
          "hidden": false
        },
        {
          "_id": "683e7cbf402acb186580d5ee",
          "name": "Jaegul Choo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64797735a68454566356b708/a-8jl8yq3KdDLnuZEaB3K.mp4"
      ],
      "publishedAt": "2025-06-01T12:57:43.000Z",
      "submittedOnDailyAt": "2025-06-03T03:25:10.796Z",
      "title": "時系列ダイバージェンス微調節による動画ダイバージェンスモデルの多様な制御",
      "submittedOnDailyBy": {
        "_id": "64797735a68454566356b708",
        "avatarUrl": "/avatars/3424d022dd8ad29b56eb41814c5c3dee.svg",
        "isPro": false,
        "fullname": "Kinam Kim",
        "user": "kinam0252",
        "type": "user"
      },
      "summary": "最近の文脈から動画への拡散モデルの進展は、高品質の動画合成を可能にしましたが、制御可能な生成は、特にデータと計算量の制限の下で難しいです。現在の条件付き生成に対する調整メソッドは、外部エンコーダーや構造的変更を依存していますが、これらは大きなデータセットを必要とし、通常は空間的に一致した条件付き生成に限定され、柔軟性とスケーラビリティを制限します。本論文では、時系列的なコンテキスト付き調整（Temporal In-Context Fine-Tuning, TIC-FT）を導入し、予えられた動画拡散モデルを多様な条件付き生成タスクに適用する効率的かつ幅広いアプローチを提案します。私たちの主なアイデアは、条件と目標フレームを時系列軸に結合し、進歩的に増加するノイズレベルを持つ中間バッフヤフレームを挿入することです。これらのバッフヤフレームは、平滑な移行を可能にし、調整プロセスが予えられたモデルの時系列的な動態に一致させます。TIC-FTは構造的な変更を必要としません、10-30件の訓練サンプルで強い性能を達成します。私たちの方法は、CogVideoX-5BやWan-14Bといった大規模な基盤モデルを使用して画像から動画へ、動画から動画への生成など様々なタスクで検証されました。拡張された実験は、条件忠実性と視覚品質の両方で既存のベースラインを超えることを示し、訓練および推論の両方でも高度に効率的です。追加の結果については、https://kinam0252.github.io/TIC-FT/ を参照してください。",
      "upvotes": 14,
      "discussionId": "683e7cc1402acb186580d663",
      "ai_summary": "Temporal In-Context Fine-Tuning (TIC-FT) enhances pretrained video diffusion models for diverse conditional generation tasks with minimal data and without architectural changes.",
      "ai_keywords": [
        "text-to-video diffusion models",
        "fine-tuning",
        "external encoders",
        "architectural modifications",
        "Temporal In-Context Fine-Tuning",
        "condition and target frames",
        "buffer frames",
        "noise levels",
        "smooth transitions",
        "pretrained video diffusion models",
        "image-to-video generation",
        "video-to-video generation",
        "CogVideoX-5B",
        "Wan-14B"
      ]
    },
    "publishedAt": "2025-06-01T08:57:43.000Z",
    "title": "Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion\n  Models",
    "summary": "Recent advances in text-to-video diffusion models have enabled high-quality\nvideo synthesis, but controllable generation remains challenging, particularly\nunder limited data and compute. Existing fine-tuning methods for conditional\ngeneration often rely on external encoders or architectural modifications,\nwhich demand large datasets and are typically restricted to spatially aligned\nconditioning, limiting flexibility and scalability. In this work, we introduce\nTemporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach\nfor adapting pretrained video diffusion models to diverse conditional\ngeneration tasks. Our key idea is to concatenate condition and target frames\nalong the temporal axis and insert intermediate buffer frames with\nprogressively increasing noise levels. These buffer frames enable smooth\ntransitions, aligning the fine-tuning process with the pretrained model's\ntemporal dynamics. TIC-FT requires no architectural changes and achieves strong\nperformance with as few as 10-30 training samples. We validate our method\nacross a range of tasks, including image-to-video and video-to-video\ngeneration, using large-scale base models such as CogVideoX-5B and Wan-14B.\nExtensive experiments show that TIC-FT outperforms existing baselines in both\ncondition fidelity and visual quality, while remaining highly efficient in both\ntraining and inference. For additional results, visit\nhttps://kinam0252.github.io/TIC-FT/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64797735a68454566356b708/a-8jl8yq3KdDLnuZEaB3K.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64797735a68454566356b708",
      "avatarUrl": "/avatars/3424d022dd8ad29b56eb41814c5c3dee.svg",
      "fullname": "Kinam Kim",
      "name": "kinam0252",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24846",
      "authors": [
        {
          "_id": "683e82f2fa7ede4842f95214",
          "name": "Jingyan Shen",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f95215",
          "user": {
            "_id": "66f8689725464a7989b75845",
            "avatarUrl": "/avatars/43a61a528c5779103eaf5687ba44ee14.svg",
            "isPro": false,
            "fullname": "Jiarui Yao",
            "user": "FlippyDora",
            "type": "user"
          },
          "name": "Jiarui Yao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:40:58.100Z",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f95216",
          "user": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "isPro": false,
            "fullname": "Rui Yang",
            "user": "Ray2333",
            "type": "user"
          },
          "name": "Rui Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:40:16.572Z",
          "hidden": true
        },
        {
          "_id": "683e82f2fa7ede4842f95217",
          "name": "Yifan Sun",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f95218",
          "name": "Feng Luo",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f95219",
          "name": "Rui Pan",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f9521a",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "683e82f2fa7ede4842f9521b",
          "name": "Han Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:44:28.000Z",
      "submittedOnDailyAt": "2025-06-03T03:51:59.115Z",
      "title": "MiCRo: ミクスモデリングとコンテキストに関するルーティングを用いた個人化好み学習",
      "submittedOnDailyBy": {
        "_id": "64d45451c34a346181b130dd",
        "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
        "isPro": false,
        "fullname": "Rui Yang",
        "user": "Ray2333",
        "type": "user"
      },
      "summary": "レベルモデリングは、人間の反饋を元に強化学習を適用して大規模な言語モデル（LLMs）として安全な基盤モデルを構築する際の重要なステップです。しかし、Bradley-Terry（BT）モデルに基づくレベルモデリングは、一意的な報酬関数を仮定し、固有の多様性と異なり性のある人間の好みを捉えずにいます。そのような簡略化は、LLMsが個人化および多様的な対位を支えることを限定します。\n\n理論的には、人間の好みが多様なグループの混和分布に従う場合、一つのBTモデルは不可解な誤差を持つことが示されます。現在の解決策では、多目的学習と細かい注釈を用いることなどがあり、この問題を解決することができますが、これらは高額であり、事前定義された属性に制限され、人間の価値の豊富さを完全に捉えることができません。\n\n本論文では、大規模な二値好みデータセットを利用して個人化の好み学習を強化するための2ステップフレームワークMiCRoを介して、明確な細かい注釈を必要としないものとします。1ステップ目では、MiCRoは上下文に関わる混和モデリングアプローチを導入し、多様な人間の好みを捉えます。2ステップ目では、MiCRoは特定の上下文中に基づいて動的に混和重みを調整するオンラインルーティング戦略を組み込み、不明確性を解決し、最小限の追加のステラチンを必要としないように、効率的でスケーラブルな好みの適応を可能にします。複数の好みデータセットにおける実験は、MiCRoが効果的に多様な人間の好みを捉え、下流の個人化を大幅に向上させることを示しました。",
      "upvotes": 11,
      "discussionId": "683e82f3fa7ede4842f95246",
      "ai_summary": "MiCRo, a two-stage framework, improves personalized preference learning for large language models by leveraging binary preference datasets and dynamically adapting mixture weights based on context, effectively capturing diverse human preferences.",
      "ai_keywords": [
        "Reward modeling",
        "reinforcement learning from human feedback (RLHF)",
        "Large Language Models (LLMs)",
        "Bradley-Terry (BT) model",
        "mixture distribution",
        "personalization",
        "pluralistic alignment",
        "multi-objective learning",
        "context-aware mixture modeling",
        "online routing strategy"
      ]
    },
    "publishedAt": "2025-05-30T13:44:28.000Z",
    "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning",
    "summary": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d45451c34a346181b130dd",
      "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
      "fullname": "Rui Yang",
      "name": "Ray2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24298",
      "authors": [
        {
          "_id": "683d12963aa5ac98190e1eda",
          "name": "Wei Fu",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1edb",
          "name": "Jiaxuan Gao",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1edc",
          "name": "Xujie Shen",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1edd",
          "name": "Chen Zhu",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ede",
          "name": "Zhiyu Mei",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1edf",
          "name": "Chuyi He",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee0",
          "name": "Shusheng Xu",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee1",
          "name": "Guo Wei",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee2",
          "name": "Jun Mei",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee3",
          "name": "Jiashu Wang",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee4",
          "name": "Tongkai Yang",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee5",
          "name": "Binhang Yuan",
          "hidden": false
        },
        {
          "_id": "683d12963aa5ac98190e1ee6",
          "name": "Yi Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T07:18:25.000Z",
      "submittedOnDailyAt": "2025-06-03T05:39:21.066Z",
      "title": "AReaL: シェイプの大規模な非同期強化学習システムで、言語論理を対象とします。",
      "submittedOnDailyBy": {
        "_id": "63159678915d0b80682fe9f9",
        "avatarUrl": "/avatars/a19e44ffe681099a155b8f8ecb53f0ce.svg",
        "isPro": false,
        "fullname": "Shusheng Xu",
        "user": "xssstory",
        "type": "user"
      },
      "summary": "強化学習（RL）は、特に理由論されたタスクに対して、大規模な言語モデル（LLMs）の訓練においてブーミングしているパラダイムとなっています。LLMsの効果的なRLは、巨大な並列化が必要で、効率的な訓練システムの急迫な要望を強調しています。現在の多くの大規模なLLMsのRLシステムは、同時訓練と生成の交換を行うバッチ設定で、各訓練バッチ内で同じもの（または最新のモデル）で出力を生成しています。これはRLの訓練を安定させるが、厳しいシステムレベルの無用りを招く問題を抱えています。生成は、バッチ内で最も長い出力が完成するまで待つ必要があり、これによりGPUの利用率が低下します。我々は、AReaLという完全な非同期RLシステムを紹介します。AReaLは、生成と訓練を完全に離れるもので、出力を続けて生成しながら、訓練ワーカーはデータが集まったらモデルを更新します。AReaLは、GPUの利用率を大幅に高めるために、システムレベルの最適化を採用しています。RLの訓練を安定させるために、AReaLは出力と訓練ワーカーの仕事量をバランスし、データの過期を制御し、過期度を強化したPPOの変体を採用して、過期した訓練サンプルをより良く扱うことを目指しています。数学とコードの理由論ベンチマークにおいての拡散的な実験により、AReaLは同じ数のGPUを使用し、最良の同期システムと同じまたは改善された最終的な性能を示した場合、2.57倍の訓練スピードアップを達成します。AReaLのコードは、https://github.com/inclusionAI/AReaL/ に公開されています。",
      "upvotes": 10,
      "discussionId": "683d12973aa5ac98190e1f19",
      "ai_summary": "AReaL, a fully asynchronous reinforcement learning system, decouples generation and training to achieve higher GPU utilization and up to 2.57x training speedup for large language models on reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "asynchronous system",
        "rollouts",
        "model update",
        "GPU utilization",
        "PPO",
        "data staleness",
        "training speedup"
      ]
    },
    "publishedAt": "2025-05-30T03:18:25.000Z",
    "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning",
    "summary": "Reinforcement learning (RL) has become a trending paradigm for training large\nlanguage models (LLMs), particularly for reasoning tasks. Effective RL for LLMs\nrequires massive parallelization and poses an urgent need for efficient\ntraining systems. Most existing large-scale RL systems for LLMs are synchronous\nby alternating generation and training in a batch setting, where the rollouts\nin each training batch are generated by the same (or latest) model. This\nstabilizes RL training but suffers from severe system-level inefficiency.\nGeneration must wait until the longest output in the batch is completed before\nmodel update, resulting in GPU underutilization. We present AReaL, a\nfully asynchronous RL system that completely decouples generation from\ntraining. Rollout workers in AReaL continuously generate new outputs without\nwaiting, while training workers update the model whenever a batch of data is\ncollected. AReaL also incorporates a collection of system-level optimizations,\nleading to substantially higher GPU utilization. To stabilize RL training,\nAReaL balances the workload of rollout and training workers to control data\nstaleness, and adopts a staleness-enhanced PPO variant to better handle\noutdated training samples. Extensive experiments on math and code reasoning\nbenchmarks show that AReaL achieves up to 2.57times training\nspeedup compared to the best synchronous systems with the same number of GPUs\nand matched or even improved final performance. The code of AReaL is available\nat https://github.com/inclusionAI/AReaL/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24298.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63159678915d0b80682fe9f9",
      "avatarUrl": "/avatars/a19e44ffe681099a155b8f8ecb53f0ce.svg",
      "fullname": "Shusheng Xu",
      "name": "xssstory",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23907",
      "authors": [
        {
          "_id": "683e6838a6815c77acb18ea3",
          "user": {
            "_id": "650e3abcae507a2c7c847baa",
            "avatarUrl": "/avatars/a138bc6c9ba80a486fb36f4e1aa16b33.svg",
            "isPro": false,
            "fullname": "Amirhossein Alimohammadi",
            "user": "Amirhossein-Alimohammadi",
            "type": "user"
          },
          "name": "Amirhossein Almohammadi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:59.757Z",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea4",
          "name": "Aryan Mikaeili",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea5",
          "name": "Sauradip Nag",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea6",
          "name": "Negar Hassanpour",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea7",
          "name": "Andrea Tagliasacchi",
          "hidden": false
        },
        {
          "_id": "683e6838a6815c77acb18ea8",
          "name": "Ali Mahdavi-Amiri",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/650e3abcae507a2c7c847baa/dRNPZqV4HrW79gs506jk7.mp4"
      ],
      "publishedAt": "2025-05-29T18:00:56.000Z",
      "submittedOnDailyAt": "2025-06-03T01:51:49.601Z",
      "title": "コーラ: 少ないステップでのディフュージョンによる通信に関する画像編集",
      "submittedOnDailyBy": {
        "_id": "650e3abcae507a2c7c847baa",
        "avatarUrl": "/avatars/a138bc6c9ba80a486fb36f4e1aa16b33.svg",
        "isPro": false,
        "fullname": "Amirhossein Alimohammadi",
        "user": "Amirhossein-Alimohammadi",
        "type": "user"
      },
      "summary": "画像編集は、コンピューターグラフィック、ビジョン、VFXなどの重要なタスクであり、最近の拡散ベースの方法は高速で高品質な結果を実現しています。しかし、非則異形変形、オブジェクトの修正、またはコンテンツの生成などの構造的な変更が必要な編集は難しいといえます。現在の少ないステップの編集アプローチは、無関係なテクスチャや、元画像の鍵属性（例：姿勢）の保存に苦戦します。私たちは、対応関係によるノイズ補正とインタープールされたアテンションマップを導入することで、これらの制限を解決する新しい編集フレームワークCoraを紹介します。私たちの方法は、ソースとターゲット画像のテクスチャや構造を対応関係によってアラインし、必要に応じて新しいコンテンツを生成することで、正確なテクスチャ転送を可能にします。Coraは、コンテンツの生成と保存のバランスの制御を提供します。拡張された実験は、定量的および定性的に、Coraは構造、テクスチャ、そして多様な編集中の同一性を保ち、姿勢変化、オブジェクト追加、テクスチャの精練などの編集中の優れた性能を示します。ユーザーステージスは、Coraが上位の結果を提供し、代替方法を超えていることを確認しました。",
      "upvotes": 8,
      "discussionId": "683e683aa6815c77acb18f03",
      "projectPage": "https://cora-edit.github.io/",
      "githubRepo": "https://github.com/alimohammadiamirhossein/cora",
      "ai_summary": "Cora framework enhances image editing through correspondence-aware noise correction and interpolated attention maps, excelling in structure and texture preservation and generation.",
      "ai_keywords": [
        "diffusion-based methods",
        "non-rigid deformations",
        "object modifications",
        "content generation",
        "correspondence-aware noise correction",
        "interpolated attention maps",
        "semantic correspondence",
        "texture transfer",
        "content generation",
        "preservation",
        "pose changes",
        "object addition",
        "texture refinements"
      ]
    },
    "publishedAt": "2025-05-29T14:00:56.000Z",
    "title": "Cora: Correspondence-aware image editing using few step diffusion",
    "summary": "Image editing is an important task in computer graphics, vision, and VFX,\nwith recent diffusion-based methods achieving fast and high-quality results.\nHowever, edits requiring significant structural changes, such as non-rigid\ndeformations, object modifications, or content generation, remain challenging.\nExisting few step editing approaches produce artifacts such as irrelevant\ntexture or struggle to preserve key attributes of the source image (e.g.,\npose). We introduce Cora, a novel editing framework that addresses these\nlimitations by introducing correspondence-aware noise correction and\ninterpolated attention maps. Our method aligns textures and structures between\nthe source and target images through semantic correspondence, enabling accurate\ntexture transfer while generating new content when necessary. Cora offers\ncontrol over the balance between content generation and preservation. Extensive\nexperiments demonstrate that, quantitatively and qualitatively, Cora excels in\nmaintaining structure, textures, and identity across diverse edits, including\npose changes, object addition, and texture refinements. User studies confirm\nthat Cora delivers superior results, outperforming alternatives.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650e3abcae507a2c7c847baa/dRNPZqV4HrW79gs506jk7.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23907.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650e3abcae507a2c7c847baa",
      "avatarUrl": "/avatars/a138bc6c9ba80a486fb36f4e1aa16b33.svg",
      "fullname": "Amirhossein Alimohammadi",
      "name": "Amirhossein-Alimohammadi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23001",
      "authors": [
        {
          "_id": "6839f87c49d173e7b23f220b",
          "user": {
            "_id": "66fa2c61c25c3fcb32f9f131",
            "avatarUrl": "/avatars/05387c30f2d1803fa0a5b176c3706772.svg",
            "isPro": false,
            "fullname": "Yize Cheng",
            "user": "yizecheng",
            "type": "user"
          },
          "name": "Yize Cheng",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T18:27:47.059Z",
          "hidden": false
        },
        {
          "_id": "6839f87c49d173e7b23f220c",
          "user": {
            "_id": "659dc02d72238596c24d49f5",
            "avatarUrl": "/avatars/d4600d23ccc72f296fab7f626d5895e7.svg",
            "isPro": false,
            "fullname": "Wenxiao Wang",
            "user": "wangwenxiao",
            "type": "user"
          },
          "name": "Wenxiao Wang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T18:28:18.975Z",
          "hidden": false
        },
        {
          "_id": "6839f87c49d173e7b23f220d",
          "user": {
            "_id": "63449874ee1504dbcd59af3d",
            "avatarUrl": "/avatars/57a805d82d16de9544c98585bd7a3e55.svg",
            "isPro": false,
            "fullname": "MazdaM",
            "user": "mmoayeri",
            "type": "user"
          },
          "name": "Mazda Moayeri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T20:01:34.913Z",
          "hidden": false
        },
        {
          "_id": "6839f87c49d173e7b23f220e",
          "name": "Soheil Feizi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T02:22:14.000Z",
      "submittedOnDailyAt": "2025-06-03T00:16:12.678Z",
      "title": "ダイパック：LLMs でのバックドアを使用してテストセットの汚染を確実にフラギングする方法",
      "submittedOnDailyBy": {
        "_id": "66fa2c61c25c3fcb32f9f131",
        "avatarUrl": "/avatars/05387c30f2d1803fa0a5b176c3706772.svg",
        "isPro": false,
        "fullname": "Yize Cheng",
        "user": "yizecheng",
        "type": "user"
      },
      "summary": "オープンベンチマークは、大規模な言語モデルの評価と進歩に不可欠であり、再現性と透明性を提供します。しかし、そのアクセシビリティが、テストセット汚染の際に軽くなる可能性があります。本稿では、バックドア攻撃を活用して、モデルがベンチマークテストセットを使用したかどうかを特定するためのフレームワーク「DyePack」を介しています。バックドアサンプルをテストデータと混ぜ合わせて、モデルがそれに学習したことをフラグすることを目的としています。バンクがドライバックパックをお金と混ぜて、盗み人を識別するようにしているようなように、DyePackはバックドアサンプルをテストデータと混ぜ合わせ、モデルがそれに学習したことをフラグすることを目的としています。また、複数のバックドアを含む原則的な設計を提案し、フラグされたモデルの全てに対して、確率的なファルシュポジティブ率（FPR）を計算することができます。これにより、偽騙訴訟を防ぎながら、汚染のそのようなことが見つかった場合に強い証拠を提供します。DyePackは、5つのモデルを3つのデータセットで評価し、複数選択と開放的な生成タスクをカバーしています。複数選択問題では、MMLU-ProとBig-Bench-Hardでは、8つのバックドアを使用して、確率的なFPRが低く、0.000073%までの確認で、すべての汚染モデルを成功に検出します。開放的な生成タスクでは、モデルの全てを汚染されたモデルとして検出し、6つのバックドアを使用して、確率的なFPRが0.127%で、すべての汚染モデルを検出します。",
      "upvotes": 8,
      "discussionId": "6839f87c49d173e7b23f222c",
      "githubRepo": "https://github.com/chengez/DyePack",
      "ai_summary": "DyePack, a framework using backdoor attacks, identifies models that leveraged benchmark test sets during training by introducing benign backdoor samples, ensuring precise false positive rates while preventing false accusations.",
      "ai_keywords": [
        "backdoor attacks",
        "test set contamination",
        "false positive rate",
        "FPR",
        "DyePack",
        "multiple backdoors",
        "stochastic targets",
        "MMLU-Pro",
        "Big-Bench-Hard",
        "Alpaca",
        "multiple-choice questions",
        "open-ended generation tasks"
      ]
    },
    "publishedAt": "2025-05-28T22:22:14.000Z",
    "title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using\n  Backdoors",
    "summary": "Open benchmarks are essential for evaluating and advancing large language\nmodels, offering reproducibility and transparency. However, their accessibility\nmakes them likely targets of test set contamination. In this work, we introduce\nDyePack, a framework that leverages backdoor attacks to identify models that\nused benchmark test sets during training, without requiring access to the loss,\nlogits, or any internal details of the model. Like how banks mix dye packs with\ntheir money to mark robbers, DyePack mixes backdoor samples with the test data\nto flag models that trained on it. We propose a principled design incorporating\nmultiple backdoors with stochastic targets, enabling exact false positive rate\n(FPR) computation when flagging every model. This provably prevents false\naccusations while providing strong evidence for every detected case of\ncontamination. We evaluate DyePack on five models across three datasets,\ncovering both multiple-choice and open-ended generation tasks. For\nmultiple-choice questions, it successfully detects all contaminated models with\nguaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard\nusing eight backdoors. For open-ended generation tasks, it generalizes well and\nidentifies all contaminated models on Alpaca with a guaranteed false positive\nrate of just 0.127% using six backdoors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66fa2c61c25c3fcb32f9f131",
      "avatarUrl": "/avatars/05387c30f2d1803fa0a5b176c3706772.svg",
      "fullname": "Yize Cheng",
      "name": "yizecheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00577",
      "authors": [
        {
          "_id": "683e646de9f216ff5a3e5dea",
          "user": {
            "_id": "658ab894c4b2004663dff3ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658ab894c4b2004663dff3ae/oPRnFuW2Imaa2KkYWNbSf.jpeg",
            "isPro": false,
            "fullname": "YUFA ZHOU",
            "user": "MasterZhou",
            "type": "user"
          },
          "name": "Yufa Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:13.058Z",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5deb",
          "user": {
            "_id": "66968099c952e09a4cb29f78",
            "avatarUrl": "/avatars/bd3a361fe5315e26e9ae328071704eed.svg",
            "isPro": false,
            "fullname": "Wang",
            "user": "Steven-Shaobo",
            "type": "user"
          },
          "name": "Shaobo Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:16.609Z",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5dec",
          "name": "Xingyu Dong",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5ded",
          "name": "Xiangqi Jin",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5dee",
          "name": "Yifang Chen",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5def",
          "name": "Yue Min",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5df0",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5df1",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5df2",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "683e646de9f216ff5a3e5df3",
          "name": "Linfeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T14:22:40.000Z",
      "submittedOnDailyAt": "2025-06-03T01:32:07.834Z",
      "title": "「经济学者のように理由をつくる：経済問題におけるトレーニング後による戦略的な一般化を引き起こす」\n\n(Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs)",
      "submittedOnDailyBy": {
        "_id": "658ab894c4b2004663dff3ae",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658ab894c4b2004663dff3ae/oPRnFuW2Imaa2KkYWNbSf.jpeg",
        "isPro": false,
        "fullname": "YUFA ZHOU",
        "user": "MasterZhou",
        "type": "user"
      },
      "summary": "直接訓練大語言モデル（LLMs）マルチアガントシステム（MAS）は、複雑な報酬モデリング、動的なアガント相互作用、および強い一般化要求により難しいと考えられます。本論文では、後訓練技術、特に観察可能な報酬を含む学習検証（SFT）と強化学習（RLVR）について、マルチアガントシナリオに適用できるかどうかを調査します。我々は、経済的論理をテストベンダーとし、数学とゲーム理論の強い基盤、構造化された分析的論理の必要性、市場設計、リソース配分、政策分析などの実世界の応用に関連していることを活用します。我々は、Recon（ECONomistのように論理する）を紹介します。Reconは、2,100件の高品質の経済的論理問題の手動精選データセットで7BパラメータのオープンソースLLMを後訓練しました。経済的論理ベンチマークとマルチアガントゲームの詳細な評価により、構造化された論理と経済的合理性に明らかな向上が見られました。これらの結果は、領域対応の後訓練が論理とアガントの対応を向上させる可能性を強調し、SFTとRLがモデルの行動を形成する役割について光を引きます。コードは、https://github.com/MasterZhou1/Recon にアクセスできます。",
      "upvotes": 7,
      "discussionId": "683e646ee9f216ff5a3e5e2c",
      "githubRepo": "https://github.com/MasterZhou1/Recon",
      "ai_summary": "Post-training techniques such as Supervised Fine-Tuning and Reinforcement Learning with Verifiable Rewards improve the reasoning and economic rationality of Large Language Models in multi-agent scenarios through domain-aligned training.",
      "ai_keywords": [
        "Large Language Models",
        "Multi-Agent Systems",
        "Supervised Fine-Tuning",
        "Reinforcement Learning with Verifiable Rewards",
        "economic reasoning",
        "domain-aligned post-training"
      ]
    },
    "publishedAt": "2025-05-31T10:22:40.000Z",
    "title": "Reasoning Like an Economist: Post-Training on Economic Problems Induces\n  Strategic Generalization in LLMs",
    "summary": "Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS)\nremains challenging due to intricate reward modeling, dynamic agent\ninteractions, and demanding generalization requirements. This paper explores\nwhether post-training techniques, specifically Supervised Fine-Tuning (SFT) and\nReinforcement Learning with Verifiable Rewards (RLVR), can effectively\ngeneralize to multi-agent scenarios. We use economic reasoning as a\ntestbed, leveraging its strong foundations in mathematics and game theory, its\ndemand for structured analytical reasoning, and its relevance to real-world\napplications such as market design, resource allocation, and policy analysis.\nWe introduce Recon (Reasoning like an\nECONomist), a 7B-parameter open-source LLM post-trained on a\nhand-curated dataset of 2,100 high-quality economic reasoning problems.\nComprehensive evaluation on economic reasoning benchmarks and multi-agent games\nreveals clear improvements in structured reasoning and economic rationality.\nThese results underscore the promise of domain-aligned post-training for\nenhancing reasoning and agent alignment, shedding light on the roles of SFT and\nRL in shaping model behavior. Code is available at\nhttps://github.com/MasterZhou1/Recon .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00577.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658ab894c4b2004663dff3ae",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658ab894c4b2004663dff3ae/oPRnFuW2Imaa2KkYWNbSf.jpeg",
      "fullname": "YUFA ZHOU",
      "name": "MasterZhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23977",
      "authors": [
        {
          "_id": "683e380cb028cae60270adcf",
          "user": {
            "_id": "6702d9e2db3b7a57f9420e8d",
            "avatarUrl": "/avatars/2e65e83e8d13ca129f6382deb6e8bdfc.svg",
            "isPro": false,
            "fullname": "Yichen Feng",
            "user": "EthanSta",
            "type": "user"
          },
          "name": "Yichen Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:47:01.523Z",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add0",
          "user": {
            "_id": "653df1323479e9ebbe3eb6cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
            "isPro": true,
            "fullname": "Zhangchen Xu",
            "user": "zhangchenxu",
            "type": "user"
          },
          "name": "Zhangchen Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:58.968Z",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add1",
          "name": "Fengqing Jiang",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add2",
          "name": "Yuetai Li",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add3",
          "name": "Bhaskar Ramasubramanian",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add4",
          "name": "Luyao Niu",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add5",
          "name": "Bill Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "683e380cb028cae60270add6",
          "name": "Radha Poovendran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T20:08:36.000Z",
      "submittedOnDailyAt": "2025-06-03T01:08:43.749Z",
      "title": "VisualSphinx: 大規模合成視覚ロジックパズルのRL用",
      "submittedOnDailyBy": {
        "_id": "653df1323479e9ebbe3eb6cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
        "isPro": true,
        "fullname": "Zhangchen Xu",
        "user": "zhangchenxu",
        "type": "user"
      },
      "summary": "ビジョン言語モデル（VLMs）は、効果的な多モード論理とロジック的に一貫した判断を行うことが期待されています。これは、図解読や空間問題解決などの任務において重要です。しかし、現在のVLMの論理は、大規模なおよび構造的に良い訓練データセットに欠けています。この隙を埋めるために、私たちはVisualSphinxを提案します。これは、最初のような大規模な合成的な視覚的なロジック論理訓練データです。画像合成における答えの基礎化の挑戦を解決するために、私たちはルールから画像合成のパイプラインを提案します。これは、種の質問から謎のルールを抽出し、拡張し、謎のサンプルの組み立てに向けた基礎化合成画像のコードを生成します。実験は、VisualSphinx上でGRPOを用いて訓練されたVLMは、我々のデータセットのロジック的な一貫性と読解性からベータを受け、論理論理タスクにおける性能向上を示します。VisualSphinxから開発された拡張された論理能力は、代数論理、算術論理、および幾何論理などの他の論理タスクにも利益を与えます。",
      "upvotes": 7,
      "discussionId": "683e380eb028cae60270ae82",
      "ai_summary": "VisualSphinx provides a large-scale synthetic dataset to improve multimodal reasoning in vision language models, enhancing performance on various logical reasoning tasks.",
      "ai_keywords": [
        "vision language models",
        "multimodal reasoning",
        "logical reasoning",
        "large-scale synthetic visual logical reasoning",
        "image synthesis",
        "rule-to-image synthesis",
        "GRPO"
      ]
    },
    "publishedAt": "2025-05-29T16:08:36.000Z",
    "title": "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL",
    "summary": "Vision language models (VLMs) are expected to perform effective multimodal\nreasoning and make logically coherent decisions, which is critical to tasks\nsuch as diagram understanding and spatial problem solving. However, current VLM\nreasoning lacks large-scale and well-structured training datasets. To bridge\nthis gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic\nvisual logical reasoning training data. To tackle the challenge of image\nsynthesis with grounding answers, we propose a rule-to-image synthesis\npipeline, which extracts and expands puzzle rules from seed questions and\ngenerates the code of grounding synthesis image synthesis for puzzle sample\nassembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx\nbenefit from logical coherence and readability of our dataset and exhibit\nimproved performance on logical reasoning tasks. The enhanced reasoning\ncapabilities developed from VisualSphinx also benefit other reasoning tasks\nsuch as algebraic reasoning, arithmetic reasoning and geometry reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "zhangchenxu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23059",
      "authors": [
        {
          "_id": "683b4b86c4b9677f3f7125e5",
          "user": {
            "_id": "6683c3b04905815dcffe7a21",
            "avatarUrl": "/avatars/73c50843d99b6fb8b1ee8fe11106c4ce.svg",
            "isPro": false,
            "fullname": "Dohyeon Lee",
            "user": "waylight3",
            "type": "user"
          },
          "name": "Dohyeon Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:50:07.024Z",
          "hidden": false
        },
        {
          "_id": "683b4b86c4b9677f3f7125e6",
          "user": {
            "_id": "645aedd221ab438e732bff43",
            "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
            "isPro": false,
            "fullname": "Yeonseok Jeong",
            "user": "yeonseokjeong",
            "type": "user"
          },
          "name": "Yeonseok Jeong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:42:20.758Z",
          "hidden": false
        },
        {
          "_id": "683b4b86c4b9677f3f7125e7",
          "name": "Seung-won Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T04:04:25.000Z",
      "submittedOnDailyAt": "2025-06-03T01:42:51.938Z",
      "title": "トークンから行動へ：状態機の理由論を活用して情報検索における過度考慮を軽減する",
      "submittedOnDailyBy": {
        "_id": "645aedd221ab438e732bff43",
        "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
        "isPro": false,
        "fullname": "Yeonseok Jeong",
        "user": "yeonseokjeong",
        "type": "user"
      },
      "summary": "Chain-of-Thought (CoT) promptingは、複雑な理由論を大規模な言語モデル（LLMs）で可能にします。これは情報検索（IR）へのアプリケーションも含む。しかし、これは過度な考え方により、モデルが過多に長いさまざまな意味的な冗長な跡を生成し、ほとんどの利益もないことにより、モデルの性能が低下することがあります。我々は、IRにおける2つの重要な課題を特定しました。1つは、類似した状態を再訪問する冗長な軌跡、2つは、ユーザーの意図から離れた理由論です。これらを解決するために、我々は、遷移ベースの理由論フレームワークであるState Machine Reasoning（SMR）を提案しました。SMRは、早終了と細かい制御を支える離散な行動（Refine、Rerank、Stop）からなるものです。BEIRとBRIGHTベンチマークでの実験は、SMRはトークン使用量を74.4%減少させながら、nDCG@10での検索性能を3.4%向上させることを示しました。SMRは、LLMsと検索モデルにわたって一般化可能で、タスク専用のチューニングが必要なく、単なるCoT理由論の実用的な代替として提供されます。コードと詳細は、https://github.com/ldilab/SMRにアクセスできます。",
      "upvotes": 7,
      "discussionId": "683b4b87c4b9677f3f712609",
      "githubRepo": "https://github.com/ldilab/SMR",
      "ai_summary": "State Machine Reasoning (SMR) improves information retrieval performance and reduces token usage in large language models by addressing overthinking through a discrete action framework.",
      "ai_keywords": [
        "Chain-of-Thought",
        "State Machine Reasoning",
        "IR",
        "redundant trajectories",
        "misguided reasoning",
        "early stopping",
        "nDCG@10"
      ]
    },
    "publishedAt": "2025-05-29T00:04:25.000Z",
    "title": "From Token to Action: State Machine Reasoning to Mitigate Overthinking\n  in Information Retrieval",
    "summary": "Chain-of-Thought (CoT) prompting enables complex reasoning in large language\nmodels (LLMs), including applications in information retrieval (IR). However,\nit often leads to overthinking, where models produce excessively long and\nsemantically redundant traces with little or no benefit. We identify two key\nchallenges in IR: redundant trajectories that revisit similar states and\nmisguided reasoning that diverges from user intent. To address these, we\npropose State Machine Reasoning (SMR), a transition-based reasoning framework\ncomposed of discrete actions (Refine, Rerank, Stop) that support early stopping\nand fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show\nthat SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token\nusage by 74.4%. It generalizes across LLMs and retrievers without requiring\ntask-specific tuning, offering a practical alternative to conventional CoT\nreasoning. The code and details are available at https://github.com/ldilab/SMR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23059.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645aedd221ab438e732bff43",
      "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
      "fullname": "Yeonseok Jeong",
      "name": "yeonseokjeong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01881",
      "authors": [
        {
          "_id": "683e6708ef9c250c6642783c",
          "user": {
            "_id": "65c431a609672feb8cac22e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c431a609672feb8cac22e7/obXCXUOgEoCuJbGuzYOGA.jpeg",
            "isPro": false,
            "fullname": "Yaoyao Qian",
            "user": "FreaxRuby",
            "type": "user"
          },
          "name": "Yaoyao Qian",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T03:07:55.037Z",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c6642783d",
          "name": "Jindan Huang",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c6642783e",
          "name": "Yuanli Wang",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c6642783f",
          "user": {
            "_id": "636681feaa6a4af6073ba73e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636681feaa6a4af6073ba73e/u_0moYNu6as9Sszp-ej95.png",
            "isPro": true,
            "fullname": "Simon Yu",
            "user": "simonycl",
            "type": "user"
          },
          "name": "Simon Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:06.559Z",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c66427840",
          "name": "Kyrie Zhixuan Zhou",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c66427841",
          "name": "Jiayuan Mao",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c66427842",
          "name": "Mingfu Liang",
          "hidden": false
        },
        {
          "_id": "683e6708ef9c250c66427843",
          "name": "Hanhan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:11:10.000Z",
      "submittedOnDailyAt": "2025-06-03T01:38:44.220Z",
      "title": "タスク向けダイアロジーでの意図のトリガー可能性をモデル化するための構造的なトラジェクトの時間の選び方：行動する時、待つ時",
      "submittedOnDailyBy": {
        "_id": "65c431a609672feb8cac22e7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c431a609672feb8cac22e7/obXCXUOgEoCuJbGuzYOGA.jpeg",
        "isPro": false,
        "fullname": "Yaoyao Qian",
        "user": "FreaxRuby",
        "type": "user"
      },
      "summary": "タスク取向ダイアログシステムは、ユーザーのユーターンが語義的に完全であるが、適切なシステムアクションに必要な構造的な情報を欠く場合に困難を見せます。これは、ユーザーが自分の需要を完全に理解していない一方、システムが細かくなる意図を定義する必要があるために起きます。現在のLLMベースのアグリートは、言語的に完全であるかどうかやコンテキストによってテキストを引き起こすことができるかを効果的に区別できないため、コラボレーション的な意図形成のフレームワークを欠くことにより問題を見出しています。私たちは、STORMというフレームワークを紹介します。これは、UserLLM（完全な内部アクセス）とAgentLLM（観察可能な行動のみ）の会話で対称的な情報動力学をモデリングします。STORMは、表現の軌跡と潜在的な認知的なトランジションを捉える注釈付きコーパスを生成し、コラボレーション的な理解の開発をシステマティックに分析することができます。私たちの貢献は、(1) ダイアログシステムの対称的な情報処理の形式化、(2) 意図形成の軌跡とコラボレーション的な理解の進化、(3) 任務性能とともに内部的な認知の向上を測定する評価指標にあります。4つの言語モデルを経験的に検証した結果から、中度の不確実性（40-60%）は、特定のスキャンデラーで完全な透明性を上回ることができることがわかり、モデル特有のパターンは人間とAIのコラボレーションの最適な情報の完全性の再考を促すことを示しています。これらの発見は、対称的な理由の動態を理解することに貢献し、不確実性調整されたダイアログシステムの設計に情報を与えます。",
      "upvotes": 6,
      "discussionId": "683e670bef9c250c664278be",
      "projectPage": "https://nanostorm.netlify.app/",
      "githubRepo": "https://github.com/H-Freax/Storm",
      "ai_summary": "STORM frameworks facilitates collaborative intent formation in task-oriented dialogue systems by modeling asymmetric information dynamics between UserLLM and AgentLLM.",
      "ai_keywords": [
        "UserLLM",
        "AgentLLM",
        "asymmetric information dynamics",
        "collaborative understanding",
        "intent formation",
        "expression trajectories",
        "latent cognitive transitions",
        "uncertainty-calibrated dialogue systems"
      ]
    },
    "publishedAt": "2025-06-02T13:11:10.000Z",
    "title": "WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent\n  Triggerability in Task-Oriented Dialogue",
    "summary": "Task-oriented dialogue systems often face difficulties when user utterances\nseem semantically complete but lack necessary structural information for\nappropriate system action. This arises because users frequently do not fully\nunderstand their own needs, while systems require precise intent definitions.\nCurrent LLM-based agents cannot effectively distinguish between linguistically\ncomplete and contextually triggerable expressions, lacking frameworks for\ncollaborative intent formation. We present STORM, a framework modeling\nasymmetric information dynamics through conversations between UserLLM (full\ninternal access) and AgentLLM (observable behavior only). STORM produces\nannotated corpora capturing expression trajectories and latent cognitive\ntransitions, enabling systematic analysis of collaborative understanding\ndevelopment. Our contributions include: (1) formalizing asymmetric information\nprocessing in dialogue systems; (2) modeling intent formation tracking\ncollaborative understanding evolution; and (3) evaluation metrics measuring\ninternal cognitive improvements alongside task performance. Experiments across\nfour language models reveal that moderate uncertainty (40-60%) can outperform\ncomplete transparency in certain scenarios, with model-specific patterns\nsuggesting reconsideration of optimal information completeness in human-AI\ncollaboration. These findings contribute to understanding asymmetric reasoning\ndynamics and inform uncertainty-calibrated dialogue system design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01881.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c431a609672feb8cac22e7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c431a609672feb8cac22e7/obXCXUOgEoCuJbGuzYOGA.jpeg",
      "fullname": "Yaoyao Qian",
      "name": "FreaxRuby",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01667",
      "authors": [
        {
          "_id": "683eaa8ed8d42fc832445ebd",
          "name": "Yan Shu",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ebe",
          "name": "Bin Ren",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ebf",
          "name": "Zhitong Xiong",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec0",
          "name": "Danda Pani Paudel",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec1",
          "name": "Luc Van Gool",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec2",
          "name": "Begum Demir",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec3",
          "name": "Nicu Sebe",
          "hidden": false
        },
        {
          "_id": "683eaa8ed8d42fc832445ec4",
          "name": "Paolo Rota",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T13:36:05.000Z",
      "submittedOnDailyAt": "2025-06-03T06:26:41.885Z",
      "title": "EarthMind: エアーミンド への向けて 多粒度と多センサーの地球観測\n\nwith Large Multimodal Models\n\nエアーミンド への向けて、大規模な多モデルを用いた多粒度と多センサーの地球観測に向けて",
      "submittedOnDailyBy": {
        "_id": "65c4f99b27736b5b86c2cbda",
        "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
        "isPro": false,
        "fullname": "Yan Shu",
        "user": "sy1998",
        "type": "user"
      },
      "summary": "大規模多模態モデル（LMMs）は、多様な視覚言語タスクで強い性能を示しています。しかし、地球観測（EO）データの全体的な理解には難しいことがあり、環境の観測および人類活動の影響を見るために重要です。本論文では、新しい視覚言語フレームワーク「EarthMind」を紹介します。EarthMindは、多粒度と多センサーのEOデータの理解を目的としています。EarthMindには2つの核心成分が含まれています：（1）空間アテンションプロンプティング（SAP）、LLMのアテンションを再分配してピクセルレベルの理解を強化する；（2）クロスモードフュージョン、不適合なモードエルを共有スペースに調整し、情報密度に基づいてトークンを適応的に重み付けして効果的なフュージョンを実現する。多センサーフュージョン評価を促進するために、EarthMind-Benchという詳細なベンチマークを提案します。EarthMind-Benchは、2,000点以上の人間注釈された多センサー画像-質問ペアを含み、広範囲の観測と理由論理タスクを収録しています。拡張された実験は、EarthMindの効果性を示しています。EarthMindは、EarthMind-Benchで最先端の性能を達成し、GPT-4oを超えることができます。また、EarthMindは、現在の方法よりも複数の公開EOベンチマークで優れています。これらの結果は、EarthMindが一つの統合的なフレームワークで多粒度と多センサーの挑戦を軽鬆に扱うことを示しています。",
      "upvotes": 6,
      "discussionId": "683eaa94d8d42fc832446013",
      "githubRepo": "https://github.com/shuyansy/EarthMind",
      "ai_summary": "EarthMind is a vision-language framework that uses spatial attention prompting and cross-modal fusion for efficient multi-granular and multi-sensor Earth Observation data understanding, outperforming larger models on specialized benchmarks.",
      "ai_keywords": [
        "Spatial Attention Prompting",
        "Cross-modal Fusion",
        "Earth Observation",
        "multi-granular",
        "multi-sensor",
        "EarthMind-Bench"
      ]
    },
    "publishedAt": "2025-06-02T09:36:05.000Z",
    "title": "EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation\n  with Large Multimodal Models",
    "summary": "Large Multimodal Models (LMMs) have demonstrated strong performance in\nvarious vision-language tasks. However, they often struggle to comprehensively\nunderstand Earth Observation (EO) data, which is critical for monitoring the\nenvironment and the effects of human activity on it. In this work, we present\nEarthMind, a novel vision-language framework for multi-granular and\nmulti-sensor EO data understanding. EarthMind features two core components: (1)\nSpatial Attention Prompting (SAP), which reallocates attention within the LLM\nto enhance pixel-level understanding; and (2) Cross-modal Fusion, which aligns\nheterogeneous modalities into a shared space and adaptively reweighs tokens\nbased on their information density for effective fusion. To facilitate\nmulti-sensor fusion evaluation, we propose EarthMind-Bench, a comprehensive\nbenchmark with over 2,000 human-annotated multi-sensor image-question pairs,\ncovering a wide range of perception and reasoning tasks. Extensive experiments\ndemonstrate the effectiveness of EarthMind. It achieves state-of-the-art\nperformance on EarthMind-Bench, surpassing GPT-4o despite being only 4B in\nscale. Moreover, EarthMind outperforms existing methods on multiple public EO\nbenchmarks, showcasing its potential to handle both multi-granular and\nmulti-sensor challenges in a unified framework.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01667.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c4f99b27736b5b86c2cbda",
      "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
      "fullname": "Yan Shu",
      "name": "sy1998",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24625",
      "authors": [
        {
          "_id": "683e5569fce31842c60675d7",
          "user": {
            "_id": "646e2fcaf813cfe153f1af6c",
            "avatarUrl": "/avatars/2f87d0e5c071000990da29cd744bc03d.svg",
            "isPro": false,
            "fullname": "Duo Zheng",
            "user": "zd11024",
            "type": "user"
          },
          "name": "Duo Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:45.196Z",
          "hidden": false
        },
        {
          "_id": "683e5569fce31842c60675d8",
          "name": "Shijia Huang",
          "hidden": false
        },
        {
          "_id": "683e5569fce31842c60675d9",
          "name": "Yanyang Li",
          "hidden": false
        },
        {
          "_id": "683e5569fce31842c60675da",
          "name": "Liwei Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T14:16:41.000Z",
      "submittedOnDailyAt": "2025-06-03T00:24:09.506Z",
      "title": "ビデオから学習する3次元世界：3次元視覚でMLLMを向上させる\nギオメトリープロフィード",
      "submittedOnDailyBy": {
        "_id": "646e2fcaf813cfe153f1af6c",
        "avatarUrl": "/avatars/2f87d0e5c071000990da29cd744bc03d.svg",
        "isPro": false,
        "fullname": "Duo Zheng",
        "user": "zd11024",
        "type": "user"
      },
      "summary": "以前の研究は、ビデオで読み解くことで3Dスケーンを理解するためのMultimodal Large Language Models（MLLMs）の応用について調査しました。これらのアプローチは通常、3Dデータの入力を必要とすることが多いです。例えば、ポイントクラスターや再構築したBird's-Eye View（BEV）マップなどです。我々の研究では、これらの研究に進展し、ビデオデータから直接3Dスペースでの理解と理由を行うMLLMの能力を向上させることでこの分野を進展しました。我々は、Video-3D Geometry Large Language Model（VG LLM）という新しいエファシェントな方法を提案しました。我々のアプローチは、3D視覚ギゼオメトリーエンコーダーを使用し、ビデオシーケンスから3D先驚信息を抽出します。この情報は、視覚トークンと統合され、MLLMに入力されます。拡張した実験は、3Dスケーン理解と空間理由に関する様々なタスクで我々の方法が大幅に改善を収めたことを示しました。印象的に、我々の4Bモデルは、明示的な3Dデータの入力を必要としないことで、現在の最先端の方法と比較しても競争的な結果を収め、VSI-Bench評価ではGemini-1.5-Proを超えたことを示しました。",
      "upvotes": 6,
      "discussionId": "683e556efce31842c6067737",
      "projectPage": "https://lavi-lab.github.io/VG-LLM/",
      "githubRepo": "https://github.com/LaVi-Lab/VG-LLM",
      "ai_summary": "A novel Video-3D Geometry Large Language Model (VG LLM) extracts 3D information directly from video sequences to enhance 3D scene understanding without additional 3D data, achieving competitive results in various tasks.",
      "ai_keywords": [
        "MLLMs",
        "Video-3D Geometry Large Language Model",
        "VG LLM",
        "3D visual geometry encoder",
        "VSI-Bench"
      ]
    },
    "publishedAt": "2025-05-30T10:16:41.000Z",
    "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors",
    "summary": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method, the Video-3D Geometry Large\nLanguage Model (VG LLM). Our approach employs a 3D visual geometry encoder that\nextracts 3D prior information from video sequences. This information is\nintegrated with visual tokens and fed into the MLLM. Extensive experiments have\nshown that our method has achieved substantial improvements in various tasks\nrelated to 3D scene understanding and spatial reasoning, all directly learned\nfrom video sources. Impressively, our 4B model, which does not rely on explicit\n3D data inputs, achieves competitive results compared to existing\nstate-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646e2fcaf813cfe153f1af6c",
      "avatarUrl": "/avatars/2f87d0e5c071000990da29cd744bc03d.svg",
      "fullname": "Duo Zheng",
      "name": "zd11024",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01413",
      "authors": [
        {
          "_id": "683e6aa057738c5cc3616d70",
          "user": {
            "_id": "6390525c00fb8ec4a424e0ff",
            "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
            "isPro": false,
            "fullname": "Yulei Qin",
            "user": "yolay",
            "type": "user"
          },
          "name": "Yulei Qin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T03:23:18.288Z",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d71",
          "name": "Gang Li",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d72",
          "name": "Zongyi Li",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d73",
          "name": "Zihan Xu",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d74",
          "name": "Yuchen Shi",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d75",
          "name": "Zhekai Lin",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d76",
          "name": "Xiao Cui",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d77",
          "name": "Ke Li",
          "hidden": false
        },
        {
          "_id": "683e6aa057738c5cc3616d78",
          "name": "Xing Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T08:11:44.000Z",
      "submittedOnDailyAt": "2025-06-03T02:06:35.145Z",
      "title": "インセンティブズライジング・リジュースンフォローイング・アドバンスド・インストラクション・フォローイング・大規模・言語・モデル",
      "submittedOnDailyBy": {
        "_id": "6390525c00fb8ec4a424e0ff",
        "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
        "isPro": false,
        "fullname": "Yulei Qin",
        "user": "yolay",
        "type": "user"
      },
      "summary": "現在の大規模言語モデル（LLMs）は、複雑な指示を従う際に、特に複数の制約が並列的、連鎖的、分岐的構造に配置されている場合には、課題を見出しています。直感的な解決策として、chain-of-thought（CoT）がLLMsの能力を普遍的に向上させることを期待されています。しかし、ベージャーなCoTは、指示の簡単な再語化の表面的な理由パターンによって、性能に負の影響を及ぼしています。それは、種類と次元の階層構造の間での制約の関係を特定するために、制約の構成を剥ぎ取ることができません。\n\nこの点に対して、我々は、テスト時の計算スケーリングによる理由を促すために、複雑な指示を処理するためのLLMsの能力を向上させる方法を提案します。まず、現在のタクノロジーの分解に基づいて、複雑な指示の再現可能なデータの取得方法を提案します。次に、可証明的なルールセンター的リバイワリング（RL）を利用して、指示従いの理由を特に養成することを試みます。複雑な指示の理由の浅い、非重要な性質を解決するために、サンプル単位の対比を用いて、上位のCoTの強制を実現します。また、専門家の行動クローニングを利用して、迅速な思考のLLMsからスキルフルな理由者への穩定的な分布の移行を促進します。\n\n7つの詳細なベンチマークでの拡大的な評価は、提案された方法の有効性を確認し、1.5B LLMは8B LLMと同等の性能を達成し、11.74%の効果を獲得しました。コードとデータは、https://github.com/yuleiqin/RAIFにアクセスできます。",
      "upvotes": 5,
      "discussionId": "683e6aa657738c5cc3616ecc",
      "projectPage": "https://huggingface.co/collections/yolay/raif-arxivorg-pdf-250601413-682b16e5c0c2fa9b73811369",
      "githubRepo": "https://github.com/yuleiqin/RAIF",
      "ai_summary": "A method is proposed to enhance large language models in handling complex instructions through incentivized reasoning and reinforcement learning, improving performance and reducing computational load.",
      "ai_keywords": [
        "chain-of-thought (CoT)",
        "reinforcement learning (RL)",
        "rule-centric reward signals",
        "sample-wise contrast",
        "behavior cloning",
        "instruction following",
        "decomposition of complex instructions"
      ]
    },
    "publishedAt": "2025-06-02T04:11:44.000Z",
    "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models",
    "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nare available at https://github.com/yuleiqin/RAIF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01413.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6390525c00fb8ec4a424e0ff",
      "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
      "fullname": "Yulei Qin",
      "name": "yolay",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24452",
      "authors": [
        {
          "_id": "683e8abd8e6e97efe0bf20d9",
          "name": "Anda Tang",
          "hidden": false
        },
        {
          "_id": "683e8abd8e6e97efe0bf20da",
          "name": "Yiming Dong",
          "hidden": false
        },
        {
          "_id": "683e8abd8e6e97efe0bf20db",
          "user": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "isPro": false,
            "fullname": "Yutao Zeng",
            "user": "Taoer",
            "type": "user"
          },
          "name": "Yutao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:39:15.864Z",
          "hidden": false
        },
        {
          "_id": "683e8abd8e6e97efe0bf20dc",
          "name": "zhou Xun",
          "hidden": false
        },
        {
          "_id": "683e8abd8e6e97efe0bf20dd",
          "name": "Zhouchen Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/GfsrISs7G7K3rMBB1uws5.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/eQqbJxhzxMNKVlyoqQPRA.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/q1H4zp_jhjZw57UYMnECl.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/TpjaRwRNbGOzWfL7y5C4P.png"
      ],
      "publishedAt": "2025-05-30T10:38:03.000Z",
      "submittedOnDailyAt": "2025-06-03T04:32:00.766Z",
      "title": "ステップサイズのどれか：マルチプロセス学習率スケジュールの統合",
      "submittedOnDailyBy": {
        "_id": "6371128eafbe42caa5a5222b",
        "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
        "isPro": false,
        "fullname": "Yutao Zeng",
        "user": "Taoer",
        "type": "user"
      },
      "summary": "拡大する計算コストと限られたリソースにより、予算バッチ訓練の重要性が強調されています。これは、予約された訓練バッチ内で最適な学習を達成するために設計されています。学習率スケジュールは、異なるネットワークとタスクの性能を支配し、特に予算バッチ訓練の場合、その設計は主にヒューリスティックであり、理論的な基盤が欠けています。また、最適な学習率スケジュールの選択は、試行錯誤の多くのテストによる選択により、訓練プロセスが低効率です。本稿では、異なるアーキテクチャとタスクの間で、異なる制約された訓練予算の下でも一貫して優れた性能を示す、理論的に基づいた学習率スケジュールを提案します。まず、新しい訓練予算に関連付けられた最適化フレームワークを構築し、曲率の変化に対する強固性を明確に考慮します。このフレームワークから、理論的に基づいたUnified Budget-Aware (UBA)スケジュールを得ます。UBAスケジュールは、一つの超パラメーターvarphiで制御され、柔軟性と簡単のバランスを提供し、ネットワークごとの数値最適化の必要を削減します。また、varphiと条件数の間の理論的な関係を確立し、我々のアプローチに説明と正当化を追加します。また、varphiの異なる値についての収束を証明します。理論的解析と実験結果により、varphiの選択の実用的なガイドラインを提供します。拡大する実験結果は、UBAが多様な視覚と言語タスクの幅広い範囲で、異なるネットワークアーキテクチャ（例：ResNet, OLMo）とスケールの範囲で、異なる訓練イテレーション予算の下で通常のスケジュールを超えることを示しています。",
      "upvotes": 5,
      "discussionId": "683e8abf8e6e97efe0bf2155",
      "ai_summary": "A unified budget-aware learning rate schedule is proposed to optimize training within limited iteration budgets, outperforming traditional schedules across various tasks and network architectures.",
      "ai_keywords": [
        "budgeted-iteration training",
        "learning rate schedules",
        "Unified Budget-Aware (UBA) schedule",
        "training budget-aware optimization framework",
        "robustness to landscape curvature variations",
        "condition number",
        "convergence",
        "ResNet",
        "OLMo"
      ]
    },
    "publishedAt": "2025-05-30T06:38:03.000Z",
    "title": "Stepsize anything: A unified learning rate schedule for\n  budgeted-iteration training",
    "summary": "The expanding computational costs and limited resources underscore the\ncritical need for budgeted-iteration training, which aims to achieve optimal\nlearning within predetermined iteration budgets.While learning rate schedules\nfundamentally govern the performance of different networks and tasks,\nparticularly in budgeted-iteration scenarios, their design remains largely\nheuristic, lacking theoretical foundations.In addition, the optimal learning\nrate schedule requires extensive trial-and-error selection, making the training\nprocess inefficient.In this work, we propose the Unified Budget-Aware (UBA)\nschedule, a theoretically grounded learning rate schedule that consistently\noutperforms commonly-used schedules among diverse architectures and tasks under\ndifferent constrained training budgets.First, we bridge the gap by constructing\na novel training budget-aware optimization framework, which explicitly accounts\nfor the robustness to landscape curvature variations.From this framework, we\nderive the UBA schedule, controlled by a single hyper-parameter varphi that\nprovides a trade-off between flexibility and simplicity, eliminating the need\nfor per-network numerical optimization. Moreover, we establish a theoretical\nconnection between varphi and the condition number, adding interpretation\nand justification to our approach. Besides, we prove the convergence for\ndifferent values of varphi.We offer practical guidelines for its selection\nvia theoretical analysis and empirical results.xtensive experimental results\nshow that UBA consistently surpasses the commonly-used schedules\nacross diverse vision and language tasks, spanning network architectures (e.g.,\nResNet, OLMo) and scales, under different training-iteration budgets.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/GfsrISs7G7K3rMBB1uws5.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/eQqbJxhzxMNKVlyoqQPRA.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/q1H4zp_jhjZw57UYMnECl.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/TpjaRwRNbGOzWfL7y5C4P.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24452.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6371128eafbe42caa5a5222b",
      "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
      "fullname": "Yutao Zeng",
      "name": "Taoer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00338",
      "authors": [
        {
          "_id": "683e64eb3e5a54d05365ddc6",
          "user": {
            "_id": "61809f31a367a8f5351ef353",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61809f31a367a8f5351ef353/s5eQ00YeoirakzE_rJ0cy.jpeg",
            "isPro": false,
            "fullname": "Yifan Peng",
            "user": "pyf98",
            "type": "user"
          },
          "name": "Yifan Peng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:46:09.029Z",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddc7",
          "name": "Shakeel Muhammad",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddc8",
          "name": "Yui Sudo",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddc9",
          "name": "William Chen",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddca",
          "name": "Jinchuan Tian",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddcb",
          "name": "Chyi-Jiunn Lin",
          "hidden": false
        },
        {
          "_id": "683e64eb3e5a54d05365ddcc",
          "name": "Shinji Watanabe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T01:44:44.000Z",
      "submittedOnDailyAt": "2025-06-03T01:39:05.927Z",
      "title": "OWSM v4: データスケーリングとクリーニングによるOpen Whisperスタイルの音声モデルの改善",
      "submittedOnDailyBy": {
        "_id": "61809f31a367a8f5351ef353",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61809f31a367a8f5351ef353/s5eQ00YeoirakzE_rJ0cy.jpeg",
        "isPro": false,
        "fullname": "Yifan Peng",
        "user": "pyf98",
        "type": "user"
      },
      "summary": "Open Whisper-style Speech Models (OWSM)プロジェクトは、学術用レジズル資源を使用して完全に開放的な音声基盤モデルを開発しましたが、トレーニングデータは十分ではありません。本論文は、Creative Commons 許諾の大規模なウェブクローリングデータセット YODAS を統合することで OWSM を強化します。しかし、YODAS の統合は、不正な言語ラベルと音声-テキストの非対応などの課題を引き起こすことで非単純です。これに対処し、公開ツールキットを使用してスケーラブルなデータクリーニングパイプラインを開発し、75言語で166,000時間の音声を構成するデータセットを収集しました。このようなカレーレッドデータセットと既存のOWSMデータを用いてトレーニングした新しいOWSM v4モデルは、多言語ベンチマークで以前のバージョンより大幅に優れています。また、モデルはWhisperやMMSといった先進的な工業モデルとの比較でも、複数のシナリオでフロンティアに達しています。公開により、クリーンデータ、事前学習モデルとすべての関連シクリプトをESPnetツールキットを通じて公開します。",
      "upvotes": 4,
      "discussionId": "683e64ec3e5a54d05365ddee",
      "projectPage": "https://www.wavlab.org/activities/2024/owsm/",
      "githubRepo": "https://github.com/espnet/espnet",
      "ai_summary": "The OWSM project is enhanced with a large-scale, cleaned web dataset, leading to improved multilingual speech models comparable to leading industrial models.",
      "ai_keywords": [
        "speach foundation models",
        "YODAS",
        "data-cleaning pipeline",
        "multilingual benchmarks",
        "Whisper",
        "MMS",
        "ESPnet toolkit"
      ]
    },
    "publishedAt": "2025-05-30T21:44:44.000Z",
    "title": "OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and\n  Cleaning",
    "summary": "The Open Whisper-style Speech Models (OWSM) project has developed a series of\nfully open speech foundation models using academic-scale resources, but their\ntraining data remains insufficient. This work enhances OWSM by integrating\nYODAS, a large-scale web-crawled dataset with a Creative Commons license.\nHowever, incorporating YODAS is nontrivial due to its wild nature, which\nintroduces challenges such as incorrect language labels and audio-text\nmisalignments. To address this, we develop a scalable data-cleaning pipeline\nusing public toolkits, yielding a dataset with 166,000 hours of speech across\n75 languages. Our new series of OWSM v4 models, trained on this curated dataset\nalongside existing OWSM data, significantly outperform previous versions on\nmultilingual benchmarks. Our models even match or surpass frontier industrial\nmodels like Whisper and MMS in multiple scenarios. We will publicly release the\ncleaned YODAS data, pre-trained models, and all associated scripts via the\nESPnet toolkit.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61809f31a367a8f5351ef353",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61809f31a367a8f5351ef353/s5eQ00YeoirakzE_rJ0cy.jpeg",
      "fullname": "Yifan Peng",
      "name": "pyf98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24183",
      "authors": [
        {
          "_id": "683e9a174de2ca71b8bc915b",
          "user": {
            "_id": "67de68f4f38795c545310088",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8604Li6_OlATOsTdY9oHL.png",
            "isPro": false,
            "fullname": "Yaoyu Zhu",
            "user": "zhuyaoyu",
            "type": "user"
          },
          "name": "Yaoyu Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:39:09.237Z",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc915c",
          "user": {
            "_id": "62c581177b48ba0bb8cdb737",
            "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
            "isPro": false,
            "fullname": "di huang",
            "user": "dihuang",
            "type": "user"
          },
          "name": "Di Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:37:07.655Z",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc915d",
          "name": "Hanqi Lyu",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc915e",
          "name": "Xiaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc915f",
          "name": "Chongxiao Li",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9160",
          "name": "Wenxuan Shi",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9161",
          "name": "Yutong Wu",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9162",
          "name": "Jianan Mu",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9163",
          "name": "Jinghua Wang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9164",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9165",
          "name": "Pengwei Jin",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9166",
          "name": "Shuyao Cheng",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9167",
          "name": "Shengwen Liang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9168",
          "name": "Xishan Zhang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc9169",
          "name": "Rui Zhang",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc916a",
          "name": "Zidong Du",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc916b",
          "name": "Qi Guo",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc916c",
          "name": "Xing Hu",
          "hidden": false
        },
        {
          "_id": "683e9a174de2ca71b8bc916d",
          "name": "Yunji Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T03:51:06.000Z",
      "submittedOnDailyAt": "2025-06-03T05:58:04.258Z",
      "title": "CodeV-R1: 理由付きVerilog生成",
      "submittedOnDailyBy": {
        "_id": "62c581177b48ba0bb8cdb737",
        "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
        "isPro": false,
        "fullname": "di huang",
        "user": "dihuang",
        "type": "user"
      },
      "summary": "大型言語モデル（LLMs）は、可証明な報酬（RLVR）による強化学習を用いて、ソフトウェアプログラミングや数学問題のような明確で自動可能な確認が可能なタスクにおいて進歩を達成しました。これらの効果を電子設計自動化（EDA）に拡張し、特に自然言語（NL）の規定からハードウェア記述言語（HDL）のようなVerilogを自動的に生成することについては、3つの主要な課題があります：自動化されたおおきく正確な確認環境のないこと、高品質のNL-コードペアの不足、RLVRの計算コストの高さ。このために、CodeV-R1というRLVRフレームワークを紹介します。まず、ロールベースのテストベンチジェネレータを開発し、ゴールデンリファレンスとの強固な等価性チェックを行います。次に、オープンソースのVerilogスニペットとLLMで生成されたNL説明を組み合わせ、生成されたテストベンチによりコード-NL-コードの一致性を確認し、不等価な例を除去して高品質なデータセットを生成します。最後に、2段階の「ディスティルしてからRL」の訓練パイプラインを用います：理由能力の初期段階のディスティルを行い、自動的にサンプリングレートを調整して訓練コストを削減できる新しいRLVRアルゴリズム（DAPO）を用います。このもとに、CodeV-R1-7BはVerilogEval v2とRTLLM v1.1ではそれぞれ68.6%と72.9%のpass@1を達成し、前の最先端を12~20%超え、または671B DeepSeek-R1の性能を追い越すことができました。モデル、訓練パイプライン、データセットを公開し、EDAやLLMコミュニティの研究に役立てることを目的とします。",
      "upvotes": 4,
      "discussionId": "683e9a184de2ca71b8bc91b3",
      "projectPage": "https://iprc-dip.github.io/CodeV-R1",
      "ai_summary": "CodeV-R1, an RLVR framework for Verilog generation, achieves state-of-the-art performance in EDA using a rule-based testbench, round-trip data synthesis, and adaptive RLVR training.",
      "ai_keywords": [
        "reinforcement learning with verifiable reward",
        "RLVR",
        "electronic design automation",
        "EDA",
        "hardware description languages",
        "HDLs",
        "Verilog",
        "natural-language",
        "NL",
        "testbench generator",
        "equivalence checking",
        "round-trip data synthesis",
        "dataset",
        "two-stage training pipeline",
        "distillation",
        "adaptive DAPO",
        "VerilogEval",
        "RTLLM"
      ]
    },
    "publishedAt": "2025-05-29T23:51:06.000Z",
    "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
    "summary": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24183.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c581177b48ba0bb8cdb737",
      "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
      "fullname": "di huang",
      "name": "dihuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23504",
      "authors": [
        {
          "_id": "683e501c89dc42ba0515a4d8",
          "name": "Liyun Zhu",
          "hidden": false
        },
        {
          "_id": "683e501c89dc42ba0515a4d9",
          "name": "Qixiang Chen",
          "hidden": false
        },
        {
          "_id": "683e501c89dc42ba0515a4da",
          "name": "Xi Shen",
          "hidden": false
        },
        {
          "_id": "683e501c89dc42ba0515a4db",
          "name": "Xiaodong Cun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T14:48:10.000Z",
      "submittedOnDailyAt": "2025-06-03T00:01:06.695Z",
      "title": "VAU-R1: 強化学習による異常検知の理解を進める",
      "submittedOnDailyBy": {
        "_id": "63184c517ca1b876d99b7e0e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
        "isPro": false,
        "fullname": "Xiaodong Cun",
        "user": "vinthony",
        "type": "user"
      },
      "summary": "Video Anomaly Understanding (VAU)は、スマートシティ、安全監視、および災害警報システムなどの應用に重要ですが、空間時間的詳細な認識と難解性のある状況下での強固な理由論を求めることによって難しい。異常検出の進歩に伴い、現在の方法は解釈性の不足と異常イベントの原因とコンテキスト的な面貌の捉えやすさに欠点があります。この制限は、異常検出の理由論能力の評価に関する詳細なベンチマークの欠け方によってさらに厳しくなります。これらの両方の挑戦に対処するために、私たちはVAU-R1を紹介します。VAU-R1は、Reinforcement Fine-Tuning (RFT)を通じてMultimodal Large Language Models (MLLMs)に基づくデータ効率的なフレームワークで、異常理由論を強化します。また、私たちはVAU-Benchを提案します。VAU-Benchは、異常理由論に適したChain-of-Thoughtベンチマークで、複数選択QA、詳細な理由、時系列注釈、説明的なキャプションを特徴とします。実験結果から、VAU-R1は多様なコンテキストでの問答の正確性、時系列のグローディング、理由の一致性を大幅に向上させます。これらの方法とベンチマークは、説明可能な理由論を取り入れた映像異常理解の強い基盤を築きます。コードは、https://github.com/GVCLab/VAU-R1に公開されています。",
      "upvotes": 4,
      "discussionId": "683e502189dc42ba0515a5e1",
      "ai_summary": "VAU-R1 uses Multimodal Large Language Models with Reinforcement Fine-Tuning to enhance video anomaly reasoning, complemented by VAU-Bench, a Chain-of-Thought benchmark for evaluating anomaly understanding.",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Reinforcement Fine-Tuning (RFT)",
        "Chain-of-Thought",
        "benchmark",
        "question answering",
        "temporal grounding",
        "reasoning coherence"
      ]
    },
    "publishedAt": "2025-05-29T10:48:10.000Z",
    "title": "VAU-R1: Advancing Video Anomaly Understanding via Reinforcement\n  Fine-Tuning",
    "summary": "Video Anomaly Understanding (VAU) is essential for applications such as smart\ncities, security surveillance, and disaster alert systems, yet remains\nchallenging due to its demand for fine-grained spatio-temporal perception and\nrobust reasoning under ambiguity. Despite advances in anomaly detection,\nexisting methods often lack interpretability and struggle to capture the causal\nand contextual aspects of abnormal events. This limitation is further\ncompounded by the absence of comprehensive benchmarks for evaluating reasoning\nability in anomaly scenarios. To address both challenges, we introduce VAU-R1,\na data-efficient framework built upon Multimodal Large Language Models (MLLMs),\nwhich enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT).\nBesides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored\nfor video anomaly reasoning, featuring multiple-choice QA, detailed rationales,\ntemporal annotations, and descriptive captions. Empirical results show that\nVAU-R1 significantly improves question answering accuracy, temporal grounding,\nand reasoning coherence across diverse contexts. Together, our method and\nbenchmark establish a strong foundation for interpretable and reasoning-aware\nvideo anomaly understanding. Our code is available at\nhttps://github.com/GVCLab/VAU-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23504.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63184c517ca1b876d99b7e0e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
      "fullname": "Xiaodong Cun",
      "name": "vinthony",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 323
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21179",
      "authors": [
        {
          "_id": "6838c66dc60fb2fc462cec9f",
          "user": {
            "_id": "64d0eb731ed6649d70afb136",
            "avatarUrl": "/avatars/4c591c86c575c82760126c39af5a02b4.svg",
            "isPro": true,
            "fullname": "Chen Dar-Yen",
            "user": "ChenDY",
            "type": "user"
          },
          "name": "Dar-Yen Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:47:40.163Z",
          "hidden": false
        },
        {
          "_id": "6838c66dc60fb2fc462ceca0",
          "user": {
            "_id": "638c81fa61eb51017518fa31",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/f0eCrzBrxz7Y9n25WkZ2v.png",
            "isPro": false,
            "fullname": "Hmrishav Bandyopadhyay",
            "user": "Hmrishav",
            "type": "user"
          },
          "name": "Hmrishav Bandyopadhyay",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T13:40:02.354Z",
          "hidden": false
        },
        {
          "_id": "6838c66dc60fb2fc462ceca1",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "6838c66dc60fb2fc462ceca2",
          "name": "Yi-Zhe Song",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64d0eb731ed6649d70afb136/BtItSeWg7RJJDSWqnZWIB.mp4"
      ],
      "publishedAt": "2025-05-27T13:30:46.000Z",
      "submittedOnDailyAt": "2025-06-03T07:58:27.904Z",
      "title": "正規化アテンションガイドニング：普遍的負面ガイドニングのDiffusionモデル",
      "submittedOnDailyBy": {
        "_id": "64d0eb731ed6649d70afb136",
        "avatarUrl": "/avatars/4c591c86c575c82760126c39af5a02b4.svg",
        "isPro": true,
        "fullname": "Chen Dar-Yen",
        "user": "ChenDY",
        "type": "user"
      },
      "summary": "負のガイドニング──明示的に不満足の属性を抑制する──は、拡散モデルでは特に少ステップサンプリングレジミンでは基本的な課題です。クラスファイザー自由ガイドニング（CFG）は通常の設定では効果的ですが、正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるために厳しいサンプリングステップの圧縮で失敗します。私たちは、L1基準の正負バ分支の予測が離れるた",
      "upvotes": 4,
      "discussionId": "6838c673c60fb2fc462cee10",
      "projectPage": "https://chendaryen.github.io/NAG.github.io/",
      "githubRepo": "https://github.com/ChenDarYen/Normalized-Attention-Guidance",
      "ai_summary": "Normalized Attention Guidance (NAG) enhances diffusion models by providing effective negative guidance across regimes and modalities without retraining.",
      "ai_keywords": [
        "negative guidance",
        "diffusion models",
        "few-step sampling",
        "Classifier-Free Guidance (CFG)",
        "Normalized Attention Guidance (NAG)",
        "attention space",
        "L1-based normalization",
        "extrapolation",
        "fidelity",
        "CLIP Score",
        "FID",
        "PFID",
        "ImageReward",
        "UNet",
        "DiT",
        "image",
        "video",
        "model-agnostic inference-time approach"
      ]
    },
    "publishedAt": "2025-05-27T09:30:46.000Z",
    "title": "Normalized Attention Guidance: Universal Negative Guidance for Diffusion\n  Model",
    "summary": "Negative guidance -- explicitly suppressing unwanted attributes -- remains a\nfundamental challenge in diffusion models, particularly in few-step sampling\nregimes. While Classifier-Free Guidance (CFG) works well in standard settings,\nit fails under aggressive sampling step compression due to divergent\npredictions between positive and negative branches. We present Normalized\nAttention Guidance (NAG), an efficient, training-free mechanism that applies\nextrapolation in attention space with L1-based normalization and refinement.\nNAG restores effective negative guidance where CFG collapses while maintaining\nfidelity. Unlike existing approaches, NAG generalizes across architectures\n(UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image,\nvideo), functioning as a universal plug-in with minimal computational\noverhead. Through extensive experimentation, we demonstrate consistent\nimprovements in text alignment (CLIP Score), fidelity (FID, PFID), and\nhuman-perceived quality (ImageReward). Our ablation studies validate each\ndesign component, while user studies confirm significant preference for\nNAG-guided outputs. As a model-agnostic inference-time approach requiring no\nretraining, NAG provides effortless negative guidance for all modern diffusion\nframeworks -- pseudocode in the Appendix!",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64d0eb731ed6649d70afb136/BtItSeWg7RJJDSWqnZWIB.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21179.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64d0eb731ed6649d70afb136",
      "avatarUrl": "/avatars/4c591c86c575c82760126c39af5a02b4.svg",
      "fullname": "Chen Dar-Yen",
      "name": "ChenDY",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01084",
      "authors": [
        {
          "_id": "683ea4388be2e40086ea9056",
          "name": "Saibo Geng",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea9057",
          "user": {
            "_id": "6420afc71ccd411979dc12dc",
            "avatarUrl": "/avatars/ee4a89ebc7a0716e21deaebc86e062e6.svg",
            "isPro": false,
            "fullname": "nathan ranchin",
            "user": "nathanrchn",
            "type": "user"
          },
          "name": "Nathan Ranchin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T09:39:27.313Z",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea9058",
          "name": "Yunzhen yao",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea9059",
          "name": "Maxime Peyrard",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea905a",
          "name": "Chris Wendler",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea905b",
          "name": "Michael Gastpar",
          "hidden": false
        },
        {
          "_id": "683ea4388be2e40086ea905c",
          "name": "Robert West",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/VnqCCx4RG5-62le1tQcaW.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/EGD7DC-lDXkTXOLD6r0IP.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/YDPF1VlfQ0Yfynq9peK5s.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/QY0t5DXqhIVBWrLpJvZm8.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/af0uwzA8bV-7byretzGwW.png"
      ],
      "publishedAt": "2025-06-01T17:03:02.000Z",
      "submittedOnDailyAt": "2025-06-03T06:05:31.112Z",
      "title": "zip2zip: 言語モデルの推論時に適応可能なボキャブラリーをトークン圧縮によって",
      "submittedOnDailyBy": {
        "_id": "5fce0cfeb3dbf216ad31836a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625748458774-5fce0cfeb3dbf216ad31836a.png",
        "isPro": false,
        "fullname": "Saibo-creator",
        "user": "Saibo-creator",
        "type": "user"
      },
      "summary": "トークナイゼーションの効率性は、大規模な言語モデル（LLMs）の性能とコストに重要な役割を果たしていますが、ほとんどのモデルは一般用コーパスに最適化された静的トークナイゼーターを使用しています。これらのトークナイゼーターの固定ボキャベラリーは、領域や言語に特化された入力に適応しないことが多いですが、これによりトークンシーケンスが長く、計算コストが高まります。私たちは、zip2zipというフレームワークを紹介します。これは、LLMsが推論時にトークンボキャベラリーを動的に調整できるようにし、生成されるトークンを減らし、これにより推論が速まるようにするものです。zip2zipは3つの重要なコンポーネントからなります：1) Lempel-Ziv-Welch（LZW）圧縮に基づくトークナイゼーター、トークンを動的に圧縮し、再利用可能な「ハイパートークン」に変換するものです。2) 新しいハイパートークンに対する埋め込みを計算する埋め込み層です。3) 因果言語モデリングの変体で、モデルをハイパートークン化された圧縮シーケンスに対して動作させるものです。私たちは、既存のLLMをzip2zipされることが10GPU時間で可能であることを示します。その結果、zip2zip LLMsは推論時にハイパートークンを使用することで、入力と出力シーケンスの長さを20-60%減らし、推論の遅延に大幅な改善を収めます。",
      "upvotes": 3,
      "discussionId": "683ea4398be2e40086ea90b7",
      "githubRepo": "https://github.com/epfl-dlab/zip2zip",
      "ai_summary": "A framework called zip2zip dynamically adjusts token vocabulary in LLMs at inference time using LZW compression, reducing token sequence length and improving inference speed.",
      "ai_keywords": [
        "LZW compression",
        "hypertokens",
        "embedding layer",
        "causal language modeling",
        "parameter-efficient finetuning"
      ]
    },
    "publishedAt": "2025-06-01T13:03:02.000Z",
    "title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via\n  Token Compression",
    "summary": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized for general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a\nframework that enables LLMs to dynamically adjust token vocabulary at inference\ntime, allowing for fewer generated tokens and thus faster inference. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\n(LZW) compression that incrementally compresses tokens into reusable\n\"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for\nnewly formed hypertokens at runtime; and (3) a causal language modeling variant\nthat trains the model to operate on hypertokenized, compressed sequences. We\nshow that an existing LLM can be zip2zip-fied in 10 GPU-hours via\nparameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to\nuse hypertokens at inference time, reducing input and output sequence length by\n20-60\\%, with significant improvements in inference latency.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/VnqCCx4RG5-62le1tQcaW.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/EGD7DC-lDXkTXOLD6r0IP.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/YDPF1VlfQ0Yfynq9peK5s.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/QY0t5DXqhIVBWrLpJvZm8.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/af0uwzA8bV-7byretzGwW.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fce0cfeb3dbf216ad31836a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625748458774-5fce0cfeb3dbf216ad31836a.png",
      "fullname": "Saibo-creator",
      "name": "Saibo-creator",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00643",
      "authors": [
        {
          "_id": "683e61338ebad8b7519bc7f3",
          "user": {
            "_id": "63e3f57754f51ea342ce26be",
            "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
            "isPro": false,
            "fullname": "Weijie Xu",
            "user": "xwjzds",
            "type": "user"
          },
          "name": "Weijie Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-03T02:43:01.430Z",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f4",
          "name": "Shixian Cui",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f5",
          "name": "Xi Fang",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f6",
          "name": "Chi Xue",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f7",
          "name": "Stephanie Eckman",
          "hidden": false
        },
        {
          "_id": "683e61338ebad8b7519bc7f8",
          "name": "Chandan Reddy",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/lDZvyKeHC66Fk1-x6VgbB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/l-YQ7eiVk7rb7n3Hi51p5.png"
      ],
      "publishedAt": "2025-05-31T17:14:21.000Z",
      "submittedOnDailyAt": "2025-06-03T01:22:58.893Z",
      "title": "SATA-BENCH: 複数選択ベンチマーク\n  質問",
      "submittedOnDailyBy": {
        "_id": "63e3f57754f51ea342ce26be",
        "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
        "isPro": false,
        "fullname": "Weijie Xu",
        "user": "xwjzds",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、単一回答の複数選択タスクでの評価が増加していますが、多くの実世界の問題は、オプションのセットからすべての正しい回答を特定する必要があります。この能力はまだ調査が不足しています。私たちは、選ぶすべてのものが当てはまる（SATA）クエスチョンを評価するための最初の専門的なベンチマークであるSATA-BENCHを紹介します。このベンチマークは、読解、法律、バイオミクスなどの多様な領域においてLLMsを評価します。27つのオープンソースおよび所有権モデルの評価により、显著な間違いが明らかになりました：最強のモデルでも、正確なマッチはそれほど高くなりません、すなわちLLMsはすべての正しい回答を信頼的に特定することができません。この弱点は、2つの核心的な課題から来ることを見出しました：選択バイアス - モデルは、内容に関係なく特定の選択肢を好みます、カウントバイアス - モデルは正しい回答の数を予測できません。これらの問題に対処するために、私たちは、トークンデビアスと適応ディスクリミネーションを組み合わせた解釈モデルによる選択ファンナル（Choice Funnel）を提案します。Choice Funnelは、相対的な基準に比べて、正確なマッチを最高29%上げ、推論コストを64%以上削減します。この見つかりは、現在のLLMsの基本的な制限を明らかにし、多回答の理由論を診断し改善する新しいフレームワークを導入します。SATA-BENCHとChoice Funnelを公開し、実用的な、多回答のアプリケーションでの強固な決定策に向けたLLMsの開発を促進します。",
      "upvotes": 3,
      "discussionId": "683e61358ebad8b7519bc8cf",
      "githubRepo": "https://github.com/sata-bench/sata-bench",
      "ai_summary": "SATA-BENCH evaluates LLMs on multi-answer questions, revealing selections biases and proposing Choice Funnel to improve accuracy and reduce costs in multi-answer reasoning tasks.",
      "ai_keywords": [
        "Select All That Apply (SATA) questions",
        "SATA-BENCH",
        "token debiasing",
        "adaptive thresholding",
        "Choice Funnel"
      ]
    },
    "publishedAt": "2025-05-31T13:14:21.000Z",
    "title": "SATA-BENCH: Select All That Apply Benchmark for Multiple Choice\n  Questions",
    "summary": "Large language models (LLMs) are increasingly evaluated on single-answer\nmultiple-choice tasks, yet many real-world problems require identifying all\ncorrect answers from a set of options. This capability remains underexplored.\nWe introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on\nSelect All That Apply (SATA) questions across diverse domains, including\nreading comprehension, law, and biomedicine. Our evaluation of 27 open-source\nand proprietary models reveals a significant gap: even the strongest model\nachieves only 41.8% exact match, exposing LLMs' inability to reliably identify\nall correct answers. We find that this weakness stems from two core challenges:\nselection bias - models favor certain choices regardless of content, and count\nbias - models fail to predict the correct number of answers. To address these\nissues, we propose Choice Funnel, a decoding strategy that combines token\ndebiasing with adaptive thresholding to guide models toward complete and\naccurate selections. Choice Funnel achieves up to 29% higher exact match than\ncompetitive baselines while reducing inference cost by over 64%. Our findings\nexpose fundamental limitations in current LLMs and introduce a new framework\nfor diagnosing and improving multi-answer reasoning. We release SATA-BENCH and\nChoice Funnel to promote LLM development for robust decision-making in\nrealistic, multi-answer applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/lDZvyKeHC66Fk1-x6VgbB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63e3f57754f51ea342ce26be/l-YQ7eiVk7rb7n3Hi51p5.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00643.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e3f57754f51ea342ce26be",
      "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
      "fullname": "Weijie Xu",
      "name": "xwjzds",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24842",
      "authors": [
        {
          "_id": "683e9da357738c5cc36d5175",
          "name": "Harsh Chaudhari",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d5176",
          "name": "Jamie Hayes",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d5177",
          "name": "Matthew Jagielski",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d5178",
          "name": "Ilia Shumailov",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d5179",
          "name": "Milad Nasr",
          "hidden": false
        },
        {
          "_id": "683e9da357738c5cc36d517a",
          "name": "Alina Oprea",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:41:58.000Z",
      "submittedOnDailyAt": "2025-06-03T05:35:48.034Z",
      "title": "言語モデルにおける注入から煉熱までの連鎖的敵意バイアス",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "モデルの結果を返します。\n\nモデルの結果を返します。",
      "upvotes": 3,
      "discussionId": "683e9da457738c5cc36d51b2",
      "ai_summary": "Adversarial injection of biased content can significantly propagate from teacher to student models during distillation, leading to frequent biased responses in both targeted and untargeted scenarios across various bias types and modalities.",
      "ai_keywords": [
        "model distillation",
        "language models",
        "adversarial manipulation",
        "data poisoning",
        "bias injection",
        "Untargeted Propagation",
        "Targeted Propagation",
        "perplexity filtering",
        "bias detection systems",
        "LLM-based autorater frameworks"
      ]
    },
    "publishedAt": "2025-05-30T13:41:58.000Z",
    "title": "Cascading Adversarial Bias from Injection to Distillation in Language\n  Models",
    "summary": "Model distillation has become essential for creating smaller, deployable\nlanguage models that retain larger system capabilities. However, widespread\ndeployment raises concerns about resilience to adversarial manipulation. This\npaper investigates vulnerability of distilled models to adversarial injection\nof biased content during training. We demonstrate that adversaries can inject\nsubtle biases into teacher models through minimal data poisoning, which\npropagates to student models and becomes significantly amplified. We propose\ntwo propagation modes: Untargeted Propagation, where bias affects multiple\ntasks, and Targeted Propagation, focusing on specific tasks while maintaining\nnormal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning\nrate), student models generate biased responses 76.9% of the time in targeted\nscenarios - higher than 69.4% in teacher models. For untargeted propagation,\nadversarial bias appears 6x-29x more frequently in student models on unseen\ntasks. We validate findings across six bias types (targeted advertisements,\nphishing links, narrative manipulations, insecure coding practices), various\ndistillation methods, and different modalities spanning text and code\ngeneration. Our evaluation reveals shortcomings in current defenses -\nperplexity filtering, bias detection systems, and LLM-based autorater\nframeworks - against these attacks. Results expose significant security\nvulnerabilities in distilled models, highlighting need for specialized\nsafeguards. We propose practical design principles for building effective\nadversarial bias mitigation strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24842.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24086",
      "authors": [
        {
          "_id": "683ebe67140f76a0a5485d51",
          "user": {
            "_id": "668e6b47f59574a8ec2ae078",
            "avatarUrl": "/avatars/1cbc80ee4fb4a832783bd3dbee032d6e.svg",
            "isPro": false,
            "fullname": "Zeeshan Khan",
            "user": "zk95",
            "type": "user"
          },
          "name": "Zeeshan Khan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T09:39:29.585Z",
          "hidden": false
        },
        {
          "_id": "683ebe67140f76a0a5485d52",
          "name": "Shizhe Chen",
          "hidden": false
        },
        {
          "_id": "683ebe67140f76a0a5485d53",
          "name": "Cordelia Schmid",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638878e0c9a44f05de452e91/V8gVhCAESrievXk5RWL_7.png"
      ],
      "publishedAt": "2025-05-30T00:13:36.000Z",
      "submittedOnDailyAt": "2025-06-03T07:53:33.269Z",
      "title": "コンペスト・アニソン：テキストから画像生成のための合成物体の先驅",
      "submittedOnDailyBy": {
        "_id": "638878e0c9a44f05de452e91",
        "avatarUrl": "/avatars/5748195a0ac761e2776548aabc3a53e3.svg",
        "isPro": false,
        "fullname": "Matthieu Futeral",
        "user": "matthieufp",
        "type": "user"
      },
      "summary": "コンプォジェスト・アニソーティング：文から画像を生成する際に複雑なお新しい物体の配置を扱うことが現在の文から画像への変換モデル（T2I）にとっては重要な課題です。先行のライブアウトベースの方法は、2次元ライブアウトを用いて空間制約を使用して物体の配置を改善することができますが、3次元の位置情報を捉えやすく、品質と一致性を失わせています。本稿では、現存するT2Iモデルの再学習を避けることで構成論的な画像生成を改善するための新しいフレームワーク「ComposeAnything」を紹介します。我々のアプローチは、LLMの連鎖オフィンシング論理能力を活用して、文から2.5次元の語義的なライブアウトを生成します。これは、2次元の物体バウンディングボックスに深さ情報と詳細なキャプションを追加したものです。このライブアウトに基づいて、我々は、空間と深さに関心を持つ物体の粗略な構成を生成し、設計された構成を捉え、ディフュージョンベースのT2Iモデルでの確率的なノイズ初期化を置き換える強力な解釈可能な先驅として役立ちます。この先驅は、物体先驅強化と空間制御されたデノイズ処理を通じて、構成論的な物体と一貫した背景の無難な生成を可能にし、不正確な先驅の精練を許容します。ComposeAnythingは、T2I-CompBenchとNSR-1Kベンチマークで、2D/3Dの空間配置、高い物体数、サーファリエフェクトの構成に対して最先端の方法を超える性能を示します。人間評価も、我々のモデルが文を忠実に反映した高品質の画像を生成することを示しています。",
      "upvotes": 3,
      "discussionId": "683ebe6a140f76a0a5485e06",
      "ai_summary": "ComposeAnything improves text-to-image generation by using LLMs for 2.5D semantic layouts, enhancing object placement and coherence in diffusion-based models.",
      "ai_keywords": [
        "LLMs",
        "chain-of-thought reasoning",
        "2.5D semantic layouts",
        "object bounding boxes",
        "depth information",
        "spatial and depth aware",
        "coarse composite",
        "denoising process",
        "object prior reinforcement",
        "spatial-controlled denoising",
        "diffusion-based T2I models",
        "T2I-CompBench",
        "NSR-1K benchmarks"
      ]
    },
    "publishedAt": "2025-05-29T20:13:36.000Z",
    "title": "ComposeAnything: Composite Object Priors for Text-to-Image Generation",
    "summary": "Generating images from text involving complex and novel object arrangements\nremains a significant challenge for current text-to-image (T2I) models.\nAlthough prior layout-based methods improve object arrangements using spatial\nconstraints with 2D layouts, they often struggle to capture 3D positioning and\nsacrifice quality and coherence. In this work, we introduce ComposeAnything, a\nnovel framework for improving compositional image generation without retraining\nexisting T2I models. Our approach first leverages the chain-of-thought\nreasoning abilities of LLMs to produce 2.5D semantic layouts from text,\nconsisting of 2D object bounding boxes enriched with depth information and\ndetailed captions. Based on this layout, we generate a spatial and depth aware\ncoarse composite of objects that captures the intended composition, serving as\na strong and interpretable prior that replaces stochastic noise initialization\nin diffusion-based T2I models. This prior guides the denoising process through\nobject prior reinforcement and spatial-controlled denoising, enabling seamless\ngeneration of compositional objects and coherent backgrounds, while allowing\nrefinement of inaccurate priors. ComposeAnything outperforms state-of-the-art\nmethods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D\nspatial arrangements, high object counts, and surreal compositions. Human\nevaluations further demonstrate that our model generates high-quality images\nwith compositions that faithfully reflect the text.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638878e0c9a44f05de452e91/V8gVhCAESrievXk5RWL_7.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24086.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638878e0c9a44f05de452e91",
      "avatarUrl": "/avatars/5748195a0ac761e2776548aabc3a53e3.svg",
      "fullname": "Matthieu Futeral",
      "name": "matthieufp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01952",
      "authors": [
        {
          "_id": "683ebebfb5052f5f8741c7f5",
          "user": {
            "_id": "6527b37c0ae663e384eb1b85",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
            "isPro": true,
            "fullname": "Atsuyuki Miyai",
            "user": "AtsuMiyai",
            "type": "user"
          },
          "name": "Atsuyuki Miyai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T09:39:31.731Z",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7f6",
          "name": "Zaiying Zhao",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7f7",
          "name": "Kazuki Egashira",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7f8",
          "name": "Atsuki Sato",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7f9",
          "name": "Tatsumi Sunada",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fa",
          "name": "Shota Onohara",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fb",
          "name": "Hiromasa Yamanishi",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fc",
          "name": "Mashiro Toyooka",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fd",
          "name": "Kunato Nishina",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7fe",
          "name": "Ryoma Maeda",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c7ff",
          "name": "Kiyoharu Aizawa",
          "hidden": false
        },
        {
          "_id": "683ebebfb5052f5f8741c800",
          "name": "Toshihiko Yamasaki",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:59:45.000Z",
      "submittedOnDailyAt": "2025-06-03T07:52:49.121Z",
      "title": "WebChoreArena: 実用的難しいWebタスクでのWebブラウザーアガントの評価",
      "submittedOnDailyBy": {
        "_id": "6527b37c0ae663e384eb1b85",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
        "isPro": true,
        "fullname": "Atsuyuki Miyai",
        "user": "AtsuMiyai",
        "type": "user"
      },
      "summary": "ラージェットラングジャングルム（LLM）をポーチしたウェブブラウザーアガントは、人間のようにウェブブラウザーを操作し、様々な日常的な仕事を自動化するための非常に透明な道を提供します。ウェブアガントが増えてきて、一般的なブラウザータスクでの優秀な動作を示すことになるにつれ、重要な質問が出ます：それらは、冗長で複雑な仕事や人間が自分で回避する家事を厳密に扱うことができるかどうかですか？ 本論文では、WebChoreArenaという新しい完全に再現可能なベンチマークを紹介します。これは、532つのよく選ばれたタスクを含み、一般的なブラウザータスクを超えて、より労働費用の高いや冗長な仕事に向けて視野を拡張します。WebChoreArenaは、3つのキーの挑戦をシステマチックに統合しています：（i）大規模なメモリタスク、観察からの大量の情報の正確な検索が必要，（ii）計算タスク、数学的な理由の正確な計算が必要，（iii）長期モデリングタスク、複数のウェブページ間での長期モデリングが必要です。WebChoreArenaは、完全に再現可能でウェブアガントに広く採用されている4つのウェブアガントシミュレーション環境の上に構築されており、厳格な再現性を確保し、既存のWebArenaベンチマークとの公平な直接な比較を可能にします。これは、アガントの進歩についての重要なエイリアスを提供します。実験結果によると、LLMの進化を表すGPT-4o、Claude 3.7 Sonnet、およびGemini 2.5 Proによって、WebChoreArenaでの性能に大きな向上が見られます。これらの発見は、WebChoreArenaが最先端のLLMの進歩を明確に測定するに最適であることを示します。しかし、もしもGemini 2.5 Proの場合、WebArenaに比べてもっとも大きな向上の余地が残っていることも示し、WebChoreArenaによる増加された挑戦を明らかにします。",
      "upvotes": 2,
      "discussionId": "683ebec0b5052f5f8741c847",
      "ai_summary": "WebChoreArena, a new benchmark comprising 532 tasks, extends the scope of WebArena to more complex and tedious web browsing tasks, measuring advancements in LLM capabilities.",
      "ai_keywords": [
        "LLM",
        "Web browsing agent",
        "WebChoreArena",
        "benchmark",
        "general browsing",
        "Massive Memory tasks",
        "Calculation tasks",
        "Long-Term Memory tasks",
        "WebArena simulation environments",
        "GPT-4o",
        "Claude 3.7 Sonnet",
        "Gemini 2.5 Pro"
      ]
    },
    "publishedAt": "2025-06-02T13:59:45.000Z",
    "title": "WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web\n  Tasks",
    "summary": "Powered by a large language model (LLM), a web browsing agent operates web\nbrowsers in a human-like manner and offers a highly transparent path toward\nautomating a wide range of everyday tasks. As web agents become increasingly\ncapable and demonstrate proficiency in general browsing tasks, a critical\nquestion emerges: Can they go beyond general browsing to robustly handle tasks\nthat are tedious and complex, or chores that humans often avoid doing\nthemselves? In this paper, we introduce WebChoreArena, a new fully reproducible\nbenchmark comprising 532 carefully curated tasks designed to extend the scope\nof WebArena beyond general browsing to more labor-intensive and tedious tasks.\nWebChoreArena systematically integrates three key challenges: (i) Massive\nMemory tasks requiring accurate retrieval of large amounts of information in\nthe observations, (ii) Calculation tasks demanding precise mathematical\nreasoning, and (iii) Long-Term Memory tasks necessitating long-term memory\nacross multiple webpages. Built on top of the fully reproducible and widely\nadopted four WebArena simulation environments, WebChoreArena ensures strict\nreproducibility and enables fair, direct comparisons with the established\nWebArena benchmark, offering key insights into agent progress. Our experimental\nresults demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7\nSonnet, and Gemini 2.5 Pro, significant improvements in performance are\nobserved on WebChoreArena. These findings suggest that WebChoreArena is\nwell-suited to measure the advancement of state-of-the-art LLMs with greater\nclarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro,\nthere remains substantial room for improvement compared to WebArena,\nhighlighting the increased challenges posed by WebChoreArena.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6527b37c0ae663e384eb1b85",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
      "fullname": "Atsuyuki Miyai",
      "name": "AtsuMiyai",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01484",
      "authors": [
        {
          "_id": "683eb167b3f3b41729e1d2e8",
          "name": "Shuzhou Yuan",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2e9",
          "name": "Ercong Nie",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2ea",
          "name": "Lukas Kouba",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2eb",
          "name": "Ashish Yashwanth Kangen",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2ec",
          "name": "Helmut Schmid",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2ed",
          "name": "Hinrich Schutze",
          "hidden": false
        },
        {
          "_id": "683eb167b3f3b41729e1d2ee",
          "name": "Michael Farber",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/662ce44c8b8705f30371fba8/7b4Mhx2INBhnjYZc55g7h.png"
      ],
      "publishedAt": "2025-06-02T09:45:05.000Z",
      "submittedOnDailyAt": "2025-06-03T08:19:44.458Z",
      "title": "LLM in the Loop: パレーデハイトデータセットの作成とハートスピーチの対策\n\nDetoxification: 毒素除去モデルの作成とハートスピーチの対策",
      "submittedOnDailyBy": {
        "_id": "662ce44c8b8705f30371fba8",
        "avatarUrl": "/avatars/b96a25a8c124e7caa9de06b7188bdc15.svg",
        "isPro": false,
        "fullname": "Shuzhou Yuan",
        "user": "shuzyuan",
        "type": "user"
      },
      "summary": "デトキシファイシング、有害な言葉を無害なテキストに改訂する任務は、オンラインで毒性内容の増加に伴い重要になってきました。しかし、デトキシファイシングに適した高品質の平行データセット、特に恨み言葉に対しては、人間の注釈の費用と敏感性により稀少です。本論文では、GPT-4o-miniを機械的にデトキシファイシングに利用する新しいLLM-in-the-loopパイプラインを提案します。最初に、人間の注釈をLLMに置き換えてParaDetoxパイプラインを再現し、LLMが人間の注釈と同等の性能を示すことを示します。これに基づいて、PARADEHATEという大規模な平行データセットを構築し、特に恨み言葉のデトキシファイシングに対して用意します。PARADEHATEは8K以上の恨み/非恨みのテキストペアを含み、ベンチマークとして公開し、様々な基準方法を評価します。実験結果から、PARADEHATEで微調節されたモデル（例えばBART）はスタイルの正確性、内容の保存、流れの良さによってより良い性能を示すことを示し、LLMで生成されたデトキシファイシングテキストの可換性と効果性を示します。",
      "upvotes": 2,
      "discussionId": "683eb169b3f3b41729e1d370",
      "ai_summary": "A novel pipeline using GPT-4o-mini generates a large-scale dataset for hate speech detoxification, improving baseline model performance in style accuracy, content preservation, and fluency.",
      "ai_keywords": [
        "LLM-in-the-loop",
        "GPT-4o-mini",
        "ParaDetox",
        "PARADEHATE",
        "BART",
        "style accuracy",
        "content preservation",
        "fluency"
      ]
    },
    "publishedAt": "2025-06-02T05:45:05.000Z",
    "title": "LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech\n  Detoxification",
    "summary": "Detoxification, the task of rewriting harmful language into non-toxic text,\nhas become increasingly important amid the growing prevalence of toxic content\nonline. However, high-quality parallel datasets for detoxification, especially\nfor hate speech, remain scarce due to the cost and sensitivity of human\nannotation. In this paper, we propose a novel LLM-in-the-loop pipeline\nleveraging GPT-4o-mini for automated detoxification. We first replicate the\nParaDetox pipeline by replacing human annotators with an LLM and show that the\nLLM performs comparably to human annotation. Building on this, we construct\nPARADEHATE, a large-scale parallel dataset specifically for hatespeech\ndetoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate\ntext pairs and evaluate a wide range of baseline methods. Experimental results\nshow that models such as BART, fine-tuned on PARADEHATE, achieve better\nperformance in style accuracy, content preservation, and fluency, demonstrating\nthe effectiveness of LLM-generated detoxification text as a scalable\nalternative to human annotation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/662ce44c8b8705f30371fba8/7b4Mhx2INBhnjYZc55g7h.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01484.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "662ce44c8b8705f30371fba8",
      "avatarUrl": "/avatars/b96a25a8c124e7caa9de06b7188bdc15.svg",
      "fullname": "Shuzhou Yuan",
      "name": "shuzyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00512",
      "authors": [
        {
          "_id": "683eb1f54c5b9f381d5b42ad",
          "name": "Yang Zheng",
          "hidden": false
        },
        {
          "_id": "683eb1f54c5b9f381d5b42ae",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "683eb1f54c5b9f381d5b42af",
          "user": {
            "_id": "6629d7c9fa14eaccf07d8633",
            "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
            "isPro": false,
            "fullname": "Nan Chen",
            "user": "CNcreator0331",
            "type": "user"
          },
          "name": "Nan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:36:45.797Z",
          "hidden": false
        },
        {
          "_id": "683eb1f54c5b9f381d5b42b0",
          "name": "Zhendong Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T11:11:55.000Z",
      "submittedOnDailyAt": "2025-06-03T07:11:36.988Z",
      "title": "Pro3D-Editor : 進歩的ビューの視点からの一致したおおごとしと正確な3D編集",
      "submittedOnDailyBy": {
        "_id": "6629d7c9fa14eaccf07d8633",
        "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
        "isPro": false,
        "fullname": "Nan Chen",
        "user": "CNcreator0331",
        "type": "user"
      },
      "summary": "テキストガイドド3D編集は、語義的に関連する局所的な3D領域を極めて正確に編集することを目的としています。これは3Dゲームだけでなく映画制作など多様な実用的な応用にも大きな潜力があります。現在の方法は通常、視点無関係パラダイムを軌跡にしています：2D視点を無関係に編集し、それを3D空間に戻します。しかし、これらは視点間の異なる相互依存関係を遺漏し、多視点編集の不統一性を招きます。本研究では、理想的な統一的な3D編集は、進歩的視点パラダイムを通じて実現できることを主張しています。このパラダイムでは、編集された視点から他の編集された視点への編集語義を伝播させます。特に、プロ3Dエディターという新しいフレームワークを提案しています。これは、主視点サンプライダー、キー視点レンダリング、全視点リファイナーを含みます。主視点サンプライダーは、最も編集された視点を主視点として動的にサンプリングし、編集します。キー視点レンダリングは、Mixture-of-View-Experts Low-Rank Adaption (MoVE-LoRA)を通じて主視点から他のキー視点への編集語義を正確に伝播させます。全視点リファイナーは、編集された多視点に基づいて3Dオブジェクトを編集し、リファイナリングします。拡張的な実験は、我々の方法が編集精度と空間的な一貫性において現在の方法を上回ることを示しています。",
      "upvotes": 2,
      "discussionId": "683eb1f64c5b9f381d5b42ed",
      "projectPage": "https://shuoyueli4519.github.io/Pro3D-Editor",
      "ai_summary": "A progressive-views paradigm with Pro3D-Editor achieves consistent 3D editing by propagating semantics from key views to less edited ones.",
      "ai_keywords": [
        "progressive-views paradigm",
        "Primary-view Sampler",
        "Key-view Render",
        "Full-view Refiner",
        "Mixture-of-View-Experts Low-Rank Adaptation",
        "MoVE-LoRA"
      ]
    },
    "publishedAt": "2025-05-31T07:11:55.000Z",
    "title": "Pro3D-Editor : A Progressive-Views Perspective for Consistent and\n  Precise 3D Editing",
    "summary": "Text-guided 3D editing aims to precisely edit semantically relevant local 3D\nregions, which has significant potential for various practical applications\nranging from 3D games to film production. Existing methods typically follow a\nview-indiscriminate paradigm: editing 2D views indiscriminately and projecting\nthem back into 3D space. However, they overlook the different cross-view\ninterdependencies, resulting in inconsistent multi-view editing. In this study,\nwe argue that ideal consistent 3D editing can be achieved through a\nprogressive-views paradigm, which propagates editing semantics from\nthe editing-salient view to other editing-sparse views. Specifically, we\npropose Pro3D-Editor, a novel framework, which mainly includes\nPrimary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view\nSampler dynamically samples and edits the most editing-salient view as the\nprimary view. Key-view Render accurately propagates editing semantics from the\nprimary view to other key views through its Mixture-of-View-Experts Low-Rank\nAdaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based\non the edited multi-views. Extensive experiments demonstrate that our method\noutperforms existing methods in editing accuracy and spatial consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00512.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6629d7c9fa14eaccf07d8633",
      "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
      "fullname": "Nan Chen",
      "name": "CNcreator0331",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00385",
      "authors": [
        {
          "_id": "683e707763e27c6256f58a51",
          "name": "Yakun Song",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a52",
          "name": "Jiawei Chen",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a53",
          "user": {
            "_id": "63774ca43a63a2983ffc12f9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/j_VcANnAXvvrIuu6QYxn6.png",
            "isPro": false,
            "fullname": "xiaobin zhuang",
            "user": "xiaobinzhuang",
            "type": "user"
          },
          "name": "Xiaobin Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:45:12.616Z",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a54",
          "name": "Chenpeng Du",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a55",
          "name": "Ziyang Ma",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a56",
          "name": "Jian Wu",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a57",
          "name": "Jian Cong",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a58",
          "name": "Dongya Jia",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a59",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a5a",
          "name": "Yuping Wang",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a5b",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "683e707763e27c6256f58a5c",
          "name": "Xie Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T04:31:02.000Z",
      "submittedOnDailyAt": "2025-06-03T04:28:21.280Z",
      "title": "MagiCodec: 簡単なマスクガウスジェノード注入コーデックで高品質の再構築と生成を実現",
      "submittedOnDailyBy": {
        "_id": "63774ca43a63a2983ffc12f9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/j_VcANnAXvvrIuu6QYxn6.png",
        "isPro": false,
        "fullname": "xiaobin zhuang",
        "user": "xiaobinzhuang",
        "type": "user"
      },
      "summary": "ニューラル音声コーデックは、生ノートアウディオワーブレイフを離散トークン表現に効率的に変換することで、現代の音声生成モデルの基盤を提供しています。しかし、現存するコーデックは主に再構築質量を最適化しており、コーデックトークンの下流モデル可能性にとってはよりもより重要なものではありません。このボトルネックを克服するために、私たちはMagiCodec、新しい単層、ストリーミングTransformerベースの音声コーデックを紹介します。MagiCodecは、Gaussianノイズ注入と潜在正規化を含む多段階訓練プイルオプによって設計されており、生成されるコードの意味表現性の向上を明記したまま高い再構築フィデティーを維持することを明確に目指しています。周波数領域でのノイズ注入の効果を分析的に求め、高周波成分の減衰と強固なトークニゼーションの促進の効果を示しました。拡張的な実験評価により、MagiCodecは再構築質量と下流タスクの両方で最先端のコーデックを超えています。特に、MagiCodecが生成するトークンは、自然言語におけるようなジープ分布を示し、言語モデルベースの生成アーキテクチャとの相性を向上させます。コードと事前学習モデルは、https://github.com/Ereboas/MagiCodecにアクセスできます。",
      "upvotes": 2,
      "discussionId": "683e707963e27c6256f58a98",
      "ai_summary": "MagiCodec, a Transformer-based audio codec, enhances semantic tokenization while maintaining high reconstruction quality, improving compatibility with generative models.",
      "ai_keywords": [
        "Transformer",
        "Gaussian noise injection",
        "latent regularization",
        "frequency domain",
        "Zipf-like distributions",
        "generative models"
      ]
    },
    "publishedAt": "2025-05-31T00:31:02.000Z",
    "title": "MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity\n  Reconstruction and Generation",
    "summary": "Neural audio codecs have made significant strides in efficiently mapping raw\naudio waveforms into discrete token representations, which are foundational for\ncontemporary audio generative models. However, most existing codecs are\noptimized primarily for reconstruction quality, often at the expense of the\ndownstream modelability of the encoded tokens. Motivated by the need to\novercome this bottleneck, we introduce MagiCodec, a novel\nsingle-layer, streaming Transformer-based audio codec. MagiCodec is designed\nwith a multistage training pipeline that incorporates Gaussian noise injection\nand latent regularization, explicitly targeting the enhancement of semantic\nexpressiveness in the generated codes while preserving high reconstruction\nfidelity. We analytically derive the effect of noise injection in the frequency\ndomain, demonstrating its efficacy in attenuating high-frequency components and\nfostering robust tokenization. Extensive experimental evaluations show that\nMagiCodec surpasses state-of-the-art codecs in both reconstruction quality and\ndownstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like\ndistributions, as observed in natural languages, thereby improving\ncompatibility with language-model-based generative architectures. The code and\npre-trained models are available at https://github.com/Ereboas/MagiCodec.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00385.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63774ca43a63a2983ffc12f9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/j_VcANnAXvvrIuu6QYxn6.png",
      "fullname": "xiaobin zhuang",
      "name": "xiaobinzhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21724",
      "authors": [
        {
          "_id": "683b44583f2842f6afcc5e6f",
          "name": "Cheng Luo",
          "hidden": false
        },
        {
          "_id": "683b44583f2842f6afcc5e70",
          "name": "Jianghui Wang",
          "hidden": false
        },
        {
          "_id": "683b44583f2842f6afcc5e71",
          "name": "Bing Li",
          "hidden": false
        },
        {
          "_id": "683b44583f2842f6afcc5e72",
          "name": "Siyang Song",
          "hidden": false
        },
        {
          "_id": "683b44583f2842f6afcc5e73",
          "name": "Bernard Ghanem",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T20:12:46.000Z",
      "submittedOnDailyAt": "2025-06-03T08:26:23.533Z",
      "title": "OmniResponse: オンラインモノモダル対話システムのダイアリックインタラクションでの応答生成",
      "submittedOnDailyBy": {
        "_id": "666ddb45c0f3d5afc27e85ba",
        "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
        "isPro": false,
        "fullname": "Bing Li",
        "user": "bing-li-ai",
        "type": "user"
      },
      "summary": "本論文では、オンライン多モーダル会話的な反応生成（Online Multimodal Conversational Response Generation、OMCRG）という新しいタスクを介紹します。このタスクは、スピーカーの多モード入力に基づいて、同期された語的的と非語的的リスナーの反応をオンラインで生成することを目的としています。OMCRGは自然なダイアインターセクションを反映し、生成された音声とリスナーの顔の反応の同期を達成するために新たな挑戦を帯びています。これらの挑戦に対処するために、テキストを中間モードとして紹介し、音声と顔の反応を結ぶために新たに創造します。そこで、OmniResponseというMultimodal Large Language Model（MLLM）を提案します。OmniResponseは、2つの新しい機能、Chrono-TextとTempoVoiceを持つ学習済みLLMを利用し、高品質の多モードリスナーの反応を自動的に生成します。Chrono-Textは生成されたテキストトークンを時間的に固定し、TempoVoiceは顔の反応と同期された音声を生成します。OMCRGの進展を支援するために、ResponseNetという新しいデータセットを紹介します。このデータセットは696件の高品質なダイアインターセクションを含み、同期されたスプリットシーンビデオ、多チャンネル音声、テキスト、顔の行動のアノテーションを特徴としています。ResponseNetにおける詳細な評価は、OmniResponseが語意的な音声内容、音声ビジュアル同期、生成品質において基準モデルより显著に優れていることを示しています。",
      "upvotes": 2,
      "discussionId": "683b445c3f2842f6afcc5f49",
      "ai_summary": "OmniResponse, a Multimodal Large Language Model, generates high-quality synchronized verbal and non-verbal listener responses using text as an intermediate modality.",
      "ai_keywords": [
        "Online Multimodal Conversational Response Generation",
        "OmniResponse",
        "Multimodal Large Language Model",
        "Chrono-Text",
        "TempoVoice",
        "ResponseNet",
        "audio-visual synchronization"
      ]
    },
    "publishedAt": "2025-05-27T16:12:46.000Z",
    "title": "OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions",
    "summary": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task that aims to online generate synchronized\nverbal and non-verbal listener feedback, conditioned on the speaker's\nmultimodal input. OMCRG reflects natural dyadic interactions and poses new\nchallenges in achieving synchronization between the generated audio and facial\nresponses of the listener. To address these challenges, we innovatively\nintroduce text as an intermediate modality to bridge the audio and facial\nresponses. We hence propose OmniResponse, a Multimodal Large Language Model\n(MLLM) that autoregressively generates high-quality multi-modal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two novel\ncomponents: Chrono-Text, which temporally anchors generated text tokens, and\nTempoVoice, a controllable online TTS module that produces speech synchronized\nwith facial reactions. To support further OMCRG research, we present\nResponseNet, a new dataset comprising 696 high-quality dyadic interactions\nfeaturing synchronized split-screen videos, multichannel audio, transcripts,\nand facial behavior annotations. Comprehensive evaluations conducted on\nResponseNet demonstrate that OmniResponse significantly outperforms baseline\nmodels in terms of semantic speech content, audio-visual synchronization, and\ngeneration quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21724.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "666ddb45c0f3d5afc27e85ba",
      "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
      "fullname": "Bing Li",
      "name": "bing-li-ai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19621",
      "authors": [
        {
          "_id": "683ea7297e58553a7f73c210",
          "name": "George Kour",
          "hidden": false
        },
        {
          "_id": "683ea7297e58553a7f73c211",
          "name": "Itay Nakash",
          "hidden": false
        },
        {
          "_id": "683ea7297e58553a7f73c212",
          "name": "Ateret Anaby-Tavor",
          "hidden": false
        },
        {
          "_id": "683ea7297e58553a7f73c213",
          "name": "Michal Shmueli-Scheuer",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/dQxbCsfx-MqMegYL0zCWu.png"
      ],
      "publishedAt": "2025-05-26T07:41:21.000Z",
      "submittedOnDailyAt": "2025-06-03T06:13:25.326Z",
      "title": "再考えてみて！ テストタイムコンピューティングの影響について、大規模言語モデルの好み、意見、信念について",
      "submittedOnDailyBy": {
        "_id": "671f8106d677d3a764a6f9a5",
        "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
        "isPro": false,
        "fullname": "itay nakash",
        "user": "itaynakash",
        "type": "user"
      },
      "summary": "LLMsは人間の生活に深く組織され、決策に影響を与えるようになっています。これらのモデルが主観的な好み、意見、信念を表現するかどうか、そしてどの程度であるかを評価することが重要です。これらの傾向は、モデルの中に含まれるバイアスから生まれ、モデルの行動を影響し、ユーザーに提供されるアドバイスやリコメンドを影響し、特定の視点を強化する可能性があります。本論文では、社会的、文化的、倫理的、個人的な領域での主観的な傾向を評価するための基準として、好み、意見、信念調査（POBs）を提案します。この基準を用いて、先進的な開放的およびクローズドソースLLMsを評価し、信頼性、中立性、一貫性などの望ましい特徴を測定しました。また、テスト時の計算量を増加させることによる影響を調査し、理由論と自己反省機能によるメリットを評価しました。これらの機能は他のタスクでは効果的であることを示していますが、本論文では、これらの機能が本論文の領域では限られた効果を示すことを示しています。また、新しいモデルバージョンが特定の視点に偏って一貫性を下げ、バイアスを増加させることを示し、盲点と懸念の傾向を明らかにしています。POBS: https://ibm.github.io/POBS",
      "upvotes": 2,
      "discussionId": "683ea72b7e58553a7f73c277",
      "ai_summary": "The Preference, Opinion, and Belief survey assesses the subjective tendencies and biases of Large Language Models across various domains and highlights a trend of increased bias in newer model versions.",
      "ai_keywords": [
        "Large Language Models",
        "Preference",
        "Opinion",
        "and Belief survey",
        "reliability",
        "neutrality",
        "consistency",
        "reasoning mechanisms",
        "self-reflection mechanisms"
      ]
    },
    "publishedAt": "2025-05-26T03:41:21.000Z",
    "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions,\n  and Beliefs of Large Language Models",
    "summary": "As Large Language Models (LLMs) become deeply integrated into human life and\nincreasingly influence decision-making, it's crucial to evaluate whether and to\nwhat extent they exhibit subjective preferences, opinions, and beliefs. These\ntendencies may stem from biases within the models, which may shape their\nbehavior, influence the advice and recommendations they offer to users, and\npotentially reinforce certain viewpoints. This paper presents the Preference,\nOpinion, and Belief survey (POBs), a benchmark developed to assess LLMs'\nsubjective inclinations across societal, cultural, ethical, and personal\ndomains. We applied our benchmark to evaluate leading open- and closed-source\nLLMs, measuring desired properties such as reliability, neutrality, and\nconsistency. In addition, we investigated the effect of increasing the\ntest-time compute, through reasoning and self-reflection mechanisms, on those\nmetrics. While effective in other tasks, our results show that these mechanisms\noffer only limited gains in our domain. Furthermore, we reveal that newer model\nversions are becoming less consistent and more biased toward specific\nviewpoints, highlighting a blind spot and a concerning trend. POBS:\nhttps://ibm.github.io/POBS",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/dQxbCsfx-MqMegYL0zCWu.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671f8106d677d3a764a6f9a5",
      "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
      "fullname": "itay nakash",
      "name": "itaynakash",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00772",
      "authors": [
        {
          "_id": "683ec2d53c81cc903bbb418c",
          "name": "Zihang Liu",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb418d",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb418e",
          "name": "Oleg Balabanov",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb418f",
          "name": "Chaoqun Yang",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb4190",
          "name": "Tianjin Huang",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb4191",
          "name": "Lu Yin",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb4192",
          "name": "Yaoqing Yang",
          "hidden": false
        },
        {
          "_id": "683ec2d53c81cc903bbb4193",
          "name": "Shiwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T01:31:50.000Z",
      "submittedOnDailyAt": "2025-06-03T08:10:49.247Z",
      "title": "LIFT the Veil for the Truth: プロフェッショナル重みがランク減少後に浮き上がる、理由フォーカスのサブジェクト調整",
      "submittedOnDailyBy": {
        "_id": "65b04d2291e63920a7898c9e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b04d2291e63920a7898c9e/iUHs235G4bqK-KnH_94ti.jpeg",
        "isPro": false,
        "fullname": "Liu",
        "user": "Shiweiliuiiiiiii",
        "type": "user"
      },
      "summary": "最近の研究により、LLMに小数の高品質なデータセットを用いて規範的な微調校を行うことで強力的な理由論能力を獲得することができることが明らかになっています。しかし、完全な微調校（Full FT）は強力であるが、計算費用が高く、オーバーフィッティングとカタストロフィックな忘却に脆弱で、特にデータが限られた場合はその問題が強くなります。稀疏な微調校は、モデルパラメータの小さな部分だけを更新することで前に顕著な成功を収めたが、LLM時代には、理由論に関する重要なパラメータを特定することの難しさにより進展が遅れていました。本研究では、低頻度近似後の最大の絶対値を持つ重みが微調校に重要な重みであることを主張し、これらの重みをPrincipal Weightsと呼びます。意外に、絶対値に基づく稀疏な微調校はLLMの微調校の基準としては劣りますが、順位削減後には非常に効果的になります。これらのインサイトにより、Low-rank Informed Sparse Fine-Tuning（LIFT）の方法を提案します。LIFTは、全学習期間において上位5%のPrincipal Weightsだけを更新し、Full FTよりも理由論タスクに対してより良い性能を維持し、プロパーティフィールドな微調校方法と同じレベルのメモリ効率を維持します。また、算術的な理由論などの特定の領域で強い性能を示すことを除けば、LIFTはFull FTとLoRAに比べて20%以上のソース領域の知識を維持します。本研究のコードは以下のURLで提供されています：https://github.com/zihanghliu/LIFT。",
      "upvotes": 1,
      "discussionId": "683ec2d53c81cc903bbb41c4",
      "ai_summary": "Leveraging low-rank approximation to identify critical weights for sparse fine-tuning of large language models enhances performance and efficiency compared to full fine-tuning.",
      "ai_keywords": [
        "LLMs",
        "supervised fine-tuning",
        "full fine-tuning",
        "sparse fine-tuning",
        "low-rank approximation",
        "Principal Weights",
        "Low-rank Informed Sparse Fine-Tuning",
        "LIFT",
        "memory efficiency",
        "parameter-efficient fine-tuning",
        "reasoning tasks",
        "arithmetic reasoning",
        "source-domain knowledge",
        "LoRA"
      ]
    },
    "publishedAt": "2025-05-31T21:31:50.000Z",
    "title": "LIFT the Veil for the Truth: Principal Weights Emerge after Rank\n  Reduction for Reasoning-Focused Supervised Fine-Tuning",
    "summary": "Recent studies have shown that supervised fine-tuning of LLMs on a small\nnumber of high-quality datasets can yield strong reasoning capabilities.\nHowever, full fine-tuning (Full FT), while powerful, is computationally\nexpensive and susceptible to overfitting and catastrophic forgetting,\nparticularly when data is limited. Sparse fine-tuning, which previously\nachieved notable success by updating only a small subset of model parameters,\noffers a promising trade-off between efficiency and effectiveness. Yet, it has\nlagged behind in the LLM era due to the difficulty of identifying parameters\ntruly critical for reasoning. In this work, we state that weights with the\nlargest magnitude after low-rank approximation are critical weights for\nfine-tuning, which we call Principal Weights. Surprisingly, while\nmagnitude-based sparse fine-tuning performs poorly as a baseline on LLM\nfine-tuning, it becomes highly effective after rank reduction. These insights\nmotivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only\nupdates the top 5% Principal Weights throughout training and consistently\nachieves better performance on reasoning tasks than Full FT, while maintaining\nmemory efficiency on par with popular parameter-efficient fine-tuning methods.\nIn addition to strong performance on target domains such as arithmetic\nreasoning, LIFT also retains up to 20% more source-domain knowledge, compared\nto Full FT and LoRA. Our code is available at:\nhttps://github.com/zihanghliu/LIFT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00772.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b04d2291e63920a7898c9e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b04d2291e63920a7898c9e/iUHs235G4bqK-KnH_94ti.jpeg",
      "fullname": "Liu",
      "name": "Shiweiliuiiiiiii",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.00469",
      "authors": [
        {
          "_id": "683e9e0a1c5320ac91b85a19",
          "user": {
            "_id": "617a92e16f37340367d5d791",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png",
            "isPro": false,
            "fullname": "Shaoxiong",
            "user": "jisx",
            "type": "user"
          },
          "name": "Shaoxiong Ji",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-03T08:37:04.382Z",
          "hidden": true
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1a",
          "name": "Zihao Li",
          "hidden": false
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1b",
          "name": "Jaakko Paavola",
          "hidden": false
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1c",
          "name": "Indraneil Paul",
          "hidden": false
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1d",
          "name": "Hengyu Luo",
          "hidden": false
        },
        {
          "_id": "683e9e0a1c5320ac91b85a1e",
          "name": "Jörg Tiedemann",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T08:37:17.000Z",
      "submittedOnDailyAt": "2025-06-03T05:43:58.004Z",
      "title": "大規模多言語における大規模言語モデルの適応におけるバイリンガル翻訳データの使用",
      "submittedOnDailyBy": {
        "_id": "617a92e16f37340367d5d791",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png",
        "isPro": false,
        "fullname": "Shaoxiong",
        "user": "jisx",
        "type": "user"
      },
      "summary": "この論文は、マジックスティール多言語の継続的な事前学習の実践での重要な設計決定について調査します。特に、平行データの含め方について調査します。具体的には、Llama3モデルの500言語の大規模な多言語調整における単語訳データの影響を調査します。そこで、MaLA単語訳データコーパスを構築し、2,500言語ペア以上のデータを含むものとします。その後、EMMA-500 Llama 3システムとして4つの大規模な多言語モデルを開発します。これらのモデルは、Llama3ベースモデルからの様々なデータミックスで極めて広範囲に継続的に事前学習されています。また、単語訳データの有無による継続的な事前学習の影響を調査します。7つのタスクと12つのベンチマークでの詳細な評価は、単語訳データが言語の移行と性能を高めることを示し、特に資源の少ない言語においても特に効果があることを示します。MaLAコーパス、EMMA-500 Llama 3システムのアーテキュレート、コード、モデルの生成を公開します。",
      "upvotes": 1,
      "discussionId": "683e9e0a1c5320ac91b85a50",
      "projectPage": "https://mala-lm.github.io/emma-500-gen2.html",
      "githubRepo": "https://github.com/MaLA-LM/emma-500/",
      "ai_summary": "Bilingual translation data enhances language transfer and performance in massively multilingual language adaptation of the Llama3 family of models.",
      "ai_keywords": [
        "massively multilingual continual pre-training",
        "bilingual translation data",
        "Llama3",
        "MaLA bilingual translation corpus",
        "EMMA-500 Llama 3 suite",
        "continual pre-training",
        "language transfer",
        "low-resource languages"
      ]
    },
    "publishedAt": "2025-05-31T04:37:17.000Z",
    "title": "Massively Multilingual Adaptation of Large Language Models Using\n  Bilingual Translation Data",
    "summary": "This paper investigates a critical design decision in the practice of\nmassively multilingual continual pre-training -- the inclusion of parallel\ndata. Specifically, we study the impact of bilingual translation data for\nmassively multilingual language adaptation of the Llama3 family of models to\n500 languages. To this end, we construct the MaLA bilingual translation corpus,\ncontaining data from more than 2,500 language pairs. Subsequently, we develop\nthe EMMA-500 Llama 3 suite of four massively multilingual models -- continually\npre-trained from the Llama 3 family of base models extensively on diverse data\nmixes up to 671B tokens -- and explore the effect of continual pre-training\nwith or without bilingual translation data. Comprehensive evaluation across 7\ntasks and 12 benchmarks demonstrates that bilingual data tends to enhance\nlanguage transfer and performance, particularly for low-resource languages. We\nopen-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model\ngenerations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00469.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "617a92e16f37340367d5d791",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png",
      "fullname": "Shaoxiong",
      "name": "jisx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01920",
      "authors": [
        {
          "_id": "683ec5047ec12b4ee9a21215",
          "name": "Serry Sibaee",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21216",
          "name": "Omer Nacar",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21217",
          "name": "Adel Ammar",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21218",
          "name": "Yasser Al-Habashi",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21219",
          "name": "Abdulrahman Al-Batati",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a2121a",
          "name": "Wadii Boulila",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:39:50.000Z",
      "submittedOnDailyAt": "2025-06-03T08:20:38.744Z",
      "title": "ガイドラインから実践へ：アラビア語言モデル評価の新しいパラダイム",
      "submittedOnDailyBy": {
        "_id": "628f7a71dd993507cfcbe587",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
        "isPro": true,
        "fullname": "Omartificial Intelligence Space",
        "user": "Omartificial-Intelligence-Space",
        "type": "user"
      },
      "summary": "この論文は、アラビア語言モデル評価における重要な欠陥を解決するため、詳細な理論的ガイドラインを構築し、新しい評価フレームワークを導入します。まず、現在のアラビア評価データセットを分析し、言語正確性、文化の一致性、そして方法学的な厳密性における重要な問題を識別します。これらのLLMsの制限を解決するために、アラビア語の深さミニデータセット（ADMD）を提出します。ADMDは、10つの主要な領域（42つのサブデータベース、図1参照）に跨ぐ490問の難しい質問からなります。ADMDを使用して、GPT-4、Claude 3.5 Sonnet、Gemini Flash 1.5、CommandR 100B、Qwen-Maxの5つの先進的な言語モデルを評価します。結果から、違う領域でのモデルの性能における顕著な差異が明らかになり、深い文化理解と専門的な知識が必要な領域で特に課題があることがわかります。Claude 3.5 Sonnetは、アラビア語の数学理論、アラビア語、イスラム領域での相対的な強さを示し、全体的な正確性は30%で最も高いです。この研究は、アラビア語言モデル評価の向上における理論的基盤と実用的な洞察を提供し、文化調和と技術能力の両方の重要性を強調します。",
      "upvotes": 0,
      "discussionId": "683ec5137ec12b4ee9a21496",
      "ai_summary": "A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.",
      "ai_keywords": [
        "evaluation framework",
        "Arabic Depth Mini Dataset (ADMD)",
        "GPT-4",
        "Claude 3.5 Sonnet",
        "Gemini Flash 1.5",
        "CommandR 100B",
        "Qwen-Max",
        "cultural understanding",
        "specialized knowledge",
        "cultural competence"
      ]
    },
    "publishedAt": "2025-06-02T13:39:50.000Z",
    "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation",
    "summary": "This paper addresses critical gaps in Arabic language model evaluation by\nestablishing comprehensive theoretical guidelines and introducing a novel\nevaluation framework. We first analyze existing Arabic evaluation datasets,\nidentifying significant issues in linguistic accuracy, cultural alignment, and\nmethodological rigor. To address these limitations in LLMs, we present the\nArabic Depth Mini Dataset (ADMD), a carefully curated collection of 490\nchallenging questions spanning ten major domains (42 sub-domains, see Figure 1.\nUsing ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,\nGemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant\nvariations in model performance across different domains, with particular\nchallenges in areas requiring deep cultural understanding and specialized\nknowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%,\nshowing relative strength in mathematical theory in Arabic, Arabic language,\nand islamic domains. This work provides both theoretical foundations and\npractical insights for improving Arabic language model evaluation, emphasizing\nthe importance of cultural competence alongside technical capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01920.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "628f7a71dd993507cfcbe587",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
      "fullname": "Omartificial Intelligence Space",
      "name": "Omartificial-Intelligence-Space",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 100
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01920",
      "authors": [
        {
          "_id": "683ec5047ec12b4ee9a21215",
          "name": "Serry Sibaee",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21216",
          "name": "Omer Nacar",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21217",
          "name": "Adel Ammar",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21218",
          "name": "Yasser Al-Habashi",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a21219",
          "name": "Abdulrahman Al-Batati",
          "hidden": false
        },
        {
          "_id": "683ec5047ec12b4ee9a2121a",
          "name": "Wadii Boulila",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:39:50.000Z",
      "submittedOnDailyAt": "2025-06-03T08:20:38.744Z",
      "title": "ガイドラインから実践へ：アラビア語言モデル評価の新しいパラダイム",
      "submittedOnDailyBy": {
        "_id": "628f7a71dd993507cfcbe587",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
        "isPro": true,
        "fullname": "Omartificial Intelligence Space",
        "user": "Omartificial-Intelligence-Space",
        "type": "user"
      },
      "summary": "この論文は、アラビア語言モデル評価の重要な欠陥を解決するために、詳細な理論的ガイドラインを設定し、新しい評価フレームワークを導入します。まず、既存のアラビア評価データセットを分析し、言語精度、文化の適合性、そして方法学的な厳密性における重要な問題点を特定します。これらのLLMの制限を解決するために、アラビア語の深さミニデータセット（ADMD）を提出します。ADMDは、10つの主要な領域（42つのサブ領域、図1参照）を横断する490問の挑戦的な問題からなります。ADMDを用いて、GPT-4、Claude 3.5 Sonnet、Gemini Flash 1.5、CommandR 100B、Qwen-Maxの5つの先進モデルを評価します。結果から、違う領域でのモデルの性能が显著な差異を示し、深い文化的理解と専門知識が必要な領域で特に課題があることが明らかになります。Claude 3.5 Sonnetは、アラビア語の数学理論、アラビア語、イスラム領域での強さを示し、全体的な正確性が30%で最も高くなります。この研究は、アラビア語言モデル評価の改善における理論的基盤と実用的なフィードバックを提供し、文化の理解と技術的な能力の両方の重要性を強調します。",
      "upvotes": 0,
      "discussionId": "683ec5137ec12b4ee9a21496",
      "ai_summary": "A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.",
      "ai_keywords": [
        "evaluation framework",
        "Arabic Depth Mini Dataset (ADMD)",
        "GPT-4",
        "Claude 3.5 Sonnet",
        "Gemini Flash 1.5",
        "CommandR 100B",
        "Qwen-Max",
        "cultural understanding",
        "specialized knowledge",
        "cultural competence"
      ]
    },
    "publishedAt": "2025-06-02T13:39:50.000Z",
    "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation",
    "summary": "This paper addresses critical gaps in Arabic language model evaluation by\nestablishing comprehensive theoretical guidelines and introducing a novel\nevaluation framework. We first analyze existing Arabic evaluation datasets,\nidentifying significant issues in linguistic accuracy, cultural alignment, and\nmethodological rigor. To address these limitations in LLMs, we present the\nArabic Depth Mini Dataset (ADMD), a carefully curated collection of 490\nchallenging questions spanning ten major domains (42 sub-domains, see Figure 1.\nUsing ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,\nGemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant\nvariations in model performance across different domains, with particular\nchallenges in areas requiring deep cultural understanding and specialized\nknowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%,\nshowing relative strength in mathematical theory in Arabic, Arabic language,\nand islamic domains. This work provides both theoretical foundations and\npractical insights for improving Arabic language model evaluation, emphasizing\nthe importance of cultural competence alongside technical capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01920.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "628f7a71dd993507cfcbe587",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
      "fullname": "Omartificial Intelligence Space",
      "name": "Omartificial-Intelligence-Space",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 100
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01713",
      "authors": [
        {
          "_id": "683ec81753981b08324ce57b",
          "name": "Zhongwei Wan",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce57c",
          "name": "Zhihao Dou",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce57d",
          "name": "Che Liu",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce57e",
          "name": "Yu Zhang",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce57f",
          "name": "Dongfei Cui",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce580",
          "name": "Qinjian Zhao",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce581",
          "name": "Hui Shen",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce582",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce583",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce584",
          "name": "Yifan Jiang",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce585",
          "name": "Yangfan He",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce586",
          "name": "Mi Zhang",
          "hidden": false
        },
        {
          "_id": "683ec81753981b08324ce587",
          "name": "Shen Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T14:21:44.000Z",
      "submittedOnDailyAt": "2025-06-03T08:34:07.706Z",
      "title": "SRPO: 反省意識に関する強化学習をもとめる多モデルLLMの理由論の向上",
      "submittedOnDailyBy": {
        "_id": "631b9ff5824f2502e3557c7e",
        "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
        "isPro": true,
        "fullname": "liu",
        "user": "che111",
        "type": "user"
      },
      "summary": "多モダル大語言モデル（MLLMs）は、理由タスクで有望な能力を示していますが、明示的な自己反省と自己補正を必要とする複雑な問題に対しては、その対応が困難であり、特にユニモダルのテキストベースのコンタラピーよりもより困難です。現在の反省方法は簡単であり、意味のあるおそりさえも生成できなく、理由能力と知識の限界が初期訓練時に大きく固定されているためです。これらの挑戦を克服するために、私たちは、多モダルLLMの理由を強化するための、グループ相対ポリシー最適化（SRPO）を提案します。SRPOは、2段階の反省意識付きの強化学習（RL）フレームワークであり、理由を強化するために特に設計されています。1段階目に、先進的なMLLMのガイダンスの下で、高品質で反省フォーカスされたデータセットを構築し、初期の回答に基づいた反省を生成して、政策モデルが理由と自己反省を両方学習することを促すことです。2段階目に、GRPOフレームワーク内で新しい報酬機構を導入し、冗長を避けながら、簡潔で認知的に意味のある反省を促すことです。MathVista、MathVision、MathVerse、MMMU-Proなどの複数の多モダル理由ベンチマークでの拡張的な実験、Qwen-2.5-VL-7BとQwen-2.5-VL-32Bを使用して、SRPOは最先端のモデルを大幅に超え、理由の精度と反省の品質において著しい向上を実現しました。",
      "upvotes": 0,
      "discussionId": "683ec81853981b08324ce5f1",
      "projectPage": "https://srpo.pages.dev/"
    },
    "publishedAt": "2025-06-02T10:21:44.000Z",
    "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware\n  Reinforcement Learning",
    "summary": "Multimodal large language models (MLLMs) have shown promising capabilities in\nreasoning tasks, yet still struggle with complex problems requiring explicit\nself-reflection and self-correction, especially compared to their unimodal\ntext-based counterparts. Existing reflection methods are simplistic and\nstruggle to generate meaningful and instructive feedback, as the reasoning\nability and knowledge limits of pre-trained models are largely fixed during\ninitial training. To overcome these challenges, we propose Multimodal\nSelf-Reflection enhanced reasoning with Group Relative Policy Optimization\n(SRPO), a two-stage reflection-aware reinforcement learning (RL) framework\nexplicitly designed to enhance multimodal LLM reasoning. In the first stage, we\nconstruct a high-quality, reflection-focused dataset under the guidance of an\nadvanced MLLM, which generates reflections based on initial responses to help\nthe policy model learn both reasoning and self-reflection. In the second stage,\nwe introduce a novel reward mechanism within the GRPO framework that encourages\nconcise and cognitively meaningful reflection while avoiding redundancy.\nExtensive experiments across multiple multimodal reasoning benchmarks,\nincluding MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B\nand Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms\nstate-of-the-art models, achieving notable improvements in both reasoning\naccuracy and reflection quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01713.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631b9ff5824f2502e3557c7e",
      "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
      "fullname": "liu",
      "name": "che111",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15772",
      "authors": [
        {
          "_id": "683ec87a4246cd3c413046e1",
          "name": "Yifan Cheng",
          "hidden": false
        },
        {
          "_id": "683ec87a4246cd3c413046e2",
          "name": "Ruoyi Zhang",
          "hidden": false
        },
        {
          "_id": "683ec87a4246cd3c413046e3",
          "name": "Jiatong Shi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:23:12.000Z",
      "submittedOnDailyAt": "2025-06-03T08:35:09.353Z",
      "title": "MIKU-PAL: 音声の語学的的な意味を除く多様的な情報を自動的に標準化して検出する方法",
      "submittedOnDailyBy": {
        "_id": "6607d9c2d81d6112498810b9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6607d9c2d81d6112498810b9/mmwx-SFEP-6gjnAdjbcxb.png",
        "isPro": false,
        "fullname": "PoTaTo",
        "user": "PoTaTo721",
        "type": "user"
      },
      "summary": "大規模の感情付きのスピーチデータの収集に強い一致性を持つものが、スピーチ合成にとっては難題です。本論文では、未ラベルのビデオデータから高品質で一致性の高い感情付きスピーチを抽出するための完全自動化マルチモデルパイプライン「MIKU-PAL」を紹介します。顔検出と追跡アルゴリズムを活用し、多モデル大語言モデル（MLLM）を基にした自動的な感情分析システムを開発しました。結果として、MIKU-PALは人間レベルの精度（MELDで68.5%）と上級の一致性（Fleiss kappaスコア0.93）を実現でき、人間のアノテーションよりも廉價で速いことがわかりました。MIKU-PALから得られる高品質で柔軟な一致性のあるアノテーションを利用して、26つの細分化されたスピーチ感情カテゴリーを記録でき、83%の理性評価を受けた人間アノテーターが評価しました。この提案システムに基づいて、また、感情付きのテキストからスピーチおよび可視化ベイクコロニングの新たなベンチマークとして、131.2時間の細分化された感情付きスピーチデータセット「MIKU-EmoBench」をリリースしました。",
      "upvotes": 0,
      "discussionId": "683ec87a4246cd3c41304708"
    },
    "publishedAt": "2025-05-21T13:23:12.000Z",
    "title": "MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech\n  Paralinguistic and Affect Labeling",
    "summary": "Acquiring large-scale emotional speech data with strong consistency remains a\nchallenge for speech synthesis. This paper presents MIKU-PAL, a fully automated\nmultimodal pipeline for extracting high-consistency emotional speech from\nunlabeled video data. Leveraging face detection and tracking algorithms, we\ndeveloped an automatic emotion analysis system using a multimodal large\nlanguage model (MLLM). Our results demonstrate that MIKU-PAL can achieve\nhuman-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss\nkappa score) while being much cheaper and faster than human annotation. With\nthe high-quality, flexible, and consistent annotation from MIKU-PAL, we can\nannotate fine-grained speech emotion categories of up to 26 types, validated by\nhuman annotators with 83% rationality ratings. Based on our proposed system, we\nfurther released a fine-grained emotional speech dataset MIKU-EmoBench(131.2\nhours) as a new benchmark for emotional text-to-speech and visual voice\ncloning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15772.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6607d9c2d81d6112498810b9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6607d9c2d81d6112498810b9/mmwx-SFEP-6gjnAdjbcxb.png",
      "fullname": "PoTaTo",
      "name": "PoTaTo721",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  }
]