[
  {
    "paper": {
      "id": "2506.09513",
      "authors": [
        {
          "_id": "684b8dbd3b733ba33368701b",
          "user": {
            "_id": "6723079ad1306fe9c76a1d29",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
            "isPro": false,
            "fullname": "Yu Sun",
            "user": "YuSun-AI",
            "type": "user"
          },
          "name": "Yu Sun",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:32:30.652Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701c",
          "name": "Xingyu Qian",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701d",
          "name": "Weiwen Xu",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701e",
          "user": {
            "_id": "64b7cd74ff6d81ae297feded",
            "avatarUrl": "/avatars/880fbc96cc093f5e901ce84f32a1d21d.svg",
            "isPro": false,
            "fullname": "ZHANG HAO",
            "user": "26hzhang",
            "type": "user"
          },
          "name": "Hao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:43.056Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba33368701f",
          "name": "Chenghao Xiao",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687020",
          "name": "Long Li",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687021",
          "user": {
            "_id": "642eecbf9b2484d7d8526781",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642eecbf9b2484d7d8526781/4IvGbd66s49Wx5pZyZGHA.png",
            "isPro": false,
            "fullname": "Yu Rong",
            "user": "Swrooy",
            "type": "user"
          },
          "name": "Yu Rong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:40.908Z",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687022",
          "name": "Wenbing Huang",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687023",
          "name": "Qifeng Bai",
          "hidden": false
        },
        {
          "_id": "684b8dbd3b733ba333687024",
          "name": "Tingyang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T08:36:55.000Z",
      "submittedOnDailyAt": "2025-06-13T01:06:46.741Z",
      "title": "ReasonMed: 370K 多エージェント生成データセット 医療論理の進歩に向けて",
      "submittedOnDailyBy": {
        "_id": "6723079ad1306fe9c76a1d29",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
        "isPro": false,
        "fullname": "Yu Sun",
        "user": "YuSun-AI",
        "type": "user"
      },
      "summary": "Reasoningベースの大規模な言語モデル（LLMs）は数学とプログラミングにおいて優れた性能を示しますが、知識密集型の医療問い合わせの解答においては、その能力は調査が不足しています。これに対して、私たちはReasonMedという最大の医療理由データセットを紹介します。ReasonMedは、170万の最初の理由パスから抜粋された高品質の例37万件から構成されています。ReasonMedは、誤りの検出と修正を行うバリデーターがフラグした誤りのリスクのあるステップを特定し、それらを修正して理由パスを強化するためのError Refinerを設計し、多効的な調査と改良プロセスを通じて構築されています。ReasonMedを活用し、医療理由モデルの最適なトレーニングプラクティスを系統的に調査し、詳細なChain-of-Thought（CoT）理由と簡潔な回答の要約を組み合わせた最適な微調プラクティスを発見しました。このプラクティスに基づいて、ReasonMed-7Bをトレーニングし、10Bモデルの新たなベンチマークを設定し、先週の最善のモデルより4.17%よりも優秀で、PubMedQAではLLaMA3.1-70Bよりも4.60%よりも優秀です。",
      "upvotes": 46,
      "discussionId": "684b8dbe3b733ba333687025",
      "githubRepo": "https://github.com/YuSun-Work/ReasonMed",
      "ai_summary": "ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.",
      "ai_keywords": [
        "reasoning-based large language models",
        "LLMs",
        "medical question answering",
        "ReasonMed",
        "multi-agent verification",
        "Error Refiner",
        "Chain-of-Thought",
        "CoT reasoning",
        "ReasonMed-7B",
        "PubMedQA"
      ]
    },
    "publishedAt": "2025-06-11T04:36:55.000Z",
    "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning",
    "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a multi-agent\nverification and refinement process, where we design an Error Refiner\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09513.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6723079ad1306fe9c76a1d29",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
      "fullname": "Yu Sun",
      "name": "YuSun-AI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10954",
      "authors": [
        {
          "_id": "684b7ea83b733ba333686f8a",
          "name": "Lianghong Guo",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8b",
          "name": "Yanlin Wang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8c",
          "name": "Caihua Li",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8d",
          "name": "Pengyu Yang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8e",
          "name": "Jiachi Chen",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f8f",
          "user": {
            "_id": "6355473d525beaee688b7ba1",
            "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
            "isPro": false,
            "fullname": "Wei Tao",
            "user": "itaowe",
            "type": "user"
          },
          "name": "Wei Tao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:58.091Z",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f90",
          "name": "Yingtian Zou",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f91",
          "name": "Duyu Tang",
          "hidden": false
        },
        {
          "_id": "684b7ea83b733ba333686f92",
          "name": "Zibin Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:54:17.000Z",
      "submittedOnDailyAt": "2025-06-13T00:07:20.052Z",
      "title": "SWE-Factory: 問題解決データと評価ベンチマークの自動化ファクトリー",
      "submittedOnDailyBy": {
        "_id": "6355473d525beaee688b7ba1",
        "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
        "isPro": false,
        "fullname": "Wei Tao",
        "user": "itaowe",
        "type": "user"
      },
      "summary": "GitHub 問題解決タスクのための大規模データセットの構築は、大規模言語モデル（LLMs）のソフトウェア開発能力の訓練と評価において重要である。しかし、これらのベンチマークの作成の伝統的な手順は、評価環境の設定、テスト結果の評価、タスクインスタンスの検証などのステップで特に難しく労力が必要なことが知られている。本論文では、これらの挑戦を解決するために、SWE-Factoryという自動化パイプラインを提案します。このパイプラインは3つの核心的な自動化コンポーネントを組み合わせて、これらの問題を解決するために設計されています。まず、SWE-Builderという多エージェントシステムを介して、評価環境の構築を自動化し、4つの特殊化エージェントが協力し、環境メモリポールを利用して効率化を実現します。次に、標準化された、エクスターシスプコードに基づく評価方法を導入し、カスタムパーサーの手動書き込みを省略します。最後に、これらの信頼性のあるエクスターシスプコードシグナルを使用して、fail2passの検証プロセスを自動化します。4つのプログラミング言語の671問題に対する実験結果から、我々のパイプラインは有効なタスクインスタンスの構築に効果的であることが示されました。例えば、GPT-4.1-miniを使用しては、269個の有効なインスタンスを0.045インスタンス当たりで構築し、Gemini-2.5-flashを使用しては、最も低コストで0.024インスタンス当たりで類似の性能を達成しました。また、エクスターシスプコードに基づく評価は、手動検証と比較して100%の精度を達成し、自動化ファイル2パスの検証は精度0.92と再現率1.00を達成しました。我々は、この自動化パイプラインが、サイズと品質の高いGitHub問題解決データセットの収集を加速し、そのデータセットの訓練と評価に役立てることを望むと考えています。我々のコードとデータセットは、https://github.com/DeepSoftwareAnalytics/swe-factory に公開されています。",
      "upvotes": 28,
      "discussionId": "684b7ea83b733ba333686f93",
      "githubRepo": "https://github.com/DeepSoftwareAnalytics/swe-factory",
      "ai_summary": "A pipeline named SWE-Factory automates the creation and validation of GitHub issue resolution datasets for training and evaluating Large Language Models, using SWE-Builder for environment setup, exit-code-based grading, and automated fail2pass validation.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "SWE-Factory",
        "SWE-Builder",
        "multi-agent system",
        "environment memory pool",
        "exit-code-based grading",
        "automated fail2pass validation",
        "GPT-4.1-mini",
        "Gemini-2.5-flash",
        "precision",
        "recall"
      ]
    },
    "publishedAt": "2025-06-12T13:54:17.000Z",
    "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks",
    "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of 0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10954.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6355473d525beaee688b7ba1",
      "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
      "fullname": "Wei Tao",
      "name": "itaowe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09993",
      "authors": [
        {
          "_id": "684ae204dbd21a9cc27b0fba",
          "user": {
            "_id": "66012e9c9e1cf5eb41ee0c4c",
            "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
            "isPro": false,
            "fullname": "Jaewon Min",
            "user": "Min-Jaewon",
            "type": "user"
          },
          "name": "Jaewon Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:25.024Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbb",
          "user": {
            "_id": "65ec3449a69aaabb431db0da",
            "avatarUrl": "/avatars/d7b507be0175a61a8fc21176eea45001.svg",
            "isPro": false,
            "fullname": "Jin Hyeon Kim",
            "user": "jinlovespho",
            "type": "user"
          },
          "name": "Jin Hyeon Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:22.776Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbc",
          "user": {
            "_id": "6752b6315281c3cae4b0783f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xmcyVEl2xBhk3G5_7dmpz.png",
            "isPro": false,
            "fullname": "Paul Hyunbin Cho",
            "user": "paulcho98",
            "type": "user"
          },
          "name": "Paul Hyunbin Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:20.327Z",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbd",
          "name": "Jaeeun Lee",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbe",
          "name": "Jihye Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fbf",
          "name": "Minkyu Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc0",
          "name": "Sangpil Kim",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc1",
          "name": "Hyunhee Park",
          "hidden": false
        },
        {
          "_id": "684ae204dbd21a9cc27b0fc2",
          "name": "Seungryong Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:59:46.000Z",
      "submittedOnDailyAt": "2025-06-13T00:32:01.285Z",
      "title": "ディフューションモデルを用いた文脈に関心のある画像のリストニング",
      "submittedOnDailyBy": {
        "_id": "66012e9c9e1cf5eb41ee0c4c",
        "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
        "isPro": false,
        "fullname": "Jaewon Min",
        "user": "Min-Jaewon",
        "type": "user"
      },
      "summary": "画像リファイル化の目的は、劣化した画像を復元することです。しかし、現在の存在する、拡散ベースのリファイル化手法は、自然画像のリファイル化において大成功を収めているのに対し、劣化画像のテキスト領域の忠実な再構築を難しくなっています。これらの手法は、テキストのようなパターンを生成し、この現象を「テキスト画像のハロウィン」と呼び、これを誤ったものとして見なします。本論文では、テキストに関心を持つ画像リファイル化（TAIR）を紹介します。これは、視覚内容とテキストの忠実性の同時的な復元を要求する新しいリファイル化タスクです。このタスクを解決するために、SA-Textを提案します。これは、100Kの高品質なスケーン画像の大規模なベンチマークで、多様かつ複雑なテキストインスタンスを密集的にアノテートしたものです。また、TeReDiffという多タスク拡散フレームワークを提案します。これは、拡散モデルからの内部的な特徴をテキストスポッティングモジュールに統合し、両方のコンポーネントが共通学習から利益を得ることを可能にします。これにより、豊富なテキスト表現を抽出することができ、それらは後続のデノイズステップでのプロンプトとして利用されます。拡張された実験は、我々のアプローチが最先端のリファイル化手法を一致的に上回り、テキスト認識精度において显著な効果を収めることを示します。プロジェクトページを参照：https://cvlab-kaist.github.io/TAIR/",
      "upvotes": 28,
      "discussionId": "684ae204dbd21a9cc27b0fc5",
      "projectPage": "https://cvlab-kaist.github.io/TAIR/",
      "githubRepo": "https://github.com/cvlab-kaist/TAIR",
      "ai_summary": "The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.",
      "ai_keywords": [
        "diffusion-based restoration",
        "text-image hallucination",
        "Text-Aware Image Restoration (TAIR)",
        "SA-Text",
        "multi-task diffusion framework",
        "TeReDiff",
        "text-spotting module",
        "text recognition accuracy"
      ]
    },
    "publishedAt": "2025-06-11T13:59:46.000Z",
    "title": "Text-Aware Image Restoration with Diffusion Models",
    "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09993.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66012e9c9e1cf5eb41ee0c4c",
      "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
      "fullname": "Jaewon Min",
      "name": "Min-Jaewon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10857",
      "authors": [
        {
          "_id": "684b817e3b733ba333686f95",
          "user": {
            "_id": "64b89a14cf14c2fabe96664c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
            "isPro": false,
            "fullname": "Jiashuo Yu",
            "user": "awojustin",
            "type": "user"
          },
          "name": "Jiashuo Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:55.618Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f96",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f97",
          "name": "Meng Chu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f98",
          "name": "Zhifei Ren",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f99",
          "name": "Zizheng Huang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9a",
          "user": {
            "_id": "64c9beb2904317f42de06dd8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c9beb2904317f42de06dd8/he3rxfyzfwEd1vLuK6_o2.jpeg",
            "isPro": false,
            "fullname": "Pei Chu",
            "user": "chupei",
            "type": "user"
          },
          "name": "Pei Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:51.884Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9b",
          "name": "Ruijie Zhang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9c",
          "name": "Yinan He",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9d",
          "name": "Qirui Li",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9e",
          "user": {
            "_id": "64acbbd51aee69ece03c6c0c",
            "avatarUrl": "/avatars/604df1cabc5faeda55022ae4c1997e56.svg",
            "isPro": false,
            "fullname": "Songze Li",
            "user": "LarryLee",
            "type": "user"
          },
          "name": "Songze Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:53.776Z",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686f9f",
          "name": "Zhenxiang Li",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa0",
          "name": "Zhongying Tu",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa1",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa2",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa3",
          "name": "Yali Wang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa4",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "684b817e3b733ba333686fa5",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T16:17:17.000Z",
      "submittedOnDailyAt": "2025-06-13T00:10:47.082Z",
      "title": "VRBench: 長ビデオの多段階理由に適したベンチマーク",
      "submittedOnDailyBy": {
        "_id": "64b89a14cf14c2fabe96664c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
        "isPro": false,
        "fullname": "Jiashuo Yu",
        "user": "awojustin",
        "type": "user"
      },
      "summary": "VRBenchは、大規模モデルの多段階推理能力を評価するために作成された最初の長篇ニューレイティングビデオベンチマークです。現在の評価における時間的推理と手順の正当性の欠点を解決します。これは1,010部の長ビデオ（平均1.6時間）を含み、9,468件の人間ラベル付き多段階質問回答ペアと30,292件の理由論の時間スタンプを持っています。これらのビデオは、プロットの一貫性を優先するための多段階フィルタリングプロセスによりカレーティングされています。人間とAIの協力フレームワークを開発し、7タイプの理由論（例：イベントの説明、隠れエピストロジー）を含む時間的に基づく複数の段階を必要とする一意の理由論コースを生成します。VRBenchは、結果とプロセスの両方においてモデルを評価するための多段階評価プロセスを設計しています。最終結果のためのMCQだけでなく、理由論コースの質の評価を複数の次元から構成的に行うための進捗レベルLLMガイドドラインスコアメトリックを提案しています。VRBench上で12タイプのLLMと16タイプのVLMを検証し、詳細な分析を行い、多段階推理の分野における有効なヒントを提供します。",
      "upvotes": 23,
      "discussionId": "684b817e3b733ba333686fa6",
      "projectPage": "https://vrbench.github.io/",
      "githubRepo": "https://github.com/OpenGVLab/VRBench",
      "ai_summary": "VRBench is a long narrative video benchmark designed to evaluate models' multi-step reasoning and procedural validity through human-labeled question-answering pairs and a human-AI collaborative framework with a multi-phase evaluation pipeline.",
      "ai_keywords": [
        "VRBench",
        "multi-step reasoning",
        "temporal reasoning",
        "procedural validity",
        "long videos",
        "human-labeled",
        "multi-step question-answering",
        "expert inter-rater reviewing",
        "coherent reasoning chains",
        "event attribution",
        "implicit inference",
        "multi-phase evaluation",
        "progress-level LLM-guided scoring metric",
        "LLMs",
        "VLMs"
      ]
    },
    "publishedAt": "2025-06-12T12:17:17.000Z",
    "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
    "summary": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10857.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b89a14cf14c2fabe96664c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
      "fullname": "Jiashuo Yu",
      "name": "awojustin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10540",
      "authors": [
        {
          "_id": "684bad683b733ba3336870b6",
          "user": {
            "_id": "652fb8bcc9dd2692a25ef2e3",
            "avatarUrl": "/avatars/461e6cc1c3441cde18192b080b0b8576.svg",
            "isPro": false,
            "fullname": "Haoyuan Shi",
            "user": "MrSunshy",
            "type": "user"
          },
          "name": "Haoyuan Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:52.713Z",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b7",
          "user": {
            "_id": "62fdb01bc1588e1d4c6c1a7c",
            "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
            "isPro": false,
            "fullname": "Yunxin Li",
            "user": "YunxinLi",
            "type": "user"
          },
          "name": "Yunxin Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T04:47:39.539Z",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b8",
          "name": "Xinyu Chen",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870b9",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870ba",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "684bad683b733ba3336870bb",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T10:06:21.000Z",
      "submittedOnDailyAt": "2025-06-13T03:26:17.710Z",
      "title": "AniMaker: MCTSドリブンドの自動化マルチアグエントアニメーションストーリーテリングとクリップ生成",
      "submittedOnDailyBy": {
        "_id": "62fdb01bc1588e1d4c6c1a7c",
        "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
        "isPro": false,
        "fullname": "Yunxin Li",
        "user": "YunxinLi",
        "type": "user"
      },
      "summary": "電視生成モデルの急速な進歩にもかかわらず、多くのスケーンとキャラクターを組み合わせたコンティネイティーの連続的な物語を生成することは難しい。現在の方法は、予め生成されたキーフレームを固定長のクリップに変換し、連続なニューレットとページング問題を発生させることが多い。また、電視生成モデルの内在的な不確実性は、一つの低品質のクリップも全体の出力アニメーションの論理的な一貫性と視覚的な連続性を大幅に低下させることができる。これらの障害を克服するために、我々はAniMakerを紹介します。AniMakerは、多候補のクリップの効率的な生成と物語に関連付けられたクリップ選択を可能にするための多アガントフレームワークです。このフレームワークは、ストーリーボード生成のためのディレクターアガント、クリップ生成のためのフォトグラフィーアガント、評価のためのレビューアガント、編集とボイスオーバーのためのポストプロダクションアガントを構成しています。AniMakerのアプローチの中心的な部分は、フォトグラフィーアガントのMCTS-GenとレビューアガントのAniEvalの2つの技術的なコンポーネントです。MCTS-Genは、賢明に候補空間を歩み、リソース使用を最適化しながら高エネルギーのクリップを生成するための、MCTSにヒントを得た効率的なモンテカルロ木検索（MCTS）風の戦略です。AniEvalは、多スライドアニメーションの評価に特に設計された最初のフレームワークで、ストーリーレベルの一貫性、アクションの完了、アニメーションの特有の機能などの重要な面で、前後のクリップのコンテキストを考慮して評価します。実験は、VBenchや我々が提案したAniEvalフレームワークを用いて愛用メトリックでの上品さを示し、多候補の生成の効率を大幅に向上させ、AI生成の物語アニメーションを生産標準に近づけることを示しています。",
      "upvotes": 23,
      "discussionId": "684bad683b733ba3336870bc",
      "ai_summary": "AniMaker, a multi-agent framework using MCTS-Gen and AniEval, generates coherent storytelling videos from text input, outperforming existing models with better quality and efficiency.",
      "ai_keywords": [
        "multi-agent framework",
        "Director Agent",
        "Photography Agent",
        "Reviewer Agent",
        "Post-Production Agent",
        "Monte Carlo Tree Search (MCTS)",
        "AniEval",
        "VBench",
        "action completion",
        "story-level consistency",
        "animation-specific features"
      ]
    },
    "publishedAt": "2025-06-12T06:06:21.000Z",
    "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation",
    "summary": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity. To overcome these\nobstacles, we introduce AniMaker, a multi-agent framework enabling efficient\nmulti-candidate clip generation and storytelling-aware clip selection, thus\ncreating globally consistent and story-coherent animation solely from text\ninput. The framework is structured around specialized agents, including the\nDirector Agent for storyboard generation, the Photography Agent for video clip\ngeneration, the Reviewer Agent for evaluation, and the Post-Production Agent\nfor editing and voiceover. Central to AniMaker's approach are two key technical\ncomponents: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search\n(MCTS)-inspired strategy that intelligently navigates the candidate space to\ngenerate high-potential clips while optimizing resource usage; and AniEval in\nReviewer Agent, the first framework specifically designed for multi-shot\nanimation evaluation, which assesses critical aspects such as story-level\nconsistency, action completion, and animation-specific features by considering\neach clip in the context of its preceding and succeeding clips. Experiments\ndemonstrate that AniMaker achieves superior quality as measured by popular\nmetrics including VBench and our proposed AniEval framework, while\nsignificantly improving the efficiency of multi-candidate generation, pushing\nAI-generated storytelling animation closer to production standards.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10540.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62fdb01bc1588e1d4c6c1a7c",
      "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
      "fullname": "Yunxin Li",
      "name": "YunxinLi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10952",
      "authors": [
        {
          "_id": "684b96403b733ba33368703a",
          "user": {
            "_id": "65e808ed7c10574cc3f8e363",
            "avatarUrl": "/avatars/ed10759d354e271bfc15afd946b66b4a.svg",
            "isPro": false,
            "fullname": "zhangmozhi",
            "user": "mzzhang",
            "type": "user"
          },
          "name": "Mozhi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:33.791Z",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703b",
          "user": {
            "_id": "6718fc605e14ff6b94a7109f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
            "isPro": false,
            "fullname": "Howe Tissue",
            "user": "Howe77",
            "type": "user"
          },
          "name": "Howe Tissue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:31.965Z",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703c",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "684b96403b733ba33368703d",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:53:51.000Z",
      "submittedOnDailyAt": "2025-06-13T01:43:47.223Z",
      "title": "ドメイン2ベクトル：トレーニングなしで最適なデータ混合を見つけるデータセットのベクトル化",
      "submittedOnDailyBy": {
        "_id": "6718fc605e14ff6b94a7109f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
        "isPro": false,
        "fullname": "Howe Tissue",
        "user": "Howe77",
        "type": "user"
      },
      "summary": "ドメイン2ベクトル（Domain2Vec）は、データセットを、新しい概念であるメタドメイン（meta-domains）の線形結合に分解する新しいアプローチです。このアプローチは、データセットの隠れた特徴を捉えるために設計されています。Domain2Vecはメタドメインのバーコレーションを保持し、与えられたデータセットをこのバーコレーションに対応するドメインベクトルに分解するクラス分類器を使用します。これらのドメインベクトルは、訓練フリーで最適なデータ混合を識別し、語言モデル（LM）の事前学習の性能を向上させるために、分布一致性仮定（DA^2）のもとで効果的に使用できます。この仮定は、訓練セットと検証セットのデータ分布がより良く一致したほど、検証損失が低くなることを示しています。また、Domain2Vecは、ドメインベクトルとLMの性能の関係をモデル化する先行研究と無間違く統合可能で、先行方法の効率とスケーラビリティを大幅に向上させます。拡張的な実験は、Domain2Vecが最小限の計算オーバーヘッドで下流タスクの性能を向上させるデータ混合を見つけることを示します。特に、Pile-CCデータセットの元の混合による訓練で必要な計算量の51.5%だけで同じ検証損失を達成し、等価な計算バジュードでは、下流タスクの性能が平均で2.83%向上します。",
      "upvotes": 14,
      "discussionId": "684b96413b733ba33368703e",
      "ai_summary": "Domain2Vec decomposes datasets into meta-domains to optimize language model pretraining and downstream performance with reduced computational cost.",
      "ai_keywords": [
        "Domain2Vec",
        "meta-domains",
        "domain vector",
        "distribution alignment assumption",
        "DA²",
        "language model",
        "pretraining",
        "downstream task performance",
        "Pile-CC",
        "The Pile dataset"
      ]
    },
    "publishedAt": "2025-06-12T13:53:51.000Z",
    "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training",
    "summary": "We introduce~Domain2Vec, a novel approach that decomposes any\ndataset into a linear combination of several meta-domains, a new concept\ndesigned to capture the key underlying features of datasets.\nDomain2Vec maintains a vocabulary of meta-domains and uses a\nclassifier to decompose any given dataset into a domain vector that corresponds\nto a distribution over this vocabulary. These domain vectors enable the\nidentification of the optimal data mixture for language model (LM) pretraining\nin a training-free manner under the \\textbf{Distribution\nAlignment Assumption} (DA^{2}), which suggests that when\nthe data distributions of the training set and the validation set are better\naligned, a lower validation loss is achieved. Moreover, Domain2vec can\nbe seamlessly integrated into previous works to model the relationship between\ndomain vectors and LM performance, greatly enhancing the efficiency and\nscalability of previous methods. Extensive experiments demonstrate that\nDomain2Vec helps find the data mixture that enhances downstream task\nperformance with minimal computational overhead. Specifically,\nDomain2Vec achieves the same validation loss on Pile-CC using only\n51.5% of the computation required when training on the original mixture of\nThe Pile dataset. Under equivalent compute budget, Domain2Vec improves\ndownstream performance by an average of 2.83%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6718fc605e14ff6b94a7109f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
      "fullname": "Howe Tissue",
      "name": "Howe77",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10357",
      "authors": [
        {
          "_id": "684b86bf3b733ba333686fbe",
          "name": "Zaijing Li",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fbf",
          "name": "Yuquan Xie",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc0",
          "name": "Rui Shao",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc1",
          "name": "Gongwei Chen",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc2",
          "name": "Weili Guan",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc3",
          "name": "Dongmei Jiang",
          "hidden": false
        },
        {
          "_id": "684b86bf3b733ba333686fc4",
          "name": "Liqiang Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T05:29:40.000Z",
      "submittedOnDailyAt": "2025-06-13T00:37:48.793Z",
      "title": "Optimus-3: スケーラブルなタスクエキスパーを採用した一般的な多タイプマニュアルマイクロソーツアガントへの向け",
      "submittedOnDailyBy": {
        "_id": "66b45fe75d0ac130d7d82764",
        "avatarUrl": "/avatars/09253f41f82c533b36199f82620cd075.svg",
        "isPro": false,
        "fullname": "Zaijing Li",
        "user": "dawn0815",
        "type": "user"
      },
      "summary": "最近、多モデル大語言モデル（MLLM）に基づくアグエントは、様々な領域で驚異的な進歩を達成しました。しかし、プレイステートやMinecraftのような開放ウールド環境で視覚的な認識、計画、行動、基礎化、反省などの機能を持つ一般的なアグエントの構築は、難しい問題です：特定領域のデータが不足し、異なるタスクの間の干渉があり、開放ウールド設定の視覚的多様性があります。本論文では、これらの問題を解決するために、3つの主な貢献を提案します。1) データ生成パイプラインを提案し、アグエントの開発に拡張可能で高品質なデータを提供します。2) 異なるタスクの間の干渉を軽減するために、タスクレベルのルーティングを用いたMixture-of-Experts（MoE）アーキテクチャを導入します。3) Minecraftでの視覚的多様性に対するアグエントの推理能力を高めるために、Multimodal Reasoning-Augmented Reinforcement Learningアプローチを開発します。これらの革新的な機能を基に、Optimus-3、Minecraft用の一般的なアグエントを紹介します。拡張的な実験結果は、Optimus-3はMinecraft環境の様々なタスクで、一般的な多モデル大語言モデルと現在の最先端のアグエントを超えることを示します。プロジェクトページ：https://cybertronagent.github.io/Optimus-3.github.io/",
      "upvotes": 12,
      "discussionId": "684b86bf3b733ba333686fc5",
      "projectPage": "https://cybertronagent.github.io/Optimus-3.github.io/",
      "githubRepo": "https://github.com/JiuTian-VL/Optimus-3",
      "ai_summary": "Optimus-3, a multimodal large language model agent, uses knowledge-enhanced data generation, a Mixture-of-Experts architecture, and multimodal reasoning-augmented reinforcement learning to achieve superior performance across various tasks in Minecraft.",
      "ai_keywords": [
        "multimodal large language models",
        "knowledge-enhanced data generation",
        "Mixture-of-Experts",
        "task-level routing",
        "multimodal reasoning-augmented reinforcement learning"
      ]
    },
    "publishedAt": "2025-06-12T01:29:40.000Z",
    "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts",
    "summary": "Recently, agents based on multimodal large language models (MLLMs) have\nachieved remarkable progress across various domains. However, building a\ngeneralist agent with capabilities such as perception, planning, action,\ngrounding, and reflection in open-world environments like Minecraft remains\nchallenges: insufficient domain-specific data, interference among heterogeneous\ntasks, and visual diversity in open-world settings. In this paper, we address\nthese challenges through three key contributions. 1) We propose a\nknowledge-enhanced data generation pipeline to provide scalable and\nhigh-quality training data for agent development. 2) To mitigate interference\namong heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture\nwith task-level routing. 3) We develop a Multimodal Reasoning-Augmented\nReinforcement Learning approach to enhance the agent's reasoning ability for\nvisual diversity in Minecraft. Built upon these innovations, we present\nOptimus-3, a general-purpose agent for Minecraft. Extensive experimental\nresults demonstrate that Optimus-3 surpasses both generalist multimodal large\nlanguage models and existing state-of-the-art agents across a wide range of\ntasks in the Minecraft environment. Project page:\nhttps://cybertronagent.github.io/Optimus-3.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10357.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b45fe75d0ac130d7d82764",
      "avatarUrl": "/avatars/09253f41f82c533b36199f82620cd075.svg",
      "fullname": "Zaijing Li",
      "name": "dawn0815",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10974",
      "authors": [
        {
          "_id": "684b8e193b733ba333687028",
          "user": {
            "_id": "6241749cf80bd930bd99f3dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669210243382-6241749cf80bd930bd99f3dd.jpeg",
            "isPro": false,
            "fullname": "Ou Yixin",
            "user": "OE-Heart",
            "type": "user"
          },
          "name": "Yixin Ou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:36.148Z",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba333687029",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702a",
          "name": "Jingsheng Zheng",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702b",
          "name": "Lanning Wei",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702c",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702d",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702e",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba33368702f",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "684b8e193b733ba333687030",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:37.984Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:59:32.000Z",
      "submittedOnDailyAt": "2025-06-13T03:44:21.173Z",
      "title": "AutoMind: 自動データサイエンス向けの適応的知識型アガント",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "大語言モデル（LLM）アガントは、実世界的データサイエンス問題を解決するために大きなポテンシャルを示しています。LLM駆動データサイエンスアガントは、全ての機械学習パイプラインを自動化することを承諾していますが、実世界的効果性は限られています。現在のフレームワークは、厳格な、事前定義されたワークフローと不変的なコーディング戦略に依存しており、相対的に簡単な古典的な問題での優れた性能を示すだけで、複雑なイノベーシブなタスクにおける人間の実証的知識を捉えることができません。本研究では、3つのキーの進歩を通じてこれらの欠点を克服した自動的な知識を持つLLMアガントフレームワーク「AutoMind」を紹介します。これらの進歩は、(1)専門家の知識ベース、(2)アガント知識を持つ木検索アルゴリズム、(3)自動調整コーディング戦略です。2つの自動データサイエンスベンチマークでの評価により、AutoMindは最先端のベースラインと比較して上位の性能を示しています。追加的分析は、有効性、効率性、質的な解決策の品質の良さを確認し、AutoMindは完全自動化されたデータサイエンスへの適切かつ強固なステップとしての役割を果たしていることを明らかにしています。",
      "upvotes": 10,
      "discussionId": "684b8e193b733ba333687031",
      "ai_summary": "AutoMind, a flexible and knowledgeable LLM-agent framework, improves automated data science through expert knowledge integration, strategic solution exploration, and adaptive coding, outperforming existing systems.",
      "ai_keywords": [
        "LLM",
        "data science agents",
        "machine learning pipeline",
        "expert knowledge base",
        "agentic knowledgeable tree search",
        "self-adaptive coding strategy"
      ]
    },
    "publishedAt": "2025-06-12T13:59:32.000Z",
    "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
    "summary": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10974.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10960",
      "authors": [
        {
          "_id": "684bb33a3b733ba3336870c5",
          "name": "Kangwei Liu",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c6",
          "name": "Siyuan Cheng",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c7",
          "name": "Bozhong Tian",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c8",
          "name": "Xiaozhuan Liang",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870c9",
          "name": "Yuyang Yin",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870ca",
          "name": "Meng Han",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cb",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:50.549Z",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cc",
          "name": "Bryan Hooi",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870cd",
          "user": {
            "_id": "635113fdcba4ff2e81cb236e",
            "avatarUrl": "/avatars/f80df906b722b4901debce9baa867073.svg",
            "isPro": false,
            "fullname": "chen",
            "user": "Jasonchen123",
            "type": "user"
          },
          "name": "Xi Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T05:12:27.526Z",
          "hidden": false
        },
        {
          "_id": "684bb33a3b733ba3336870ce",
          "name": "Shumin Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:57:05.000Z",
      "submittedOnDailyAt": "2025-06-13T03:43:12.055Z",
      "title": "中国ハームベンチャーム：中国の有害内容検出ベンチマーク",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、有害な内容検出任務に増えて自動化されています。これらは、モデレーターに政策違反を識別し、内容のレビューの全体的な効率と精度を向上させることを助けています。しかし、現在の有害な内容検出のリソースは主に英語に集中しており、中国語のデータセットは稀少で、そして範囲が限られています。私たちは、6つの代表的なカテゴリーを掲載し、全てのデータは実世界的なものから構築された、専門的に注釈されたベンチマークを提出します。この注釈プロセスは、LLMsが中国語有害な内容検出に役立つための明確な専門知識を提供する知識ルールベースを得ます。また、私たちは、人間注釈された知識ルールと大語言モデルから得られる潜在的な知識を統合した知識付加ベースラインを提案し、これにより小さなモデルが最先端のLLMsと比較的な性能を達成することができます。コードとデータは、https://github.com/zjunlp/ChineseHarm-bench にアクセスできます。",
      "upvotes": 9,
      "discussionId": "684bb33a3b733ba3336870cf",
      "ai_summary": "A benchmark for Chinese harmful content detection is introduced, along with a knowledge-augmented model that enhances efficiency and accuracy using human-annotated rules and LLMs.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "harmful content detection",
        "knowledge-augmented baseline",
        "annotation process",
        "knowledge rule base",
        "Chinese datasets"
      ]
    },
    "publishedAt": "2025-06-12T13:57:05.000Z",
    "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark",
    "summary": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which covers six representative categories and\nis constructed entirely from real-world data. Our annotation process further\nyields a knowledge rule base that provides explicit expert knowledge to assist\nLLMs in Chinese harmful content detection. In addition, we propose a\nknowledge-augmented baseline that integrates both human-annotated knowledge\nrules and implicit knowledge from large language models, enabling smaller\nmodels to achieve performance comparable to state-of-the-art LLMs. Code and\ndata are available at https://github.com/zjunlp/ChineseHarm-bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10960.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10821",
      "authors": [
        {
          "_id": "684b91c73b733ba333687033",
          "name": "Huaying Yuan",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687034",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687035",
          "name": "Junjie Zhou",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687036",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "684b91c73b733ba333687037",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T15:39:10.000Z",
      "submittedOnDailyAt": "2025-06-13T01:20:51.837Z",
      "title": "VideoDeepResearch: 長ビデオ理解に対するアウトローカルツールの使用",
      "submittedOnDailyBy": {
        "_id": "66d916a7b86f0d569aa19b60",
        "avatarUrl": "/avatars/2537cee66afecc2d999e05b01c78d319.svg",
        "isPro": false,
        "fullname": "huaying Yuan",
        "user": "avery00",
        "type": "user"
      },
      "summary": "長期ビデオ理解（LVU）は、現在の多モーダル大語言モデル（MLLMs）にとって重要な課題であり、これはタスクの固有の複雑さとコンテキストウィンドウの制限により成り立ちます。通常は、LVUタスクを解決するには、拡張されたコンテキストウィンドウ、強力的な視覚認識能力、そして専門的な知識を持つ基盤的なMLLMが必要とされています。本論文では、この一般的な信念を挑戦し、VideoDeepResearchという新しいアガントフレームワークを紹介します。我々のアプローチは、単一のテキストベースの大語言モデル（LRM）とモジュール化された多モーダルツールキットを組み合わせて構成されています。これらのツールキットには、多モーダルのリテランサーと視覚認識器が含まれ、実用的には容易に利用できます。LVUタスクごとに、システムは理由論を通じて問題解決の戦略を構築し、必要なビデオ内容を選択的にアクセスし、ツールを使用して利用します。流行りのLVUベンチマーク上で極めて広範囲な実験を実施しました。MLVU（テスト）、LVBench、LongVideoBenchにおいて、VideoDeepResearchは現在のMLLMベースラインに対して大幅な改善を収め、それぞれ9.6%、6.6%、3.9%の改善率を達成しました。これらの結果は、アガントシステムがLVUプロブラムの重要な課題を克服する可能性を明らかにしています。",
      "upvotes": 9,
      "discussionId": "684b91c73b733ba333687038",
      "ai_summary": "VideoDeepResearch, a text-only reasoning model with modular tools, surpasses existing baselines in long video understanding tasks without extending context windows or enhancing visual perception capabilities.",
      "ai_keywords": [
        "long video understanding",
        "multi-modal large language models",
        "VideoDeepResearch",
        "text-only large reasoning model",
        "multimodal retrievers",
        "visual perceivers",
        "MLVU",
        "Video-MME",
        "LVBench",
        "LongVideoBench",
        "agentic systems"
      ]
    },
    "publishedAt": "2025-06-12T11:39:10.000Z",
    "title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using",
    "summary": "Long video understanding (LVU) presents a significant challenge for current\nmulti-modal large language models (MLLMs) due to the task's inherent complexity\nand context window constraint. It is widely assumed that addressing LVU tasks\nrequires foundation MLLMs with extended context windows, strong visual\nperception capabilities, and proficient domain expertise. In this work, we\nchallenge this common belief by introducing VideoDeepResearch, a novel agentic\nframework for long video understanding. Our approach relies solely on a\ntext-only large reasoning model (LRM) combined with a modular multi-modal\ntoolkit, including multimodal retrievers and visual perceivers, all of which\nare readily available in practice. For each LVU task, the system formulates a\nproblem-solving strategy through reasoning, while selectively accessing and\nutilizing essential video content via tool using. We conduct extensive\nexperiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.\nOur results demonstrate that VideoDeepResearch achieves substantial\nimprovements over existing MLLM baselines, surpassing the previous\nstate-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and\nLongVideoBench, respectively. These findings highlight the promise of agentic\nsystems in overcoming key challenges in LVU problems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d916a7b86f0d569aa19b60",
      "avatarUrl": "/avatars/2537cee66afecc2d999e05b01c78d319.svg",
      "fullname": "huaying Yuan",
      "name": "avery00",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10741",
      "authors": [
        {
          "_id": "684b881f3b733ba333686fd4",
          "user": {
            "_id": "64966691990b342dcc9fccb5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64966691990b342dcc9fccb5/tQSrE3MkBeakk5QYfgHSo.jpeg",
            "isPro": false,
            "fullname": "sixiang chen",
            "user": "Ephemeral182",
            "type": "user"
          },
          "name": "SiXiang Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:47.414Z",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd5",
          "name": "Jianyu Lai",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd6",
          "name": "Jialin Gao",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd7",
          "name": "Tian Ye",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd8",
          "name": "Haoyu Chen",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fd9",
          "name": "Hengyu Shi",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fda",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdb",
          "name": "Yunlong Lin",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdc",
          "name": "Song Fei",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdd",
          "name": "Zhaohu Xing",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fde",
          "name": "Yeying Jin",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fdf",
          "name": "Junfeng Luo",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fe0",
          "name": "Xiaoming Wei",
          "hidden": false
        },
        {
          "_id": "684b881f3b733ba333686fe1",
          "name": "Lei Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T14:28:12.000Z",
      "submittedOnDailyAt": "2025-06-13T04:30:38.214Z",
      "title": "PosterCraft: 統一的フレームワークでの高品質美術ポスター生成の再考",
      "submittedOnDailyBy": {
        "_id": "66015e8aa4d296af07de538e",
        "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
        "isPro": false,
        "fullname": "Ye",
        "user": "Owen777",
        "type": "user"
      },
      "summary": "美術海报の生成は、簡単なデザイン画像よりも難しい：\nそれは、テキストの精密な描画だけでなく、抽象的な芸術的な内容の無間違いなエンジニアリング、魅力的なレイアウト、および全体的なスタイリッシュの和諧性を要求しているからである。これに対処するために、私たちはPosterCraftという統一的なフレームワークを提案しています。これは、先ほどのモジュール化プイルプリンと厳格な、事前定義されたレイアウトを捨て、モデルが自由にコヒーレントな、視覚的に誘惑的な構成を探ることを許可します。PosterCraftは、高美術的な海报の生成を最適化するために、謹めたワークフローを構築しています： (i) 新たに追加されたText-Render-2Mデータセットにおける大規模なテキスト描画最適化； (ii) HQ-Poster100Kにおける領域に関する規範付きの微調校； (iii) 美術的なテキストの強化学習を行うベストのnの好み最適化； (iv) 共同的な視覚言語のフィードバックのリファインメント。各ステップは、その特定の需要に合わせた全自動化データ構築プイルプリンをもってサポートされ、複雑なアーキテクチャの変更を除いて強固なトレーニングを可能にします。複数の実験で評価されたPosterCraftは、レンダリング精度、レイアウトのコヒーレンシ、全体的な視覚的な魅力において、開放ソースベースラインティングを大幅に超え、最先端の商業システムの品質に近づきます。私たちのコード、モデル、データセットは、プロジェクトページにあります：https://ephemeral182.github.io/PosterCraft",
      "upvotes": 9,
      "discussionId": "684b881f3b733ba333686fe2",
      "projectPage": "https://ephemeral182.github.io/PosterCraft/",
      "githubRepo": "https://github.com/Ephemeral182/PosterCraft",
      "ai_summary": "PosterCraft improves aesthetic poster generation through a unified, modular pipeline with enhanced text rendering, region-aware fine-tuning, aesthetic reinforcement learning, and joint vision-language refinement.",
      "ai_keywords": [
        "text-rendering optimization",
        "Text-Render-2M",
        "region-aware supervised fine-tuning",
        "HQ-Poster100K",
        "aesthetic-text-reinforcement learning",
        "best-of-n preference optimization",
        "joint vision-language feedback refinement"
      ]
    },
    "publishedAt": "2025-06-12T10:28:12.000Z",
    "title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a\n  Unified Framework",
    "summary": "Generating aesthetic posters is more challenging than simple design images:\nit requires not only precise text rendering but also the seamless integration\nof abstract artistic content, striking layouts, and overall stylistic harmony.\nTo address this, we propose PosterCraft, a unified framework that abandons\nprior modular pipelines and rigid, predefined layouts, allowing the model to\nfreely explore coherent, visually compelling compositions. PosterCraft employs\na carefully designed, cascaded workflow to optimize the generation of\nhigh-aesthetic posters: (i) large-scale text-rendering optimization on our\nnewly introduced Text-Render-2M dataset; (ii) region-aware supervised\nfine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via\nbest-of-n preference optimization; and (iv) joint vision-language feedback\nrefinement. Each stage is supported by a fully automated data-construction\npipeline tailored to its specific needs, enabling robust training without\ncomplex architectural modifications. Evaluated on multiple experiments,\nPosterCraft significantly outperforms open-source baselines in rendering\naccuracy, layout coherence, and overall visual appeal-approaching the quality\nof SOTA commercial systems. Our code, models, and datasets can be found in the\nProject page: https://ephemeral182.github.io/PosterCraft",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10741.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66015e8aa4d296af07de538e",
      "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
      "fullname": "Ye",
      "name": "Owen777",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10890",
      "authors": [
        {
          "_id": "684b8b533b733ba333686fe4",
          "user": {
            "_id": "62bc1adacaf01b9bec398547",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
            "isPro": false,
            "fullname": "Zhao Zhang",
            "user": "zbrl",
            "type": "user"
          },
          "name": "Zhao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:45.153Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe5",
          "name": "Yutao Cheng",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe6",
          "user": {
            "_id": "6669a0cc9f28880b31d7c4ef",
            "avatarUrl": "/avatars/bd66a6f68a9af2bf7ee40510579e57fe.svg",
            "isPro": false,
            "fullname": "dexiang hong",
            "user": "hxxxl",
            "type": "user"
          },
          "name": "Dexiang Hong",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:22:14.202Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe7",
          "user": {
            "_id": "63fd7279ed9eead590fd02ed",
            "avatarUrl": "/avatars/4cf6f005069412ee87ed07cd81500f1e.svg",
            "isPro": false,
            "fullname": "YangMaoke",
            "user": "YangMaoke",
            "type": "user"
          },
          "name": "Maoke Yang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-13T02:22:14.202Z",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fe8",
          "user": {
            "_id": "6436619ead9b9147de287a24",
            "avatarUrl": "/avatars/180c43c79e552dd345636a47db80e3e9.svg",
            "isPro": false,
            "fullname": "ShiLayne",
            "user": "ShiLayne",
            "type": "user"
          },
          "name": "Gonglei Shi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-13T02:23:52.345Z",
          "hidden": true
        },
        {
          "_id": "684b8b533b733ba333686fe9",
          "name": "Lei Ma",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fea",
          "name": "Hui Zhang",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686feb",
          "name": "Jie Shao",
          "hidden": false
        },
        {
          "_id": "684b8b533b733ba333686fec",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T16:54:39.000Z",
      "submittedOnDailyAt": "2025-06-13T00:55:02.473Z",
      "title": "CreatiPoster: マルチレイヤーグラフィックデザインの編集可能と制御可能な生成に向けて",
      "submittedOnDailyBy": {
        "_id": "62bc1adacaf01b9bec398547",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
        "isPro": false,
        "fullname": "Zhao Zhang",
        "user": "zbrl",
        "type": "user"
      },
      "summary": "グラフィックデザインは、商業的なものと個人的なものの両方で重要な役割を果たしていますが、高品質で編集可能で美しいグラフィックコンポージションの作成は、特に初心者にとって時間のかかりそうでスキルの要求が高い任務です。現在のAIツールはワークフローの一部を自動化していますが、ユーザーが提供したアセットを正確に含め、編集可能性を維持し、プロフェッショナルな視覚的な魅力を達成することには難しい。商業的なシステムでは、Canva Magic Designなどは大きなテンプレートライブラリを基にしていますが、これらを再現するのは実用的ではありません。本論文では、CreatiPosterというフレームワークを紹介します。このフレームワークは、選択可能な自然言語の指示またはアセットから編集可能な多層コンポージションを生成します。プロトコルモデルとRGBA大規模な多モーダルモデルは、テキストまたはアセットのそれぞれのレイヤーについて、精密な配置、階層、内容とスタイルを詳細に記述したJSONスペシフィカルを生成します。そして、簡潔な背景プロンプトも含む。次に、このレイヤーを基に条件付きの背景モデルは、この描画されたフォロウグラウンドレイヤーに基づいて一貫した背景を合成します。グラフィックデザインの生成に関する自動化マーケットを用いてベンチマークを構築し、CreatiPosterは先進的なオープンソースアプローチと特許ポータブルな商業システムを超えることを示します。フィードバックを促進するために、100,000の多層デザインのコピー権無償なコーパスを公開します。CreatiPosterは、カンバス編集、テキストオーバーライド、レスポンシブなリサイズ、多言語対応、アニメーションポスターなどの多様なアプリケーションをサポートし、AIを助けるグラフィックデザインの民主化に進むことを促進します。プロジェクトホームページ：https://github.com/graphic-design-ai/creatiposter",
      "upvotes": 7,
      "discussionId": "684b8b533b733ba333686fed",
      "githubRepo": "https://github.com/graphic-design-ai/creatiposter",
      "ai_summary": "CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.",
      "ai_keywords": [
        "RGBA large multimodal model",
        "JSON specification",
        "conditional background model",
        "automated metrics",
        "graphic-design generation",
        "multi-layer designs",
        "AI-assisted graphic design"
      ]
    },
    "publishedAt": "2025-06-12T12:54:39.000Z",
    "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation",
    "summary": "Graphic design plays a crucial role in both commercial and personal contexts,\nyet creating high-quality, editable, and aesthetically pleasing graphic\ncompositions remains a time-consuming and skill-intensive task, especially for\nbeginners. Current AI tools automate parts of the workflow, but struggle to\naccurately incorporate user-supplied assets, maintain editability, and achieve\nprofessional visual appeal. Commercial systems, like Canva Magic Design, rely\non vast template libraries, which are impractical for replicate. In this paper,\nwe introduce CreatiPoster, a framework that generates editable, multi-layer\ncompositions from optional natural-language instructions or assets. A protocol\nmodel, an RGBA large multimodal model, first produces a JSON specification\ndetailing every layer (text or asset) with precise layout, hierarchy, content\nand style, plus a concise background prompt. A conditional background model\nthen synthesizes a coherent background conditioned on this rendered foreground\nlayers. We construct a benchmark with automated metrics for graphic-design\ngeneration and show that CreatiPoster surpasses leading open-source approaches\nand proprietary commercial systems. To catalyze further research, we release a\ncopyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports\ndiverse applications such as canvas editing, text overlay, responsive resizing,\nmultilingual adaptation, and animated posters, advancing the democratization of\nAI-assisted graphic design. Project homepage:\nhttps://github.com/graphic-design-ai/creatiposter",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10890.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bc1adacaf01b9bec398547",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
      "fullname": "Zhao Zhang",
      "name": "zbrl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09967",
      "authors": [
        {
          "_id": "684ae1eedbd21a9cc27b0f10",
          "user": {
            "_id": "67469d6a8407f929491dce06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
            "isPro": true,
            "fullname": "Shangshang Wang",
            "user": "upup-ashton-wang",
            "type": "user"
          },
          "name": "Shangshang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:31.479Z",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f11",
          "name": "Julian Asilis",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f12",
          "name": "Ömer Faruk Akgül",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f13",
          "name": "Enes Burak Bilgin",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f14",
          "name": "Ollie Liu",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f15",
          "user": {
            "_id": "63c8454e46421a2efe82709d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
            "isPro": true,
            "fullname": "Deqing Fu",
            "user": "deqing",
            "type": "user"
          },
          "name": "Deqing Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:29.284Z",
          "hidden": false
        },
        {
          "_id": "684ae1eedbd21a9cc27b0f16",
          "user": {
            "_id": "644bf65522d211df6444a7f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf65522d211df6444a7f4/k_ddZdQDg2fzhwjI1EXyx.jpeg",
            "isPro": false,
            "fullname": "Willie Neiswanger",
            "user": "willieneis",
            "type": "user"
          },
          "name": "Willie Neiswanger",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:27.301Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:44:01.000Z",
      "submittedOnDailyAt": "2025-06-13T02:39:37.215Z",
      "title": "レサ: 透明な理由論理モデルをSAEsを通じて作成する",
      "submittedOnDailyBy": {
        "_id": "67469d6a8407f929491dce06",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
        "isPro": true,
        "fullname": "Shangshang Wang",
        "user": "upup-ashton-wang",
        "type": "user"
      },
      "summary": "モデルの潜在的な表現を活用して、言語モデルに強い理由論をコスト効率的に引き出せるかどうかを調べる。この質問に答えるのはResaです。Resaは、新しいエフエクティブな稀疏自動エンコーダー調整（SAE-Tuning）手順を用いて、150M単位の理由論モデルの家族です。この方法は、まず、ソースモデルから理由論能力を捉えるためにSAEを訓練し、その訓練されたSAEを用いて、標準的な超フィニットチューニングプロセスをガイドし、理由論能力をターゲットモデルに引き出す。これは、理由論トレースがない確認された質問回答データを使用して行われます。特に、さらなるRLポストトレーニング前にこれを特定の基礎モデルに適用すると、SAE-Tuningは、RL訓練モデルの97%以上の理由論性能を維持し、トレーニングコストを2000倍以上減少し、約1ドルと約20分のトレーニング時間を削減します。また、これは、例えば2ガプラ上で1時間以内の軽いRL訓練モデルに適用すると、AIME24のPass@1が43.33%、AMC23のPass@1が90%の理由論性能をそのほとんどの追加コストで実現できます。それが驚き的なことですが、SAEから抽出される理由論能力は、潜在的に一般化可能でモジュール化可能です。一般性は、データセットから抽出された能力が、より大きい重複したコーパスでも性能を向上させることを意味します。モジュール化は、QwenやQwen-Mathから抽出された能力が、テスト時にR1-Distillモデルに付け加えられ、リトレーニングなしで比較的収益を得ることを意味します。拡張的な試験はこれらの発見を証明し、すべてのアーティファクトは完全にオープンソースに提供されています。",
      "upvotes": 6,
      "discussionId": "684ae1eedbd21a9cc27b0f17",
      "projectPage": "https://shangshangwang.notion.site/resa",
      "githubRepo": "https://github.com/shangshang-wang/Resa",
      "ai_summary": "SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.",
      "ai_keywords": [
        "sparse autoencoder tuning",
        "SAE-Tuning",
        "reasoning models",
        "verification",
        "sparse autoencoders",
        "supervised fine-tuning",
        "RL post-training",
        "Pass@1",
        "AIME24",
        "AMC23",
        "generality",
        "modularity",
        "R1-Distill",
        "Qwen",
        "Qwen-Math"
      ]
    },
    "publishedAt": "2025-06-11T13:44:01.000Z",
    "title": "Resa: Transparent Reasoning Models via SAEs",
    "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround 1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09967.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67469d6a8407f929491dce06",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
      "fullname": "Shangshang Wang",
      "name": "upup-ashton-wang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10910",
      "authors": [
        {
          "_id": "684bbe273b733ba3336870ed",
          "name": "Mistral-AI",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870ef",
          "name": "Abhinav Rastogi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f0",
          "name": "Albert Q. Jiang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f1",
          "name": "Andy Lo",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f2",
          "name": "Gabrielle Berrada",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f3",
          "name": "Guillaume Lample",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f4",
          "name": "Jason Rute",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f5",
          "name": "Joep Barmentlo",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f6",
          "name": "Karmesh Yadav",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f7",
          "name": "Kartik Khandelwal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f8",
          "name": "Khyathi Raghavi Chandu",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870f9",
          "name": "Léonard Blier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fa",
          "name": "Lucile Saulnier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fb",
          "name": "Matthieu Dinot",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fc",
          "name": "Maxime Darrin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fd",
          "name": "Neha Gupta",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870fe",
          "name": "Roman Soletskyi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba3336870ff",
          "name": "Sagar Vaze",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687100",
          "name": "Teven Le Scao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687101",
          "name": "Yihan Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687102",
          "name": "Adam Yang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687103",
          "name": "Alexander H. Liu",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687104",
          "name": "Alexandre Sablayrolles",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687105",
          "name": "Amélie Héliou",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687106",
          "name": "Amélie Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687107",
          "name": "Andy Ehrenberg",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687108",
          "name": "Anmol Agarwal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687109",
          "name": "Antoine Roux",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710a",
          "name": "Arthur Darcet",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710b",
          "name": "Arthur Mensch",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710c",
          "name": "Baptiste Bout",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710d",
          "name": "Baptiste Rozière",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710e",
          "name": "Baudouin De Monicault",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368710f",
          "name": "Chris Bamford",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687110",
          "name": "Christian Wallenwein",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687111",
          "name": "Christophe Renaudin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687112",
          "name": "Clémence Lanfranchi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687113",
          "name": "Darius Dabert",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687114",
          "name": "Devon Mizelle",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687115",
          "name": "Diego de las Casas",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687116",
          "name": "Elliot Chane-Sane",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687117",
          "name": "Emilien Fugier",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687118",
          "name": "Emma Bou Hanna",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687119",
          "name": "Gauthier Delerce",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711a",
          "name": "Gauthier Guinet",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711b",
          "name": "Georgii Novikov",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711c",
          "name": "Guillaume Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711d",
          "name": "Himanshu Jaju",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711e",
          "name": "Jan Ludziejewski",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368711f",
          "name": "Jean-Hadrien Chabran",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687120",
          "name": "Jean-Malo Delignon",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687121",
          "name": "Joachim Studnia",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687122",
          "name": "Jonas Amar",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687123",
          "name": "Josselin Somerville Roberts",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687124",
          "name": "Julien Denize",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687125",
          "name": "Karan Saxena",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687126",
          "name": "Kush Jain",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687127",
          "name": "Lingxiao Zhao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687128",
          "name": "Louis Martin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687129",
          "name": "Luyu Gao",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712a",
          "name": "Lélio Renard Lavaud",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712b",
          "name": "Marie Pellat",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712c",
          "name": "Mathilde Guillaumin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712d",
          "name": "Mathis Felardos",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712e",
          "name": "Maximilian Augustin",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368712f",
          "name": "Mickaël Seznec",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687130",
          "name": "Nikhil Raghuraman",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687131",
          "name": "Olivier Duchenne",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687132",
          "name": "Patricia Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687133",
          "name": "Patrick von Platen",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687134",
          "name": "Patryk Saffer",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687135",
          "name": "Paul Jacob",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687136",
          "name": "Paul Wambergue",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687137",
          "name": "Paula Kurylowicz",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687138",
          "name": "Pavankumar Reddy Muddireddy",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687139",
          "name": "Philomène Chagniot",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713a",
          "name": "Pierre Stock",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713b",
          "name": "Pravesh Agrawal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713c",
          "name": "Romain Sauvestre",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713d",
          "name": "Rémi Delacourt",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713e",
          "name": "Sanchit Gandhi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368713f",
          "name": "Sandeep Subramanian",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687140",
          "name": "Shashwat Dalal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687141",
          "name": "Siddharth Gandhi",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687142",
          "name": "Soham Ghosh",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687143",
          "name": "Srijan Mishra",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687144",
          "name": "Sumukh Aithal",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687145",
          "name": "Szymon Antoniak",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687146",
          "name": "Thibault Schueller",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687147",
          "name": "Thibaut Lavril",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687148",
          "name": "Thomas Robert",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687149",
          "name": "Thomas Wang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714a",
          "name": "Timothée Lacroix",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714b",
          "name": "Valeriia Nemychnikova",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714c",
          "name": "Victor Paltz",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714d",
          "name": "Virgile Richard",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714e",
          "name": "Wen-Ding Li",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba33368714f",
          "name": "William Marshall",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687150",
          "name": "Xuanyu Zhang",
          "hidden": false
        },
        {
          "_id": "684bbe273b733ba333687151",
          "name": "Yunhao Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:22:37.000Z",
      "submittedOnDailyAt": "2025-06-13T04:29:39.974Z",
      "title": "マジスタル",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "マジストラル、Mistralの最初の推理モデルと自社のスケーラブルな強化学習（RL）パイプラインを紹介します。現有の実装と先行モデルから汲み出されたRLツールチャイブに依存しないで、自社のモデルとインフラをだけに依存して、シンプルなアプローチを採用します。特に、我々は、LLMの純粋なRL訓練の限界を探索するためのスタックを示し、モデルの理由論の言語を強制する簡単な方法を示し、文書データのみでのRLは初期チェックポイントの能力を大して保持します。文書データに対するRLは、多タイプ的理解、指示従い、関数呼び出しを保持または向上させることを見出しました。Magistral Medium、Mistral Medium 3の上に理由論を学習させたモデルを紹介し、Magistral Small（Apache 2.0）を公開します。Magistral Smallは、Magistral Mediumからの冷やかなスタートデータを含むことで、より強力なモデルとなります。",
      "upvotes": 5,
      "discussionId": "684bbe283b733ba333687152",
      "ai_summary": "Magistral, a scalable reinforcement learning pipeline, demonstrates that RL can enhance multimodal understanding and instruction following in large language models without requiring existing RL traces.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "LLMs",
        "multimodal understanding",
        "instruction following",
        "function calling",
        "cold-start data"
      ]
    },
    "publishedAt": "2025-06-12T13:22:37.000Z",
    "title": "Magistral",
    "summary": "We introduce Magistral, Mistral's first reasoning model and our own scalable\nreinforcement learning (RL) pipeline. Instead of relying on existing\nimplementations and RL traces distilled from prior models, we follow a ground\nup approach, relying solely on our own models and infrastructure. Notably, we\ndemonstrate a stack that enabled us to explore the limits of pure RL training\nof LLMs, present a simple method to force the reasoning language of the model,\nand show that RL on text data alone maintains most of the initial checkpoint's\ncapabilities. We find that RL on text maintains or improves multimodal\nunderstanding, instruction following and function calling. We present Magistral\nMedium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we\nopen-source Magistral Small (Apache 2.0) which further includes cold-start data\nfrom Magistral Medium.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10910.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2746
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09344",
      "authors": [
        {
          "_id": "684ae277dbd21a9cc27b118d",
          "name": "Inclusion AI",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b118e",
          "name": "Biao Gong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b118f",
          "name": "Cheng Zou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1190",
          "name": "Chuanyang Zheng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1191",
          "name": "Chunluan Zhou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1192",
          "name": "Canxiang Yan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1193",
          "name": "Chunxiang Jin",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1194",
          "name": "Chunjie Shen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1195",
          "name": "Dandan Zheng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1196",
          "name": "Fudong Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1197",
          "name": "Furong Xu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1198",
          "name": "GuangMing Yao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b1199",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119a",
          "name": "Jingdong Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119b",
          "name": "Jianxin Sun",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119c",
          "name": "Jiajia Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119d",
          "name": "Jianjiang Zhu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119e",
          "name": "Jun Peng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b119f",
          "name": "Kaixiang Ji",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a0",
          "name": "Kaiyou Song",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a1",
          "name": "Kaimeng Ren",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a2",
          "name": "Libin Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a3",
          "name": "Lixiang Ru",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a4",
          "name": "Lele Xie",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a5",
          "name": "Longhua Tan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a6",
          "name": "Lyuxin Xue",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a7",
          "name": "Lan Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a8",
          "name": "Mochen Bai",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11a9",
          "name": "Ning Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11aa",
          "name": "Pei Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ab",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ac",
          "name": "Qinglong Zhang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ad",
          "name": "Qiang Xu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ae",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11af",
          "name": "Ruijie Xiong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b0",
          "name": "Sirui Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b1",
          "name": "Tinghao Liu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b2",
          "name": "Taisong Li",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b3",
          "name": "Weilong Chai",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b4",
          "name": "Xinyu Xiao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b5",
          "name": "Xiaomei Wang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b6",
          "name": "Xiaoxue Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b7",
          "name": "Xiao Lu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b8",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11b9",
          "name": "Xingning Dong",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11ba",
          "name": "Xuzheng Yu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bb",
          "name": "Yi Yuan",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bc",
          "name": "Yuting Gao",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bd",
          "name": "Yunxiao Sun",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11be",
          "name": "Yipeng Chen",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11bf",
          "name": "Yifei Wu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c0",
          "name": "Yongjie Lyu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c1",
          "name": "Ziping Ma",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c2",
          "name": "Zipeng Feng",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c3",
          "name": "Zhijiang Fang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c4",
          "name": "Zhihao Qiu",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c5",
          "name": "Ziyuan Huang",
          "hidden": false
        },
        {
          "_id": "684ae277dbd21a9cc27b11c6",
          "name": "Zhengyu He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T02:50:49.000Z",
      "submittedOnDailyAt": "2025-06-13T01:53:15.172Z",
      "title": "明-オムニ：感知と生成の統合モデル",
      "submittedOnDailyBy": {
        "_id": "644fcbea4f7316588267dc80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
        "isPro": false,
        "fullname": "Biao Gong",
        "user": "BiaoGong",
        "type": "user"
      },
      "summary": "明-Omniは、画像、テキスト、音声、ビデオを処理でき、同時にスピーチと画像生成に強い実力を示す統一モダルモデルです。明-Omniは、異なるモダルティードからトークンを抽出するための専用エンコーダーを使用し、それらは新たに提案されたモダルティード専用ルーターを掛け付けたMoEアーキテクチャのLingで処理されます。この設計は、統一フレームワーク内で効率的に多モダル入力を処理し、融合することを可能にし、異なるタスクを実行するには別々のモデル、タスク専用のファイナルチューニング、または構造的な再設計を必要としません。重要なことに、明-Omniは、音声と画像生成をサポートすることで、価値を高めます。これは、自然な音声を生成する高級な音声デコーダーと高品質な画像生成を行う明-Lite-Uniの組み合わせで実現され、モデルはコンテキストに関連付けられたチャット、テキストからスピーチの変換、多様な画像編集を行うことができます。実験結果によると、明-Omniはすべてのモダルティードでの統一ポーションと生成に強力な解決策を提供します。特に、明-OmniはGPT-4oと同じモダルティードサポートを実現する最初の開放ソースモデルであり、すべてのコードとモデル重みを公開し、コミュニティでの進歩を促進することを目的としています。",
      "upvotes": 4,
      "discussionId": "684ae277dbd21a9cc27b11c7",
      "ai_summary": "Ming-Omni is a unified multimodal model with dedicated encoders and modality-specific routers that can process images, text, audio, and video, and performs tasks like speech and image generation, context-aware chatting, and versatile image editing.",
      "ai_keywords": [
        "multimodal model",
        "encoders",
        "tokens",
        "MoE architecture",
        "modality-specific routers",
        "audio decoder",
        "Ming-Lite-Uni",
        "context-aware chatting",
        "text-to-speech conversion",
        "image editing",
        "unified perception",
        "generation",
        "open-source"
      ]
    },
    "publishedAt": "2025-06-10T22:50:49.000Z",
    "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
    "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644fcbea4f7316588267dc80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
      "fullname": "Biao Gong",
      "name": "BiaoGong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08060",
      "authors": [
        {
          "_id": "6848e0b042e4f9106973f280",
          "user": {
            "_id": "62f32eab52ad88c930bb3f3b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
            "isPro": true,
            "fullname": "Asankhaya Sharma",
            "user": "codelion",
            "type": "user"
          },
          "name": "Asankhaya Sharma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:08.045Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T08:37:19.000Z",
      "submittedOnDailyAt": "2025-06-13T00:31:17.814Z",
      "title": "推論時によるFine-Tuned Transformerの能力の引き出し",
      "submittedOnDailyBy": {
        "_id": "62f32eab52ad88c930bb3f3b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
        "isPro": true,
        "fullname": "Asankhaya Sharma",
        "user": "codelion",
        "type": "user"
      },
      "summary": "大語言モデルは自然言語処理を変えましたが、規範的な微調校編（SFT）は計算量の多いです。本論文では、理想的な仮定を満たす場合、無限計算コマンドのサポートと微調校編データセットのアクセスを含む前提で、推論時の手法で特にコンテキスト学習（ICL）を用いて、モデルパラメータを変更しない限り、SFTで得られた能力を基礎モデルに近似できることを正式に証明します。これらの結果は、有限なコンテキスト長と部分データセットアクセスの実用的なシナリオに拡張されます。固定された出力長さlの文字生成タスクにおいて、エラー率εでmコンテキスト内の微調校編の行動を近似するためには、ボキャブラリーサイズVと失敗確率δを含むOleft( m V{varepsilon^2} log m{delta} right)または、有界コンテキストの場合、Oleft( l log V{varepsilon^2} log 1{delta} right)のデータセットが十分です。線形分類においては、Oleft( d{varepsilon} right)または、固定されたコンテキストの場合、Oleft( 1{varepsilon^2} log 1{delta} right)のデータセットが十分です。これらの結果は、トーリング完全性に基づくモデルの理論的な基盤を提供し、大語言モデルの資源効率的な採用につなげ、実用的な手法です。",
      "upvotes": 4,
      "discussionId": "6848e0b042e4f9106973f281",
      "githubRepo": "https://github.com/codelion/optillm",
      "ai_summary": "Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.",
      "ai_keywords": [
        "supervised fine-tuning",
        "in-context learning",
        "base transformer model",
        "Turing completeness",
        "retrieval-augmented generation",
        "text generation",
        "linear classification"
      ]
    },
    "publishedAt": "2025-06-09T04:37:19.000Z",
    "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time\n  Techniques",
    "summary": "Large language models have transformed natural language processing, yet\nsupervised fine-tuning (SFT) remains computationally intensive. This paper\nformally proves that capabilities acquired through SFT can be approximated by a\nbase transformer model using inference-time techniques, specifically in-context\nlearning (ICL), without altering model parameters, under idealized assumptions\nincluding unbounded computational resources and access to the fine-tuning\ndataset. We extend these results to practical scenarios with finite context\nlengths and partial dataset access. For text generation tasks with fixed output\nlength l, datasets of size Oleft( m V{varepsilon^2} log\nm{delta} right) or, with bounded context, Oleft( l\nlog V{varepsilon^2} log 1{delta} right) suffice to approximate\nfine-tuned behavior across m contexts within error varepsilon, where V\nis the vocabulary size and delta is the failure probability. For linear\nclassification, datasets of size Oleft( d{varepsilon}\nright) or, with fixed context, Oleft( 1{varepsilon^2} log\n1{delta} right) are sufficient, where d is the input dimension.\nGrounded in the Turing completeness of transformers, these results provide a\ntheoretical foundation for resource-efficient deployment of large language\nmodels, with practical techniques like retrieval-augmented generation bridging\ntheory to real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f32eab52ad88c930bb3f3b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
      "fullname": "Asankhaya Sharma",
      "name": "codelion",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 91
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09952",
      "authors": [
        {
          "_id": "684ae226dbd21a9cc27b107a",
          "user": {
            "_id": "63579b21a8e247a69d4e13de",
            "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
            "isPro": false,
            "fullname": "Ziyi Wang",
            "user": "LavenderLA",
            "type": "user"
          },
          "name": "Ziyi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:40:18.063Z",
          "hidden": false
        },
        {
          "_id": "684ae226dbd21a9cc27b107b",
          "user": {
            "_id": "661cfae9a853782abad2a495",
            "avatarUrl": "/avatars/39723a07bf9efed8278e009fe966d044.svg",
            "isPro": false,
            "fullname": "Yanran Zhang",
            "user": "Yanran21",
            "type": "user"
          },
          "name": "Yanran Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:41:09.847Z",
          "hidden": false
        },
        {
          "_id": "684ae226dbd21a9cc27b107c",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "684ae226dbd21a9cc27b107d",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:23:21.000Z",
      "submittedOnDailyAt": "2025-06-13T06:56:09.722Z",
      "title": "UniPre3D: 3D Point Cloudモデルの統合予測学習とクロスモードガウススプラッティング",
      "submittedOnDailyBy": {
        "_id": "63579b21a8e247a69d4e13de",
        "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
        "isPro": false,
        "fullname": "Ziyi Wang",
        "user": "LavenderLA",
        "type": "user"
      },
      "summary": "点云データのスケール多様性は、3Dビジョンの統一表現学習手法の開発に重大な課題をもたらしています。現在、統一的3Dモデルは少なく、対象モデルとスペースモデルの両方に対して同様に効果的な事前学習方法は存在しません。本論文では、オブジェクトやスペースレベルの点云データの任何スケールや3Dモデルの任何アーキテクチャに無間適用可能な最初の統一的事前学習方法、UniPre3Dを紹介します。我々のアプローチは、事前学習タスクとしてガウスプリミティブを予測し、微分可能なガウススプレッティングを用いて画像をレンダリングし、精確なピクセルレベルのサブジェクトと終端的な最適化を可能にします。また、事前学習タスクの複雑性を調節し、モデルの焦点を幾何構造に向けるために、事前学習された画像モデルからの2D特徴を統合し、既定のテクスチャ知識を統合します。我々の提案方法の普遍的な有効性を証明するために、多様な点云モデルをバックボーンとして、対象モデルとスペースモデルの両方の様々なタスクで極めて広範囲な実験を行いました。コードは、https://github.com/wangzy22/UniPre3D にアクセスできます。",
      "upvotes": 3,
      "discussionId": "684ae226dbd21a9cc27b107e",
      "ai_summary": "UniPre3D is a unified pre-training method for 3D point clouds and models of any scale, using Gaussian primitives and 2D feature integration for effective performance across object and scene tasks.",
      "ai_keywords": [
        "point cloud",
        "3D vision",
        "representation learning",
        "UniPre3D",
        "Gaussian primitives",
        "differentiable Gaussian splatting",
        "pixel-level supervision",
        "end-to-end optimization",
        "2D features",
        "pre-trained image models",
        "geometric structures"
      ]
    },
    "publishedAt": "2025-06-11T13:23:21.000Z",
    "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal\n  Gaussian Splatting",
    "summary": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63579b21a8e247a69d4e13de",
      "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
      "fullname": "Ziyi Wang",
      "name": "LavenderLA",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.09942",
      "authors": [
        {
          "_id": "684ae26adbd21a9cc27b1177",
          "user": {
            "_id": "625a5446f1063e7085d5178a",
            "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
            "isPro": false,
            "fullname": "Hao Peng",
            "user": "Wesleythu",
            "type": "user"
          },
          "name": "Hao Peng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:40:06.456Z",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b1178",
          "name": "Yunjia Qi",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b1179",
          "name": "Xiaozhi Wang",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117a",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117b",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "684ae26adbd21a9cc27b117c",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:10:36.000Z",
      "submittedOnDailyAt": "2025-06-13T00:15:19.828Z",
      "title": "VerIF: インストラクション従順の強化学習における検証工学",
      "submittedOnDailyBy": {
        "_id": "625a5446f1063e7085d5178a",
        "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
        "isPro": false,
        "fullname": "Hao Peng",
        "user": "Wesleythu",
        "type": "user"
      },
      "summary": "強化学習における信頼可能な報酬（RLVR）は、大規模な言語モデル（LLMs）の機能を向上させるための重要な技術となり、証明工程が中心的な役割を果たしています。しかし、強化学習における指示従いの最善策はまだ調査が浅い状態です。本研究では、指示従いにおける強化学習の証明課題を調査し、ルールベースのコード証明とLLMによる証明（例えばQwQ-32B）を組み合わせた証明方法VerIFを提案します。このアプローチを支えるために、高品質な指示従いデータセットVerInstructを構築し、約22,000のインスタンスを含むことにします。VerIFを用いた強化学習を2つのモデルに適用し、代表的な指示従いベンチマークで訓練されたモデルは、様々な性能を向上させ、比較的サイズのモデルの中で最先端の性能を達成し、新たな制約にも広範囲に対応できます。また、その一般的な能力は影響を受けず、RLVRを組み込むことで現在のモデル性能を向上させることができることを示します。本研究では、データセット、コード、モデルを公開し、将来の研究を促進するためにhttps://github.com/THU-KEG/VerIFにアクセスしてください。",
      "upvotes": 3,
      "discussionId": "684ae26adbd21a9cc27b117d",
      "githubRepo": "https://github.com/THU-KEG/VerIF",
      "ai_summary": "VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "RLVR",
        "large language models",
        "LLMs",
        "rule-based code verification",
        "QwQ-32B",
        "instruction-following",
        "VerInstruct",
        "RL training",
        "instruction-following benchmarks",
        "state-of-the-art performance",
        "existing RL recipes"
      ]
    },
    "publishedAt": "2025-06-11T13:10:36.000Z",
    "title": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09942.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625a5446f1063e7085d5178a",
      "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
      "fullname": "Hao Peng",
      "name": "Wesleythu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10953",
      "authors": [
        {
          "_id": "684b9fa13b733ba333687066",
          "user": {
            "_id": "5fa9ff3ea13e063b8b2b60cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
            "isPro": false,
            "fullname": "Xing Han Lù",
            "user": "xhluca",
            "type": "user"
          },
          "name": "Xing Han Lù",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:26.665Z",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687067",
          "name": "Gaurav Kamath",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687068",
          "name": "Marius Mosbach",
          "hidden": false
        },
        {
          "_id": "684b9fa13b733ba333687069",
          "name": "Siva Reddy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T17:53:58.000Z",
      "submittedOnDailyAt": "2025-06-13T02:22:59.640Z",
      "title": "ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェブをアウトプット用意し、アウトプットをウェブ用意することによって、ウェ",
      "submittedOnDailyBy": {
        "_id": "5fa9ff3ea13e063b8b2b60cb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
        "isPro": false,
        "fullname": "Xing Han Lù",
        "user": "xhluca",
        "type": "user"
      },
      "summary": "最近の大語言モデル（LLMs）と多様化モデルの進展により、ウェブアガント（AIシステム）の開発において大きな興味が発生しています。これらのシステムは、自動的にウェブ環境内でタスクを完了することができるものです。ウェブインターフェースとLLMの能力との本質的な違いにより、現在のアプローチは、複雑なウェブインターカプティングに対する自動化においてもっとも大きな課題を抱えています。現在の方法は、DOMツリーの巨大な処理、スクリーンショットに追加情報を追加したものや、ユーザインターフェースを完全に回避するAPIインタラクションによるものが多いです。この立場論文は、ウェブアガント研究のパラダイムシフトを主張します：ウェブアガントが人間向けのインターフェースに適応するように強制しなくてもよいでしょう。代わりに、アガント能力に最適化された新しいインターフェースパラダイムを開発するべきです。ここでは、アガント向けウェブインターフェース（AWI）の概念を紹介し、AWIの設計において安全性、効率性、標準化を強調し、主なスタkeホルダーの利益を考慮した6つの指導原則を掲載します。このリフレンスは、現在のインターフェースの基本的な制限を克服し、より効率的かつ信頼性のある透明なウェブアガントの設計を可能にします。これは、より広いMLコミュニティの協力を必要とするものです。",
      "upvotes": 2,
      "discussionId": "684b9fa13b733ba33368706a",
      "ai_summary": "A new Agentic Web Interface (AWI) design paradigm is proposed to optimize web agents for navigating websites, focusing on safety, efficiency, and standardization to address fundamental interface mismatches.",
      "ai_keywords": [
        "Large Language Models",
        "multimodal",
        "web agents",
        "Agentic Web Interface",
        "AWI",
        "DOM trees",
        "screenshots",
        "API interactions"
      ]
    },
    "publishedAt": "2025-06-12T13:53:58.000Z",
    "title": "Build the web for agents, not agents for the web",
    "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10953.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fa9ff3ea13e063b8b2b60cb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
      "fullname": "Xing Han Lù",
      "name": "xhluca",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07795",
      "authors": [
        {
          "_id": "6848dca942e4f9106973f25c",
          "user": {
            "_id": "6659b410a69183808d04b22f",
            "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
            "isPro": false,
            "fullname": "Xiaotian Ye",
            "user": "Acruxos",
            "type": "user"
          },
          "name": "Xiaotian Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-11T08:35:10.086Z",
          "hidden": false
        },
        {
          "_id": "6848dca942e4f9106973f25d",
          "name": "Mengqi Zhang",
          "hidden": false
        },
        {
          "_id": "6848dca942e4f9106973f25e",
          "name": "Shu Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T14:21:25.000Z",
      "submittedOnDailyAt": "2025-06-13T05:51:30.395Z",
      "title": "LLM Unlearning は形に依存しないべきです。",
      "submittedOnDailyBy": {
        "_id": "6659b410a69183808d04b22f",
        "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
        "isPro": false,
        "fullname": "Xiaotian Ye",
        "user": "Acruxos",
        "type": "user"
      },
      "summary": "大型言語モデル（LLM）の忘却機能は、不適切な知識を削除または抑制することで、有害なものまたは隠しものの情報を制御し、間違いのある利用を防ぐ可能性を提供します。しかし、最近の研究は、実世界のスケーラーでの効果が限られていることを明らかにし、実用的な採用を妨げています。本研究では、複数の下流フェーズの失敗における普遍的な問題を特定しました：現在の忘却方法の効果性は、学習サンプルの形式により強烈に依存し、同じ知識の違う表現に対して一般化できないことが多いです。この問題を正式的にForm-Dependent Biasとして特徴化し、各種の下流タスクでの具体的な表現パターンを系統的に調査しました。この問題の普及度を定量的に評価し、将来の研究にサポートするために、知識表現の変化に対する忘却方法の強固性を評価するための新しいベンチマーク「ORT」を介しました。結果は、現在の技術でForm-Dependent Biasが広く厳しいことを示しています。\n\nLLMの忘却機能は、実世界的な安全的なスケーラーで現れる無数の下流タスクの形に依存しないようにし、間違いのある利用を防ぐことができるようにしなければなりません。この目的に向けて、新しい無学習フリーの方法「Rank-one Concept Redirection (ROCR)」を介し、望ましい解決策のルートとして紹介します。ROCRは、下流タスクの不変量を特定し、特に活性化された危険な概念を忘却するための忘却を行います。そのために、特定の忘却ターゲット概念のモデルパラメータを秒間で変更し、モデルの認識を他の無害な概念にリダイレクトできます。拡大的な実験は、傳統的な方法と比較して忘却効果性を大幅に向上させ、高度な自然的な出力を生成することを示しています。",
      "upvotes": 2,
      "discussionId": "6848dca942e4f9106973f25f",
      "ai_summary": "Form-Dependent Bias limits the effectiveness of LLM unlearning across different knowledge expressions, and Rank-one Concept Redirection (ROCR) is proposed as a form-independent solution that enhances unlearning efficacy.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "unlearning",
        "Form-Dependent Bias",
        "ORT",
        "Rank-one Concept Redirection (ROCR)",
        "downstream tasks",
        "unlearning methods",
        "concept redirection",
        "model parameters",
        "activated dangerous concepts"
      ]
    },
    "publishedAt": "2025-06-09T10:21:25.000Z",
    "title": "LLM Unlearning Should Be Form-Independent",
    "summary": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to generalize to alternate expressions of\nthe same knowledge. We formally characterize this problem as Form-Dependent\nBias and systematically investigate its specific manifestation patterns across\nvarious downstream tasks. To quantify its prevalence and support future\nresearch, we introduce ORT, a novel benchmark designed to evaluate the\nrobustness of unlearning methods against variations in knowledge expression.\nResults reveal that Form-Dependent Bias is both widespread and severe among\ncurrent techniques.\n  We argue that LLM unlearning should be form-independent to address the\nendless forms of downstream tasks encountered in real-world security-critical\nscenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),\na novel training-free method, as a promising solution path. ROCR performs\nunlearning by targeting the invariants in downstream tasks, specifically the\nactivated dangerous concepts. It is capable of modifying model parameters\nwithin seconds to redirect the model's perception of a specific unlearning\ntarget concept to another harmless concept. Extensive experiments demonstrate\nthat ROCR significantly improves unlearning effectiveness compared to\ntraditional methods while generating highly natural outputs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07795.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6659b410a69183808d04b22f",
      "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
      "fullname": "Xiaotian Ye",
      "name": "Acruxos",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06694",
      "authors": [
        {
          "_id": "684ba6bc3b733ba333687093",
          "name": "Yuan Yuan",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687094",
          "name": "Yukun Liu",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687095",
          "name": "Chonghua Han",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687096",
          "user": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "isPro": false,
            "fullname": "Jie Feng",
            "user": "JJ-TMT",
            "type": "user"
          },
          "name": "Jie Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:13.023Z",
          "hidden": false
        },
        {
          "_id": "684ba6bc3b733ba333687097",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/LfNEmKBa7cYZhNNRnZRIx.png"
      ],
      "publishedAt": "2025-06-07T07:19:11.000Z",
      "submittedOnDailyAt": "2025-06-13T02:53:49.430Z",
      "title": "データシロールを破壊する：開放的でスケーラブルな移動基盤を目指して、生成的学習を通じてモデルを構築する",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "基礎モデルは、自然言語処理やコンピュータビジョンの分野を革命的に変えてきた。これらのモデルは、多様なタスクとデータセットに対する一般的な学習を可能にしている。しかし、人間の移動に関する基礎モデルの構築は、移動データの隠れらしさとそれによる機関間のデータシルオールの存在により難しい。この隙を埋めるために、私たちは、生成的な継続的学習を介した移動ファンダメンタルモデルの訓練のスケーラブルなプライバシー保存フレームワーク「MoveGCL」を提案しています。ラボーグラデーションされたデータを共有しないように、MoveGCLは、フリーズンド教師モデルから生成された合成的なトラジェクトを再演して分散的で進歩的なモデル進化を可能にし、カタストロフィックフォーゲッティングを軽減するティアリゼット戦略を通じて知識保存を強化しています。移動パターンの異なりを対処するために、MoveGCLは、移動に関する知識を持つエクスパートルーティング機構を持つMixture-of-Experts Transformerを採用し、階層的な進歩的なアダプタイブスティックを用いて継続的な更新を安定化しています。6つの実世界的な都市データセットにおける実験は、MoveGCLは、ジョイントトレーニングと同等の性能を達成し、フェデレートローディングベースラインニングを大幅に超えることを示し、強いプライバシープロテクションを提供しています。MoveGCLは、移動に関する基礎モデルの開発の重要なステップを記録し、基礎モデルの時代における開放的でスケーラブルなプライバシー保存モデル開発の実用的なプランを提供しています。",
      "upvotes": 2,
      "discussionId": "684ba6bc3b733ba333687098",
      "githubRepo": "https://github.com/ScottLiu2003/MoveGCL",
      "ai_summary": "MoveGCL is a privacy-preserving framework using generative continual learning and a Mixture-of-Experts Transformer for training mobility foundation models without sharing raw data.",
      "ai_keywords": [
        "generative continual learning",
        "privacy-preserving",
        "Mixture-of-Experts Transformer",
        "mobility-aware expert routing mechanism",
        "layer-wise progressive adaptation",
        "catastrophic forgetting",
        "federated learning"
      ]
    },
    "publishedAt": "2025-06-07T03:19:11.000Z",
    "title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation\n  Models via Generative Continual Learning",
    "summary": "Foundation models have revolutionized fields such as natural language\nprocessing and computer vision by enabling general-purpose learning across\ndiverse tasks and datasets. However, building analogous models for human\nmobility remains challenging due to the privacy-sensitive nature of mobility\ndata and the resulting data silos across institutions. To bridge this gap, we\npropose MoveGCL, a scalable and privacy-preserving framework for training\nmobility foundation models via generative continual learning. Without sharing\nraw data, MoveGCL enables decentralized and progressive model evolution by\nreplaying synthetic trajectories generated from a frozen teacher model, and\nreinforces knowledge retention through a tailored distillation strategy that\nmitigates catastrophic forgetting. To address the heterogeneity of mobility\npatterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a\nmobility-aware expert routing mechanism, and employs a layer-wise progressive\nadaptation strategy to stabilize continual updates. Experiments on six\nreal-world urban datasets demonstrate that MoveGCL achieves performance\ncomparable to joint training and significantly outperforms federated learning\nbaselines, while offering strong privacy protection. MoveGCL marks a crucial\nstep toward unlocking foundation models for mobility, offering a practical\nblueprint for open, scalable, and privacy-preserving model development in the\nera of foundation models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/LfNEmKBa7cYZhNNRnZRIx.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.10036",
      "authors": [
        {
          "_id": "684bc8db3b733ba33368718c",
          "name": "Javad Rajabi",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718d",
          "name": "Soroush Mehraban",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718e",
          "user": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "isPro": false,
            "fullname": "Seyedmorteza Sadat",
            "user": "msadat97",
            "type": "user"
          },
          "name": "Seyedmorteza Sadat",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:38:32.584Z",
          "hidden": false
        },
        {
          "_id": "684bc8db3b733ba33368718f",
          "name": "Babak Taati",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T21:25:46.000Z",
      "submittedOnDailyAt": "2025-06-13T05:21:38.595Z",
      "title": "トークンパーバーバージャングイドフォライブドモデル",
      "submittedOnDailyBy": {
        "_id": "63b4b02a103617b0a5b0ee2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
        "isPro": false,
        "fullname": "Seyedmorteza Sadat",
        "user": "msadat97",
        "type": "user"
      },
      "summary": "Classifier-free guidance (CFG)は、現代のディフュージョンモデルの重要な構成要素として、生成品質と入力条件との一致性を向上させるために不可欠である。しかし、CFGは特定のトレーニング手順が必要であり、条件付き生成に限定されている。これらの制限を解決するために、我々はToken Perturbation Guidance (TPG)という新しい方法を提案しています。TPGは、ディフュージョンネットワーク内の中間トークン表現に直接的にポーランス行列を適用しています。TPGは、ノルム保持したシャッフル操作を使用して、構造変更を伴っても生成品質を向上させる効果的で安定したガイド信号を提供します。このように、TPGはトレーニング不要で入力条件に依存しないことで、条件付き生成と非条件付き生成に適用できます。また、TPGが提供するガイド項を進一度に分析し、現在のトレーニング不要ガイドメソッドと比較してCFGのような影響を示すことを示しています。SDXLとStable Diffusion 2.1においての拡張的な実験は、TPGはSDXLベースラインと比べて非条件付き生成のFIDに近似2倍の改善を実現し、提示の一致性にもCFGと類似した性能を示しています。これらの結果は、TPGが一般的で条件依存性なしのガイドメソッドであり、ディフュージョンモデルの広範囲にCFGのような利点を提供することを確立しています。コードは以下のURLで提供されています。\nhttps://github.com/TaatiTeam/Token-Perturbation-Guidance",
      "upvotes": 1,
      "discussionId": "684bc8db3b733ba333687190",
      "githubRepo": "https://github.com/TaatiTeam/Token-Perturbation-Guidance",
      "ai_summary": "Token Perturbation Guidance (TPG) enhances diffusion models with condition-agnostic, training-free guidance, similar to classifier-free guidance (CFG), without requiring architectural changes.",
      "ai_keywords": [
        "classifier-free guidance (CFG)",
        "Token Perturbation Guidance (TPG)",
        "perturbation matrices",
        "intermediate token representations",
        "norm-preserving shuffling",
        "FID",
        "prompt alignment",
        "SDXL",
        "Stable Diffusion 2.1"
      ]
    },
    "publishedAt": "2025-06-10T17:25:46.000Z",
    "title": "Token Perturbation Guidance for Diffusion Models",
    "summary": "Classifier-free guidance (CFG) has become an essential component of modern\ndiffusion models to enhance both generation quality and alignment with input\nconditions. However, CFG requires specific training procedures and is limited\nto conditional generation. To address these limitations, we propose Token\nPerturbation Guidance (TPG), a novel method that applies perturbation matrices\ndirectly to intermediate token representations within the diffusion network.\nTPG employs a norm-preserving shuffling operation to provide effective and\nstable guidance signals that improve generation quality without architectural\nchanges. As a result, TPG is training-free and agnostic to input conditions,\nmaking it readily applicable to both conditional and unconditional generation.\nWe further analyze the guidance term provided by TPG and show that its effect\non sampling more closely resembles CFG compared to existing training-free\nguidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1\nshow that TPG achieves nearly a 2times improvement in FID for unconditional\ngeneration over the SDXL baseline, while closely matching CFG in prompt\nalignment. These results establish TPG as a general, condition-agnostic\nguidance method that brings CFG-like benefits to a broader class of diffusion\nmodels. The code is available at\nhttps://github.com/TaatiTeam/Token-Perturbation-Guidance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10036.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b4b02a103617b0a5b0ee2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
      "fullname": "Seyedmorteza Sadat",
      "name": "msadat97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08373",
      "authors": [
        {
          "_id": "684ae1f3dbd21a9cc27b0f32",
          "name": "Kevin Galim",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f33",
          "name": "Ethan Ewer",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f34",
          "name": "Wonjun Kang",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f35",
          "name": "Minjae Lee",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f36",
          "name": "Hyung Il Koo",
          "hidden": false
        },
        {
          "_id": "684ae1f3dbd21a9cc27b0f37",
          "name": "Kangwook Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T02:37:46.000Z",
      "submittedOnDailyAt": "2025-06-13T00:35:36.456Z",
      "title": "ドラフトベースの近似推論を用いたLLM",
      "submittedOnDailyBy": {
        "_id": "630c90123dc31beba6e8f406",
        "avatarUrl": "/avatars/2188b41fff122d4f5683b46c529ed79d.svg",
        "isPro": false,
        "fullname": "Kevin Galim",
        "user": "kev95",
        "type": "user"
      },
      "summary": "長文脈の大規模言語モデル（LLMs）の推論を最適化することは、Transformerの二次元計算量と線形メモリ複雑性により重要になっています。現在の近似手法では、キーバリュー（KV）キャッシュの切り捨て、関数參数の稀疏化、プロンプトの圧縮などが、トークンまたはKVペアの重要性を粗略に予測するものが主な手法です。私たちは、小さなドラフトモデルを使用してトークンとKVペアの重要性を更に正確に予測するための新しいフレームワークを提案します。特に、私たちは以下の2つの実装を提案します：（i）SpecKVは、ドラフト出力を使用して、より効果的なKVキャッシュの切り捨てに向けて各KVペアの重要性を正確に評価します。（ii）SpecPCは、ドラフトモデルの注意活性化を使用して、重要なプロンプトトークンを特定して切り捨てます。私たちの知識によると、これはドラフトモデルを使用して近似LLM推論加速に適用する最初の研究です。これは、伝統的な無失格的推測解码よりもドラフトモデルの役割を拡張します。理論的および実験的な分析を用いて、私たちの方法を説明し、ドラフトモデルと目標モデルの注意パターンの関連性を示します。長文脈ベンチマークでの拡濶な実験により、私たちの方法は、既存の基準と比較しても、精度が高く、メモリ使用量、ラタンシー、サイクル数の改善が同じです。私たちのコードは、https://github.com/furiosa-ai/draft-based-approx-llm に公開されています。",
      "upvotes": 1,
      "discussionId": "684ae1f3dbd21a9cc27b0f38",
      "ai_summary": "A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.",
      "ai_keywords": [
        "Large Language Models",
        "Transformers",
        "key-value cache dropping",
        "sparse attention",
        "prompt compression",
        "draft models",
        "SpecKV",
        "SpecPC",
        "attention activations",
        "long-context benchmarks"
      ]
    },
    "publishedAt": "2025-06-09T22:37:46.000Z",
    "title": "Draft-based Approximate Inference for LLMs",
    "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08373.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630c90123dc31beba6e8f406",
      "avatarUrl": "/avatars/2188b41fff122d4f5683b46c529ed79d.svg",
      "fullname": "Kevin Galim",
      "name": "kev95",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08234",
      "authors": [
        {
          "_id": "684ae1dddbd21a9cc27b0edc",
          "name": "Yu-Ang Lee",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0edd",
          "name": "Guan-Ting Yi",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0ede",
          "name": "Mei-Yi Liu",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0edf",
          "name": "Jui-Chao Lu",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0ee0",
          "name": "Guan-Bo Yang",
          "hidden": false
        },
        {
          "_id": "684ae1dddbd21a9cc27b0ee1",
          "name": "Yun-Nung Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T21:04:14.000Z",
      "submittedOnDailyAt": "2025-06-13T07:22:29.081Z",
      "title": "化合物AIシステム最適化：方法、課題と未来の方向への概覧",
      "submittedOnDailyBy": {
        "_id": "6615752da15c52fa7ab3e2f7",
        "avatarUrl": "/avatars/37e72cfb829a42630d229080ad8d60f3.svg",
        "isPro": false,
        "fullname": "Lee",
        "user": "Speeeed",
        "type": "user"
      },
      "summary": "最近の大規模言語モデル（LLMs）とAIシステムの進歩は、複雑なAIワークフローの設計と最適化におけるパラダイムの変更を引き起こしました。複数のコンポーネントを統合することで、複合AIシステムは複雑なタスクを行うことになりました。しかし、これらのシステムが複雑化するにつれて、個々のコンポーネントの最適化だけでなく、その相互作用の最適化において新たな課題が発生します。テキストに基づく監督的微調節（SFT）と強化学習（RL）などの伝統的な最適化方法は基盤として残っていますが、自然言語フィードバックの増加は、特に非微分可計算なシステムの最適化において、希望のある新しいアプローチを導き出しました。本論文は、複合AIシステムの最適化における最近の進歩をシステマティックに調査し、数値的および言語ベースの技術を含むものです。複合AIシステムの最適化の概念を形式化し、既存の方法を数えられる鍵の次元に沿ってクラスフィードし、この急速に変化する分野における開放的研究課題と将来の方向を明らかにします。調査した論文のリストは、https://github.com/MiuLab/AISysOpt-Survey で公開しています。",
      "upvotes": 1,
      "discussionId": "684ae1dedbd21a9cc27b0ee2",
      "ai_summary": "Recent advancements in optimizing compound AI systems highlight challenges in integrating various components, with an emphasis on natural language feedback methods for non-differentiable systems.",
      "ai_keywords": [
        "large language models",
        "AI systems",
        "compound AI systems",
        "supervised fine-tuning",
        "reinforcement learning",
        "natural language feedback",
        "non-differentiable systems"
      ]
    },
    "publishedAt": "2025-06-09T17:04:14.000Z",
    "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and\n  Future Directions",
    "summary": "Recent advancements in large language models (LLMs) and AI systems have led\nto a paradigm shift in the design and optimization of complex AI workflows. By\nintegrating multiple components, compound AI systems have become increasingly\nadept at performing sophisticated tasks. However, as these systems grow in\ncomplexity, new challenges arise in optimizing not only individual components\nbut also their interactions. While traditional optimization methods such as\nsupervised fine-tuning (SFT) and reinforcement learning (RL) remain\nfoundational, the rise of natural language feedback introduces promising new\napproaches, especially for optimizing non-differentiable systems. This paper\nprovides a systematic review of recent progress in optimizing compound AI\nsystems, encompassing both numerical and language-based techniques. We\nformalize the notion of compound AI system optimization, classify existing\nmethods along several key dimensions, and highlight open research challenges\nand future directions in this rapidly evolving field. A list of surveyed papers\nis publicly available at https://github.com/MiuLab/AISysOpt-Survey.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08234.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6615752da15c52fa7ab3e2f7",
      "avatarUrl": "/avatars/37e72cfb829a42630d229080ad8d60f3.svg",
      "fullname": "Lee",
      "name": "Speeeed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06950",
      "authors": [
        {
          "_id": "684ae1fbdbd21a9cc27b0f51",
          "name": "Do Xuan Long",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f52",
          "name": "Duy Dinh",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f53",
          "name": "Ngoc-Hai Nguyen",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f54",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f55",
          "name": "Nancy F. Chen",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f56",
          "name": "Shafiq Joty",
          "hidden": false
        },
        {
          "_id": "684ae1fbdbd21a9cc27b0f57",
          "name": "Min-Yen Kan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-07T23:19:27.000Z",
      "submittedOnDailyAt": "2025-06-13T03:14:57.042Z",
      "title": "何かが良い自然言語プロンプトを作るのに必要な要素は何ですか？",
      "submittedOnDailyBy": {
        "_id": "63a9a0d13453852ef53c0b37",
        "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
        "isPro": false,
        "fullname": "Do Xuan Long",
        "user": "dxlong2000",
        "type": "user"
      },
      "summary": "LLMが人間ほぼ同じようなコミュニケーションに向けて進化し、プロンプトは決定的な要素として出現しました。しかし、自然言語プロンプトを定量化するための具体的な概念的な共通認識は限られています。私たちは、2022年至2025年の先進的なNLPとAIコンファレンスからの150点以上のプロンプト関連の論文とブログを調査して、この問題を解決するために試みます。私たちは、21プロパティを6ディメンションに分類したプロパティと人間センタリックなフレームワークを提案します。そして、現在の研究がLLMにどのような影響を与えているかを調べ、モデルやタスクのバランスが不均衡であり、研究の欠陥が大きいことを明らかにします。また、高品質の自然言語プロンプトのプロパティ間の相関を分析し、プロンプトの推奨を得ます。そして、理由論タスクでの多プロパティプロンプトの強化を実験的に調べ、単一プロパティの強化が最も大きな影響を与えることを見出します。最後に、プロパティを拡張したプロンプトに対する教訓チューニングで理由論モデルが改善されることを見出します。我々の発見は、プロパティセンタリックなプロンプト評価と最適化の基盤を奠め、人間とAIのコミュニケーションの間の隙を埋め、新たなプロンプト研究の方向を開拓します。",
      "upvotes": 1,
      "discussionId": "684ae1fbdbd21a9cc27b0f58",
      "ai_summary": "A framework for evaluating and optimizing natural language prompts in large language models is proposed, revealing correlations between prompt properties and their impact on reasoning tasks.",
      "ai_keywords": [
        "large language models",
        "prompting",
        "meta-analysis",
        "property-centric framework",
        "instruction-tuning",
        "reasoning tasks"
      ]
    },
    "publishedAt": "2025-06-07T19:19:27.000Z",
    "title": "What Makes a Good Natural Language Prompt?",
    "summary": "As large language models (LLMs) have progressed towards more human-like and\nhuman--AI communications have become prevalent, prompting has emerged as a\ndecisive component. However, there is limited conceptual consensus on what\nexactly quantifies natural language prompts. We attempt to address this\nquestion by conducting a meta-analysis surveying more than 150\nprompting-related papers from leading NLP and AI conferences from 2022 to 2025\nand blogs. We propose a property- and human-centric framework for evaluating\nprompt quality, encompassing 21 properties categorized into six dimensions. We\nthen examine how existing studies assess their impact on LLMs, revealing their\nimbalanced support across models and tasks, and substantial research gaps.\nFurther, we analyze correlations among properties in high-quality natural\nlanguage prompts, deriving prompting recommendations. We then empirically\nexplore multi-property prompt enhancements in reasoning tasks, observing that\nsingle-property enhancements often have the greatest impact. Finally, we\ndiscover that instruction-tuning on property-enhanced prompts can result in\nbetter reasoning models. Our findings establish a foundation for\nproperty-centric prompt evaluation and optimization, bridging the gaps between\nhuman--AI communication and opening new prompting research directions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06950.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a9a0d13453852ef53c0b37",
      "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
      "fullname": "Do Xuan Long",
      "name": "dxlong2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06561",
      "authors": [
        {
          "_id": "684b88113b733ba333686fc7",
          "name": "Ho Yin 'Sam' Ng",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fc8",
          "name": "Ting-Yao Hsu",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fc9",
          "name": "Aashish Anantha Ramakrishnan",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fca",
          "name": "Branislav Kveton",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcb",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcc",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-13T07:39:49.692Z",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcd",
          "name": "Dongwon Lee",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fce",
          "name": "Tong Yu",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fcf",
          "name": "Sungchul Kim",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fd0",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "684b88113b733ba333686fd1",
          "name": "Ting-Hao 'Kenneth' Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T22:16:16.000Z",
      "submittedOnDailyAt": "2025-06-13T00:38:33.715Z",
      "title": "LaMP-Cap: 多モーダルフィギュアプロフィーによる個人化フィギュアキャプション生成",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "グラフのタイトルは、読者がグラフの主なメッセージを理解し、記憶するために重要です。多くのモデルがこれらのタイトルを生成するために開発されました、これらは著者がより良い質のタイトルを作成することを容易にすることを助けます。しかし、著者は、グラフの書き込みスタイルと領域のスタイルに合わせるために、標準的なAIゲンダードタイトルを編集する必要があります、これにより個別化の必要性が明らかになります。言語モデルの個別化（LaMP）の進歩にもかかわらず、これらのテクノロジーはほとんど言語だけの設定に焦点を当てていて、入力とプロファイルが多タイプの場合を少なく調査しません。本論文では、多タイプのグラフプロファイルを用いた個別化グラフタイトル生成のデータセット、LaMP-Capを紹介します。ターゲットのグラフに対して、LaMP-Capは、グラフ画像などの必要な入力を提供しながら、同じ文書からの他の3つのグラフの画像、タイトル、グラフを提及した段落をも含むプロファイルを提供します、これをコンテキストを特徴化するために。4つのLLMとの実験は、プロファイル情報の使用はタイトルを生成することを助け、元の著者が書いたものに近づけることができることを示しました。消去試験は、プロファイルの画像は、グラフを提及した段落よりもより役立ち、多タイプのプロファイルを使用することが言語だけのものよりも優れていることを明らかにしました。",
      "upvotes": 1,
      "discussionId": "684b88123b733ba333686fd2",
      "ai_summary": "LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.",
      "ai_keywords": [
        "LaMP-Cap",
        "personalized figure caption generation",
        "multimodal figures",
        "figure profiles",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-06-06T18:16:16.000Z",
    "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles",
    "summary": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06561.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05982",
      "authors": [
        {
          "_id": "684b86913b733ba333686fb8",
          "name": "Zonglin Wu",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fb9",
          "name": "Yule Xue",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fba",
          "name": "Xin Wei",
          "hidden": false
        },
        {
          "_id": "684b86913b733ba333686fbb",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T11:02:01.000Z",
      "submittedOnDailyAt": "2025-06-13T00:33:34.648Z",
      "title": "MCA-Bench: VLMベース攻撃に対するCAPTCHAの鉤力評価のための多様評価基準",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "自動化攻撃手法が急速に進化している中、CAPTCHAは悪意のボットを防ぐための重要な防御機構です。しかし、現在のCAPTCHAスキームは多様なモダリティーを含み、静的に歪みされた文字や雑蔓された画像から、交互させたクリック、スライディングパズル、ロジックベースの質問まで幅広く範囲を広げていますが、コミュニティはまだ一つの統一的な、大規模な、多モダリティーのベンチマークを持っていません。この欠点を解決するために、MCA-Benchという、一つの評価プロトコルに統合された多様なCAPTCHAタイプを含む詳細で再現可能なベンチマークシステムを紹介します。共有したビジョン言語モデルのバックボードを利用して、各CAPTCHAカテゴリに専門化されたクラッシングアガントを微調整し、一貫した、クロスモダリティー評価を可能にします。拡大的な実験により、MCA-Benchは現代のCAPTCHAデザインの脆弱性スペクトラムを効果的にバンドワードに変換し、そして重要なポイントとして、チャレンジの複雑性、交互作用の深さ、モデルの解決可能性との相互関係を最初の定量的な分析を提供します。これらの発見に基づいて、3つの行動可能なデザイン原則を提案し、主な開放されている課題を特定し、システム的なCAPTCHAの強化、公平なベンチマーク、およびより広いコミュニティの協力の基盤を立てます。データセットとコードはオンラインで提供されています。",
      "upvotes": 1,
      "discussionId": "684b86923b733ba333686fbc",
      "ai_summary": "MCA-Bench provides a unified benchmark for evaluating CAPTCHA security using a shared vision-language model and attackers specialized for each type of CAPTCHA.",
      "ai_keywords": [
        "vision-language model",
        "CAPTCHA",
        "benchmark",
        "evaluation protocol",
        "cracking agents",
        "vulnerability spectrum",
        "challenge complexity",
        "interaction depth",
        "model solvability"
      ]
    },
    "publishedAt": "2025-06-06T07:02:01.000Z",
    "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks",
    "summary": "As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking\nsuite that integrates heterogeneous CAPTCHA types into a single evaluation\nprotocol. Leveraging a shared vision-language model backbone, we fine-tune\nspecialized cracking agents for each CAPTCHA category, enabling consistent,\ncross-modal assessments. Extensive experiments reveal that MCA-Bench\neffectively maps the vulnerability spectrum of modern CAPTCHA designs under\nvaried attack settings, and crucially offers the first quantitative analysis of\nhow challenge complexity, interaction depth, and model solvability interrelate.\nBased on these findings, we propose three actionable design principles and\nidentify key open challenges, laying the groundwork for systematic CAPTCHA\nhardening, fair benchmarking, and broader community collaboration. Datasets and\ncode are available online.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05982.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10378",
      "authors": [
        {
          "_id": "684bb08e3b733ba3336870bf",
          "name": "Jikai Jin",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c0",
          "name": "Vasilis Syrgkanis",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c1",
          "name": "Sham Kakade",
          "hidden": false
        },
        {
          "_id": "684bb08e3b733ba3336870c2",
          "name": "Hanlin Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-12T06:07:42.000Z",
      "submittedOnDailyAt": "2025-06-13T03:36:14.455Z",
      "title": "ディスカバリング・ヒューリラクシャル・レーテント・キャパビリティーフォラムモデルによるカウスレイプレジュレーション学習",
      "submittedOnDailyBy": {
        "_id": "624054bcc2c17da6a63eb539",
        "avatarUrl": "/avatars/bf52dc0683b4100733f8696a97696d0e.svg",
        "isPro": false,
        "fullname": "hlzhang109",
        "user": "hlzhang109",
        "type": "user"
      },
      "summary": "信頼性のある言語モデル能力の評価は、行動可能な洞察を提供することを目的としたモデル開発において重要である。しかし、この領域では、複雑な混雑効果と複数のモデルの拡大訓練に伴う計算費用の高さなど、厳密な因果的な評価には大きな方法学的な課題がある。これらの課題を解決するために、我々は、観測されたベンチマーク性能を数少ない潜在的な能力因子の線形変換としてモデル化する因果表現学習フレームワークを提案している。重要なことに、これらの潜在的な因子は、基礎モデルを共通の混雑劑として適切に制御した後に因果的に相互関係を持つものと識別される。このアプローチを、Open LLM Leaderboardから6つのベンチマークを挙げる評価を通じた1500点以上のモデルを含む詳細なデータセットに適用した結果、観測された性能の変化を信頼性のある3ノードの線形因果構造で説明できることを明らかにした。この因果構造の進一づの解釈は、簡単な数値のランキングよりも科学的な洞察を提供し、特に、一般的な問題解決能力から始まり、指示従いの熟練を通じて、数学的な理由論能力に至る明確な因果的な方向を示している。我々の結果は、評価中に基礎モデルの変化を慎重に制御する重要性を強調し、潜在的なモデル能力の間の潜在的な因果関係を正確に明らかにするためには不可欠なステップであることを説明している。",
      "upvotes": 0,
      "discussionId": "684bb08e3b733ba3336870c3",
      "ai_summary": "A causal representation learning framework identifies a concise causal structure to explain performance variations in language models across benchmarks by controlling for base model variations.",
      "ai_keywords": [
        "causal representation learning",
        "latent capability factors",
        "causal interrelated",
        "base model confounder",
        "Open LLM Leaderboard",
        "linear causal structure",
        "general problem-solving capabilities",
        "instruction-following proficiency",
        "mathematical reasoning ability"
      ]
    },
    "publishedAt": "2025-06-12T02:07:42.000Z",
    "title": "Discovering Hierarchical Latent Capabilities of Language Models via\n  Causal Representation Learning",
    "summary": "Faithful evaluation of language model capabilities is crucial for deriving\nactionable insights that can inform model development. However, rigorous causal\nevaluations in this domain face significant methodological challenges,\nincluding complex confounding effects and prohibitive computational costs\nassociated with extensive retraining. To tackle these challenges, we propose a\ncausal representation learning framework wherein observed benchmark performance\nis modeled as a linear transformation of a few latent capability factors.\nCrucially, these latent factors are identified as causally interrelated after\nappropriately controlling for the base model as a common confounder. Applying\nthis approach to a comprehensive dataset encompassing over 1500 models\nevaluated across six benchmarks from the Open LLM Leaderboard, we identify a\nconcise three-node linear causal structure that reliably explains the observed\nperformance variations. Further interpretation of this causal structure\nprovides substantial scientific insights beyond simple numerical rankings:\nspecifically, we reveal a clear causal direction starting from general\nproblem-solving capabilities, advancing through instruction-following\nproficiency, and culminating in mathematical reasoning ability. Our results\nunderscore the essential role of carefully controlling base model variations\nduring evaluation, a step critical to accurately uncovering the underlying\ncausal relationships among latent model capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "624054bcc2c17da6a63eb539",
      "avatarUrl": "/avatars/bf52dc0683b4100733f8696a97696d0e.svg",
      "fullname": "hlzhang109",
      "name": "hlzhang109",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08862",
      "authors": [
        {
          "_id": "684ae26ddbd21a9cc27b117f",
          "name": "Zike Wu",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1180",
          "name": "Qi Yan",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1181",
          "name": "Xuanyu Yi",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1182",
          "name": "Lele Wang",
          "hidden": false
        },
        {
          "_id": "684ae26ddbd21a9cc27b1183",
          "name": "Renjie Liao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T14:52:36.000Z",
      "submittedOnDailyAt": "2025-06-13T07:33:07.278Z",
      "title": "StreamSplat: 向けて、未校正のビデオストリームからのオンラインダイナミック3D再構築へ",
      "submittedOnDailyBy": {
        "_id": "648058ff8c6a3b8f11f77893",
        "avatarUrl": "/avatars/043c832314a8d6713af90d7c255fc2f2.svg",
        "isPro": false,
        "fullname": "Wu Zike",
        "user": "Nickwzk",
        "type": "user"
      },
      "summary": "動的3Dスペースの無チャレンジテックスのビデオストリームからの時間単位での実時間再構築は、複数の実世界のアプリケーションに重要です。しかし、既存の方法は、3つの主要な課題を同時に解決することが難しい：1) 無チャレンジテックスの入力を時間単位で処理する、2) 動的なスペースの進化を正確にモデル化する、3) 長期間の安定性と計算効率を維持する。これに対して、我々は、任意の長さの無チャレンジテックスのビデオストリームを動的な3Dガウススプレッド（3DGS）表現に変換するための最初の完全なフィードフォワードフレームワークを導入します。これは、時間的に近い観測からスペースの動きを復元することができる。我々は、2つの鍵の技術的なイノベーションを提案します：静的エンコーダーでの3DGS位置予測の確率的なサンプリング機構、および動的デコーダーでのバイデリクシャルな変形フィールド、これらは動的なモデリングに強固で効率的なものになります。静的および動的ベンチマーク上の拡張な実験により、StreamSplatは、前の仕事と比較して、再構築品質と動的なスペースモデリングにおいて一貫して優れていることが示され、特に、任意の長さのビデオストリームのオンライン再構築をサポートします。コードとモデルは、https://github.com/nickwzk/StreamSplat に提供されています。",
      "upvotes": 0,
      "discussionId": "684ae26ddbd21a9cc27b1184",
      "ai_summary": "StreamSplat, a fully feed-forward framework, addresses real-time 3D scene reconstruction from uncalibrated video with accurate dynamics and long-term stability.",
      "ai_keywords": [
        "3D Gaussian Splatting",
        "3DGS",
        "probabilistic sampling mechanism",
        "bidirectional deformation field",
        "online reconstruction"
      ]
    },
    "publishedAt": "2025-06-10T10:52:36.000Z",
    "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated\n  Video Streams",
    "summary": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams\nis crucial for numerous real-world applications. However, existing methods\nstruggle to jointly address three key challenges: 1) processing uncalibrated\ninputs in real time, 2) accurately modeling dynamic scene evolution, and 3)\nmaintaining long-term stability and computational efficiency. To this end, we\nintroduce StreamSplat, the first fully feed-forward framework that transforms\nuncalibrated video streams of arbitrary length into dynamic 3D Gaussian\nSplatting (3DGS) representations in an online manner, capable of recovering\nscene dynamics from temporally local observations. We propose two key technical\ninnovations: a probabilistic sampling mechanism in the static encoder for 3DGS\nposition prediction, and a bidirectional deformation field in the dynamic\ndecoder that enables robust and efficient dynamic modeling. Extensive\nexperiments on static and dynamic benchmarks demonstrate that StreamSplat\nconsistently outperforms prior works in both reconstruction quality and dynamic\nscene modeling, while uniquely supporting online reconstruction of arbitrarily\nlong video streams. Code and models are available at\nhttps://github.com/nickwzk/StreamSplat.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08862.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648058ff8c6a3b8f11f77893",
      "avatarUrl": "/avatars/043c832314a8d6713af90d7c255fc2f2.svg",
      "fullname": "Wu Zike",
      "name": "Nickwzk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]