[
  {
    "paper": {
      "id": "2504.17761",
      "authors": [
        {
          "_id": "680af2df3b93130c9b2b90a7",
          "name": "Shiyu Liu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90a8",
          "name": "Yucheng Han",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90a9",
          "name": "Peng Xing",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90aa",
          "name": "Fukun Yin",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ab",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ac",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:36.757Z",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ad",
          "name": "Jiaqi Liao",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ae",
          "name": "Yingming Wang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90af",
          "name": "Honghao Fu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b0",
          "name": "Chunrui Han",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b1",
          "name": "Guopeng Li",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b2",
          "name": "Yuang Peng",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b3",
          "name": "Quan Sun",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b4",
          "name": "Jingwei Wu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b5",
          "name": "Yan Cai",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b6",
          "name": "Zheng Ge",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b7",
          "name": "Ranchen Ming",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b8",
          "name": "Lei Xia",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90b9",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90ba",
          "name": "Yibo Zhu",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bb",
          "name": "Binxing Jiao",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bc",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90bd",
          "user": {
            "_id": "63417332c5565a4b8d43a0d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
            "isPro": false,
            "fullname": "Gang Yu",
            "user": "skicy",
            "type": "user"
          },
          "name": "Gang Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:34.650Z",
          "hidden": false
        },
        {
          "_id": "680af2df3b93130c9b2b90be",
          "name": "Daxin Jiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/lRHqqMDr1SxDfhelcO26J.mp4"
      ],
      "publishedAt": "2025-04-24T17:25:12.000Z",
      "submittedOnDailyAt": "2025-04-25T01:12:44.269Z",
      "title": "ステップ1X-エディット：一般画像編集の実用的フレームワーク",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "近年、画像編集モデルは驚異的な迅速な進歩を見せています。最近、GPT-4oやGemini2 Flashといった先端の多モデルの公開で、高度な画像編集能力が導入されました。これらのモデルは、ユーザー駆動の編集要求を満たすために誇り高さのある能力を示し、画像操作領域では大きな進歩を示しています。しかし、これらのクローズドソースモデルとの間には大きな間違いがあります。そこで、本論文では、Step1X-Editという最先端の画像編集モデルをリリースし、クローズドソースモデルと同等の性能を提供できるように試みます。具体的には、参考画像とユーザーの編集指示をMultimodal LLMで処理し、潜在的な埋め込みを抽出し、ディフューション画像デコーダーと統合して目標画像を取得します。モデルの訓練には、高品質なデータセットを生成するためのデータ生成パイプラインを構築し、評価には、実世界的なユーザー指示に基づく新しいベンチマークGEdit-Benchを開発しました。GEdit-Benchでの実験結果は、Step1X-Editが現在の開放ソースベースのベースラインより大幅に優位を示し、先進的なクローズドソースモデルの性能に近づき、画像編集領域に大きな貢献を果たします。",
      "upvotes": 38,
      "discussionId": "680af2e13b93130c9b2b9132",
      "githubRepo": "https://github.com/stepfun-ai/Step1X-Edit",
      "ai_keywords": [
        "Multimodal LLM",
        "latent embedding",
        "diffusion image decoder",
        "data generation pipeline",
        "GEdit-Bench",
        "real-world user instructions"
      ]
    },
    "publishedAt": "2025-04-24T13:25:12.000Z",
    "title": "Step1X-Edit: A Practical Framework for General Image Editing",
    "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/lRHqqMDr1SxDfhelcO26J.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17761.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/udUHjj6fby82zh8LDjXhL.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17502",
      "authors": [
        {
          "_id": "680b44fb426b7d5bc2018c75",
          "user": {
            "_id": "631da07f6d6a5870f3d2c375",
            "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
            "isPro": false,
            "fullname": "Aviv Slobodkin",
            "user": "lovodkin93",
            "type": "user"
          },
          "name": "Aviv Slobodkin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-25T08:17:03.471Z",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c76",
          "name": "Hagai Taitelbaum",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c77",
          "name": "Yonatan Bitton",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c78",
          "name": "Brian Gordon",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c79",
          "name": "Michal Sokolik",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7a",
          "name": "Nitzan Bitton Guetta",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7b",
          "name": "Almog Gueta",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7c",
          "name": "Royi Rassin",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7d",
          "name": "Itay Laish",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7e",
          "name": "Dani Lischinski",
          "hidden": false
        },
        {
          "_id": "680b44fb426b7d5bc2018c7f",
          "name": "Idan Szpektor",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T12:44:51.000Z",
      "submittedOnDailyAt": "2025-04-25T06:50:21.552Z",
      "title": "RefVNLI: 主題駆動の文から画像への生成の可換的評価に向けて",
      "submittedOnDailyBy": {
        "_id": "631da07f6d6a5870f3d2c375",
        "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
        "isPro": false,
        "fullname": "Aviv Slobodkin",
        "user": "lovodkin93",
        "type": "user"
      },
      "summary": "主題駆動的文字から画像（T2I）生成は、与えられた文字記述に一致しながら、参照画像からの視覚的な身份を保持した画像を生成することを目指しています。その広いダウンストリームの適用範囲は、画像生成の拡張的なプライベート化から、映像渲染での一貫したキャラクター表現に至ることです。しかし、この分野の進展は、信頼性のある自動評価の欠如によって制限されています。現在の方法は、タスクの一つの面を評価しかしかできる（つまり、文字の一致性または主題の保持）、人間の判断に間違っているか、または費用の高いAPIベースの評価に依存しています。これに対して、私たちはRefVNLIを紹介します。RefVNLIは、一つの予測で文字の一致性と主題の保持を両方評価する低コストのメトリックです。ビデオ論理ベンチマークからの大規模なデータセットと画像の摂動から学習されています。RefVNLIは、複数のベンチマークと主題カテゴリ（例：動物、物体）で現在のベースラインを超えることや、同じ程度になることを実現し、文字の一致性については最高で6.4点の収益、主題の一致性については最高で8.5点の収益を収めます。また、知られずの概念でも優れています、87%以上の精度で人間の好みに合わせています。",
      "upvotes": 35,
      "discussionId": "680b44ff426b7d5bc2018d85",
      "ai_keywords": [
        "RefVNLI",
        "video-reasoning benchmarks",
        "image perturbations"
      ]
    },
    "publishedAt": "2025-04-24T08:44:51.000Z",
    "title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image\n  Generation",
    "summary": "Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability --\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering -- progress in this field is\nlimited by the lack of reliable automatic evaluation. Existing methods either\nassess only one aspect of the task (i.e., textual alignment or subject\npreservation), misalign with human judgments, or rely on costly API-based\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\nevaluates both textual alignment and subject preservation in a single\nprediction. Trained on a large-scale dataset derived from video-reasoning\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\nbaselines across multiple benchmarks and subject categories (e.g.,\nAnimal, Object), achieving up to 6.4-point gains in textual\nalignment and 8.5-point gains in subject consistency. It also excels with\nlesser-known concepts, aligning with human preferences at over 87\\% accuracy.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631da07f6d6a5870f3d2c375",
      "avatarUrl": "/avatars/242e344dca08057bdf1eef09f69b41b2.svg",
      "fullname": "Aviv Slobodkin",
      "name": "lovodkin93",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17192",
      "authors": [
        {
          "_id": "680aee7bcf67477f2c00ca53",
          "user": {
            "_id": "64f7bf0c7565a69eb693ad1f",
            "avatarUrl": "/avatars/aba6910aa39a3437a7f0df3f5cd49e6d.svg",
            "isPro": false,
            "fullname": "minju",
            "user": "iaminju",
            "type": "user"
          },
          "name": "Minju Seo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:41.304Z",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca54",
          "user": {
            "_id": "63036b6c5c70c21d0ea79d48",
            "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
            "isPro": false,
            "fullname": "Jinheon Baek",
            "user": "jinheon",
            "type": "user"
          },
          "name": "Jinheon Baek",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:38.982Z",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca55",
          "name": "Seongyun Lee",
          "hidden": false
        },
        {
          "_id": "680aee7bcf67477f2c00ca56",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T01:57:01.000Z",
      "submittedOnDailyAt": "2025-04-25T04:17:48.790Z",
      "title": "Paper2Code: マシン学習における科学論文からのコード自動生成",
      "submittedOnDailyBy": {
        "_id": "6550c4f27bbfce1878f5f280",
        "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
        "isPro": false,
        "fullname": "seongyun_lee",
        "user": "Seongyun",
        "type": "user"
      },
      "summary": "マシン学習研究の急速な成長に伴い、対応するコード実装は通常利用できないことにより、研究者が結果を再現し、先行研究に基づいて拡張するのは遅くて労力の多いことになっています。一方、最近の大規模言語モデル（LLMs）は科学論文を理解し、高品質のコードを生成することができます。これに励まし、PaperCoderという多エージェントLLMフレームワークを介して、機械学習論文を機能的なコードリポジトリに変換する方法を提案します。PaperCoderは計画、分析、生成の3ステップで動作します。計画ステップでは、高レベルのマップを構築し、システムアーキテクチャをディアグラムで設計し、ファイルの依存関係を特定し、設定ファイルを生成します。分析ステップでは、実装特有の詳細を解釈し、生成ステップでは、モジュール化された、依存関係を知るコードが生成されます。また、各ステップは、プイロフェッショナルなエージェントのセットでインスタンス化され、パイプラインを横断的に効果的にコラボレーションします。さらに、PaperCoderは機械学習論文からのコード実装を生成することについて、モデルベース評価と人間評価を用いて評価し、論文の原作者からのディストリビュートが利用できる場合はそれを真の物として、その効果性を示します。また、最近のPaperBenchベンチマークでは、強いベースラインを大幅に超える強みを示しています。",
      "upvotes": 31,
      "discussionId": "680aee7dcf67477f2c00ca96",
      "githubRepo": "https://github.com/going-doer/Paper2Code"
    },
    "publishedAt": "2025-04-23T21:57:01.000Z",
    "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning",
    "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17192.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6550c4f27bbfce1878f5f280",
      "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
      "fullname": "seongyun_lee",
      "name": "Seongyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17432",
      "authors": [
        {
          "_id": "680adfbe464a44cea0b843c1",
          "name": "Tiancheng Gu",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c2",
          "user": {
            "_id": "63e202f352b7578dba448ab5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
            "isPro": false,
            "fullname": "Yang",
            "user": "Kaichengalex",
            "type": "user"
          },
          "name": "Kaicheng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:46.935Z",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c3",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c4",
          "name": "Xingjun Wang",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c5",
          "name": "Yanzhao Zhang",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c6",
          "name": "Dingkun Long",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c7",
          "name": "Yingda Chen",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c8",
          "name": "Weidong Cai",
          "hidden": false
        },
        {
          "_id": "680adfbe464a44cea0b843c9",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T10:51:52.000Z",
      "submittedOnDailyAt": "2025-04-25T01:11:53.967Z",
      "title": "モディュールの壁を破り：多モディュールLLMによる普遍的な埋め込み学習",
      "submittedOnDailyBy": {
        "_id": "63e202f352b7578dba448ab5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
        "isPro": false,
        "fullname": "Yang",
        "user": "Kaichengalex",
        "type": "user"
      },
      "summary": "対比的言語画像予約訓練（CLIP）フレームワークは、多モデル表現学習に特に画像文書検索とクラスタリングにおいて広く使用されていますが、以下の3つの要因での効果性が制限されています：1) 文書トークントランケーション、2) 孤立した画像文書エンコーディング、3) ボケボケのボケボケ行為による構成性の欠陥。最近の多モデル大語言モデル（MLLM）は、一般化した視覚言語理解に関して顕著な進歩を示していますが、動的な多モデル表現の学習の可能性はまだ調査不足です。本論文では、MLLMを使って多様な下流タスクに対する判別的な表現を学習するための新しい2段階フレームワークUniME（Universal Multimodal Embedding）を提出します。1段階目には、強力なLLMベースの教師モデルからの言語成分の埋め込み能力を高めるために言語的判別的知識収納を行います。2段階目には、難しい負例を強化した指示調整を導入し、判別的表現学習を進めます。特に、最初に誤り負例の汚染を軽減し、各バッチ内の各インスタンスにおいて複数の難しい負例をサンプリングし、モデルを難しいサンプルに焦点を当てようとします。このアプローチは、判別力を向上させながらも下流タスクの指示従い能力を高めます。MMEBベンチマークと複数の検索タスクに対して拡張的な実験を行い、結果はすべてのタスクで統一的な性能向上を示し、優れた判別的と構成的な能力を見せました。",
      "upvotes": 21,
      "discussionId": "680adfbf464a44cea0b8440f",
      "projectPage": "https://garygutc.github.io/UniME/",
      "githubRepo": "https://github.com/deepglint/UniME",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training (CLIP)",
        "Multimodal Large Language Models (MLLMs)",
        "Generalized vision-language understanding",
        "UniME (Universal Multimodal Embedding)",
        "Discriminative representations",
        "Textual discriminative knowledge distillation",
        "LLM-based teacher model",
        "Hard negative enhanced instruction tuning",
        "False negative contamination",
        "Challenging samples",
        "Discriminative power",
        "Instruction-following ability",
        "MMEB benchmark",
        "Short caption retrieval",
        "Long caption retrieval",
        "Compositional retrieval"
      ]
    },
    "publishedAt": "2025-04-24T06:51:52.000Z",
    "title": "Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs",
    "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17432.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17207",
      "authors": [
        {
          "_id": "680af2bf2fa10fbf21684bde",
          "name": "Phillip Y. Lee",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684bdf",
          "name": "Jihyeon Je",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be0",
          "name": "Chanho Park",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be1",
          "name": "Mikaela Angelina Uy",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be2",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "680af2bf2fa10fbf21684be3",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T02:41:34.000Z",
      "submittedOnDailyAt": "2025-04-25T00:59:29.327Z",
      "title": "ビジョン・ラングジュードモデルにおける視点による理由論を通じてのメンタルイメージのシミュレーション",
      "submittedOnDailyBy": {
        "_id": "6342796a0875f2c99cfd313b",
        "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
        "isPro": false,
        "fullname": "Yuseung \"Phillip\" Lee",
        "user": "phillipinseoul",
        "type": "user"
      },
      "summary": "私たちは、視覚言語モデル（VLMs）における視角関係認知を通じて、心象記憶のシミュレーションを用いたフレームワークを提案します。視角取り、環境や状況を他の視点から見る能力は、人間レベルの視覚理解の重要なベンチマークであり、環境の相互作用と自動車エージェントとの協力に必要です。VLMsの空間認知の進歩に伴い、最近の研究は、現代のVLMsが显著に視角関係認知能力を欠くことを示し、自中心的な解釈に強いバイアスを示していることを明らかにしました。VLMsと人間の視覚認知の間の隙を埋めるために、心象記憶の役割に焦点を当てています。人間は抽象化された表現を通じて世界を見ることで、視角の移動を促進します。これに基づき、視覚ベースモデル（オブジェクト検出、分割、向き計測）を活用して、スペース抽象化と視角変換を可能にする「抽象視角変換（APC）」のフレームワークを提案します。合成画像と実写画像のベンチマークでの実験で、VLMsとの比較により、フレームワークを用いた視角関係認知における显著な向上を示し、空間認知の微調節モデルと新視点合成ベースのアプローチを更に上回ったことを示しました。",
      "upvotes": 14,
      "discussionId": "680af2c02fa10fbf21684c1f",
      "ai_keywords": [
        "vision-language models (VLMs)",
        "mental imagery simulation",
        "perspective-taking",
        "visual understanding",
        "environmental interaction",
        "autonomous agents",
        "spatial reasoning",
        "perspective-aware reasoning capabilities",
        "egocentric interpretations",
        "mental imagery",
        "scene abstractions",
        "perspective transformations",
        "object detection",
        "segmentation",
        "orientation estimation",
        "synthetic benchmarks",
        "real-image benchmarks",
        "fine-tuned spatial reasoning models",
        "novel-view-synthesis-based approaches"
      ]
    },
    "publishedAt": "2025-04-23T22:41:34.000Z",
    "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery\n  Simulation",
    "summary": "We present a framework for perspective-aware reasoning in vision-language\nmodels (VLMs) through mental imagery simulation. Perspective-taking, the\nability to perceive an environment or situation from an alternative viewpoint,\nis a key benchmark for human-level visual understanding, essential for\nenvironmental interaction and collaboration with autonomous agents. Despite\nadvancements in spatial reasoning within VLMs, recent research has shown that\nmodern VLMs significantly lack perspective-aware reasoning capabilities and\nexhibit a strong bias toward egocentric interpretations. To bridge the gap\nbetween VLMs and human perception, we focus on the role of mental imagery,\nwhere humans perceive the world through abstracted representations that\nfacilitate perspective shifts. Motivated by this, we propose a framework for\nperspective-aware reasoning, named Abstract Perspective Change (APC), that\neffectively leverages vision foundation models, such as object detection,\nsegmentation, and orientation estimation, to construct scene abstractions and\nenable perspective transformations. Our experiments on synthetic and real-image\nbenchmarks, compared with various VLMs, demonstrate significant improvements in\nperspective-aware reasoning with our framework, further outperforming\nfine-tuned spatial reasoning models and novel-view-synthesis-based approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17207.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6342796a0875f2c99cfd313b",
      "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
      "fullname": "Yuseung \"Phillip\" Lee",
      "name": "phillipinseoul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16511",
      "authors": [
        {
          "_id": "680b2a95c94724c1465c20dd",
          "name": "Fengze Liu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20de",
          "name": "Weidong Zhou",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20df",
          "name": "Binbin Liu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e0",
          "name": "Zhimiao Yu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e1",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e2",
          "name": "Haobin Lin",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e3",
          "name": "Yifeng Yu",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e4",
          "name": "Xiaohuan Zhou",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e5",
          "name": "Taifeng Wang",
          "hidden": false
        },
        {
          "_id": "680b2a95c94724c1465c20e6",
          "name": "Yong Cao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T08:36:50.000Z",
      "submittedOnDailyAt": "2025-04-25T04:55:18.773Z",
      "title": "QuaDMix: 質量・多様性バランスのあるデータ選択による効率的なLLM予習",
      "submittedOnDailyBy": {
        "_id": "668f5875b5b3081d776e4094",
        "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
        "isPro": false,
        "fullname": "Xiaohuan Zhou",
        "user": "XiaohuanZhou",
        "type": "user"
      },
      "summary": "品質と多様性は、大規模言語モデル（LLMs）の訓練データにおいて重要な2つの指標であり、性能向上にも正の影響を与えます。現在の研究は、これらの指標を別々に最適化していますが、通常は、品質フィルタリングを実施し、その後データの割合を調整しています。しかし、これらのアプローチは、品質と多様性の固有のトレードオフを見漏らしています。固定された訓練配額を前提として、各データ点の品質とそのデータセット全体の補間的な効果を評価することが重要です。本論文では、QuaDMixという統一的なデータ選択フレームワークを介して、LLMの事前訓練に対してデータ分布を自動的に最適化し、同時に品質と多様性をバランスすることを目的としています。特に、データの品質を測定するための複数の基準を提案し、データ点を区別するための領域分類を使用し、全体の多様性を測定します。QuaDMixは、これらの品質と多様性に関連するラベルに基づいて、各データ点のサンプリング確率を決定するための統一的なパラメタ化データサンプリング関数を使用します。QuaDMixフレームワークに関する最適化パラメータの探索を加速するために、小さなモデルに対して計算機での実験を行い、RegMixミシンをモデルとしてLightGBMを使用します。多様なモデルとデータセットの範囲での試験結果によると、QuaDMixは複数のベンチマークで平均7.2%の性能向上を達成しました。これらの結果は、独立した品質と多様性の戦略に比べても優れてい、データの品質と多様性のバランスの必要性と可能性を明らかにしています。",
      "upvotes": 13,
      "discussionId": "680b2a97c94724c1465c21a3",
      "ai_keywords": [
        "large language models (LLMs)",
        "QuaDMix",
        "data selection framework",
        "parameterized data sampling function",
        "domain classification",
        "LightGBM",
        "RegMix"
      ]
    },
    "publishedAt": "2025-04-23T04:36:50.000Z",
    "title": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining",
    "summary": "Quality and diversity are two critical metrics for the training data of large\nlanguage models (LLMs), positively impacting performance. Existing studies\noften optimize these metrics separately, typically by first applying quality\nfiltering and then adjusting data proportions. However, these approaches\noverlook the inherent trade-off between quality and diversity, necessitating\ntheir joint consideration. Given a fixed training quota, it is essential to\nevaluate both the quality of each data point and its complementary effect on\nthe overall dataset. In this paper, we introduce a unified data selection\nframework called QuaDMix, which automatically optimizes the data distribution\nfor LLM pretraining while balancing both quality and diversity. Specifically,\nwe first propose multiple criteria to measure data quality and employ domain\nclassification to distinguish data points, thereby measuring overall diversity.\nQuaDMix then employs a unified parameterized data sampling function that\ndetermines the sampling probability of each data point based on these quality\nand diversity related labels. To accelerate the search for the optimal\nparameters involved in the QuaDMix framework, we conduct simulated experiments\non smaller models and use LightGBM for parameters searching, inspired by the\nRegMix method. Our experiments across diverse models and datasets demonstrate\nthat QuaDMix achieves an average performance improvement of 7.2% across\nmultiple benchmarks. These results outperform the independent strategies for\nquality and diversity, highlighting the necessity and ability to balance data\nquality and diversity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16511.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668f5875b5b3081d776e4094",
      "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
      "fullname": "Xiaohuan Zhou",
      "name": "XiaohuanZhou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17789",
      "authors": [
        {
          "_id": "680b318bbbebf87944bc9595",
          "name": "Xu Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9596",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9597",
          "name": "Haoyu Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9598",
          "name": "Hao Tang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc9599",
          "name": "Chih-Yao Ma",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959a",
          "name": "Jialiang Wang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959b",
          "name": "Kunpeng Li",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959c",
          "name": "Xiaoliang Dai",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959d",
          "name": "Yujun Shi",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959e",
          "name": "Xuan Ju",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc959f",
          "name": "Yushi Hu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a0",
          "name": "Artsiom Sanakoyeu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a1",
          "name": "Felix Juefei-Xu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a2",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a3",
          "name": "Junjiao Tian",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a4",
          "name": "Tao Xu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a5",
          "name": "Tingbo Hou",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a6",
          "name": "Yen-Cheng Liu",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a7",
          "name": "Zecheng He",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a8",
          "name": "Zijian He",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95a9",
          "name": "Matt Feiszli",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95aa",
          "name": "Peizhao Zhang",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ab",
          "name": "Peter Vajda",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ac",
          "name": "Sam Tsai",
          "hidden": false
        },
        {
          "_id": "680b318bbbebf87944bc95ad",
          "name": "Yun Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-25T05:28:51.493Z",
      "title": "トークンシャッフル：自動帰納的モデルを用いた高解像度画像生成に向けて",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "自動回帰（AR）モデルは、言語生成において長期に支配的であり、画像合成にも増加して適用されていますが、ディフューションベースモデルに比べてよく課題のあると考えられています。主な制限は、ARモデルに必要な画像トークンの数が多いことです。これは、訓練および推論の効率や画像の解像度に制限をかけます。これを解決するために、我々はトークンシャッフルを提案します。これは新しいけれども簡単な方法で、トランスフォーメーターの画像トークンの数を減らします。我々の主なアイデアは、多模態大語言モデル（MLLMs）での視覚ボキャブラリーの次元冗長性です。ここで、視覚エンコーダーからの低次元の視覚コードは直接高次元の言語ボキャブラリーにマッピングされます。これを活用し、我々は2つの主要な操作を考えます：トークンシャッフルとトークンユンシャッフル。トークンシャッフルは、スペクトラル次元に沿って空間的に局所的なトークンを統合し、入力トークンの数を減らします。トークンユンシャッフルは、トランスフォーメーターブロックの後に推論されたトークンを結びつけ、出力の空間的な並びを復元します。文脈プラントと共に訓練すると、我々の戦略は追加の訓練済み文脈エンコーダーを必要としません。これにより、MLLMsは、言語と画像の統一された次のトークン予測の方法で非常に高解像度の画像合成を支援できます。まだ初めて、我々はARテキストから画像生成の境界を2048x2048の解像度まで推進し、満足している生成性能を収めました。GenAI-benchmarkでは、我々の2.7Bモデルは難易度の高いプラントで全体的スコア0.77を達成し、ARモデルLlamaGenを0.18、ディフューションモデルLDMを0.15より上回りました。詳細な大規模な人間評価も、言語対応性、視覚的な欠点、視覚的な外観の面で我々の顕著な画像生成能力を示しました。我々は、トークンシャッフルがMLLM内での効率的な高解像度画像生成の基盤的な設計として役立つことを望む。",
      "upvotes": 4,
      "discussionId": "680b3191bbebf87944bc9739",
      "ai_keywords": [
        "autoregressive (AR) models",
        "image synthesis",
        "diffusion-based models",
        "image tokens",
        "training and inference efficiency",
        "Transformer",
        "dimensional redundancy",
        "visual vocabularies",
        "Multimodal Large Language Models (MLLMs)",
        "visual encoder",
        "high-dimensional language vocabularies",
        "token-shuffle",
        "spatially local tokens",
        "channel dimension",
        "token-unshuffle",
        "spatial arrangement",
        "unified next-token prediction",
        "text-to-image generation",
        "resolution",
        "generation performance",
        "GenAI-benchmark",
        "textual prompts",
        "pretrained text-encoder",
        "text-alignment",
        "visual flaw",
        "visual appearance"
      ]
    },
    "publishedAt": "2025-04-24T13:59:56.000Z",
    "title": "Token-Shuffle: Towards High-Resolution Image Generation with\n  Autoregressive Models",
    "summary": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17789.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17069",
      "authors": [
        {
          "_id": "680b1b33388bb2cfd497ebdb",
          "user": {
            "_id": "62bb84f82ada492aa5775709",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62bb84f82ada492aa5775709/jv4yKL75t8QzHDHLbhBPT.png",
            "isPro": false,
            "fullname": "Rishav Pramanik",
            "user": "rishavpramanik",
            "type": "user"
          },
          "name": "Rishav Pramanik",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:27.724Z",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdc",
          "name": "Antoine Poupon",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdd",
          "name": "Juan A. Rodriguez",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebde",
          "name": "Masih Aminbeidokhti",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebdf",
          "name": "David Vazquez",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe0",
          "name": "Christopher Pal",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe1",
          "name": "Zhaozheng Yin",
          "hidden": false
        },
        {
          "_id": "680b1b33388bb2cfd497ebe2",
          "name": "Marco Pedersoli",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/NUImFkmaqDnfEIBB2l94Q.png",
        "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/OuC3dzu5XCUb8dxTlVf72.png"
      ],
      "publishedAt": "2025-04-23T19:33:58.000Z",
      "submittedOnDailyAt": "2025-04-25T03:50:09.534Z",
      "title": "シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプルな自動関数を用いて、シンプル",
      "submittedOnDailyBy": {
        "_id": "63a614d264f470027818b066",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
        "isPro": false,
        "fullname": "Juan A. Rodriguez",
        "user": "joanrodai",
        "type": "user"
      },
      "summary": "自動順次ポーチベース画像生成は、画像質とスケーラビリティにおいて最近に比較的優れた結果を示しています。これらのモデルは、視覚言語モデル内で簡単に統合され、スケーリングも可能です。しかし、自動順次モデルは、ポーチの生成に定義された順番が必要です。自然な順番は、単語の指示に基づくものであり、これは文章生成には意味がありますが、画像生成には固有の生成順番は存在しません。伝統的には、ラスタースキャン順番（左上から右下まで）が自動順次画像生成モデルをガイドしています。本論文では、この順番が最適ではありませんと主張し、画像の内容の因果関係を尊重していません：例えば、日没の視覚記述に基づいて、自動順次モデルは、雲を太陽の前に生成する可能性がありますが、雲の色は太陽の色により、逆に太陽の色により決定されるべきです。本論文では、ポーチの生成順番を任意に学習させることで、生成中にポーチの内容と位置（順番）を推定することができることを示します。そして、これらの抽出された順番を用いて、任意の順番モデルを調整し、より高品質の画像を生成することを示します。実験により、本論文では、2つのデータセットでこの新しい生成方法が、伝統的なラスタースキャンアプローチよりもより良い画像を生成することを示し、同様の学習コストと追加のアノテーションが必要ないことを示します。",
      "upvotes": 4,
      "discussionId": "680b1b35388bb2cfd497ec76",
      "ai_keywords": [
        "autoregressive patch-based image generation",
        "Vision-Language models",
        "raster-scan order",
        "causality",
        "any-given-order",
        "patch content",
        "patch location",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-04-23T15:33:58.000Z",
    "title": "Distilling semantically aware orders for autoregressive image generation",
    "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/NUImFkmaqDnfEIBB2l94Q.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/OuC3dzu5XCUb8dxTlVf72.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17069.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a614d264f470027818b066",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
      "fullname": "Juan A. Rodriguez",
      "name": "joanrodai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17040",
      "authors": [
        {
          "_id": "680af0c4175842e433ae348e",
          "name": "Zhenhailong Wang",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae348f",
          "name": "Senthil Purushwalkam",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3490",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3491",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3492",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "680af0c4175842e433ae3493",
          "name": "Ran Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T18:38:18.000Z",
      "submittedOnDailyAt": "2025-04-25T06:12:13.135Z",
      "title": "ダイナミック・メリージングとベーシック・アンメリージングの効率的なVLMsのための実装",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "DyMUは、ビジョン言語モデル（VLMs）の計算負担を効率的に減らし、高いタスク性能を維持するための、訓練不要のフレームワークです。我々のアプローチは2つの主な構成要素から成る。1. 動的トークンミーリング（DToMe）は、画像の複雑さに基づいて類似なトークンを統合し、ビジョントランスフォーマーでの固定長の出力の不適切さを解決します。2. ビュートルトークンユニメージング（VTU）は、大規模な言語モデル（LLMs）の期待されるトークンシーケンスをシミュレートし、全シーケンスの注意動作を効率的に再構築し、ファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイルタイプのファイル",
      "upvotes": 3,
      "discussionId": "680af0c7175842e433ae3544",
      "ai_keywords": [
        "Dynamic Token Merging (DToMe)",
        "Virtual Token Unmerging (VTU)",
        "vision transformers",
        "token compression",
        "attention dynamics",
        "visual encoders",
        "image complexity",
        "computational costs"
      ]
    },
    "publishedAt": "2025-04-23T14:38:18.000Z",
    "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs",
    "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17040.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16921",
      "authors": [
        {
          "_id": "680b40774d69b6950c4eabda",
          "name": "José Ángel González",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdb",
          "name": "Ian Borrego Obrador",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdc",
          "name": "Álvaro Romo Herrero",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdd",
          "name": "Areg Mikael Sarvazyan",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabde",
          "user": {
            "_id": "60f95c8fda0985b973d59d77",
            "avatarUrl": "/avatars/5606f0191b9f86e6b55f7e5ab6cc8bb6.svg",
            "isPro": false,
            "fullname": "Mara Chinea Rios",
            "user": "mchinea",
            "type": "user"
          },
          "name": "Mara Chinea-Ríos",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-25T07:58:32.606Z",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabdf",
          "name": "Angelo Basile",
          "hidden": false
        },
        {
          "_id": "680b40774d69b6950c4eabe0",
          "name": "Marc Franco-Salvador",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T17:48:25.000Z",
      "submittedOnDailyAt": "2025-04-25T06:28:29.231Z",
      "title": "IberBench: イベリア語のLLM評価",
      "submittedOnDailyBy": {
        "_id": "62308d13e4673fe4985d7fc9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655555697880-62308d13e4673fe4985d7fc9.jpeg",
        "isPro": false,
        "fullname": "Areg Mikael Sarvazyan",
        "user": "asarvazyan",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、特に英語以外の言語では高品質なデータが限られているため、全体的に評価が難しい。現在のベンチマークとリーダブードは、英語中心的であり、他の言語に関するものは少ない。これらのベンチマークは、言語の多様性を飛ばし、工業的な価値のタスクより基礎的な自然言語処理（NLP）の能力を優先し、静的であるという点で欠点がある。これらの点に気付いて、我々はIberBenchを紹介します。IberBenchは、Iberian PeninsulaとIbero-Americaで使用される言語の両方の基本的なおよび工業的なNLPタスクでLLMの性能を評価するための詳細で拡張可能なベンチマークです。IberBenchは、101データセットを統合し、感情分析、毒性検出、要約など22タスクカテゴリーをカバーしています。このベンチマークは、現在の評価プラクティスの欠点を解決し、言語の多様性と静的な評価準備の不足を解決するために、コミュニティ駆動のモデルとデータセットの提出をコミッティーの専門家で調整した連続的な更新を可能にしています。23つのLLM（100 millionから14 billionパラメータ）を評価し、それらの強みと欠点について実験的なインサイトを提供します。我々の発見は、(i)LLMは工業的なタスクでは基本的なタスクよりも性能が悪い、(ii)GalicianとBasqueの平均的な性能は低い、(iii)一部のタスクではランダムな結果と近い、(iv)その他のタスクではランダムよりも高く、共有タスクシステムよりも低いことを示しています。IberBenchは、データセットの正規化とホスティング、LLMのインクリメンタル評価、公開的にアクセス可能なリーダブードを含むオープンソース実装を提供しています。",
      "upvotes": 3,
      "discussionId": "680b407a4d69b6950c4eac96",
      "projectPage": "https://huggingface.co/spaces/iberbench/leaderboard",
      "githubRepo": "https://github.com/IberBench",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "sentiment and emotion analysis",
        "toxicity detection",
        "summarization",
        "IberBench",
        "dataset normalization",
        "incremental evaluation",
        "leaderboard"
      ]
    },
    "publishedAt": "2025-04-23T13:48:25.000Z",
    "title": "IberBench: LLM Evaluation on Iberian Languages",
    "summary": "Large Language Models (LLMs) remain difficult to evaluate comprehensively,\nparticularly for languages other than English, where high-quality data is often\nlimited. Existing benchmarks and leaderboards are predominantly\nEnglish-centric, with only a few addressing other languages. These benchmarks\nfall short in several key areas: they overlook the diversity of language\nvarieties, prioritize fundamental Natural Language Processing (NLP)\ncapabilities over tasks of industrial relevance, and are static. With these\naspects in mind, we present IberBench, a comprehensive and extensible benchmark\ndesigned to assess LLM performance on both fundamental and industry-relevant\nNLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.\nIberBench integrates 101 datasets from evaluation campaigns and recent\nbenchmarks, covering 22 task categories such as sentiment and emotion analysis,\ntoxicity detection, and summarization. The benchmark addresses key limitations\nin current evaluation practices, such as the lack of linguistic diversity and\nstatic evaluation setups by enabling continual updates and community-driven\nmodel and dataset submissions moderated by a committee of experts. We evaluate\n23 LLMs ranging from 100 million to 14 billion parameters and provide empirical\ninsights into their strengths and limitations. Our findings indicate that (i)\nLLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)\nperformance is on average lower for Galician and Basque, (iii) some tasks show\nresults close to random, and (iv) in other tasks LLMs perform above random but\nbelow shared task systems. IberBench offers open-source implementations for the\nentire evaluation pipeline, including dataset normalization and hosting,\nincremental evaluation of LLMs, and a publicly accessible leaderboard.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62308d13e4673fe4985d7fc9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655555697880-62308d13e4673fe4985d7fc9.jpeg",
      "fullname": "Areg Mikael Sarvazyan",
      "name": "asarvazyan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15921",
      "authors": [
        {
          "_id": "680ab2f808464b525df64b07",
          "name": "Jian Hu",
          "hidden": false
        },
        {
          "_id": "680ab2f808464b525df64b08",
          "name": "Dimitrios Korkinof",
          "hidden": false
        },
        {
          "_id": "680ab2f808464b525df64b09",
          "name": "Shaogang Gong",
          "hidden": false
        },
        {
          "_id": "680ab2f808464b525df64b0a",
          "name": "Mariano Beguerisse-Diaz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T14:06:01.000Z",
      "submittedOnDailyAt": "2025-04-25T06:39:24.280Z",
      "title": "ViSMaP: 無サブジェクト時間長ビデオの経験的プロンプティングによる無マニュアルの縮約",
      "submittedOnDailyBy": {
        "_id": "65e1b6e9501590df0173cbd3",
        "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
        "isPro": false,
        "fullname": "Jian Hu",
        "user": "lwpyh",
        "type": "user"
      },
      "summary": "ビズマップ（ViSMap）：無マニューバックビデオ要約化を行うメタプロンプティングシステムを紹介します。このシステムは、無マニューバックで1時間のビデオを要約することができます。現在のビデオ理解モデルは、短いビデオや事前にセグメントされたイベントのビデオではよく効果的ですが、長いビデオでは、関連するイベントが稀疏に分布して事前にセグメントされていないため、要約が困難です。また、長ビデオの理解は、複数のステップの訓練を行う必要があり、詳細な注釈が必要ですが、これは高額で時間がかかり、不均一性があるため、これらの問題を解決する必要があります。ビズマップでは、短いビデオ（ここで注釈データが豊富）と長いビデオ（ここではない）の間のギャップをカットします。LLMを使用して、短いビデオから得られるセグメントの説明を用いて、長いビデオの優化されたファクトサマリーを生成します。これらのファクトサマリーは、長ビデオの要約にモデルの訓練データとして使用され、訳注釈の必要性を回避します。特に、メタプロンプティング戦略を採用して、長いビデオのファクトサマリーを反復的に生成し、改良します。この戦略は、短いクリップの説明を用いて要約をガイドします。各イテレーションでは、3つのLLMを順次使用します：クリップの説明からファクトサマリーを生成する、それを評価する、そして、ジェネレーターのプロンプトを最適化する。このイテレーションは、ファクトサマリーの品質がジェネレーターのプロンプトに高度に依存し、ビデオ間でもその品質が極端に変化するため必要となります。ビジョンデータセットで広範囲に評価し、ビズマップは、全訓練バックで最先端のモデルと同等の性能を達成し、領域を横断しながら性能を落としません。コードは、論文公表後にリリースされます。",
      "upvotes": 3,
      "discussionId": "680ab2fb08464b525df64bd2",
      "ai_keywords": [
        "LLMs",
        "ViSMap",
        "Unsupervised Video Summarisation",
        "Meta Prompting",
        "long-form video understanding",
        "supervised hierarchical training",
        "pseudo-summaries",
        "short clip descriptions",
        "meta-prompting strategy",
        "generator prompt"
      ]
    },
    "publishedAt": "2025-04-22T10:06:01.000Z",
    "title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting",
    "summary": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e1b6e9501590df0173cbd3",
      "avatarUrl": "/avatars/a73e2139700e23eff455734c99cef5ba.svg",
      "fullname": "Jian Hu",
      "name": "lwpyh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17601",
      "authors": [
        {
          "_id": "680b2b8c6bd146aa35a48222",
          "user": {
            "_id": "64d496b04ab89be0de7fb1a9",
            "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
            "isPro": false,
            "fullname": "Erik Bergh",
            "user": "erikbergh",
            "type": "user"
          },
          "name": "Erik Bergh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-25T06:39:03.785Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T14:26:42.000Z",
      "submittedOnDailyAt": "2025-04-25T05:00:56.733Z",
      "title": "ガウス重み付き線形変換を用いた解釈可能な非線形次元削減法",
      "submittedOnDailyBy": {
        "_id": "64d496b04ab89be0de7fb1a9",
        "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
        "isPro": false,
        "fullname": "Erik Bergh",
        "user": "erikbergh",
        "type": "user"
      },
      "summary": "次元削減手法は、高次元データの分析と可視化に基礎的な役割を果たしています。t-SNEとPCAなどの既存手法は、表現力と説明性の間でトレードオフを見出しています。本論文では、このトレードオフを橋渡させる新しいアプローチを紹介し、線形手法の説明性と非線形変換の表現力を統合しています。提案されたアルゴリズムは、線形変換とガウス関数による重み付けを組み合わせて高次元と低次元のスペース間の非線形マッピングを構築しています。このアーキテクチャは、複雑な非線形変換を可能にしながら、線形手法の説明性の優れた点を保ち、各変換は独立に分析できるようになっています。その結果、ディメンション削減と同時に変換されたスペースにおける透明なインサイトを提供します。学習された変換の説明手法も提示され、抑制された次元の特定およびスペースの拡大と縮小の方法も含めています。これらのツールは、実践者が次元削減の過程で保持されたおよび変更された幾何関係を理解することを可能にします。このアルゴリズムの実用的な効用を確保するために、ユーザーフレンドリーなソフトウェアパッケージの開発を強調し、学術界と業界での採用を促進しています。",
      "upvotes": 1,
      "discussionId": "680b2b8d6bd146aa35a48252",
      "projectPage": "https://github.com/erikbergh/interpretable_dim_reduction/",
      "githubRepo": "https://github.com/erikbergh/interpretable_dim_reduction/",
      "ai_keywords": [
        "t-SNE",
        "PCA",
        "non-linear mapping",
        "Gaussian functions",
        "linear transformations",
        "interpretability",
        "dimensionality reduction",
        "suppressed dimensions",
        "geometric relationships"
      ]
    },
    "publishedAt": "2025-04-24T10:26:42.000Z",
    "title": "Interpretable non-linear dimensionality reduction using gaussian\n  weighted linear transformation",
    "summary": "Dimensionality reduction techniques are fundamental for analyzing and\nvisualizing high-dimensional data. With established methods like t-SNE and PCA\npresenting a trade-off between representational power and interpretability.\nThis paper introduces a novel approach that bridges this gap by combining the\ninterpretability of linear methods with the expressiveness of non-linear\ntransformations. The proposed algorithm constructs a non-linear mapping between\nhigh-dimensional and low-dimensional spaces through a combination of linear\ntransformations, each weighted by Gaussian functions. This architecture enables\ncomplex non-linear transformations while preserving the interpretability\nadvantages of linear methods, as each transformation can be analyzed\nindependently. The resulting model provides both powerful dimensionality\nreduction and transparent insights into the transformed space. Techniques for\ninterpreting the learned transformations are presented, including methods for\nidentifying suppressed dimensions and how space is expanded and contracted.\nThese tools enable practitioners to understand how the algorithm preserves and\nmodifies geometric relationships during dimensionality reduction. To ensure the\npractical utility of this algorithm, the creation of user-friendly software\npackages is emphasized, facilitating its adoption in both academia and\nindustry.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17601.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d496b04ab89be0de7fb1a9",
      "avatarUrl": "/avatars/fe60082c26e0d98126e62ee6257a374f.svg",
      "fullname": "Erik Bergh",
      "name": "erikbergh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17414",
      "authors": [
        {
          "_id": "680b3f12c131c3be24f80ce0",
          "name": "Min Wei",
          "hidden": false
        },
        {
          "_id": "680b3f12c131c3be24f80ce1",
          "name": "Chaohui Yu",
          "hidden": false
        },
        {
          "_id": "680b3f12c131c3be24f80ce2",
          "name": "Jingkai Zhou",
          "hidden": false
        },
        {
          "_id": "680b3f12c131c3be24f80ce3",
          "name": "Fan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T10:12:40.000Z",
      "submittedOnDailyAt": "2025-04-25T06:22:09.592Z",
      "title": "3DV-TON: ディフュージョンモデルによる3Dガイドされた一致したテクスチャー付き3Dビデオショートライン",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "ビデオチャレンジは、ビデオの服装を特定の服装に置き換えます。既存の方法は、複雑な服装パターンと多様な体勢を扱う際に高品質と時系列的な一致性の高い結果を生成することが難しいです。私たちは、高品質と時系列的な一致性の高いビデオチャレンジ結果を生成するための新しいディフュージョンベースのフレームワークを紹介します。私たちのアプローチは、生成されたアニマティブなテクスチャードラフェスを明示的なフレームレベルのガイドに使用し、外見の品質と動きの一致性の両方を重視することで、モデルが外見の品質を優先して動きの一致性を見失ってしまう問題を解決します。これは、ビデオシーケンス全体で一致した服装のテクスチャー移動を直接参照することにより実現されます。提案された方法は、動的な3Dガイドを生成する適応的なプイルプールを特徴化します：1）最初の2D画像チャレンジのキーフレームを選択し、2）オリジナルのビデオポーズと同期されたテクスチャードラフェスの再構築とアニメーションを行います。さらに、動的な人間と服装の移動によるディレクトの服装情報の漏れによるアーティファクトの伝播を成功に軽減するために、強力的な矩形マスクリング戦略を紹介します。ビデオチャレンジの研究を進めるために、130ビデオを含む多様な服装タイプとシナリオの高解像度ベンチマークデータセットを紹介します。定量的および質的な結果は、私たちの優れた性能を示しています。プロジェクトページはこちらのリンクです https://2y7c3.github.io/3DV-TON/",
      "upvotes": 1,
      "discussionId": "680b3f15c131c3be24f80d65",
      "ai_keywords": [
        "diffusion-based framework",
        "animatable textured 3D meshes",
        "frame-level guidance",
        "motion coherence",
        "garment texture movements",
        "adaptive pipeline",
        "keyframe",
        "2D image try-on",
        "textured 3D mesh",
        "synchronized with original video poses",
        "rectangular masking strategy",
        "artifact propagation",
        "HR-VVT",
        "high-resolution benchmark dataset"
      ]
    },
    "publishedAt": "2025-04-24T06:12:40.000Z",
    "title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models",
    "summary": "Video try-on replaces clothing in videos with target garments. Existing\nmethods struggle to generate high-quality and temporally consistent results\nwhen handling complex clothing patterns and diverse body poses. We present\n3DV-TON, a novel diffusion-based framework for generating high-fidelity and\ntemporally consistent video try-on results. Our approach employs generated\nanimatable textured 3D meshes as explicit frame-level guidance, alleviating the\nissue of models over-focusing on appearance fidelity at the expanse of motion\ncoherence. This is achieved by enabling direct reference to consistent garment\ntexture movements throughout video sequences. The proposed method features an\nadaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe\nfor initial 2D image try-on, followed by (2) reconstructing and animating a\ntextured 3D mesh synchronized with original video poses. We further introduce a\nrobust rectangular masking strategy that successfully mitigates artifact\npropagation caused by leaking clothing information during dynamic human and\ngarment movements. To advance video try-on research, we introduce HR-VVT, a\nhigh-resolution benchmark dataset containing 130 videos with diverse clothing\ntypes and scenarios. Quantitative and qualitative results demonstrate our\nsuperior performance over existing methods. The project page is at this link\nhttps://2y7c3.github.io/3DV-TON/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17414.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17343",
      "authors": [
        {
          "_id": "680b24471c5fbd15909bf1f9",
          "name": "Linli Yao",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fa",
          "name": "Yicheng Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fb",
          "name": "Yuancheng Wei",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fc",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fd",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1fe",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf1ff",
          "name": "Kun Ouyang",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf200",
          "name": "Lean Wang",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf201",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf202",
          "name": "Sida Li",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf203",
          "name": "Lingpeng Kong",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf204",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf205",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "680b24471c5fbd15909bf206",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T07:59:46.000Z",
      "submittedOnDailyAt": "2025-04-25T06:34:54.517Z",
      "title": "タイムチャット・オンライン：ストリーミング映画では、80%の可視トークンが自然的に冗長です",
      "submittedOnDailyBy": {
        "_id": "655ca347f426a304c6b393a1",
        "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
        "isPro": false,
        "fullname": "Linli Yao",
        "user": "yaolily",
        "type": "user"
      },
      "summary": "オンラインビデオプラットフォームの急速な成長、特にライブストリーミングサービスにより、実時間のビデオ理解システムの急迫な要望が生じています。これらのシステムは、連続的なビデオストリームを処理し、ユーザーのクエリに瞬時的に応答する必要があります。現在のVideo Large Language Models (VideoLLMs)は、完全なビデオの処理に優れていますが、流れるシナリオでは、ビデオの連続した、冗長なフレームを効率的に処理できない限界があります。時間チャット・オンライン（TimeChat-Online）を紹介します。これは新しいオンラインVideoLLMで、実時間のビデオインタラクティブモデルを革新的にします。その核心は、我々の創新的なDifferential Token Drop (DTD)モジュールです。DTDは、流れるビデオの視覚的な冗長性の基本的な課題を解決します。人間の視覚的な認識のChange Blindness現象からのエナジーを引き、DTDは時系列的な変化を保存しながら、フレーム間の静的な冗長な内容をフィルタリングします。実験では、DTDはStreamingBenchで82.8%のビデオトークン削減を達成し、性能を98%保ち、流れるビデオの80%以上の視覚的な内容が自然と冗長であることを示しました。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニングが必要としないことを示します。流れるビデオの無言語ガイドニン",
      "upvotes": 1,
      "discussionId": "680b244a1c5fbd15909bf2ff",
      "ai_keywords": [
        "Video Large Language Models (VideoLLMs)",
        "Differential Token Drop (DTD)",
        "Change Blindness phenomenon",
        "TimeChat-Online",
        "TimeChat-Online-139K",
        "StreamingBench",
        "OvOBench",
        "Video-MME",
        "MLVU",
        "Proactive Response",
        "real-time video interaction",
        "continuous video streams",
        "user queries",
        "visual redundancy",
        "dense, redundant frames",
        "visual content",
        "video tokens",
        "meaningful temporal changes",
        "static, redundant content",
        "backward-tracing",
        "current-perception",
        "future-responding scenarios"
      ]
    },
    "publishedAt": "2025-04-24T03:59:46.000Z",
    "title": "TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming\n  Videos",
    "summary": "The rapid growth of online video platforms, particularly live streaming\nservices, has created an urgent need for real-time video understanding systems.\nThese systems must process continuous video streams and respond to user queries\ninstantaneously, presenting unique challenges for current Video Large Language\nModels (VideoLLMs). While existing VideoLLMs excel at processing complete\nvideos, they face significant limitations in streaming scenarios due to their\ninability to handle dense, redundant frames efficiently. We introduce\nTimeChat-Online, a novel online VideoLLM that revolutionizes real-time video\ninteraction. At its core lies our innovative Differential Token Drop (DTD)\nmodule, which addresses the fundamental challenge of visual redundancy in\nstreaming videos. Drawing inspiration from human visual perception's Change\nBlindness phenomenon, DTD preserves meaningful temporal changes while filtering\nout static, redundant content between frames. Remarkably, our experiments\ndemonstrate that DTD achieves an 82.8% reduction in video tokens while\nmaintaining 98% performance on StreamingBench, revealing that over 80% of\nvisual content in streaming videos is naturally redundant without requiring\nlanguage guidance. To enable seamless real-time interaction, we present\nTimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse\ninteraction patterns including backward-tracing, current-perception, and\nfuture-responding scenarios. TimeChat-Online's unique Proactive Response\ncapability, naturally achieved through continuous monitoring of video scene\ntransitions via DTD, sets it apart from conventional approaches. Our extensive\nevaluation demonstrates TimeChat-Online's superior performance on streaming\nbenchmarks (StreamingBench and OvOBench) and maintaining competitive results on\nlong-form video tasks such as Video-MME and MLVU.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17343.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655ca347f426a304c6b393a1",
      "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
      "fullname": "Linli Yao",
      "name": "yaolily",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16064",
      "authors": [
        {
          "_id": "680b494c6bd146aa35ab2e1c",
          "name": "Theodoros Kouzelis",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e1d",
          "name": "Efstathios Karypidis",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e1e",
          "name": "Ioannis Kakogeorgiou",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e1f",
          "name": "Spyros Gidaris",
          "hidden": false
        },
        {
          "_id": "680b494c6bd146aa35ab2e20",
          "name": "Nikos Komodakis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/677272184d148b904333e874/QkosGJUTQX94tAZPCF46-.png"
      ],
      "publishedAt": "2025-04-22T17:41:42.000Z",
      "submittedOnDailyAt": "2025-04-25T07:32:33.466Z",
      "title": "ジョイント画像-特徴量合成による生成画像モデリングの強化",
      "submittedOnDailyBy": {
        "_id": "677272184d148b904333e874",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5dUau7gxLk4Wm1TiiJJri.jpeg",
        "isPro": false,
        "fullname": "Efstathios Karypidis",
        "user": "Sta8is",
        "type": "user"
      },
      "summary": "潜在扩散モデル（LDMs）は高品質画像生成において主導地位を占めていますが、表現学習と生成モデリングの統合は難しい課題です。我々は、ディフフェーションモデルを利用して、低レベル画像潜在量（変分オートエンコーダーから）と高レベル語意的特徴（DINOなどの事前学習された自動認識エンコーダーから）を共にモデル化するための新しい生成画像モデリングフレームワークを紹介します。我々の潜在語意ディフフェーションアプローチは、純なノイズからコンテキストの画像特徴ペアを生成することを学習し、生成品質と学習効率を大幅に向上させ、標準的なDiffusion Transformerアーキテクチャに最小限の変更を必要とします。複雑なディスティルーション目標を必要としないことで、我々の統一設計は学習を簡単化し、強力な新しい推論戦略「表現ガイドニング」を開発します。条件付きと非条件付きの両方で評価され、我々の方法は画像品質と学習収束速度に大幅な向上を収め、表現に関する生成モデリングの新しい方向をセットします。",
      "upvotes": 1,
      "discussionId": "680b494e6bd146aa35ab2e97",
      "githubRepo": "https://github.com/zelaki/ReDi",
      "ai_keywords": [
        "latent diffusion models (LDMs)",
        "generative image modeling",
        "diffusion model",
        "low-level image latents",
        "variational autoencoder",
        "high-level semantic features",
        "pretrained self-supervised encoder",
        "DINO",
        "latent-semantic diffusion",
        "coherent image-feature pairs",
        "generative quality",
        "training efficiency",
        "Diffusion Transformer architectures",
        "complex distillation objectives",
        "Representation Guidance",
        "image quality",
        "training convergence speed",
        "representation-aware generative modeling"
      ]
    },
    "publishedAt": "2025-04-22T13:41:42.000Z",
    "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
    "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/677272184d148b904333e874/QkosGJUTQX94tAZPCF46-.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16064.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "677272184d148b904333e874",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5dUau7gxLk4Wm1TiiJJri.jpeg",
      "fullname": "Efstathios Karypidis",
      "name": "Sta8is",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]