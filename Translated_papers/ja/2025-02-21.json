[
  {
    "paper": {
      "id": "2502.14739",
      "authors": [
        {
          "_id": "67b7efc26348a1df80a8ae53",
          "name": "M-A-P Team",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae54",
          "name": "Xinrun Du",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae55",
          "name": "Yifan Yao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae56",
          "name": "Kaijing Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae57",
          "name": "Bingli Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae58",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tianyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:24.002Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae59",
          "name": "Kang Zhu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5a",
          "user": {
            "_id": "6417d9ea8f689506e7148417",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
            "isPro": false,
            "fullname": "minghao",
            "user": "Liam-Liu",
            "type": "user"
          },
          "name": "Minghao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:25.894Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5b",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5c",
          "name": "Xiaolong Jin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5d",
          "name": "Zhenlin Wei",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5e",
          "user": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "isPro": false,
            "fullname": "Chujie Zheng",
            "user": "chujiezheng",
            "type": "user"
          },
          "name": "Chujie Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:34.124Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae5f",
          "name": "Kaixing Deng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae60",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae61",
          "name": "Shian Jia",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae62",
          "name": "Sichao Jiang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae63",
          "name": "Yiyan Liao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae64",
          "name": "Rui Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae65",
          "name": "Qinrui Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae66",
          "name": "Sirun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae67",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae68",
          "name": "Yunwen Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae69",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6a",
          "user": {
            "_id": "64de37ee5e192985054be575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
            "isPro": false,
            "fullname": "Yuansheng Ni",
            "user": "yuanshengni",
            "type": "user"
          },
          "name": "Yuansheng Ni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:30.371Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6b",
          "name": "Haoran Que",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6c",
          "user": {
            "_id": "64560618bfdf9c63ce2d658a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
            "isPro": false,
            "fullname": "Mathsion Wong",
            "user": "QiYao-Wang",
            "type": "user"
          },
          "name": "Qiyao Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:28.639Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6d",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6e",
          "name": "Siwei Wu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae6f",
          "name": "Tianshun Xing",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae70",
          "name": "Ming Xu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae71",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae72",
          "name": "Zekun Moore Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae73",
          "name": "Junting Zhou",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae74",
          "name": "Yuelin Bai",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae75",
          "name": "Xingyuan Bu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae76",
          "name": "Chenglin Cai",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae77",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae78",
          "name": "Yifan Chen",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae79",
          "name": "Chengtuo Cheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7a",
          "name": "Tianhao Cheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7b",
          "name": "Keyi Ding",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7c",
          "name": "Siming Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7d",
          "name": "Yun Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7e",
          "name": "Yaoru Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae7f",
          "name": "Yizhe Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae80",
          "name": "Zhaoqun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae81",
          "name": "Tianhao Liang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae82",
          "name": "Chengdong Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae83",
          "name": "Hongquan Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae84",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae85",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae86",
          "user": {
            "_id": "65adda5299c3bd19c74d6a8d",
            "avatarUrl": "/avatars/1ce504b64ab60f375b235ebaf81cafd6.svg",
            "isPro": false,
            "fullname": "PENG ZIFAN",
            "user": "Ziffer",
            "type": "user"
          },
          "name": "Zifan Peng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:20.429Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae87",
          "name": "Qige Qi",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae88",
          "name": "Shi Qiu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae89",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8a",
          "name": "Yizhou Tan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8b",
          "name": "Zili Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8c",
          "name": "Chenqing Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8d",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8e",
          "name": "Yiya Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae8f",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae90",
          "name": "Jiajun Xu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae91",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae92",
          "name": "Ruibin Yuan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae93",
          "name": "Yuanhao Yue",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae94",
          "name": "Tianyang Zhan",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae95",
          "name": "Chun Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae96",
          "name": "Jingyang Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae97",
          "name": "Xiyue Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae98",
          "name": "Xingjian Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae99",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9a",
          "name": "Yongchi Zhao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9b",
          "name": "Xiangyu Zheng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9c",
          "name": "Chenghua Zhong",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9d",
          "name": "Yang Gao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9e",
          "name": "Zhoujun Li",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8ae9f",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea0",
          "user": {
            "_id": "612ee6a7b960e78c6d2319d4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "isPro": false,
            "fullname": "Qian Liu",
            "user": "SivilTaram",
            "type": "user"
          },
          "name": "Qian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:32.399Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea1",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea2",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea3",
          "name": "Junran Peng",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea4",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea5",
          "name": "Wenbo Su",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea6",
          "name": "Guoyin Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea7",
          "name": "Shi Wang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea8",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aea9",
          "name": "Min Yang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeaa",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeab",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeac",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aead",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeae",
          "user": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "isPro": false,
            "fullname": "Jiaheng Liu",
            "user": "CheeryLJH",
            "type": "user"
          },
          "name": "Jiaheng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:22.185Z",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeaf",
          "name": "Qunshu Lin",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeb0",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67b7efc26348a1df80a8aeb1",
          "name": "Ge Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T17:05:58.000Z",
      "title": "SuperGPQA: 285 卒業学部の分野でのLLM評価のスケーリング",
      "summary": "大語言モデル（LLMs）は、数学、物理学、コンピュータ科学などの主流学術分野で驚異的な優れた効能性を示しています。しかし、人間の知識は200以上の専門分野を含み、現在のベンチマークの範囲を超えています。LLMsは、特に輸出業、農業、サービス関連分野の許多の専門分野での能力は、十分に評価されていません。この欠陥を解決するために、私たちはSuperGPQAという、285分野の大学院レベルの知識と推理能力を評価する厳密なベンチマークを紹介します。我々のベンチマークは、LLMの回答と専門家のフィードバックに基づいた反復的な改善を通じて、簡単か不明確な質問を除去する新しい人間-LLM協議フィルタリング機構を使用しています。実験結果から、現在の最先端のLLMsが多様な知識領域での性能には大幅な向上の余地があることが明らかになり、現在のモデル能力と人工知能の間の大きな間違いが明らかになります。また、私たちは、80以上の専門家アノテータと相互作用する人間-LLM協議システムを含む大規模なアノテーションプロセスの管理から、将来の研究イニシアティブにおいて有効な方法学のガイドラインを提供する詳細な見解を提供します。",
      "upvotes": 61,
      "discussionId": "67b7efc66348a1df80a8afc8"
    },
    "publishedAt": "2025-02-20T22:15:33.133Z",
    "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14739.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14786",
      "authors": [
        {
          "_id": "67b7ed0d58f6b70b18dda7b4",
          "name": "Michael Tschannen",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b5",
          "name": "Alexey Gritsenko",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b6",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b7",
          "name": "Muhammad Ferjad Naeem",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b8",
          "name": "Ibrahim Alabdulmohsin",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7b9",
          "name": "Nikhil Parthasarathy",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7ba",
          "name": "Talfan Evans",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bb",
          "name": "Lucas Beyer",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bc",
          "name": "Ye Xia",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bd",
          "name": "Basil Mustafa",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7be",
          "name": "Olivier Hénaff",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7bf",
          "name": "Jeremiah Harmsen",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7c0",
          "name": "Andreas Steiner",
          "hidden": false
        },
        {
          "_id": "67b7ed0d58f6b70b18dda7c1",
          "name": "Xiaohua Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:08:29.000Z",
      "title": "SigLIP 2: 多言語ビジョン言語エンコーダーで、構文的理解、局在化、デンス特徴量を改善",
      "summary": "シグラフィープリミューム2、新たな多言語ビジョン言語エンコーダーの家族を紹介します。これは元のシグラフィープリミュームの成功に基づいて開発されました。この2世代目では、元の画像テキストトレーニングオブジェクティブに、独立に開発された数多くの先進的な技術を統合したレシピに変更しました。これには、キャプチングベースの予ちゅう、自動転写ロス（self-distillation、マスク付き予測）およびオンラインデータカレーティオンを含みます。これらの変更により、シグラフィープリミューム2モデルは、シグラフィープリミュームモデルと比較して、コア機能のすべてにおいてもっとも優れています。特に、ゼロショットクラス分類、画像テキスト検索、ビジョン言語モデル（VLM）のビジョン表現の抽出性能においてもより優れています。また、新しいトレーニングレシピは、ロケーションタスクと密な予測タスクにおいても大幅な向上を収めます。また、多サイズ対応と入力の原生の横軸比を保ったモデルバージョンも学習しています。また、デバイステクニックを含むより多様なデータミックスで学習し、多言語理解の向上と公平性の向上を収めました。ユーザーが推論コストと性能を調整するために、ViT-B（86M）、L（303M）、So400m（400M）、g（1B）の4つのサイズのモデルチェックポイントをリリースします。",
      "upvotes": 50,
      "discussionId": "67b7ed0e58f6b70b18dda7f4"
    },
    "publishedAt": "2025-02-20T22:33:22.039Z",
    "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14786.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14382",
      "authors": [
        {
          "_id": "67b7ed3e58f6b70b18ddb4bc",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4bd",
          "user": {
            "_id": "64ebbae6895a36ab28de811a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ebbae6895a36ab28de811a/gBiaQP4paS4L13eu-yRm7.jpeg",
            "isPro": false,
            "fullname": "Shiyi Cao",
            "user": "eva98",
            "type": "user"
          },
          "name": "Shiyi Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:43.358Z",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4be",
          "name": "Chengkun Cao",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4bf",
          "name": "Xiuyu Li",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c0",
          "name": "Shangyin Tan",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c1",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c2",
          "name": "Jiarong Xing",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c3",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "67b7ed3e58f6b70b18ddb4c4",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:18:53.000Z",
      "title": "S*: コード生成の時間スケーリングテスト",
      "summary": "検査時の計算量を増やすことが、LLMの複数の領域で望ましい結果を示しているが、コード生成には数学に比べて詳細な研究がない。本論文では、生成されるコードのカバレージと選択精度を大幅に向上させる最初のハイブリッド検査時スケーリングフレームワークS*を提案します。S*は現在の並列スケーリングパラダイムに順序スケーリングを追加し、性能の限界を突き抜けます。また、新しい選択機構を活用し、対比的な入力を適応的に生成し、実行ベースの情報を組み合わせて正しい解決策を強固に特定することができます。12つの大規模言語モデルと大規模論理モデルを検証し、以下の結果を示します：1) S*はモデルの家族とサイズで一貫的に性能を向上させ、3BモデルがGPT-4o-miniを超えます。2) S*は論理モデルを超えることができ、GPT-4o-miniはLiveCodeBenchでo1-previewを3.7%超えます。3) S*は最先端の論理モデルをさらに向上させ、DeepSeek-R1-Distill-Qwen-32BはLiveCodeBenchで85.7%を達成し、o1 (high)の88.5%に近づきます。コードはhttps://github.com/NovaSky-AI/SkyThoughtから利用可能です。",
      "upvotes": 29,
      "discussionId": "67b7ed3f58f6b70b18ddb510"
    },
    "publishedAt": "2025-02-20T22:04:42.635Z",
    "title": "S*: Test Time Scaling for Code Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14382.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14258",
      "authors": [
        {
          "_id": "67b7fa96c3f48f8b3fc632fe",
          "name": "Yein Park",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc632ff",
          "name": "Chanwoong Yoon",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63300",
          "name": "Jungwoo Park",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63301",
          "name": "Minbyul Jeong",
          "hidden": false
        },
        {
          "_id": "67b7fa96c3f48f8b3fc63302",
          "name": "Jaewoo Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T04:52:05.000Z",
      "title": "時間はどこに置かれているのか？時間ヘッド：言語モデルが時間的な情報を記憶する場所",
      "summary": "この英語のテキストを日本語に翻訳します。\n\n「言語モデルが事実を引き出す能力は広く調査されていますが、それらが時間的に変化する事実をどのように處理しているのかは調査が少ないです。時間的知識を処理するための特別なアテンションヘッド「Temporal Heads」を見つけました。これらのヘッドは複数のモデルにも存在し、その位置は異なりますが、知識の種類や相対的な年数によって応答が異なります。これらのヘッドを非効能にすると、時間的知識を特定する能力が低下しますが、時間的不変性と問題解決能力は損失しません。また、これらのヘッドは「2004年」の数値条件でも「…の年」の文字的な代名詞でも活性化されます、これはそれらは簡単な数値表現よりも時間的な次元を表現していることを示しています。さらに、これらのヘッドの値を調整することで時間的知識を編集できることを示しました。」",
      "upvotes": 17,
      "discussionId": "67b7fa9ac3f48f8b3fc63452"
    },
    "publishedAt": "2025-02-20T23:02:42.672Z",
    "title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64587be872b60ae7a3817858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
      "fullname": "Minbyul Jeong",
      "name": "Minbyul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14834",
      "authors": [
        {
          "_id": "67b7f3c4d00e69f10cff219e",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff219f",
          "name": "Yucheng Wang",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a0",
          "name": "Daniel Zhang-Li",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a1",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a2",
          "name": "Jifan Yu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a3",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a4",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a5",
          "name": "Huiqin Liu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a6",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a7",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "67b7f3c4d00e69f10cff21a8",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:47:36.000Z",
      "title": "LongWriter-V: 視覚言語モデルでの超長期間と高品質な生成を可能にする",
      "summary": "現在の大規模なビジョン言語モデル（LVLMs）は、128kのビジョンとテキストトークンのコンテキスト長を扱うことができますが、1,000単語を超えるコラーニティな出力を生成することが困難です。私たちは、主な制限は、超監督学習（SFT）の際に長い出力例が存在しないことです。この問題を解決するために、LongWriter-V-22kを紹介します。これは、22,158例を含むSFTデータセットで、各例に複数の入力画像、指示、および0から10,000単語の範囲の対応出力があります。また、入力画像に高精度に対応した長い出力を実現するためには、Direct Preference Optimization（DPO）をSFTモデルに適用します。人間の反饋を長い出力（例：3,000単語）に対して集めることは高価ですので、IterDPOを提案します。これは、長い出力を区切り、記憶録の修正を使用して、元の出力との好みペアを形成します。また、MMLongBench-Writeを開発しました。これは、VLMsの長生成能力を評価するためのベンチマークで、6つのタスクを挙げています。私たちの7Bパラメータモデルは、LongWriter-V-22kとIterDPOを用いて訓練され、このベンチマークで驚異的な性能を収め、GPT-4oやその他の大きなプロプライモデルを上回ります。コードとデータは、https://github.com/THU-KEG/LongWriter-V にあります。",
      "upvotes": 15,
      "discussionId": "67b7f3c7d00e69f10cff2258"
    },
    "publishedAt": "2025-02-20T22:39:21.551Z",
    "title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/8AYx7CcK4CT6flX3nRDlB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14834.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14768",
      "authors": [
        {
          "_id": "67b7f08c357c2729ac20a81b",
          "name": "Tian Xie",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81c",
          "name": "Zitian Gao",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81d",
          "name": "Qingnan Ren",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81e",
          "name": "Haoming Luo",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a81f",
          "name": "Yuqian Hong",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a820",
          "name": "Bryan Dai",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a821",
          "name": "Joey Zhou",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a822",
          "name": "Kai Qiu",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a823",
          "name": "Zhirong Wu",
          "hidden": false
        },
        {
          "_id": "67b7f08c357c2729ac20a824",
          "name": "Chong Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T17:49:26.000Z",
      "title": "Logic-RL: ルールベースの強化学習によるLLMの理由論の解放",
      "summary": "DeepSeek-R1の成功をヒントに、大規模な理由論モデルでルールベースの強化学習（RL）の可能性を検討します。理由論の動態を分析するために、制御可能な複雑さと簡単な答えの確認が可能なため、合成的なロジックパズルをトレーニングデータとして使用します。RLの効果的かつ安定したトレーニングにつながる重要な技術的な貢献をします：思考と回答のプロセスを強調するシステムプロンプト、短縮を賞恵するように厳格なフォーマット報酬関数、そして安定した収束を達成する簡単なトレーニングレシピ。7Bモデルは、ロジックコーパスからない先進的な理由論スキルを開発します：反省、確認、要約など。特に、5Kのロジック問題でのトレーニング後に、AIMEとAMCの難しい数学ベンチマークに対しての一般化能力を示します。",
      "upvotes": 14,
      "discussionId": "67b7f08e357c2729ac20a88f"
    },
    "publishedAt": "2025-02-20T22:19:05.902Z",
    "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14768.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14282",
      "authors": [
        {
          "_id": "67b7f5587f4d732dc469270e",
          "name": "Haowei Liu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc469270f",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692710",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692711",
          "name": "Yuyang Wanyan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692712",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692713",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692714",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692715",
          "name": "Chunfeng Yuan",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692716",
          "name": "Changsheng Xu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692717",
          "name": "Weiming Hu",
          "hidden": false
        },
        {
          "_id": "67b7f5587f4d732dc4692718",
          "name": "Fei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T05:41:55.000Z",
      "title": "PC-Agent: パソコン上の複雑なタスク自動化のための階層的な多エージェント協力フレームワーク",
      "summary": "マルチランプライムラインモデル（MLLM）に基づくGUIアガントの分野で、スマートフォンと比較して、PCシナリオは互換性の高い環境を特徴とし、アプリ間の複雑なワークフローも含まれています。これらの問題を解決するために、私たちはPC-Agentというハイラーエージェントフレームワークを提案します。特に、視覚的な観点から、現在のMLLMがスクリーンショットの内容を見逃す能力を克服するために、アクティブな視覚認識モジュール（APM）を設計します。決策の観点から、複雑なユーザー指示と相互依存したサブタスクを効果的に処理するために、決策プロセスを指示-サブタスク-アクションレベルに分解するヒューリスティックな複数アガント協力アーキテクチャを提案します。このアーキテクチャでは、指示分解、進捗追跡、ステップごとの決策を行うために、Manager、Progress、Decisionの3つのアガントが設置されます。また、適時の誤りの上流へのフィードバックと調整を可能にするために、Reflectionアガントを採用します。また、25件の実世界の複雑な指示を含む新たなベンチマークPC-Evalを紹介します。PC-Evalでの実験結果によると、私たちのPC-Agentは前の最先端の方法に対してタスク成功率が約32%増加しました。コードは公開的に利用可能になります。",
      "upvotes": 11,
      "discussionId": "67b7f55b7f4d732dc46927c1"
    },
    "publishedAt": "2025-02-20T22:39:48.180Z",
    "title": "PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/feg9OYb4onJJermpjc6nh.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14282.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14499",
      "authors": [
        {
          "_id": "67b7ee1dfedfe971271dcca0",
          "user": {
            "_id": "6114c9fae7a2566ae7d1a1a7",
            "avatarUrl": "/avatars/c71ab1850322fcf5ef239cb8d31cb137.svg",
            "isPro": false,
            "fullname": "Deepak Nathani",
            "user": "dnathani",
            "type": "user"
          },
          "name": "Deepak Nathani",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-21T07:20:46.836Z",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca1",
          "name": "Lovish Madaan",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca2",
          "name": "Nicholas Roberts",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca3",
          "name": "Nikolay Bashlykov",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca4",
          "name": "Ajay Menon",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca5",
          "name": "Vincent Moens",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca6",
          "name": "Amar Budhiraja",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca7",
          "name": "Despoina Magka",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca8",
          "name": "Vladislav Vorotilov",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dcca9",
          "name": "Gaurav Chaurasia",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccaa",
          "name": "Dieuwke Hupkes",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccab",
          "name": "Ricardo Silveira Cabral",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccac",
          "name": "Tatiana Shavrina",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccad",
          "name": "Jakob Foerster",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccae",
          "name": "Yoram Bachrach",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccaf",
          "name": "William Yang Wang",
          "hidden": false
        },
        {
          "_id": "67b7ee1dfedfe971271dccb0",
          "user": {
            "_id": "633e94793a17ab61de8e2b9c",
            "avatarUrl": "/avatars/5f2f58ddeed211393660ada6b135f0d5.svg",
            "isPro": false,
            "fullname": "Roberta Raileanu",
            "user": "rraileanu",
            "type": "user"
          },
          "name": "Roberta Raileanu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-21T03:08:15.471Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T12:28:23.000Z",
      "title": "MLGym: AI研究アガントの進歩を促進する新しいフレームワークとベンチマーク",
      "summary": "Meta MLGymとMLGym-Bench、新しいフレームワークとベンチマークです。これは、LLMアガントの評価と開発を行うためのAI研究タスクにおける新しいフレームワークとベンチマークです。まず、マシン学習（ML）タスクの最初のGym環境です。これにより、このようなアガントの学習用の強化認知学習（RL）アルゴリズムの研究が可能になります。MLGym-Benchは、コンピュータビジョン、自然言語処理、強化認知学習、ゲーム理論などの多様な領域から13つの多様な開放的なAI研究タスクから構成されています。これらのタスクを解決するには、新しいアイデアと仮説の生成、データの作成と処理、ML方法の実装、モデルの訓練、実験の実行、結果の分析、そしてこのプロセスを繰り返して特定のタスクにおける改善を図るための実世界的なAI研究スキルが必要です。私たちは、Claude-3.5-Sonnet、Llama-3.1 405B、GPT-4o、o1-preview、Gemini-1.5 Proなどの数多くの先進的な大規模言語モデル（LLMs）を私たちのベンチマークで評価しています。私たちのMLGymフレームワークは、新しいタスクの追加、モデルまたはアガントの統合と評価、スケールアップでの合成データの生成、AI研究タスクでのアガントの学習用の新しいアルゴリズムの開発を簡単に行うことができます。私たちは、現在の先進的なモデルは、通常、ベースラインを改善するためにより良いパラメーターを見つけることで改善ができることを見出しましたが、新しい仮説、アルゴリズム、アーキテクチャ、または大規模な改善を生成することはできませんでした。私たちは、このフレームワークとベンチマークを開放ソースして、LLMアガントのAI研究能力の進歩を促進するための将来的な研究を支援することを目的としています。",
      "upvotes": 9,
      "discussionId": "67b7ee1ffedfe971271dcd3a"
    },
    "publishedAt": "2025-02-20T22:08:38.225Z",
    "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14499.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14844",
      "authors": [
        {
          "_id": "67b7f5ee8b3dff28b749be78",
          "name": "Rameen Abdal",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be79",
          "name": "Or Patashnik",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7a",
          "name": "Ivan Skorokhodov",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7b",
          "name": "Willi Menapace",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7c",
          "name": "Aliaksandr Siarohin",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7d",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7e",
          "name": "Daniel Cohen-Or",
          "hidden": false
        },
        {
          "_id": "67b7f5ee8b3dff28b749be7f",
          "name": "Kfir Aberman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:53:39.000Z",
      "title": "Dynamic Concepts Personalization from Single Videos",
      "summary": "個人化生成的文字から画像モデルにおける進歩は驚異的ですが、これを文字から動画モデルへの個人化に拡張すると特別な課題があります。静的な概念と違い、動画モデルの個人化は動的な概念を捉えることができることがあります。そして、その存在を表すのは、外見だけではなく、動きも含むものです。本論文では、動的な概念を用いたDiffusion Transformers（DiTs）ベースの生成的な動画モデルの個人化に向けて、新しいフレームワーク「Set-and-Sequence」を介しています。我々のアプローチは、空間と時間の特徴を明示的に分離しない構造内で空間時間的な重み空間を課します。これは2つの鍵のステージで実現されます。まず、動画の無順列のフレームを用いてLow-Rank Adaptation（LoRA）レイヤーを微調校し、時間的な干渉から外見を表すidentity LoRAベースを学習します。2つ目のステージでは、identity LoRAが固定された状態で、その係数にMotion Residualsを加え、全体の動画シーケンスにおいて微調校し、動きのダイナミクスを捉えます。「Set-and-Sequence」フレームワークは、動画モデルの出力領域に動的な概念を有効に埋め込み、前所未聞の編集可能性と組織性を設定し、動的な概念の個人化に新たなベンチマークを設定します。",
      "upvotes": 8,
      "discussionId": "67b7f5f18b3dff28b749bf45"
    },
    "publishedAt": "2025-02-20T22:41:47.210Z",
    "title": "Dynamic Concepts Personalization from Single Videos",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14844.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14372",
      "authors": [
        {
          "_id": "67b81870cc6b0136b3d84254",
          "user": {
            "_id": "6530a78069751712276d60ed",
            "avatarUrl": "/avatars/2ef4f16d0be557ed60c11d8dcef85f6f.svg",
            "isPro": false,
            "fullname": "Austin He",
            "user": "basil2115",
            "type": "user"
          },
          "name": "Austin Yubo He",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-21T06:30:16.645Z",
          "hidden": false
        },
        {
          "_id": "67b81870cc6b0136b3d84255",
          "name": "Zi-Wen Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:05:34.000Z",
      "title": "量子エラー修正コードの高効率低重量化を実現するための強化学習を用いた研究",
      "summary": "可損失許容量子計算のスケーラブル性の実現は、量子エラー補正コードに寄与して期待されています。量子エラー補正の関係でのより効率的なエラー許容性を追求する過程で、重要なコードパラメーターは、誤りを特定するための測定の重みです：高い測定重みは実装コストを高め、エラーを引き起こすことがあるため、コード設計で測定重みを最適化することが重要です。これは、量子低密度パリティチェック（qLDPC）コードに対する興味の高まりに基づいています。これらの研究は主にアスインプテクス（大コードの限界）の性質に焦点を当てていました。本研究では、強化学習（RL）に基づくステラビラーコードの重み削減の柔軟かつ計算的に効率的なアプローチを紹介します。これにより、実用的なパラメーターレイムで新しい低重みコードを生成し、現在の最高レベルに比べて大幅に優位を示します。例えば、重み6のコードに対して、現在の結果と比較して物理キューブのオーバーヘッドを1〜2オーダーのほど削減でき、近未来の実験に対しては実用的な範囲に引き込みます。また、RLフレームワークを用いてコードパラメーターの相互作用を調査し、実用的なコーディング戦略の可能性と効率性に新しい見解を提供します。全体として、本研究の結果は、RLが量子コードの発見の重要なばかりで難しい問題を進めることができることを示し、故障許容量子テクノロジーの実用的な実装により速める道を提供します。",
      "upvotes": 7,
      "discussionId": "67b81873cc6b0136b3d8430a"
    },
    "publishedAt": "2025-02-21T01:11:34.971Z",
    "title": "Discovering highly efficient low-weight quantum error-correcting codes with reinforcement learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14372.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6530a78069751712276d60ed",
      "avatarUrl": "/avatars/2ef4f16d0be557ed60c11d8dcef85f6f.svg",
      "fullname": "Austin He",
      "name": "basil2115",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14846",
      "authors": [
        {
          "_id": "67b7f4f1b15c19d57189fc5e",
          "name": "Yue Yang",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc5f",
          "name": "Ajay Patel",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc60",
          "name": "Matt Deitke",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc61",
          "name": "Tanmay Gupta",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc62",
          "name": "Luca Weihs",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc63",
          "name": "Andrew Head",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc64",
          "name": "Mark Yatskar",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc65",
          "name": "Chris Callison-Burch",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc66",
          "name": "Ranjay Krishna",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc67",
          "name": "Aniruddha Kembhavi",
          "hidden": false
        },
        {
          "_id": "67b7f4f1b15c19d57189fc68",
          "name": "Christopher Clark",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T18:55:30.000Z",
      "title": "コードガイドされた合成モノモーダルデータ生成によるテキスト豊富な画像理解のスケーリング",
      "summary": "Reasoning about images with rich text, such as charts and documents, is a critical application of vision-language models (VLMs). However, VLMs often struggle in these domains due to the scarcity of diverse text-rich vision-language data. To address this challenge, we present CoSyn, a framework that leverages the coding capabilities of text-only large language models (LLMs) to automatically create synthetic text-rich multimodal data. Given input text describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic images. With the underlying code as textual representations of the synthetic images, CoSyn can generate high-quality instruction-tuning data, again relying on a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K images and 2.7M rows of vision-language instruction-tuning data. Comprehensive experiments on seven benchmarks demonstrate that models trained on our synthetic data achieve state-of-the-art performance among competitive open-source models, including Llama 3.2, and surpass proprietary models such as GPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing data, enabling VLMs to ground information within input images, showcasing its potential for developing multimodal agents capable of acting in real-world environments.",
      "upvotes": 5,
      "discussionId": "67b7f4f2b15c19d57189fc95"
    },
    "publishedAt": "2025-02-20T22:38:36.406Z",
    "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12853",
      "authors": [
        {
          "_id": "67b69b6717ccb022c6a95b38",
          "name": "Ruotian Ma",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b39",
          "name": "Peisong Wang",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3a",
          "name": "Cheng Liu",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3b",
          "name": "Xingyan Liu",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3c",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3d",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3e",
          "name": "Xin Zhou",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b3f",
          "name": "Nan Du",
          "hidden": false
        },
        {
          "_id": "67b69b6717ccb022c6a95b40",
          "name": "Jia Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T13:40:22.000Z",
      "title": "S^2R: 強化学習による自動証明と自動修正の教養を与えるLLM",
      "summary": "最近の研究は、LLMのテスト時スケーリングの効果性を示しています。しかし、現在のLLMの深い思考能力を促す方法は、通常、大規模なデータや訓練の努力が必要となります。一方で、ベースモデルの思考能力を向上させる方法は明確ではありません。本研究では、S^2Rという効率的なフレームワークを介して、LLMの推理を強化し、推論中に自動的に自覚証と補正を行うモデルを教える方法を提案します。特に、最初に、調整されたデータによる規範的調整訓練で、イテレーション的な自覚証と補正の行動を初期化します。その後、結果レベルとプロセスレベルの強化学習により、自覚証と補正のスキルを進一步に強化し、資源の要求を最小限に抑え、推論中に推理プロセスを適応的に改善することができます。我々の結果は、3.1kの自覚証と補正の行動を初期化したもので、Qwen2.5-math-7Bの正確性は51.0%から81.6%に上がり、同量の長コシードディスティルデータで訓練されたモデルを超えています。3つのベースモデルを構成する領域内と外領域ベンチマークに基づいた拡大的な実験と分析は、S^2Rの効果性を証明しています。我々のコードとデータは、https://github.com/NineAbyss/S2Rに公開されています。",
      "upvotes": 4,
      "discussionId": "67b69b6817ccb022c6a95b6e"
    },
    "publishedAt": "2025-02-21T05:00:18.645Z",
    "title": "S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648294b2eb4befee378951c1",
      "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
      "fullname": "Ruotian Ma",
      "name": "vvibt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14669",
      "authors": [
        {
          "_id": "67b7eeddaf9f1b1bd95b878b",
          "name": "Alan Dao",
          "hidden": false
        },
        {
          "_id": "67b7eeddaf9f1b1bd95b878c",
          "name": "Dinh Bach Vu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T16:05:18.000Z",
      "title": "AlphaMaze: グローバルプロジェクト（GRPO）をもとに大規模言語モデルの空間知能を向上させる",
      "summary": "大語言モデル（LLMs）は、言語処理に卓越した能力を示していますが、真の視覚空間的理由を必要とするタスクには難しいという問題があります。本論文では、通常のLLMsに迷宮探索の視覚理由能力を付け加えるための新しい2段階トレーニングフレームワークを介しています。まず、統計的に精読された迷宮表現のコレクションデータセットに対するSupervised Fine Tuning（SFT）を利用し、モデルをステップごとの移動命令を予測することを教えます。次に、DeepSeekR1で使用されるGroup Relative Policy Optimization（GRPO）を採用し、シャープな賞与関数を用いてモデルの連続的な決策を改善し、チャイムオブサイズンブールの行動を励まします。合成的に生成された迷宮に対する実験結果によると、ベースラインモデルは迷宮を探索できないことがわかりますが、SFTトレーニングされたモデルは86%の精度を達成し、GRPOの追加トレーニングは精度を93%に上げます。質的な分析により、GRPOはより強固な自補正の理由を促進し、我々のアプローチが言語モデルと視覚空間的タスクの間の隙を埋める可能性を明らかにします。これらの発見は、ロボット工学、自動ナビゲーション、その他の領域での統合的な視覚と連続的な理由を必要とするアプリケーションにおいて、望ましい意味を持ちます。",
      "upvotes": 4,
      "discussionId": "67b7eeddaf9f1b1bd95b87c8"
    },
    "publishedAt": "2025-02-20T22:11:45.130Z",
    "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14377",
      "authors": [
        {
          "_id": "67b7f350357c2729ac216494",
          "name": "Ke Cao",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216495",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216496",
          "name": "Ao Ma",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216497",
          "name": "Jiasong Feng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216498",
          "name": "Zhanjie Zhang",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac216499",
          "name": "Xuanhua He",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649a",
          "name": "Shanyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649b",
          "name": "Bo Cheng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649c",
          "name": "Dawei Leng",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649d",
          "name": "Yuhui Yin",
          "hidden": false
        },
        {
          "_id": "67b7f350357c2729ac21649e",
          "name": "Jie Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:10:05.000Z",
      "title": "RelaCtrl: 関連性ガイドされた効率的なコントロールのためのDiffusion Transformers",
      "summary": "Diffusion Transformerは、テキストから画像およびテキストから動画の生成において重要な役割を果たし、主に機能的なスケーラビリティによって進化させています。しかし、現在の制御されたDiffusion Transformerの方法は、パラメータと計算オーバーヘッドが大きく、ドライバーレベルの制御情報の関連性の差異に対応しないため、資源の不適切な割り当てによりエフィシェンスが低下しています。これに対して、私たちはRelevance-Guided Efficient Controllable Generationフレームワーク、RelaCtrlを提案し、Diffusion Transformerに制御シグナルの効率的なフォーマット化を可能にします。まず、Diffusion Transformerの各レイヤーに対する制御情報の関連性を評価し、\"ControlNet Relevance Score\"を使用して、それぞれの制御レイヤーを省略した場合の生成品質と推論時の制御効果を評価します。この関連性の強さに基づき、次に、制御レイヤーの位置ディレクトリ、パラメータスケールとモデリング能力を調整し、必要なパラメータと冗長な計算を削減することを目指します。また、よりエフィシェンスを向上させるために、通常のコピーブロックに使用される自動注意とFFNを謹重に設計されたTwo-Dimensional Shuffle Mixer（TDSM）に置き換え、トークンミッシャーとチャネルミッシャーの両方の効率的な実装を可能にします。定性的および定量的な実験結果は、PixArt-deltaに比べてパラメータと計算複雑度が15%だけで、より優れた性能を収めることを示しています。追加の例は、https://relactrl.github.io/RelaCtrl/から利用できます。",
      "upvotes": 3,
      "discussionId": "67b7f354357c2729ac216582"
    },
    "publishedAt": "2025-02-20T22:30:51.542Z",
    "title": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14377.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6161
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13759",
      "authors": [
        {
          "_id": "67b83a1f26e7d5f7cb0b7c9d",
          "user": {
            "_id": "65407ba7a38390065750233f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
            "isPro": false,
            "fullname": "Zirui Song",
            "user": "Ziruibest",
            "type": "user"
          },
          "name": "Zirui Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:04.247Z",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7c9e",
          "name": "Jingpu Yang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7c9f",
          "name": "Yuan Huang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca0",
          "name": "Jonathan Tonglet",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca1",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca2",
          "name": "Tao Cheng",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca3",
          "name": "Meng Fang",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca4",
          "name": "Iryna Gurevych",
          "hidden": false
        },
        {
          "_id": "67b83a1f26e7d5f7cb0b7ca5",
          "name": "Xiuying Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T14:21:25.000Z",
      "title": "ジオリエクションによる実際の人間のゲームプレイデータを用いたもの：大規模なデータセットと人間のような論理フレームワーク",
      "summary": "ジオリポジティング、画像の位置を特定する任務、複雑な理由を必要とし、ナビゲーション、監視、文化保存に重要です。しかし、現在の方法は通常、粗略で不精確な位置情報を生成し、解釈可能ではありません。問題の一つは、現在のジオリポジティングデータセットの質とスケールです。これらのデータセットは通常、小規模で自動的に構築され、ノイズづけされたデータと仕事の難易度の不均一性により、答えを過度に示す画像や、信頼性のある推論に必要な十分なキープを欠く画像が含まれています。これらの課題を解決するために、私たちは、3つのキーコンポーネントを持つ厳密なジオリポジティングフレームワークを紹介します：GeoComp、大規模なデータセット、GeoCoT、新しい理由方法、GeoEval、評価指標。このフレームワークの核心は、GeoComp（ジオリポジティングコンペティションデータセット）です。これは、2年間の740Kユーザーから撮影された大規模なデータセットで、世界中の多くの地域に広がる2500万件のメタデータと300万件のジオタグされた位置情報を含み、各位置は人間ユーザーが数千から数十万回のニュース付けされています。このデータセットは、詳細な分析のための多様な難易度レベルを提供し、現在のモデルの重要な欠点を明らかにします。このデータセットに基づいて、私たちは、大規模な視覚モデル（LVMs）の理由能力を強化するための新しい多段階理由フレームワーク、Geographical Chain-of-Thought（GeoCoT）を提案します。GeoCoTは、人間のジオリポジティング理由をミミックする多段階プロセスを通じて、コンテキストと空間的キープを統合し、性能を向上させます。最後に、GeoEval指標を使用して、GeoCoTが地理位置の正確性を最高25%程度向上させ、解釈性を向上させることを示します。",
      "upvotes": 1,
      "discussionId": "67b83a2226e7d5f7cb0b7d66"
    },
    "publishedAt": "2025-02-21T03:33:28.852Z",
    "title": "Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13759.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65407ba7a38390065750233f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
      "fullname": "Zirui Song",
      "name": "Ziruibest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14409",
      "authors": [
        {
          "_id": "67b83a20a9fa331061e84ecd",
          "user": {
            "_id": "60a643b9213fe60589b8fdf9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a643b9213fe60589b8fdf9/OOXmW3MkSf88r63tAE6-n.jpeg",
            "isPro": false,
            "fullname": "Dustin Wright",
            "user": "dwright37",
            "type": "user"
          },
          "name": "Dustin Wright",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:58:02.288Z",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ece",
          "name": "Zain Muhammad Mujahid",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ecf",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ed0",
          "name": "Isabelle Augenstein",
          "hidden": false
        },
        {
          "_id": "67b83a20a9fa331061e84ed1",
          "name": "David Jurgens",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:57:42.000Z",
      "title": "無構造的証拠の属性化と長文脈クエリに焦点を当てた要約化",
      "summary": "大語言モデル（LLMs）は、ユーザークエリによって非常に長いコンテキストからコラーゼントな要約を生成することができます。証拠スパンの抜粋と正しい引用が透明性と信頼性を向上させることができます。一方、LLMsは理解して注意を集中させる情報についての位置バイアスを見せています。これは証拠の引用に影響を及ぼす可能性があります。過去の研究は、特定の粒度（例：文、段落、ドキュメントなど）での証拠引用に焦点を当てていました。しかし、我々は、無構造化の証拠引用を含む長コンテキストクエリに基づく要約の作成を提案します。我々は、現在のシステムがコンテキストから無構造化の証拠を生成し、正しく引用することが困難であることを示し、証拠は「中に迷う」ようになっていることを示します。これを軽減するために、我々は、新しいドメイン無関係的なパイプラインを用いて生成された合成データセットである「SUnsET」（Summaries with Unstructured Evidence Text）を作成しました。このデータセットは、LLMsをこのタスクに適用するためのサポートとして使用できます。5つのディファレントのサイズのLLMsと4つのデータセット（これらのドキュメントの種類と長さが異なる）を横断的に調査し、SUnsETデータを用いて適用されたLLMsは、基礎モデルよりも関連性の高いかつ事実的に一貫した証拠を生成し、コンテキストのより多様な場所から証拠を抜粋し、関連性の高いかつ一貫した要約を生成することを示しました。",
      "upvotes": 0,
      "discussionId": "67b83a21a9fa331061e84f36"
    },
    "publishedAt": "2025-02-21T03:33:40.641Z",
    "title": "Unstructured Evidence Attribution for Long Context Query Focused Summarization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14409.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60a643b9213fe60589b8fdf9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a643b9213fe60589b8fdf9/OOXmW3MkSf88r63tAE6-n.jpeg",
      "fullname": "Dustin Wright",
      "name": "dwright37",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]