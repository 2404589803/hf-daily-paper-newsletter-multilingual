[
  {
    "paper": {
      "id": "2503.19325",
      "authors": [
        {
          "_id": "67e35f6fc9d8214b5e1c64c3",
          "name": "Yuchao Gu",
          "hidden": false
        },
        {
          "_id": "67e35f6fc9d8214b5e1c64c4",
          "name": "Weijia Mao",
          "hidden": false
        },
        {
          "_id": "67e35f6fc9d8214b5e1c64c5",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63021630a35b21bd8a53305a/SL0MQs7OvQpNlGBhroTW3.png"
      ],
      "publishedAt": "2025-03-25T03:38:06.000Z",
      "submittedOnDailyAt": "2025-03-26T00:37:14.940Z",
      "title": "長文脈自動回帰ビデオモデリングと次のフレーム予測",
      "submittedOnDailyBy": {
        "_id": "63021630a35b21bd8a53305a",
        "avatarUrl": "/avatars/7a7e8b39749eda61e57d8a1908726558.svg",
        "isPro": true,
        "fullname": "Gu Yuchao",
        "user": "guyuchao",
        "type": "user"
      },
      "summary": "長文脈自動回帰モデリングは言語生成において大幅に進歩しましたが、映像生成は長期間の時間脈跡を充分に利用することが難しい。長文脈映像モデリングを調査するために、Frame AutoRegressive (FAR) を介して映像自動回帰モデリングの強力な基準を提案します。言語モデルはトークン (Token) の因果的依存関係を学習しますが、FARは連続したフレームの時間的因果的依存関係をモデル化し、Token ARや映像ディフュージョントランスフォーマーよりもより良い収束を達成します。FARを基盤に、長文脈視覚モデリングが視覚的な冗長性により課題を見出していることを見出しました。現在のRoPEは遠い文脈に対する有効な時間的減衰を持たず、長い映像シーケンスにおいても良い推論を行うことができません。また、長い映像の訓練は計算的に高価であり、視覚的なトークンは言語のトークンよりも急速に増大しています。これらの問題に対して、局所性と長距離依存関係のバランスを調整するために、FlexRoPEを提案します。FlexRoPEはRoPEに柔軟な時間的減衰を追加し、16倍長い視覚的な文脈に対する推論を可能にします。また、長短期コンテキストモデリングを提案します。高解像度の短期コンテキストウィンドウで細かい時間的な一貫性を確保し、限らずの長期コンテキストウィンドウでも少ないトークンで長距離情報をエンコードします。このアプローチでは、長い映像シーケンスを訓練できるようになります。FARは短いおよび長い映像生成で最先端の性能を達成し、映像自動回帰モデリングの簡単で効果的な基準を提供します。",
      "upvotes": 49,
      "discussionId": "67e35f72c9d8214b5e1c659b",
      "ai_keywords": [
        "Frame AutoRegressive (FAR)",
        "Token AR",
        "video autoregressive modeling",
        "visual redundancy",
        "RoPE (Rotary Position Embedding)",
        "temporal decay",
        "FlexRoPE",
        "long short-term context modeling",
        "high-resolution short-term context window",
        "long-term context window",
        "state-of-the-art performance",
        "video generation"
      ]
    },
    "publishedAt": "2025-03-24T23:38:06.000Z",
    "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
    "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context vision modeling\nfaces challenges due to visual redundancy. Existing RoPE lacks effective\ntemporal decay for remote context and fails to extrapolate well to long video\nsequences. Additionally, training on long videos is computationally expensive,\nas vision tokens grow much faster than language tokens. To tackle these issues,\nwe propose balancing locality and long-range dependency. We introduce FlexRoPE,\nan test-time technique that adds flexible temporal decay to RoPE, enabling\nextrapolation to 16x longer vision contexts. Furthermore, we propose long\nshort-term context modeling, where a high-resolution short-term context window\nensures fine-grained temporal consistency, while an unlimited long-term context\nwindow encodes long-range information using fewer tokens. With this approach,\nwe can train on long video sequences with a manageable token context length. We\ndemonstrate that FAR achieves state-of-the-art performance in both short- and\nlong-video generation, providing a simple yet effective baseline for video\nautoregressive modeling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63021630a35b21bd8a53305a/SL0MQs7OvQpNlGBhroTW3.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19325.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63021630a35b21bd8a53305a",
      "avatarUrl": "/avatars/7a7e8b39749eda61e57d8a1908726558.svg",
      "fullname": "Gu Yuchao",
      "name": "guyuchao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18931",
      "authors": [
        {
          "_id": "67e25c4d1908043170bd551d",
          "user": {
            "_id": "64651db3611ae99d14d392ea",
            "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
            "isPro": false,
            "fullname": "cyt",
            "user": "Row11n",
            "type": "user"
          },
          "name": "Yitong Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T08:18:45.692Z",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd551e",
          "name": "Lingchen Meng",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd551f",
          "name": "Wujian Peng",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd5520",
          "name": "Zuxuan Wu",
          "hidden": false
        },
        {
          "_id": "67e25c4d1908043170bd5521",
          "name": "Yu-Gang Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T17:52:47.000Z",
      "submittedOnDailyAt": "2025-03-26T01:10:42.553Z",
      "title": "CoMP: 継続的な多モデル化予備学習を用いた視覚基盤モデル",
      "submittedOnDailyBy": {
        "_id": "64651db3611ae99d14d392ea",
        "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
        "isPro": false,
        "fullname": "cyt",
        "user": "Row11n",
        "type": "user"
      },
      "summary": "予ったビジョン基盤モデル（VFMs）は、広範囲のアプリケーションに強力的な可視的表現を提供します。本文では、VFMsを多モディアルな方法で継続的に予って、異なるサイズの可視的入力を容易に処理でき、言語表現によりより一致する可視的表現を生成するようにします。このために、CoMP（Continual Multimodal Pre-training）という謹みのある多モディアル予ってプロセスを介して導入します。CoMPは、Continual Rotary Position Embeddingを使用して、原生レゾルの継続的予ってプロセスをサポートし、さらに、VisualとTextual特徴間のアライメント損失を通じて、言語プロトタイプを用いて多モディアル表現をアラインします。三段階の訓練を通じて、VFMsは、多モディアル理解だけでなく、クラス分類や分割などのダウンストリームタスクにも顕著な改善を収めます。特に、CoMP-SigLIPは、ChartQAで66.7、DocVQAで75.9のスコアを達成し、0.5B LLMを使用していますが、ImageNet-1Kで87.4%の精度、ADE20Kで49.5mIoUを維持しています。",
      "upvotes": 19,
      "discussionId": "67e25c4f1908043170bd55a8",
      "projectPage": "https://slimm-x.github.io/comp/",
      "githubRepo": "https://github.com/SliMM-X/CoMP-MM",
      "ai_keywords": [
        "Vision Foundation Models (VFMs)",
        "Continual Rotary Position Embedding",
        "Alignment Loss",
        "language prototypes",
        "multimodal pre-training pipeline",
        "three-stage training",
        "multimodal understanding",
        "classification",
        "segmentation",
        "ChartQA",
        "DocVQA",
        "LLM",
        "ImageNet-1K",
        "ADE20K",
        "frozen chunk evaluation"
      ]
    },
    "publishedAt": "2025-03-24T13:52:47.000Z",
    "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models",
    "summary": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18931.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64651db3611ae99d14d392ea",
      "avatarUrl": "/avatars/b818dc0dddc999758ab5737d5053e8c3.svg",
      "fullname": "cyt",
      "name": "Row11n",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.19385",
      "authors": [
        {
          "_id": "67e36241d8da46951f858026",
          "name": "Jaihoon Kim",
          "hidden": false
        },
        {
          "_id": "67e36241d8da46951f858027",
          "name": "Taehoon Yoon",
          "hidden": false
        },
        {
          "_id": "67e36241d8da46951f858028",
          "name": "Jisung Hwang",
          "hidden": false
        },
        {
          "_id": "67e36241d8da46951f858029",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T06:30:45.000Z",
      "submittedOnDailyAt": "2025-03-26T00:49:38.583Z",
      "title": "推論時のスケーリングを行う流れモデルにおける確率生成とローラバッジバケット強制法",
      "submittedOnDailyBy": {
        "_id": "6342796a0875f2c99cfd313b",
        "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
        "isPro": false,
        "fullname": "Yuseung \"Phillip\" Lee",
        "user": "phillipinseoul",
        "type": "user"
      },
      "summary": "私たちは予ったフローモデルの推論時スケーリングアプローチを提案します。最近、推論時スケーリングはLLMsとdiffusionモデルにおいても重要な関心を引き起こし、追加計算を利用してサンプルの質の向上やユーザーの好みとのマッチングを改善しています。diffusionモデルにおいては、粒子サンプリングは、中間のデノイズステップの乱数性により、スケーリングを効率的に行うことができます。一方、flowモデルは、state-of-the-art画像とビデオ生成モデルでは高速な生成と高品質の出力を提供して、diffusionモデルの代わりとして人気を獲得していますが、その決定的な生成プロセスにより、diffusionモデルのスケーリング方法は直接適用できません。flowモデルの推論時スケーリングの効率的な方法を実現するために、私たちは3つの主なアイデアを提案します：1) SDEベースの生成、flowモデルでの粒子サンプリングを可能にします、2) インタープロテータンの変換、検索スペースを広げ、サンプルの多様性を向上させます、3) Rollover Budget Forcing (RBF)、時間ステップごとの計算チャネルの適応的な割り当てを行い、マナーズの最大限の利用を図るものです。私たちの実験によると、SDEベースの生成、特にバリアンス保持（VP）インタープロテータンベースの生成は、flowモデルの推論時スケーリングにおける粒子サンプリング方法の性能を向上させます。また、VP-SDEとRBFの組み合わせは、すべての先行する推論時スケーリングアプローチを上回る最も良い性能を示しています。",
      "upvotes": 17,
      "discussionId": "67e36245d8da46951f85802c",
      "ai_keywords": [
        "flow models",
        "inference-time scaling",
        "LLMs",
        "diffusion models",
        "sample quality",
        "user preferences",
        "particle sampling",
        "stochasticity",
        "denoising steps",
        "generative process",
        "SDE-based generation",
        "interpolant conversion",
        "sample diversity",
        "Rollover Budget Forcing (RBF)",
        "adaptive allocation",
        "computational resources",
        "timesteps",
        "budget utilization",
        "variance-preserving (VP)",
        "VP interpolant-based generation"
      ]
    },
    "publishedAt": "2025-03-25T02:30:45.000Z",
    "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
    "summary": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19385.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6342796a0875f2c99cfd313b",
      "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
      "fullname": "Yuseung \"Phillip\" Lee",
      "name": "phillipinseoul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19622",
      "authors": [
        {
          "_id": "67e3706bc9d8214b5e219149",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914a",
          "name": "Jiashu Qu",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914b",
          "name": "Jingyi Tang",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914c",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914d",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914e",
          "name": "Hongyu Chen",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e21914f",
          "name": "Li Liang",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e219150",
          "name": "Li Su",
          "hidden": false
        },
        {
          "_id": "67e3706bc9d8214b5e219151",
          "name": "Qingming Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T13:12:17.000Z",
      "submittedOnDailyAt": "2025-03-26T01:44:03.080Z",
      "title": "ビデオでの大規模多モデルのホラーニングの調査 基準評価、分析と対策",
      "submittedOnDailyBy": {
        "_id": "62728f4f6253fe2068da1021",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
        "isPro": false,
        "fullname": "Hongcheng Gao",
        "user": "HongchengGao",
        "type": "user"
      },
      "summary": "大規模多モダルモデル（LMMs）の幻覚問題は、正しく見えるものの実際に不正な回答を提供し、信頼性と適用可能性を制限しています。本論文は、この問題を研究し、動的なモデルとしての映像モデルに比べてより複雑なバンクモデルの幻覚問題を調査するために、LMMsの映像理解タスクでの幻覚を評価するための詳細なベンチマーク「HAVEN」を提案します。このベンチマークは、幻覚の原因、幻覚の面、質問の形式の3つの次元を基礎に、6Kの質問を提供しています。また、16つのLMMsの実験で7つの影響的な要因（映像の時間、モデルのサイズ、モデルの理由）について定量的に研究します。また、最近のOpenAI o1のような思考モデルのニューアルゲストをヒントに、SRFT（補給的理由訓練）とTDPO（直接な好み最適化）を用いてLMMsの幻覚を抑制するための映像思考モデルを提案します。SRFTは理由論の能力を高め、TDPOは思考プロセスでの幻覚を減らします。拡大的な実験と分析は、幻覚評価の精度に7.65%の改善とバイアススコアの4.5%の減少を示し、このモデルの効果を明らかにします。コードとデータは、https://github.com/Hongcheng-Gao/HAVENで公開されています。",
      "upvotes": 16,
      "discussionId": "67e3706dc9d8214b5e2191e0",
      "githubRepo": "https://github.com/Hongcheng-Gao/HAVEN",
      "ai_keywords": [
        "multimodal models (LMMs)",
        "hallucination",
        "video modality",
        "video understanding",
        "HAVEN",
        "hallucination causes",
        "hallucination aspects",
        "question formats",
        "duration time",
        "model sizes",
        "model reasoning",
        "supervised reasoning fine-tuning (SRFT)",
        "direct preference optimization (TDPO)",
        "video-thinking model",
        "accuracy",
        "bias score"
      ]
    },
    "publishedAt": "2025-03-25T09:12:17.000Z",
    "title": "Exploring Hallucination of Large Multimodal Models in Video\n  Understanding: Benchmark, Analysis and Mitigation",
    "summary": "The hallucination of large multimodal models (LMMs), providing responses that\nappear correct but are actually incorrect, limits their reliability and\napplicability. This paper aims to study the hallucination problem of LMMs in\nvideo modality, which is dynamic and more challenging compared to static\nmodalities like images and text. From this motivation, we first present a\ncomprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in\nvideo understanding tasks. It is built upon three dimensions, i.e.,\nhallucination causes, hallucination aspects, and question formats, resulting in\n6K questions. Then, we quantitatively study 7 influential factors on\nhallucinations, e.g., duration time of videos, model sizes, and model\nreasoning, via experiments of 16 LMMs on the presented benchmark. In addition,\ninspired by recent thinking models like OpenAI o1, we propose a video-thinking\nmodel to mitigate the hallucinations of LMMs via supervised reasoning\nfine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT\nenhances reasoning capabilities while TDPO reduces hallucinations in the\nthinking process. Extensive experiments and analyses demonstrate the\neffectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on\nhallucination evaluation and reduces the bias score by 4.5%. The code and data\nare public at https://github.com/Hongcheng-Gao/HAVEN.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19622.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "62728f4f6253fe2068da1021",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
      "fullname": "Hongcheng Gao",
      "name": "HongchengGao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14905",
      "authors": [
        {
          "_id": "67e250450487eeecfd9a5880",
          "name": "Siwei Wen",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5881",
          "name": "Junyan Ye",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5882",
          "name": "Peilin Feng",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5883",
          "name": "Hengrui Kang",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5884",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5885",
          "name": "Yize Chen",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5886",
          "name": "Jiang Wu",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5887",
          "name": "Wenjun Wu",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5888",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67e250450487eeecfd9a5889",
          "name": "Weijia Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T05:14:44.000Z",
      "submittedOnDailyAt": "2025-03-26T04:00:13.753Z",
      "title": "スポット・ザ・ファイク：大規模多モデルベースの合成画像検出とアーティファクト説明",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "AIGCテクノロジーの急速な進歩に伴い、合成画像は日常生活でよりもらしくなり、真実性の評価と検出に新たな課題をもたらしています。現在の方法は、画像の真実性を評価し、フォレージを特定するには効果的ですが、これらの手法は通常、人間の解釈可能性が低く、合成データの拡大した複雑性に対して十分に対処していません。これらの課題を解決するために、我々はFakeVLMという特化された大規模な多モデルを介して、一般的な合成画像およびDeepFake検出タスクに適したものを紹介します。FakeVLMは、真実かどうかを単に区別するのではなく、画像のアーティファクトに対する自然な言語での説明を提供し、解釈可能性を高めます。また、我々はFakeClueという詳細なアーティファクトのコラットを含む7つのカテゴリーにわたる100,000点以上の画像を含む詳細なデータセットを紹介します。FakeVLMは、エキスパートモデルと同等の性能を示し、追加の分類器の必要性を除去し、合成データ検出の強固な解決策としての可能性を示します。多くのデータセットでの拡大的な評価は、真実性分類およびアーティファクト説明タスクでのFakeVLMの優れた性能を確認し、合成画像検出の新たなベンチマークを設定します。データセットとコードは以下のURLで公開されます：https://github.com/opendatalab/FakeVLM。",
      "upvotes": 12,
      "discussionId": "67e250490487eeecfd9a599e",
      "githubRepo": "https://github.com/opendatalab/FakeVLM",
      "ai_keywords": [
        "large multimodal model",
        "FakeVLM",
        "DeepFake detection",
        "image artifacts",
        "natural language explanations",
        "FakeClue",
        "fine-grained artifact clues"
      ]
    },
    "publishedAt": "2025-03-19T01:14:44.000Z",
    "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\n  with Artifact Explanation",
    "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) technologies, synthetic images have become increasingly prevalent in\neveryday life, posing new challenges for authenticity assessment and detection.\nDespite the effectiveness of existing methods in evaluating image authenticity\nand locating forgeries, these approaches often lack human interpretability and\ndo not fully address the growing complexity of synthetic data. To tackle these\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\nexcels in distinguishing real from fake images but also provides clear, natural\nlanguage explanations for image artifacts, enhancing interpretability.\nAdditionally, we present FakeClue, a comprehensive dataset containing over\n100,000 images across seven categories, annotated with fine-grained artifact\nclues in natural language. FakeVLM demonstrates performance comparable to\nexpert models while eliminating the need for additional classifiers, making it\na robust solution for synthetic data detection. Extensive evaluations across\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\nclassification and artifact explanation tasks, setting a new benchmark for\nsynthetic image detection. The dataset and code will be released in:\nhttps://github.com/opendatalab/FakeVLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14905.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19903",
      "authors": [
        {
          "_id": "67e375d3cc93cc8c42da7699",
          "name": "Baifeng Shi",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769a",
          "name": "Boyi Li",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769b",
          "name": "Han Cai",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769c",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769d",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769e",
          "name": "Marco Pavone",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da769f",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a0",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a1",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a2",
          "name": "Pavlo Molchanov",
          "hidden": false
        },
        {
          "_id": "67e375d3cc93cc8c42da76a3",
          "name": "Hongxu Yin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649004218f7cbbc94c782db6/F0o61glyGm9fib9Pl1i-L.mp4"
      ],
      "publishedAt": "2025-03-25T17:58:37.000Z",
      "submittedOnDailyAt": "2025-03-26T02:13:20.800Z",
      "title": "スケーリングビジョン予習練習を4Kレゾリューションに拡張",
      "submittedOnDailyBy": {
        "_id": "649004218f7cbbc94c782db6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AdgLVfAIpWlug4jXTaEK-.jpeg",
        "isPro": false,
        "fullname": "Baifeng Shi",
        "user": "bfshi",
        "type": "user"
      },
      "summary": "高解像度の視覚詳細の認識は日常的な仕事に重要です。しかし、現在の視覚予備学習は、大きな画像を処理するコストが二次元の関係になるため、低解像度（例：378 x 378ピクセル）に限られています。我々は、CLIPスタイルの視覚予備学習を4K解像度に拡張することを導入します。PS3は、対比的学習による画像の全体的な表現を代わりに、選択的に局所領域を処理し、それらと局所的な詳細なカプチャーンを対比し、高解像度の表現学習を可能にします。これにより、計算コストを大幅に減少させることができます。PS3は、低解像度で画像全体をエンコードし、もしくはテキストプロンプトに関連しているような領域を選択的に高解像度で処理することができます。PS3を多モーダルLLM（MLLM）に適用すると、VILA-HDという名前のモデルが得られ、高解像度の視覚認識を大幅に向上させ、AnyResやS^2といった高解像度の視覚予備学習がない基準と比較して、4.3倍のトークンを使用していることでも、高解像度の視覚認識を向上させます。VILA-HDは、最新の技術と比較して、NVILAやQwen2-VLといった先行のMLLMを超え、複数のベンチマークで優れています。また、最新のトークンプリンシングアプローチよりもより効率的です。最後に、現在のベンチマークは4K解像度の認識が必要としていませんが、これにより、4KProという新しいベンチマークを提案します。4KProでは、VILA-HDは、以前のMLLMをすべて超え、GPT-4oに対して14.5%の向上、Qwen2-VLに対して3.2%の向上と2.96倍のスピードアップを収めます。",
      "upvotes": 9,
      "discussionId": "67e375d9cc93cc8c42da785f",
      "projectPage": "https://nvlabs.github.io/PS3/",
      "githubRepo": "https://github.com/NVlabs/PS3",
      "ai_keywords": [
        "PS3",
        "CLIP-style vision pre-training",
        "contrastive learning",
        "local regions",
        "local detailed captions",
        "high-resolution representation learning",
        "computational overhead",
        "saliency",
        "text prompt",
        "VILA-HD",
        "multi-modal LLM",
        "high-resolution visual perception",
        "AnyRes",
        "S^2",
        "scaling properties",
        "test-time compute",
        "NVILA",
        "Qwen2-VL",
        "benchmarks",
        "token pruning approaches",
        "4KPer",
        "image QA",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-03-25T13:58:37.000Z",
    "title": "Scaling Vision Pre-Training to 4K Resolution",
    "summary": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649004218f7cbbc94c782db6/F0o61glyGm9fib9Pl1i-L.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19903.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649004218f7cbbc94c782db6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AdgLVfAIpWlug4jXTaEK-.jpeg",
      "fullname": "Baifeng Shi",
      "name": "bfshi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19855",
      "authors": [
        {
          "_id": "67e36792a281c900d76a93c8",
          "name": "Xiaoyu Tian",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93c9",
          "name": "Sitong Zhao",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93ca",
          "name": "Haotian Wang",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cb",
          "name": "Shuaiting Chen",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cc",
          "name": "Yunjie Ji",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cd",
          "name": "Yiping Peng",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93ce",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "67e36792a281c900d76a93cf",
          "name": "Xiangang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:19:38.000Z",
      "submittedOnDailyAt": "2025-03-26T01:04:39.479Z",
      "title": "「2回めに思い出して：検証時の多回テストフィードバックをスケーリングしてLLMの理由論を向上させる」",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "最新の大語言モデル（LLMs）の進展、例えばOpenAI-o1とDeepSeek-R1について、テスト時スケーリングの効果性を示しています。これは、拡張された理由過程がモデルの性能を大幅に向上させることを示しています。しかし、現在のモデルは長文の処理と強化学習（RL）のトレーニング効率による制限を受けています。これらの問題に対処するために、私たちは簡単で効果的なテスト時スケーリングアプローチ「Multi-round Thinking」を提案しています。この方法は、前回の回答をプロンプトとして、後続のロードでモデルの理由を再編集することで、モデルの理由を進歩させます。QwQ-32BやDeepSeek-R1などの複数のモデルにおいて様々なベンチマーク（AIME 2024、MATH-500、GPQA-diamond、LiveCodeBench）での極めて広範囲の実験を行い、その結果はモデルの性能の向上を示しています。例えば、AIME 2024データセットではQwQ-32Bの正確率は、Round 1で80.3%からRound 2で82.1%に上がり、DeepSeek-R1も同様に79.7%から82.0%に上がりました。これらの結果は、Multi-round Thinkingは広範囲に適用可能で、簡単なアプローチでモデルの性能の安定した向上を実現できることを示し、将来のテスト時スケーリングテクニックの開発の可能性を強調しています。主なプロンプト：{元の質問プロンプト}アシスタントの前回の回答は：<answer> {前回の回答} </answer>です。これに基づいて再回答してください。",
      "upvotes": 7,
      "discussionId": "67e36793a281c900d76a9459",
      "ai_keywords": [
        "large language models",
        "OpenAI-o1",
        "DeepSeek-R1",
        "test-time scaling",
        "extended reasoning processes",
        "reinforcement learning",
        "Multi-round Thinking",
        "iterative refinement",
        "AIME 2024",
        "MATH-500",
        "GPQA-diamond",
        "LiveCodeBench",
        "accuracy",
        "stable enhancements",
        "test-time scaling techniques"
      ]
    },
    "publishedAt": "2025-03-25T13:19:38.000Z",
    "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking",
    "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19855.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6471
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19910",
      "authors": [
        {
          "_id": "67e35e4cff080b9ee71e3295",
          "name": "Chuong Huynh",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3296",
          "name": "Jinyu Yang",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3297",
          "name": "Ashish Tawari",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3298",
          "name": "Mubarak Shah",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e3299",
          "name": "Son Tran",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e329a",
          "name": "Raffay Hamid",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e329b",
          "name": "Trishul Chilimbi",
          "hidden": false
        },
        {
          "_id": "67e35e4cff080b9ee71e329c",
          "name": "Abhinav Shrivastava",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:59:50.000Z",
      "submittedOnDailyAt": "2025-03-26T00:26:00.764Z",
      "title": "CoLLM: 合成画像検索のための大規模言語モデル",
      "submittedOnDailyBy": {
        "_id": "63a4d196cde2b28f82a56bd9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4d196cde2b28f82a56bd9/iqVFOtDteRMUScFGRcx0L.png",
        "isPro": false,
        "fullname": "Chuong Huynh",
        "user": "chuonghm",
        "type": "user"
      },
      "summary": "Composed Image Retrieval (CIR)は、多モデルクエリに基づいて画像を検索する複雑な任務です。通常の訓練データは、参照画像、望ましい変更の文字記述、および標的画像を含む三つ組で構成されています。これらのデータの取得は費用と時間がかかります。CIRデータセットの不足は、合成三つ組を利用するゼロショットアプローチや、ビジョン言語モデル（VLMs）を利用してWebクロールされた画像コメントペアを活用することによって解決されました。しかし、これらの方法には以下の制限があります：合成三つ組はスケールの限界、多様性のなさ、不自然な変更テキストがあり、画像コメントペアは三つ組データのないため、多モデルクエリの共用埋め込み学習を妨げます。また、現在のアプローチは複雑なさそしい変更テキストに対して対応しきれていません。我々は、これらの制限を解決するためにCoLLMを提案します。我々のアプローチは、画像コメントペアから三つ組を機械的に生成し、手動注釈を必要とさせないサブジェクト訓練を可能にします。我々は、大規模言語モデル（LLMs）を利用して参照画像と変更テキストの共用埋め込みを生成し、深い多モデル融合を促進します。また、我々は、Multi-Text CIR（MTCIR）を導入します。MTCIRは340万サンプルを含み、現在のCIRベンチマーク（CIRRとFashion-IQ）を改良し、評価信頼性を向上させます。実験結果は、CoLLMは複数のCIRベンチマークと設定で最先端の性能を達成しました。MTCIRは、15%の性能向上を収得し、我々の改良ベンチマークはCIRモデルの評価指標をより信頼性のあるものにし、この重要な分野の進歩に貢献します。",
      "upvotes": 6,
      "discussionId": "67e35e4eff080b9ee71e3353",
      "projectPage": "https://collm-cvpr25.github.io/",
      "ai_keywords": [
        "Composed Image Retrieval (CIR)",
        "multimodal query",
        "triplets",
        "reference image",
        "textual description",
        "target image",
        "zero-shot approaches",
        "synthetic triplets",
        "vision-language models (VLMs)",
        "web-crawled image-caption pairs",
        "joint embedding learning",
        "complex and nuanced modification texts",
        "multimodal fusion",
        "CoLLM",
        "Large Language Models (LLMs)",
        "Multi-Text CIR (MTCIR)",
        "CIRR benchmark",
        "Fashion-IQ benchmark",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-03-25T13:59:50.000Z",
    "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
    "summary": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19910.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4d196cde2b28f82a56bd9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4d196cde2b28f82a56bd9/iqVFOtDteRMUScFGRcx0L.png",
      "fullname": "Chuong Huynh",
      "name": "chuonghm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18446",
      "authors": [
        {
          "_id": "67e367ee4363e3c4bbbaca3a",
          "name": "Jinho Jeong",
          "hidden": false
        },
        {
          "_id": "67e367ee4363e3c4bbbaca3b",
          "name": "Sangmin Han",
          "hidden": false
        },
        {
          "_id": "67e367ee4363e3c4bbbaca3c",
          "name": "Jinwoo Kim",
          "hidden": false
        },
        {
          "_id": "67e367ee4363e3c4bbbaca3d",
          "name": "Seon Joo Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T08:50:15.000Z",
      "submittedOnDailyAt": "2025-03-26T01:07:15.007Z",
      "title": "潜在空間超解像化モデルとディフュージョンモデルを用いた高解像度画像生成",
      "submittedOnDailyBy": {
        "_id": "66b5f733f0c16f37f307f35e",
        "avatarUrl": "/avatars/29a97e10b4d65aa23d7eae238f809499.svg",
        "isPro": false,
        "fullname": "JinHo Jeong",
        "user": "3587jjh",
        "type": "user"
      },
      "summary": "この論文では、潜在空間での超解像化を活用した高解像度（1Kを超える）画像生成の新しいフレームワークLSRNAを提案します。現在の拡散モデルは、学習解像度を超えることに困難を抱え、構造的な異常や内容の再現を招くことが多いです。参照ベースの方法は、低解像度の参照をアップサンプリングして高解像度の生成をガイドすることで問題を解決していますが、潜在空間でのアップサンプリングは多様体の偏けを招くことで出力の質を低下させます。一方、RGB空間でのアップサンプリングは過度に平滑化された出力を生成します。これらの制限を克服するために、LSRNAは多様体の調整を目的とした潜在空間での超解像化（LSR）と、高周波の詳細を強化するための領域ごとのノイズ追加（RNA）を組み合わせています。拡散モデルの最先端の参照ベースの方法を超えることを示し、多様性とシャープさの保存において潜在空間でのアップサンプリングの重要性を明らかにしています。コードは、https://github.com/3587jjh/LSRNA に公開されています。",
      "upvotes": 4,
      "discussionId": "67e367f14363e3c4bbbacae1",
      "ai_keywords": [
        "LSRNA",
        "diffusion models",
        "latent space",
        "super-resolution",
        "structural distortions",
        "content repetition",
        "reference-based methods",
        "manifold deviation",
        "RGB space",
        "manifold alignment",
        "Region-wise Noise Addition (RNA)",
        "high-frequency details"
      ]
    },
    "publishedAt": "2025-03-24T04:50:15.000Z",
    "title": "Latent Space Super-Resolution for Higher-Resolution Image Generation\n  with Diffusion Models",
    "summary": "In this paper, we propose LSRNA, a novel framework for higher-resolution\n(exceeding 1K) image generation using diffusion models by leveraging\nsuper-resolution directly in the latent space. Existing diffusion models\nstruggle with scaling beyond their training resolutions, often leading to\nstructural distortions or content repetition. Reference-based methods address\nthe issues by upsampling a low-resolution reference to guide higher-resolution\ngeneration. However, they face significant challenges: upsampling in latent\nspace often causes manifold deviation, which degrades output quality. On the\nother hand, upsampling in RGB space tends to produce overly smoothed outputs.\nTo overcome these limitations, LSRNA combines Latent space Super-Resolution\n(LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance\nhigh-frequency details. Our extensive experiments demonstrate that integrating\nLSRNA outperforms state-of-the-art reference-based methods across various\nresolutions and metrics, while showing the critical role of latent space\nupsampling in preserving detail and sharpness. The code is available at\nhttps://github.com/3587jjh/LSRNA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18446.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "66b5f733f0c16f37f307f35e",
      "avatarUrl": "/avatars/29a97e10b4d65aa23d7eae238f809499.svg",
      "fullname": "JinHo Jeong",
      "name": "3587jjh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13964",
      "authors": [
        {
          "_id": "67e20852c0c932395394dbb0",
          "name": "Siwei Han",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb1",
          "name": "Peng Xia",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb2",
          "name": "Ruiyi Zhang",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb3",
          "name": "Tong Sun",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb4",
          "name": "Yun Li",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb5",
          "name": "Hongtu Zhu",
          "hidden": false
        },
        {
          "_id": "67e20852c0c932395394dbb6",
          "name": "Huaxiu Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/smmwVmcnReTUxH6xDnyU1.png"
      ],
      "publishedAt": "2025-03-18T06:57:21.000Z",
      "submittedOnDailyAt": "2025-03-26T03:52:37.520Z",
      "title": "MDocAgent: 文書を理解するための多モデルマルチアカウントフレームワーク",
      "submittedOnDailyBy": {
        "_id": "643e9ee6f6bb3c31a26e7bc4",
        "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
        "isPro": false,
        "fullname": "Peng Xia",
        "user": "richardxp888",
        "type": "user"
      },
      "summary": "ドキュメントクエストアンサー（DocQA）は非常に一般的なタスクです。既存の方法では、大規模言語モデル（LLMs）または大規模視覚言語モデル（LVLMs）を使用し、検索増強生成（RAG）を実行していますが、これらの方法は単一のモデルからの情報を優先し、文字と画像のカットを有効に統合することができません。これらのアプローチは複雑な多モデル論理に難しく、実世界的なドキュメントの性能に限界をつけています。私たちは、ドキュメント理解のための新しいRAGと多エージェントフレームワークであるMDocAgent（多モデル多エージェントフレームワーク）を紹介します。このフレームワークは文字と画像を両方利用しています。システムは5つの専門化エージェントを使用しています：一般エージェント、批判エージェント、文字エージェント、画像エージェントと要約エージェント。これらのエージェントは多モデルコンテキスト検索に取り組み、個々の見解を結合してドキュメントの内容をより詳細に理解します。この協力的なアプローチにより、システムは文字と可視的なコンポーネントからの情報を合成し、問い合わせの答えの精度を向上させます。MMLongBench、LongDocURLなどの5つのベンチマークでの初步的な実験は、我々のMDocAgentの効果を示し、現在の最先端の方法と比較して平均的に12.1%の改善を達成しました。この研究は、実世界的なドキュメントの複雑性を調和するためにより強固で詳細なDocQAシステムの開発に貢献します。データとコードは、https://github.com/aiming-lab/MDocAgent に公開されています。",
      "upvotes": 4,
      "discussionId": "67e20858c0c932395394dde6",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Large Vision Language Models (LVLMs)",
        "Retrieval Augmented Generation (RAG)",
        "multi-modal reasoning",
        "multi-modal multi-agent framework",
        "general agent",
        "critical agent",
        "text agent",
        "image agent",
        "summarizing agent",
        "multi-modal context retrieval"
      ]
    },
    "publishedAt": "2025-03-18T02:57:21.000Z",
    "title": "MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding",
    "summary": "Document Question Answering (DocQA) is a very common task. Existing methods\nusing Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and\nRetrieval Augmented Generation (RAG) often prioritize information from a single\nmodal, failing to effectively integrate textual and visual cues. These\napproaches struggle with complex multi-modal reasoning, limiting their\nperformance on real-world documents. We present MDocAgent (A Multi-Modal\nMulti-Agent Framework for Document Understanding), a novel RAG and multi-agent\nframework that leverages both text and image. Our system employs five\nspecialized agents: a general agent, a critical agent, a text agent, an image\nagent and a summarizing agent. These agents engage in multi-modal context\nretrieval, combining their individual insights to achieve a more comprehensive\nunderstanding of the document's content. This collaborative approach enables\nthe system to synthesize information from both textual and visual components,\nleading to improved accuracy in question answering. Preliminary experiments on\nfive benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of\nour MDocAgent, achieve an average improvement of 12.1% compared to current\nstate-of-the-art method. This work contributes to the development of more\nrobust and comprehensive DocQA systems capable of handling the complexities of\nreal-world documents containing rich textual and visual information. Our data\nand code are available at https://github.com/aiming-lab/MDocAgent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/smmwVmcnReTUxH6xDnyU1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13964.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643e9ee6f6bb3c31a26e7bc4",
      "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
      "fullname": "Peng Xia",
      "name": "richardxp888",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19470",
      "authors": [
        {
          "_id": "67e365b0dcfc2aeae1bf3da2",
          "name": "Mingyang Chen",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da3",
          "name": "Tianpeng Li",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da4",
          "name": "Haoze Sun",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da5",
          "name": "Yijie Zhou",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da6",
          "name": "Chenzheng Zhu",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da7",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da8",
          "name": "Zenan Zhou",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3da9",
          "name": "Weipeng Chen",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3daa",
          "name": "Haofen Wang",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3dab",
          "name": "Jeff Z. Pan",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3dac",
          "name": "Wen Zhang",
          "hidden": false
        },
        {
          "_id": "67e365b0dcfc2aeae1bf3dad",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T09:00:58.000Z",
      "submittedOnDailyAt": "2025-03-26T00:56:07.098Z",
      "title": "研究：LLMs における検索を用いた推論の学習を強化学習によって実現する",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "大型言語モデル（LLMs）は、理由論の優れた能力を示し、OpenAI-o1とDeepSeek-R1の成功がその例である。しかし、理由論と外部検索プロセスの統合は難しい、特に複雑な多段階質問に対して複数の検索ステップが必要な場合。私たちは、ReSearchという新しいフレームワークを提案し、理由論を検索によって学習するための強化学習を使用して理由論ステップについての監督データを使用しない。我々のアプローチは、検索操作を理由論チェーンの一体的な成分とし、検索を行う時間と方法は文章基盤的な思考によってガイドされ、検索結果はその後の理由論に影響を与える。ReSearchはQwen2.5-7B(-Instruct)とQwen2.5-32B(-Instruct)モデルにより訓練され、拡張された実験を実施した。1つのデータセットでの訓練にもかかわらず、我々のモデルは様々なベンチマークで強い一般化性能を示した。分析により、ReSearchは強化学習プロセス中に自然に進歩的な理由論能力を引き出すことがわかった、そのほか反射と自己補正など。",
      "upvotes": 3,
      "discussionId": "67e365b1dcfc2aeae1bf3df6",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "OpenAI-o1",
        "DeepSeek-R1",
        "complex multi-hop questions",
        "ReSearch",
        "reinforcement learning",
        "text-based thinking",
        "reflection",
        "self-correction",
        "Qwen2.5-7B(-Instruct)",
        "Qwen2.5-32B(-Instruct)"
      ]
    },
    "publishedAt": "2025-03-25T05:00:58.000Z",
    "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning",
    "summary": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19470.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6471
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19041",
      "authors": [
        {
          "_id": "67e35da0b1b97cc3392024b1",
          "name": "Kangwei Liu",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b2",
          "name": "Mengru Wang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b3",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b4",
          "name": "Lin Yuan",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b5",
          "name": "Mengshu Sun",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b6",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b7",
          "name": "Lei Liang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b8",
          "name": "Zhiqiang Zhang",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024b9",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "67e35da0b1b97cc3392024ba",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/f0fTHNDhXrS7zWpmuxVU-.png"
      ],
      "publishedAt": "2025-03-24T18:11:42.000Z",
      "submittedOnDailyAt": "2025-03-26T00:22:20.466Z",
      "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Fine-tuningでは、大規模言語モデル（LLMs）が特定の領域に適応することができるが、そのままにしてはそれほど安全性の調整が崩れることがある。この問題を解決するために、LookAhead Tuningを導入し、これは2つの簡単的でリソースの必要性が低い、効果的なデータ駆動の手法を構成しています。これらの手法は、学習データを部分の回答の前緒をプレビューして変更することで、モデルの固有の安全機構を保存し、初期トークン分布の摂動を最小化することで、モデルの安全性を維持することを目指しています。詳細な実験は、LookAhead Tuningが下流タスクの強固な性能を犠牲にしないままに安全性を維持することを効果的に示しています。我々の発見は、LLMsの安全で効果的な適応のために信頼できるようになるようなリカバリーティーと効率的な解決策として位置づけられています。コードは、https://github.com/zjunlp/LookAheadTuning にリリースされています。",
      "upvotes": 3,
      "discussionId": "67e35da1b1b97cc339202525",
      "ai_keywords": [
        "LookAhead Tuning",
        "safety alignment",
        "data-driven methods",
        "partial answer prefixes",
        "token distributions",
        "robust performance",
        "downstream tasks"
      ]
    },
    "publishedAt": "2025-03-24T14:11:42.000Z",
    "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
    "summary": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often undermines their previously established safety alignment. To\nmitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, which comprises two simple, low-resource, and effective\ndata-driven methods that modify training data by previewing partial answer\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\nby minimizing perturbations to initial token distributions. Comprehensive\nexperiments demonstrate that LookAhead Tuning effectively maintains model\nsafety without sacrificing robust performance on downstream tasks. Our findings\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs. Code is released at\nhttps://github.com/zjunlp/LookAheadTuning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/f0fTHNDhXrS7zWpmuxVU-.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19041.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18783",
      "authors": [
        {
          "_id": "67e2a43d5116df47da357eec",
          "user": {
            "_id": "642438eaa3adbc7142c3ca0f",
            "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
            "isPro": false,
            "fullname": "CharlesChen",
            "user": "CharlesChen2023",
            "type": "user"
          },
          "name": "Linwei Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T14:36:22.430Z",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357eed",
          "name": "Lin Gu",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357eee",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357eef",
          "name": "Chenggang Yan",
          "hidden": false
        },
        {
          "_id": "67e2a43d5116df47da357ef0",
          "name": "Ying Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T15:32:06.000Z",
      "submittedOnDailyAt": "2025-03-26T01:08:28.390Z",
      "title": "Frequency Dynamic Convolution for Dense Image Prediction",
      "submittedOnDailyBy": {
        "_id": "642438eaa3adbc7142c3ca0f",
        "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
        "isPro": false,
        "fullname": "CharlesChen",
        "user": "CharlesChen2023",
        "type": "user"
      },
      "summary": "DY-Convは、複数の並列ウェイトとアテンション機能を組み合わせて、適応的な重み選択を可能にして、顕著な性能を示しているが、これらの重みの周波数応答は高い類似性を示し、高いパラメータコストと限定的な適応性を伴う。本稿では、周波数領域で固定的なパラメータバジェットを学習することでこれらの制限を軽減する新しいアプローチFrequency Dynamic Convolution (FDConv)を紹介します。FDConvは、違う周波数インデックスを持つ周波数ベースのグループにパラメータバジェットを分割し、パラメータコストを増加させないまま周波数多様性の重みを構築することができます。適応性を進めるために、Kernel Spatial Modulation (KSM)とFrequency Band Modulation (FBM)を提案します。KSMは、各フィルタの周波数応答を空間レベルで動的に調整し、FBMは、周波数領域で重みを違う周波数バンドに分解し、ローカル内容に基づいて動的に調節することで適応性を進めます。物体検出、分割、分類において様々な実験を行い、FDConvの効果を証明します。ResNet-50に対してFDConvを適用すると、パラメータコストの少しの増加（+3.6Mパラメータ）で上位の性能を達成し、CondConv（+90Mパラメータ）やKW（+76.5Mパラメータ）と比較してパラメータコストの大幅な増加を必要としないことを示します。また、FDConvはConvNeXt、Swin-Transformerなどの様々なアーキテクチャに無難に統合可能で、現代の視覚タスクに対して柔軟かつ効率的な解決策を提供します。コードは、https://github.com/Linwei-Chen/FDConv で公開しています。",
      "upvotes": 2,
      "discussionId": "67e2a4405116df47da357ff7",
      "ai_keywords": [
        "Dynamic Convolution (DY-Conv)",
        "Frequency Dynamic Convolution (FDConv)",
        "attention mechanism",
        "parameter budget",
        "Fourier domain",
        "frequency-based groups",
        "disjoint Fourier indices",
        "frequency-diverse weights",
        "Kernel Spatial Modulation (KSM)",
        "Frequency Band Modulation (FBM)",
        "frequency response",
        "spatial level",
        "frequency bands",
        "local content",
        "object detection",
        "segmentation",
        "classification",
        "ResNet-50",
        "ConvNeXt",
        "Swin-Transformer",
        "parameter-efficient"
      ]
    },
    "publishedAt": "2025-03-24T11:32:06.000Z",
    "title": "Frequency Dynamic Convolution for Dense Image Prediction",
    "summary": "While Dynamic Convolution (DY-Conv) has shown promising performance by\nenabling adaptive weight selection through multiple parallel weights combined\nwith an attention mechanism, the frequency response of these weights tends to\nexhibit high similarity, resulting in high parameter costs but limited\nadaptability. In this work, we introduce Frequency Dynamic Convolution\n(FDConv), a novel approach that mitigates these limitations by learning a fixed\nparameter budget in the Fourier domain. FDConv divides this budget into\nfrequency-based groups with disjoint Fourier indices, enabling the construction\nof frequency-diverse weights without increasing the parameter cost. To further\nenhance adaptability, we propose Kernel Spatial Modulation (KSM) and Frequency\nBand Modulation (FBM). KSM dynamically adjusts the frequency response of each\nfilter at the spatial level, while FBM decomposes weights into distinct\nfrequency bands in the frequency domain and modulates them dynamically based on\nlocal content. Extensive experiments on object detection, segmentation, and\nclassification validate the effectiveness of FDConv. We demonstrate that when\napplied to ResNet-50, FDConv achieves superior performance with a modest\nincrease of +3.6M parameters, outperforming previous methods that require\nsubstantial increases in parameter budgets (e.g., CondConv +90M, KW +76.5M).\nMoreover, FDConv seamlessly integrates into a variety of architectures,\nincluding ConvNeXt, Swin-Transformer, offering a flexible and efficient\nsolution for modern vision tasks. The code is made publicly available at\nhttps://github.com/Linwei-Chen/FDConv.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18783.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642438eaa3adbc7142c3ca0f",
      "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg",
      "fullname": "CharlesChen",
      "name": "CharlesChen2023",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17361",
      "authors": [
        {
          "_id": "67e35ca7363374850440d91d",
          "name": "Sophia Tang",
          "hidden": false
        },
        {
          "_id": "67e35ca7363374850440d91e",
          "name": "Yinuo Zhang",
          "hidden": false
        },
        {
          "_id": "67e35ca7363374850440d91f",
          "name": "Alexander Tong",
          "hidden": false
        },
        {
          "_id": "67e35ca7363374850440d920",
          "user": {
            "_id": "64cd5b3f0494187a9e8b7c69",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
            "isPro": false,
            "fullname": "Pranam Chatterjee",
            "user": "pranamanam",
            "type": "user"
          },
          "name": "Pranam Chatterjee",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-26T01:57:51.167Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:59:43.000Z",
      "submittedOnDailyAt": "2025-03-26T00:18:51.908Z",
      "title": "ゲルマルソフトマッチングフローと直通ガイドニングを用いた制御可能な生物学的配列生成",
      "submittedOnDailyBy": {
        "_id": "64cd5b3f0494187a9e8b7c69",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
        "isPro": false,
        "fullname": "Pranam Chatterjee",
        "user": "pranamanam",
        "type": "user"
      },
      "summary": "コンティニュース簡約体におけるフローマッチングは、DNA配列設計のために有望な戦略として岀現しましたが、ペプチドおよびプロテインの生成に必要な高次元の簡約体にスケールすることが難しい。我々は、時間依存性の温度を持つ新しいゴメルバル・ソフトマックスインタープレーターを基にした簡約体上の生成フレームワーク、ゴメルバル・ソフトマックスフローとスコアマッチングを導入します。このインタープレーターを用いて、ゴメルバル・ソフトマックスフローマッチングを導出し、平滑なカテゴリカル分布から簡約体の一つの頂点に集中した分布への移動を実現するパラメータ化された速度フィールドを導入します。また、ゴメルバル・ソフトマックススコアマッチングを提出し、確率密度の勾配を学習してロジスティック回帰を行います。我々のフレームワークは、高品質で多様な生成を可能にし、高次元の簡約体にスケール可能です。トレーニング無しのガイドを可能にするために、クラスファイザーに基づくガイドフロー（STGFlow）を提案し、クリーンな配列にプレトレーンされたクラスファイザーを利用して推論時のガイドを効率的に行うことができます。STGFlowは、任意の離散フロー方法と組み合わせることができ、制御可能なデュノロジー配列生成のための強固なフレームワークを形成します。条件付きDNAプロモーター設計、シーケンスだけのプロテイン生成、稀少病症治療のための目標結合ペプチド設計の最先端の性能を示します。",
      "upvotes": 1,
      "discussionId": "67e35caa363374850440d9df",
      "ai_keywords": [
        "Gumbel-Softmax Flow",
        "Score Matching",
        "simplex",
        "Gumbel-Softmax interpolant",
        "time-dependent temperature",
        "parameterized velocity field",
        "smooth categorical distributions",
        "Gumbel-Softmax Flow Matching",
        "Straight-Through Guided Flows",
        "STGFlow",
        "straight-through estimators",
        "classifiers",
        "de novo sequence generation",
        "conditional DNA promoter design",
        "sequence-only protein generation",
        "target-binding peptide design"
      ]
    },
    "publishedAt": "2025-03-21T13:59:43.000Z",
    "title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
    "summary": "Flow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\nand Score Matching, a generative framework on the simplex based on a novel\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical\ndistributions to distributions concentrated at a single vertex of the simplex.\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\nthe gradient of the probability density. Our framework enables high-quality,\ndiverse generation and scales efficiently to higher-dimensional simplices. To\nenable training-free guidance, we propose Straight-Through Guided Flows\n(STGFlow), a classifier-based guidance method that leverages straight-through\nestimators to steer the unconditional velocity field toward optimal vertices of\nthe simplex. STGFlow enables efficient inference-time guidance using\nclassifiers pre-trained on clean sequences, and can be used with any discrete\nflow method. Together, these components form a robust framework for\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\nperformance in conditional DNA promoter design, sequence-only protein\ngeneration, and target-binding peptide design for rare disease treatment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17361.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cd5b3f0494187a9e8b7c69",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
      "fullname": "Pranam Chatterjee",
      "name": "pranamanam",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17237",
      "authors": [
        {
          "_id": "67e2b68e08c6a250edda264a",
          "user": {
            "_id": "67e2063e1ee7f6db889849d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
            "isPro": false,
            "fullname": "Yu-Hsi Chen",
            "user": "wish44165",
            "type": "user"
          },
          "name": "Yu-Hsi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-25T14:35:46.455Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67e2063e1ee7f6db889849d6/Dn_qgqu4a6nI8HIAdpDvW.mp4"
      ],
      "publishedAt": "2025-03-21T15:40:18.000Z",
      "submittedOnDailyAt": "2025-03-26T04:35:14.607Z",
      "title": "強力なベースライン：多ユーバーフライントラッキングを行うYOLOv12とBoT-SORT-ReID",
      "submittedOnDailyBy": {
        "_id": "67e2063e1ee7f6db889849d6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
        "isPro": false,
        "fullname": "Yu-Hsi Chen",
        "user": "wish44165",
        "type": "user"
      },
      "summary": "摂氏インフラレッドビデオで複数の無人飛行機（UAV）を検出と追跡するのは、低コントラスト、環境ノイズ、小さなターゲットサイズによって本質的に難しい。本論文は、最近の検出と追跡の進歩を活用して、簡単なアプローチを提供し、摂氏インフラレッドビデオでの複数UAV追跡を解決する。YOLOv5とDeepSORTパイプラインに依存しないで、YOLOv12とBoT-SORTを基盤に構築した追跡フレームワークを提案し、適切な訓練と推論戦略を採用して強化した。4世代の反UAV挑戦で用いられるメトリックに基づいて評価し、強力な性能を示した。特に、対比度強化や時系列情報融合を使用しないで、UAVの特徴を増強したものではなく、複数UAV追跡の「強力なベースライン」として特徴的な結果を収めた。実装詳細、詳細な実験分析、可能性のある改善点の議論を提供し、コードはhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReIDに公開されています。",
      "upvotes": 1,
      "discussionId": "67e2b69108c6a250edda279f",
      "githubRepo": "https://github.com/wish44165/YOLOv12-BoT-SORT-ReID",
      "ai_keywords": [
        "YOLOv12",
        "BoT-SORT",
        "multi-UAV tracking",
        "thermal infrared video",
        "detection",
        "tracking",
        "tailored training",
        "inference strategies",
        "4th Anti-UAV Challenge",
        "contrast enhancement",
        "temporal information fusion",
        "UAV features",
        "Strong Baseline"
      ]
    },
    "publishedAt": "2025-03-21T11:40:18.000Z",
    "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
    "summary": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\nour approach following the metrics from the 4th Anti-UAV Challenge and\ndemonstrate competitive performance. Notably, we achieve strong results without\nusing contrast enhancement or temporal information fusion to enrich UAV\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\ntracking task. We provide implementation details, in-depth experimental\nanalysis, and a discussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67e2063e1ee7f6db889849d6/Dn_qgqu4a6nI8HIAdpDvW.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17237.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67e2063e1ee7f6db889849d6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e2063e1ee7f6db889849d6/ihiwCCqbXlxQ2V_SSGnng.jpeg",
      "fullname": "Yu-Hsi Chen",
      "name": "wish44165",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16965",
      "authors": [
        {
          "_id": "67e35c3bf049c252c672b824",
          "name": "Zhe Hu",
          "hidden": false
        },
        {
          "_id": "67e35c3bf049c252c672b825",
          "name": "Jing Li",
          "hidden": false
        },
        {
          "_id": "67e35c3bf049c252c672b826",
          "name": "Yu Yin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T09:25:23.000Z",
      "submittedOnDailyAt": "2025-03-26T00:20:32.465Z",
      "title": "ワードが視覚よりも優れている時：VLMsはテキストだけで自動的に改善することができる、人間中心的な判断に向けたトレーニング",
      "submittedOnDailyBy": {
        "_id": "63999a6fe657365725d0d0a4",
        "avatarUrl": "/avatars/99736de1bc0d5decf4a6eda86e3c7937.svg",
        "isPro": false,
        "fullname": "Derek Zhe Hu",
        "user": "zhehuderek",
        "type": "user"
      },
      "summary": "Embodied decision-makingは、実世界の環境で動作するAIアグエントにとって基本的である。Visual Language Models (VLMs)はこの能力を進化させたが、複雑な判断にはまだ苦戦している特に、人間中心的な場合で深い理由を必要とする場合は。本研究では、多タイプ的人間中心的な判断処理タスクに対して、開放ソースのVLMsをシステマティックに評価した。LLMsは、実際の画像を処理するだけのVLMsと同じスケールであることを想定しているが、それらのみの文脈でのみ、驚くべきように優れていることを見出した。これは、画像の対応がVLMsの能力を妨げることを示していることを意味する。この挑戦に対して、合成された文脈データを用いた新しい文脈だけの訓練アプローチを提案した。この方法はVLMsの言語成分を強化し、学びた能力を多タイプ推論に移し、費用の高い画像-文脈ペアデータの必要性を除去する。また、VLMsは、LLMのコンタラピーもって生成された訓練データを用いて自己改善を行い、GPT-4のより大きな教師モデルに依存しないように、大幅な性能向上を実現できることを示した。これらの発見は、VLMsの人間中心的な判断能力を強化するためのより効率的かつスケーラブルなアプローチを奨励し、VLMsを自己改善機構を通じて最適化する新しい道を開拓する。",
      "upvotes": 1,
      "discussionId": "67e35c3cf049c252c672b859",
      "ai_keywords": [
        "Visual Language Models (VLMs)",
        "multimodal human-centered decision-making tasks",
        "Large Language Models (LLMs)",
        "textual descriptions",
        "visual alignment",
        "text-only training approach",
        "synthesized textual data",
        "self-improvement",
        "training data",
        "GPT-4",
        "human-centered decision-making capabilities"
      ]
    },
    "publishedAt": "2025-03-21T05:25:23.000Z",
    "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making",
    "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16965.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63999a6fe657365725d0d0a4",
      "avatarUrl": "/avatars/99736de1bc0d5decf4a6eda86e3c7937.svg",
      "fullname": "Derek Zhe Hu",
      "name": "zhehuderek",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.11849",
      "authors": [
        {
          "_id": "67e3d0ac304f166b665e4a67",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a68",
          "name": "Zhitong Xiong",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a69",
          "name": "Chenying Liu",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6a",
          "name": "Adam J. Stewart",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6b",
          "name": "Thomas Dujardin",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6c",
          "name": "Nikolaos Ioannis Bountos",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6d",
          "name": "Angelos Zavras",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6e",
          "name": "Franziska Gerken",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a6f",
          "name": "Ioannis Papoutsis",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a70",
          "name": "Laura Leal-Taixé",
          "hidden": false
        },
        {
          "_id": "67e3d0ac304f166b665e4a71",
          "name": "Xiao Xiang Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T20:16:48.000Z",
      "submittedOnDailyAt": "2025-03-26T08:34:37.209Z",
      "title": "コーピニケフォーディングモデルの地球観に向けて",
      "submittedOnDailyBy": {
        "_id": "64cba974a81988d0734c9925",
        "avatarUrl": "/avatars/645c326ca38eb751144f356076cef60f.svg",
        "isPro": false,
        "fullname": "Yi Wang",
        "user": "wangyi111",
        "type": "user"
      },
      "summary": "地球観測（EO）の基礎モデルの進歩は、大規模な衛星データから空間からの一般的な表現を学習する可能性を開発し、地球にとって重要な多様な下流アプリケーションに利かせています。しかし、現在のほとんどの努力は固定スペクトルセンサーに限定され、地球の表面だけを焦点とし、画像よりも有價のメタデータを無視しています。この研究では、次世代のEO基礎モデルに向けたステップを踏み出し、3つのキーコンポーネントを採用しています：1) Copernicus-Pretrain、1870万枚のセンタリストミッションからのアラインメント画像を統合した巨大スケールの予ちデータセット、地球の表面から大気まで幅広く範囲を広げています。2) Copernicus-FM、拡張された動的ハイパーネットワークと柔軟なメタデータエンコーディングを用いて、どのスペクトルや非スペクトルセンサーモデルを処理できる統一的な基礎モデル。3) Copernicus-Bench、前処理から各センタリストミッションの特化されたアプリケーションまでの15つの階層的な下流タスクを構成し、システム的に評価するベンチマーク。我々のデータセット、モデル、ベンチマークは、EO基礎モデルのスケーラビリティ、バリエーション、モデル多様性を大幅に向上させ、EO、気象、気候研究とのつながりを新たに創造します。コード、データセット、モデルは、https://github.com/zhu-xlab/Copernicus-FM から利用可能です。",
      "upvotes": 0,
      "discussionId": "67e3d0af304f166b665e4b68",
      "githubRepo": "https://github.com/zhu-xlab/Copernicus-FM"
    },
    "publishedAt": "2025-03-14T16:16:48.000Z",
    "title": "Towards a Unified Copernicus Foundation Model for Earth Vision",
    "summary": "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps://github.com/zhu-xlab/Copernicus-FM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11849.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cba974a81988d0734c9925",
      "avatarUrl": "/avatars/645c326ca38eb751144f356076cef60f.svg",
      "fullname": "Yi Wang",
      "name": "wangyi111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  }
]