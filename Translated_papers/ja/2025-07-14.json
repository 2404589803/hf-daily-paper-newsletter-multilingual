[
  {
    "paper": {
      "id": "2507.01951",
      "authors": [
        {
          "_id": "6868daac213f123a1f88b9c8",
          "name": "Zixiao Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9c9",
          "name": "Yuxin Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9ca",
          "name": "Xiaorui Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cb",
          "name": "Mengting Xing",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cc",
          "name": "Jie Gao",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cd",
          "name": "Jianjun Xu",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9ce",
          "name": "Guangcan Liu",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9cf",
          "name": "Chenhui Jin",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9d0",
          "name": "Zhuo Wang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9d1",
          "name": "Shengzhuo Zhang",
          "hidden": false
        },
        {
          "_id": "6868daac213f123a1f88b9d2",
          "name": "Hongtao Xie",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638700c723da90491eb72722/mRIN68aSkxejrT4UqCaKz.jpeg"
      ],
      "publishedAt": "2025-07-02T17:58:01.000Z",
      "submittedOnDailyAt": "2025-07-14T00:42:27.924Z",
      "title": "テストタイムスケーリングによるリフレクタイブ生成モデル",
      "submittedOnDailyBy": {
        "_id": "638700c723da90491eb72722",
        "avatarUrl": "/avatars/6dcddca5c31121b60e45aab0816e11be.svg",
        "isPro": false,
        "fullname": "Yuxin Wang",
        "user": "wangyuxin87",
        "type": "user"
      },
      "summary": "私たちは、自分サポートプロセス報酬モデル（SPRM）を用いてOpenAI o3の性能を実現した最初の反省的な生成モデル、MetaStone-S1を紹介します。SPRMは、バックボーンネットワークを共有し、次のトークン予測とプロセススコアの評価に対してタスク専門のヘッドを使用し、無駄なプロセス注釈を必要とせずに、政策モデルとプロセス報酬モデル（PRM）を一つの統合フィードバックにし、PRMパラメーターを99%以上削減し、効率的な推理に適したものとして成功しました。SPRMを搭載したMetaStone-S1は、自然とテスト時スケーリング（TTS）に適しています。我々は、制御可能な思考長さに基づいて、低、中、高の3つの理由の努力モードを提供しています。また、我々は、総合的な思考計算とTTS性能の関係を明確にするスケーリングラーーを実験的に立てました。実験により、我々のMetaStone-S1は、32BパラメーターサイズのみでOpenAI-o3-miniのシリーズと比較的な性能を達成しました。研究コミュニティの支援のために、MetaStone-S1を公開ソースとしてGitHubで提供しています（https://github.com/MetaStone-AI/MetaStone-S1）。",
      "upvotes": 58,
      "discussionId": "6868daac213f123a1f88b9d3",
      "githubRepo": "https://github.com/MetaStone-AI/MetaStone-S1",
      "ai_summary": "MetaStone-S1, a reflective generative model using a self-supervised process reward model, achieves efficient reasoning and scalable performance with fewer parameters compared to existing models.",
      "ai_keywords": [
        "reflective generative model",
        "self-supervised process reward model",
        "backbone network",
        "task-specific heads",
        "policy model",
        "process reward model",
        "test time scaling",
        "controllable thinking length",
        "scaling law",
        "total thinking computation"
      ],
      "githubStars": 41
    },
    "publishedAt": "2025-07-02T13:58:01.000Z",
    "title": "Test-Time Scaling with Reflective Generative Model",
    "summary": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638700c723da90491eb72722/mRIN68aSkxejrT4UqCaKz.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01951.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "638700c723da90491eb72722",
      "avatarUrl": "/avatars/6dcddca5c31121b60e45aab0816e11be.svg",
      "fullname": "Yuxin Wang",
      "name": "wangyuxin87",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08776",
      "authors": [
        {
          "_id": "68745e0f257d4f0435370288",
          "name": "Zhengqing Wang",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f0435370289",
          "name": "Yuefan Wu",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f043537028a",
          "name": "Jiacheng Chen",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f043537028b",
          "name": "Fuyang Zhang",
          "hidden": false
        },
        {
          "_id": "68745e0f257d4f043537028c",
          "name": "Yasutaka Furukawa",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64d97c5bfd0b55d501ba00cf/xQVrjtR_Sb4SOz-GghYdp.mp4"
      ],
      "publishedAt": "2025-07-11T17:38:52.000Z",
      "submittedOnDailyAt": "2025-07-14T00:07:38.676Z",
      "title": "CLiFT: 圧縮ライトフィールドトークンへの計算効率と適応性を図る",
      "submittedOnDailyBy": {
        "_id": "64d97c5bfd0b55d501ba00cf",
        "avatarUrl": "/avatars/47505f2a573acea7176a96f538226ecb.svg",
        "isPro": false,
        "fullname": "Zhengqing Wang",
        "user": "EricW123456",
        "type": "user"
      },
      "summary": "この論文は、「圧縮光場トークン（CLiFTs）」という表現手法を提案し、スケーンの豊富な外観と幾何情報を保持します。CLiFTは、トークンの圧縮により計算効率的なレンダリングを可能にし、同じトレーニングネットワークでスケーンのトークン数を変更して新しい視点をレンダリングできます。具體には、画像のセットを与えると、多点のカメラの姿勢で画像をトークン化します。潜在空間のK-means法は、トークンを使用して減少された光線の集合をクラスタ中心として選択します。多点の「凝縮器」は、すべてのトークンの情報をクラスタ中心トークンに凝縮してCLiFTsを構築します。テスト時には、目標の視点と計算バジュット（CLiFTsの数）を与えると、指定された数の近いトークンを集め、計算効率的なレンダリングプロセスで新しい視点を合成します。RealEstate10KとDL3DVデータセットの実験は、定量的および質的な評価で我々のアプローチを検証し、比較的レンダリング質量でも最高の全体的なレンダリングスコアを達成し、データサイズ、レンダリング質量、レンダリング速度の補損を提供します。",
      "upvotes": 38,
      "discussionId": "68745e10257d4f043537028d",
      "projectPage": "https://clift-nvs.github.io/",
      "githubRepo": "https://github.com/eric-zqwang/CLiFT",
      "ai_summary": "A neural rendering method uses compressed light-field tokens to efficiently represent scenes and render novel views with varying compute budgets.",
      "ai_keywords": [
        "neural rendering",
        "compressed light-field tokens",
        "CLiFTs",
        "multi-view encoder",
        "latent-space K-means",
        "condenser",
        "compute-adaptive renderer",
        "RealEstate10K",
        "DL3DV datasets"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-07-11T13:38:52.000Z",
    "title": "CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive\n  Neural Rendering",
    "summary": "This paper proposes a neural rendering approach that represents a scene as\n\"compressed light-field tokens (CLiFTs)\", retaining rich appearance and\ngeometric information of a scene. CLiFT enables compute-efficient rendering by\ncompressed tokens, while being capable of changing the number of tokens to\nrepresent a scene or render a novel view with one trained network. Concretely,\ngiven a set of images, multi-view encoder tokenizes the images with the camera\nposes. Latent-space K-means selects a reduced set of rays as cluster centroids\nusing the tokens. The multi-view ``condenser'' compresses the information of\nall the tokens into the centroid tokens to construct CLiFTs. At test time,\ngiven a target view and a compute budget (i.e., the number of CLiFTs), the\nsystem collects the specified number of nearby tokens and synthesizes a novel\nview using a compute-adaptive renderer. Extensive experiments on RealEstate10K\nand DL3DV datasets quantitatively and qualitatively validate our approach,\nachieving significant data reduction with comparable rendering quality and the\nhighest overall rendering score, while providing trade-offs of data size,\nrendering quality, and rendering speed.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64d97c5bfd0b55d501ba00cf/xQVrjtR_Sb4SOz-GghYdp.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d97c5bfd0b55d501ba00cf",
      "avatarUrl": "/avatars/47505f2a573acea7176a96f538226ecb.svg",
      "fullname": "Zhengqing Wang",
      "name": "EricW123456",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08800",
      "authors": [
        {
          "_id": "6874615e257d4f043537028f",
          "name": "Luke Rivard",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370290",
          "name": "Sun Sun",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370291",
          "name": "Hongyu Guo",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370292",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "6874615e257d4f0435370293",
          "name": "Yuntian Deng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/8bwvqBSzlNYlfcdn-pVWu.mp4"
      ],
      "publishedAt": "2025-07-11T17:59:40.000Z",
      "submittedOnDailyAt": "2025-07-14T01:10:17.755Z",
      "title": "NeuralOS: ニューラルジェネレータモデルを用いたオペレーティングシステムのシミュレーションに向けて",
      "submittedOnDailyBy": {
        "_id": "63081e15a670ed10f9d44229",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
        "isPro": true,
        "fullname": "Yuntian Deng",
        "user": "yuntian-deng",
        "type": "user"
      },
      "summary": "ニューラルオス、ネットワークフレームワークを紹介します。これは、オペレーティングシステムのグラフィックユーザインターフェース（GUI）を直接予測することで、マウスの移動、クリック、キーボードイベントなどのユーザ入力に対応します。ニューラルオスは、コンピュータの状態を追跡するリカレントニューラルネットワーク（RNN）と、ディフュージョンベースのニューラルレンダリングモデルを組み合わせて構成されています。モデルは、ユーザインターフェースのデータセットで訓練されており、これは、AIアガントが生成したリアルショックなインターセクションと、ランダムに生成されたインターセクションの両方を含みます。実験は、ニューラルオスが写実的なGUIシーケンスを成功してレンダリングし、マウスインターセクションを正確に捉え、アプリケーションの起動などの状態タンスクションを信頼的に予測することを示しています。しかし、キーボードインターセクションの細かな構造を精密にモデル化するのは難しいですが、ニューラルオスは、未来のヒューマンコンピュータインターフェースシステムのための完全に適応的な生成可能なニューラルインターフェースの作り方に向けて踏み出しています。",
      "upvotes": 23,
      "discussionId": "6874615e257d4f0435370294",
      "projectPage": "https://neural-os.com/",
      "githubRepo": "https://github.com/yuntian-group/neural-os",
      "ai_summary": "NeuralOS uses a combination of RNNs and diffusion-based rendering to simulate OS GUIs by predicting screen frames from user inputs, demonstrating realistic GUI rendering and state transitions.",
      "ai_keywords": [
        "recurrent neural network",
        "RNN",
        "diffusion-based neural renderer",
        "GUI",
        "user inputs",
        "mouse interactions",
        "keyboard events",
        "state transitions",
        "application launches"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-11T13:59:40.000Z",
    "title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative\n  Models",
    "summary": "We introduce NeuralOS, a neural framework that simulates graphical user\ninterfaces (GUIs) of operating systems by directly predicting screen frames in\nresponse to user inputs such as mouse movements, clicks, and keyboard events.\nNeuralOS combines a recurrent neural network (RNN), which tracks computer\nstate, with a diffusion-based neural renderer that generates screen images. The\nmodel is trained on a large-scale dataset of Ubuntu XFCE recordings, which\ninclude both randomly generated interactions and realistic interactions\nproduced by AI agents. Experiments show that NeuralOS successfully renders\nrealistic GUI sequences, accurately captures mouse interactions, and reliably\npredicts state transitions like application launches. Although modeling\nfine-grained keyboard interactions precisely remains challenging, NeuralOS\noffers a step toward creating fully adaptive, generative neural interfaces for\nfuture human-computer interaction systems.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63081e15a670ed10f9d44229/8bwvqBSzlNYlfcdn-pVWu.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08800.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63081e15a670ed10f9d44229",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63081e15a670ed10f9d44229/w1b9uq-9774bMMgJbSPsS.jpeg",
      "fullname": "Yuntian Deng",
      "name": "yuntian-deng",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 245
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05397",
      "authors": [
        {
          "_id": "6874a1ff257d4f0435370344",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370345",
          "name": "Jie Xia",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370346",
          "name": "Xiaopeng Peng",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370347",
          "name": "Wangbo Zhao",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370348",
          "name": "Zilong Ye",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370349",
          "name": "Zekai Li",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034a",
          "name": "Suorong Yang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034b",
          "name": "Jiadong Pan",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034c",
          "name": "Yuanxiang Chen",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034d",
          "name": "Ziqiao Wang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034e",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f043537034f",
          "name": "Qian Zheng",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370350",
          "name": "Xiaojun Chang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370351",
          "name": "Gang Pan",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370352",
          "name": "Shurong Dong",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370353",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "6874a1ff257d4f0435370354",
          "name": "Yang You",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/KhRuhMNBeK8P8MfGydldo.png"
      ],
      "publishedAt": "2025-07-07T18:31:50.000Z",
      "submittedOnDailyAt": "2025-07-14T04:53:28.591Z",
      "title": "ニューラルネットワーク駆動画像編集",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "傳統画像編集は通常手動プロンプトを依存し、労働費用が高く、手の動作制御能力や言語能力が限られた人々にアクセス可能ではありません。最近の頭脳-コンピュータインターフェース（BCIs）と生成モデルの進歩を活用して、LoongXという手無し画像編集アプローチを提案します。LoongXは、23,928枚の画像編集ペアの最先端のディフューションモデルを用いて、同期された電子エンジェルグラフィー（EEG）、機能的近赤外線スペクトログラフィー（fNIRS）、ポットルティシモグラフィー（PPG）、頭運動信号とセンシングして、ユーザーの意図を捉えます。これらの信号の異質性を有効に対処するために、LoongXは2つのキーモジュールを組み合わせています。クロススケール状態スペース（CS3）モジュールは情報的なモデルソフト特定の特徴量をエンコードします。動的なゲート融合（DGF）モジュールはこれらの特徴量を一つの統合ラテンスペースにまとめ、ディフューショントランスフォーマー（DiT）での微調込みでこのラテンスペースを編集セマンティクスと一致させます。また、コントラスト訓練を用いてエンコーダーを事前訓練し、認知状態と埋め込みされた自然言語からのセマンティクスの意図を一致させます。拡張された実験により、LoongXはテキスト駆動メソッドと比較して同等の性能を達成し、神経信号とスピーチの組み合わせでそれらを超えます（CLIP-T: 0.2588 vs. 0.2549）。これらの結果は、神経駆動生成モデルがアクセス可能で直感的な画像編集を可能にし、認知駆動コンテナティブテクノロジーの新しい方向を開拓することを示します。データセットとコードは将来の研究やこの新しい領域の進歩を促進するために公開されます。",
      "upvotes": 16,
      "discussionId": "6874a200257d4f0435370355",
      "projectPage": "https://loongx1.github.io/",
      "githubRepo": "https://github.com/LanceZPF/loongx",
      "ai_summary": "LoongX uses multimodal neurophysiological signals and diffusion models for hands-free image editing, achieving performance comparable to text-driven methods and outperforming them when combined with speech.",
      "ai_keywords": [
        "diffusion models",
        "electroencephalography (EEG)",
        "functional near-infrared spectroscopy (fNIRS)",
        "photoplethysmography (PPG)",
        "head motion signals",
        "cross-scale state space (CS3) module",
        "dynamic gated fusion (DGF) module",
        "diffusion transformer (DiT)",
        "contrastive learning"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-07-07T14:31:50.000Z",
    "title": "Neural-Driven Image Editing",
    "summary": "Traditional image editing typically relies on manual prompting, making it\nlabor-intensive and inaccessible to individuals with limited motor control or\nlanguage abilities. Leveraging recent advances in brain-computer interfaces\n(BCIs) and generative models, we propose LoongX, a hands-free image editing\napproach driven by multimodal neurophysiological signals. LoongX utilizes\nstate-of-the-art diffusion models trained on a comprehensive dataset of 23,928\nimage editing pairs, each paired with synchronized electroencephalography\n(EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography\n(PPG), and head motion signals that capture user intent. To effectively address\nthe heterogeneity of these signals, LoongX integrates two key modules. The\ncross-scale state space (CS3) module encodes informative modality-specific\nfeatures. The dynamic gated fusion (DGF) module further aggregates these\nfeatures into a unified latent space, which is then aligned with edit semantics\nvia fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train\nthe encoders using contrastive learning to align cognitive states with semantic\nintentions from embedded natural language. Extensive experiments demonstrate\nthat LoongX achieves performance comparable to text-driven methods (CLIP-I:\n0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural\nsignals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results\nhighlight the promise of neural-driven generative models in enabling\naccessible, intuitive image editing and open new directions for\ncognitive-driven creative technologies. Datasets and code will be released to\nsupport future work and foster progress in this emerging area.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/KhRuhMNBeK8P8MfGydldo.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05397.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08799",
      "authors": [
        {
          "_id": "6874ac1e257d4f0435370389",
          "name": "Max Belitsky",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038a",
          "name": "Dawid J. Kopiczko",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038b",
          "name": "Michael Dorkenwald",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038c",
          "name": "M. Jehanzeb Mirza",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038d",
          "name": "Cees G. M. Snoek",
          "hidden": false
        },
        {
          "_id": "6874ac1e257d4f043537038e",
          "name": "Yuki M. Asano",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/637d21239a5217b88b7549c3/w5UPBRgGM_-9_ELxgOBlL.png"
      ],
      "publishedAt": "2025-07-11T17:59:36.000Z",
      "submittedOnDailyAt": "2025-07-14T05:40:13.268Z",
      "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
      "submittedOnDailyBy": {
        "_id": "637d21239a5217b88b7549c3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d21239a5217b88b7549c3/LrIGPiva5VGVZG87rTAJz.jpeg",
        "isPro": false,
        "fullname": "Yuki Asano",
        "user": "yukimasano",
        "type": "user"
      },
      "summary": "We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach leverages GPT-4o-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of hyperparameter stability, inference-time efficiency, and ease of integration, making it a more robust and practical solution for controlled generation.",
      "upvotes": 15,
      "discussionId": "6874ac1e257d4f043537038f",
      "ai_summary": "Cache steering improves reasoning in language models through a single intervention in the key-value cache, enhancing both reasoning structure and task performance.",
      "ai_keywords": [
        "cache steering",
        "key-value cache",
        "chain-of-thought reasoning",
        "GPT-4o",
        "steering vectors",
        "multi-step reasoning",
        "activation steering",
        "hyperparameter stability",
        "inference-time efficiency",
        "ease of integration",
        "controlled generation"
      ]
    },
    "publishedAt": "2025-07-11T13:59:36.000Z",
    "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
    "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/637d21239a5217b88b7549c3/w5UPBRgGM_-9_ELxgOBlL.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08799.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "637d21239a5217b88b7549c3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d21239a5217b88b7549c3/LrIGPiva5VGVZG87rTAJz.jpeg",
      "fullname": "Yuki Asano",
      "name": "yukimasano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08801",
      "authors": [
        {
          "_id": "68746fc3257d4f04353702ce",
          "name": "Hangjie Yuan",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702cf",
          "name": "Weihua Chen",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d0",
          "name": "Jun Cen",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d1",
          "name": "Hu Yu",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d2",
          "name": "Jingyun Liang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d3",
          "name": "Shuning Chang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d4",
          "name": "Zhihui Lin",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d5",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d6",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d7",
          "name": "Jiazheng Xing",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d8",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702d9",
          "name": "Jiasheng Tang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702da",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "68746fc3257d4f04353702db",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:59:42.000Z",
      "submittedOnDailyAt": "2025-07-14T01:18:10.056Z",
      "title": "Lumos-1: 統一モデルからの自動復元ビデオ生成の視点",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "自動回帰型大語言モデル（LLMs）は、複数の言語タスクを統一し、自動回帰型のビデオ生成において初期的な努力を促しました。現在の自動回帰型ビデオジェネレータは、標準的なLLMアーキテクチャから離れ、膨大な外部のテキストエンコーダーに依存し、次のトークンの解釈による遅延が課題です。本論文では、LLMアーキテクチャを最小限の改変で維持する自動回帰型ビデオジェネレータLumos-1を紹介します。LLMsに空間時間的相関を注入するために、3D RoPEの効果を認識し、頻率スペクトルの不均衡範囲を診断しました。そこで、MM-RoPEを提案します。MM-RoPEは、元のテキストのRoPEを維持しながら、複数モデル化するための拡張された頻率スペクトルとスケーリングされた3D位置を提供します。また、Lumos-1は、時系列的因果性を遵守する際のトークンの依存関係を採用します。この依存関係に基づいて、空間情報の冗長によるフレーム毎の損失の不均衡問題を識別し、自動回帰型離散ディフフェローマンションフォーシング（AR-DF）を提案します。AR-DFは、訓練時に時系列のチューブマスクを使用し、推論時のマスクポリシーと相容しながら質量の低下を防ぎます。メモリーエフフィシェントな訓練テクニックを使用して、Lumos-1は48グラフィックス上での予ち編集中、GenEvalでEMU3と同等の性能、VBench-I2VでCOSMOS-Video2Worldと同等の性能、VBench-T2VでOpenSoraPlanと同等の性能を実現しました。コードとモデルは、https://github.com/alibaba-damo-academy/Lumosから利用可能です。",
      "upvotes": 13,
      "discussionId": "68746fc3257d4f04353702dc",
      "githubRepo": "https://github.com/alibaba-damo-academy/Lumos",
      "ai_summary": "Lumos-1 is an autoregressive video generator that uses a modified LLM architecture with MM-RoPE and AR-DF to address spatiotemporal correlation and frame-wise loss imbalance, achieving competitive performance with fewer resources.",
      "ai_keywords": [
        "autoregressive large language models",
        "LLMs",
        "autoregressive video generation",
        "3D RoPE",
        "MM-RoPE",
        "token dependency strategy",
        "intra-frame bidirectionality",
        "inter-frame temporal causality",
        "frame-wise loss imbalance",
        "Autoregressive Discrete Diffusion Forcing",
        "AR-DF",
        "temporal tube masking",
        "GenEval",
        "COSMOS-Video2World",
        "VBench-I2V",
        "OpenSoraPlan",
        "VBench-T2V"
      ],
      "githubStars": 26
    },
    "publishedAt": "2025-07-11T13:59:42.000Z",
    "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective",
    "summary": "Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08801.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08772",
      "authors": [
        {
          "_id": "68746c95257d4f04353702b7",
          "name": "Shaocong Dong",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702b8",
          "name": "Lihe Ding",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702b9",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702ba",
          "name": "Yaokun Li",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bb",
          "name": "Yuxin Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bc",
          "name": "Yucheng Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bd",
          "name": "Qi Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702be",
          "name": "Jaehyeok Kim",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702bf",
          "name": "Chenjian Gao",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c0",
          "name": "Zhanpeng Huang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c1",
          "name": "Zibin Wang",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c2",
          "name": "Tianfan Xue",
          "hidden": false
        },
        {
          "_id": "68746c95257d4f04353702c3",
          "name": "Dan Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:33:18.000Z",
      "submittedOnDailyAt": "2025-07-14T01:05:21.741Z",
      "title": "オンネートモード：3D生成のコンテキスト付きパーツ潜伏",
      "submittedOnDailyBy": {
        "_id": "63ae91af2314b93f9e6dde42",
        "avatarUrl": "/avatars/792ce138cbee85b8754fdcec7fb1ff52.svg",
        "isPro": false,
        "fullname": "Shaocong Dong",
        "user": "dscdyc",
        "type": "user"
      },
      "summary": "最近の3D生成の進展は、多視点2Dレンダリングアプローチから真実データでの幾何学的先驚を利用する3D固有の潜在扩散フレームワークに転移しました。進歩があるにも関わらず、3つの主な制限が残っています：1）単一の潜在表現は複雑な多部の幾何学を捉えず、詳細の減衰を招きます；2）全体的な潜在コーディングは、構成設計に必要な部独立性と相互関係を遺しています；3）グローバル条件付け機構は微軟な操作性を欠けています。人間の3D設計ワークフローをモデル化し、CoPart - 3Dオブジェクトをコンテキスト付きの部の潜在表現に分解して、コンフォームな多部生成を可能にする部識別の扩散フレームワークを提案します。このパラダイムは3つの優みを提供します：i）部の分解により、エンコーディング複雑性を減らします；ii）明示的な部関係モデリングを可能にします；iii）部レベルの条件付けをサポートします。また、関係付きのニューラルネットワークを用いて、預習されたディフフェーションモデルを共通の部の潜在デノイズに最適化するための相互ガイドニング戦略を開発します。大規模なトレーニングを可能にするために、Partverse - Objaverseから自動マッシュ分割と人間の確認付きのアノテーションを通じて作成された新しい3D部データセットを構築しました。広範囲の実験では、CoPartの部レベルの編集、アーチテクチャオブジェクトの生成、そして前所未有の操作性を擁するスケーンの構成において、上位の能力を示しています。",
      "upvotes": 9,
      "discussionId": "68746c96257d4f04353702c4",
      "projectPage": "https://hkdsc.github.io/project/copart/",
      "githubRepo": "https://github.com/hkdsc/copart",
      "ai_summary": "A part-aware diffusion framework, CoPart, enhances 3D generation by decomposing objects into contextual parts, improving complexity handling, relationship modeling, and part-level conditioning.",
      "ai_keywords": [
        "latent diffusion frameworks",
        "geometric priors",
        "single-latent representations",
        "holistic latent coding",
        "part independence",
        "interrelationships",
        "compositional design",
        "global conditioning mechanisms",
        "fine-grained controllability",
        "human 3D design workflows",
        "part-aware diffusion framework",
        "contextual part latents",
        "coherent multi-part generation",
        "encoding complexity",
        "part relationship modeling",
        "part-level conditioning",
        "mutual guidance strategy",
        "joint part latent denoising",
        "geometric coherence",
        "foundation model priors",
        "Partverse",
        "Objaverse",
        "automated mesh segmentation",
        "human-verified annotations",
        "part-level editing",
        "articulated object generation",
        "scene composition"
      ],
      "githubStars": 38
    },
    "publishedAt": "2025-07-11T13:33:18.000Z",
    "title": "From One to More: Contextual Part Latents for 3D Generation",
    "summary": "Recent advances in 3D generation have transitioned from multi-view 2D\nrendering approaches to 3D-native latent diffusion frameworks that exploit\ngeometric priors in ground truth data. Despite progress, three key limitations\npersist: (1) Single-latent representations fail to capture complex multi-part\ngeometries, causing detail degradation; (2) Holistic latent coding neglects\npart independence and interrelationships critical for compositional design; (3)\nGlobal conditioning mechanisms lack fine-grained controllability. Inspired by\nhuman 3D design workflows, we propose CoPart - a part-aware diffusion framework\nthat decomposes 3D objects into contextual part latents for coherent multi-part\ngeneration. This paradigm offers three advantages: i) Reduces encoding\ncomplexity through part decomposition; ii) Enables explicit part relationship\nmodeling; iii) Supports part-level conditioning. We further develop a mutual\nguidance strategy to fine-tune pre-trained diffusion models for joint part\nlatent denoising, ensuring both geometric coherence and foundation model\npriors. To enable large-scale training, we construct Partverse - a novel 3D\npart dataset derived from Objaverse through automated mesh segmentation and\nhuman-verified annotations. Extensive experiments demonstrate CoPart's superior\ncapabilities in part-level editing, articulated object generation, and scene\ncomposition with unprecedented controllability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08772.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63ae91af2314b93f9e6dde42",
      "avatarUrl": "/avatars/792ce138cbee85b8754fdcec7fb1ff52.svg",
      "fullname": "Shaocong Dong",
      "name": "dscdyc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08794",
      "authors": [
        {
          "_id": "68747a58257d4f04353702f9",
          "name": "Yulai Zhao",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fa",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fb",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fc",
          "name": "S. Y. Kung",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fd",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "68747a58257d4f04353702fe",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:55:22.000Z",
      "submittedOnDailyAt": "2025-07-14T02:04:25.605Z",
      "title": "One Token to Fool LLM-as-a-Judge\n\nこのテキストは、日本語に翻訳される必要がありますが、その翻訳は以下の通りです：\n\n1 TokenでLLM-as-a-Judgeを愚弄する方法\n\nこのテキストは、LLM（大語言モデル）をジャッジの役割に立てるシステム（LLM-as-a-Judge）を愚弄するための方法を述べています。「1 Token」は、この方法が非常に効果的で、たった1つのトークン（単語や文脈の基本単位）でもLLM-as-a-Judgeを愚弄できることを意味します。",
      "submittedOnDailyBy": {
        "_id": "62d58fd53bf5e059f7cc3245",
        "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
        "isPro": false,
        "fullname": "Dian Yu",
        "user": "yudian",
        "type": "user"
      },
      "summary": "生成賞奨モデル（もしくはLLMs-as-judges）は、大規模な言語モデル（LLMs）を使用して回答の質量を評価するもので、確認可能な報酬を含む強化学習（RLVR）では増加して採用されています。則約的なルールベースメトリックよりも好まれています、特に、自由形式の出力を含む複雑な理由論のタスクにおいて。このパラダイムでは、LLMは通常、候補回答と事実的な参照を比較し、正確性を示す二値賞奨を割り当てます。この比較タスクの見える簡単さに違い、生成賞奨モデルは表面的な操作に脆弱です：非単語記号（例：「:」または「.」）や理由論の始まりのような「考えの過程:」や「この問題をステップごとに解決しましょう:」が、間違った正答賞奨を引き起こすことが多いです。この弱点がLLMs、データセット、プロンプトフォーマットの幅広い範囲で広がっていることを示し、賞奨モデルを基にした核心アルゴリズムパラダイム（例：拒否サンプリング、好み最適化、RLVR）に対して厳しい威脅を帯びています。この問題を軽減するために、私たちは単純で効果的なデータ拡張戦略を導入し、新しい生成賞奨モデルを学習し、大幅に改善された強固性を持つものを作りました。我々の見つけは、より信頼性のあるLLMベースの評価方法の緊急な必要を強調します。私たちは、https://huggingface.co/sarosavo/Master-RMとhttps://huggingface.co/datasets/sarosavo/Master-RMで強固で一般領域の賞奨モデルとその合成データを公開します。",
      "upvotes": 7,
      "discussionId": "68747a58257d4f04353702ff",
      "ai_summary": "Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.",
      "ai_keywords": [
        "generative reward models",
        "LLMs-as-judges",
        "large language models",
        "reinforcement learning with verifiable rewards",
        "RLVR",
        "binary reward",
        "rejection sampling",
        "preference optimization"
      ]
    },
    "publishedAt": "2025-07-11T13:55:22.000Z",
    "title": "One Token to Fool LLM-as-a-Judge",
    "summary": "Generative reward models (also known as LLMs-as-judges), which use large\nlanguage models (LLMs) to evaluate answer quality, are increasingly adopted in\nreinforcement learning with verifiable rewards (RLVR). They are often preferred\nover rigid rule-based metrics, especially for complex reasoning tasks involving\nfree-form outputs. In this paradigm, an LLM is typically prompted to compare a\ncandidate answer against a ground-truth reference and assign a binary reward\nindicating correctness. Despite the seeming simplicity of this comparison task,\nwe find that generative reward models exhibit surprising vulnerabilities to\nsuperficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning\nopeners like \"Thought process:\" and \"Let's solve this problem step by step.\"\ncan often lead to false positive rewards. We demonstrate that this weakness is\nwidespread across LLMs, datasets, and prompt formats, posing a serious threat\nfor core algorithmic paradigms that rely on generative reward models, such as\nrejection sampling, preference optimization, and RLVR. To mitigate this issue,\nwe introduce a simple yet effective data augmentation strategy and train a new\ngenerative reward model with substantially improved robustness. Our findings\nhighlight the urgent need for more reliable LLM-based evaluation methods. We\nrelease our robust, general-domain reward model and its synthetic training data\nat https://huggingface.co/sarosavo/Master-RM and\nhttps://huggingface.co/datasets/sarosavo/Master-RM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08794.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d58fd53bf5e059f7cc3245",
      "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
      "fullname": "Dian Yu",
      "name": "yudian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08441",
      "authors": [
        {
          "_id": "6874b80f257d4f04353703a8",
          "name": "Anlin Zheng",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703a9",
          "name": "Xin Wen",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703aa",
          "name": "Xuanyang Zhang",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ab",
          "name": "Chuofan Ma",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ac",
          "name": "Tiancai Wang",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ad",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703ae",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "6874b80f257d4f04353703af",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T09:32:45.000Z",
      "submittedOnDailyAt": "2025-07-14T06:26:25.095Z",
      "title": "ビジョンファンデーションモデルを自動復元画像生成の効果的な可視トークナイザーとして扱うための視点",
      "submittedOnDailyBy": {
        "_id": "63483629ac5172169929da0e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665676793089-noauth.jpeg",
        "isPro": false,
        "fullname": "Xin Wen",
        "user": "xwen99",
        "type": "user"
      },
      "summary": "レビューフォービジョンファンダメンタルモデルの強力な表現を活用し、これらのモデルの上に直接イメージトーキナイザーを構築する新しい方向を検討します。特に、この領域は大きく探索されていません。具体的には、フリーズドレビューフォービジョンファンダメンタルモデルをトーキナイザーのエンコーダーとして使用します。これらのモデルの効果を向上させるために、2つの主要な構成要素を導入します。1. 2Dグリッド上の予っちゃった特徴量の冗長性を減らす領域適応減量フレームワーク。2. トーキナイザーの出力をファンダメンタルモデルの表現と一致させ、セマンティックの忠実性を維持するセマンティック再構成オブジェクティブ。これらの設計に基づいて、提案されたイメージトーキナイザー、VFMTokは、画像再構成および生成の品質に大幅な向上を収め、トーキンの効率化も増します。また、自動復元（AR）生成を促進し、ImageNetベンチマークでgFIDが2.07を達成し、モデルの収束を3倍に加速し、クラス条件付きの高品質合成を可能にします。コードは公開的にリリースされ、コミュニティに利益を与えます。",
      "upvotes": 4,
      "discussionId": "6874b80f257d4f04353703b0",
      "ai_summary": "A novel image tokenizer built on pre-trained vision foundation models improves image reconstruction, generation quality, and token efficiency, enhancing autoregressive generation and class-conditional synthesis.",
      "ai_keywords": [
        "pre-trained vision foundation models",
        "image tokenizer",
        "region-adaptive quantization framework",
        "semantic reconstruction objective",
        "VFMTok",
        "gFID",
        "autoregressive generation",
        "class-conditional synthesis",
        "classifier-free guidance"
      ]
    },
    "publishedAt": "2025-07-11T05:32:45.000Z",
    "title": "Vision Foundation Models as Effective Visual Tokenizers for\n  Autoregressive Image Generation",
    "summary": "Leveraging the powerful representations of pre-trained vision foundation\nmodels -- traditionally used for visual comprehension -- we explore a novel\ndirection: building an image tokenizer directly atop such models, a largely\nunderexplored area. Specifically, we employ a frozen vision foundation model as\nthe encoder of our tokenizer. To enhance its effectiveness, we introduce two\nkey components: (1) a region-adaptive quantization framework that reduces\nredundancy in the pre-trained features on regular 2D grids, and (2) a semantic\nreconstruction objective that aligns the tokenizer's outputs with the\nfoundation model's representations to preserve semantic fidelity. Based on\nthese designs, our proposed image tokenizer, VFMTok, achieves substantial\nimprovements in image reconstruction and generation quality, while also\nenhancing token efficiency. It further boosts autoregressive (AR) generation --\nachieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model\nconvergence by three times, and enabling high-fidelity class-conditional\nsynthesis without the need for classifier-free guidance (CFG). The code will be\nreleased publicly to benefit the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08441.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63483629ac5172169929da0e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665676793089-noauth.jpeg",
      "fullname": "Xin Wen",
      "name": "xwen99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06952",
      "authors": [
        {
          "_id": "68749c03257d4f043537033e",
          "name": "Keyon Vafa",
          "hidden": false
        },
        {
          "_id": "68749c03257d4f043537033f",
          "name": "Peter G. Chang",
          "hidden": false
        },
        {
          "_id": "68749c03257d4f0435370340",
          "name": "Ashesh Rambachan",
          "hidden": false
        },
        {
          "_id": "68749c03257d4f0435370341",
          "name": "Sendhil Mullainathan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T15:36:15.000Z",
      "submittedOnDailyAt": "2025-07-14T04:27:13.989Z",
      "title": "基礎モデルは何を見つけたのか？ 推論的バイアスを用いて世界モデルを探る",
      "submittedOnDailyBy": {
        "_id": "64d98ef7a4839890b25eb78b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
        "isPro": true,
        "fullname": "Fangyuan Yu",
        "user": "Ksgk-fy",
        "type": "user"
      },
      "summary": "基礎モデルは、順序予測が深いドメインの理解を明らかにすることを前提としています。これはカイプタールの惑星の動きの予測がニュートンの力学の発見につながったようなものです。しかし、これらのモデルが本質的な構造を真に捉えているかどうかを評価するのは難しいです。我々は、基礎モデルの評価に適した技術を開発し、この技術は、モデルが設定された世界モデルから生成された合成データセットにどのように適応するかを調べます。この技術は、基礎モデルのインデクターブライスが世界モデルに合致しているかを評価し、そのためにインデクターブライスプローブと呼びます。複数のドメインで、基礎モデルは訓練タスクで優れているが、新しいタスクに適応すると、世界モデルに関するインデクターブライスを開発しないことを見出しました。特に、軌道のプロジェクトにより訓練された基礎モデルは、新しい物理タスクに適応するとニュートンの力学を適用しないことを一貫して見出しました。進みに、これらのモデルはタスクに適したヒューリスティクスを開発し、それらが一般化できないようなものとして行動していることが明らかになりました。",
      "upvotes": 4,
      "discussionId": "68749c03257d4f0435370342",
      "ai_summary": "Foundation models, despite excelling in training tasks, often fail to generalize to new tasks due to task-specific heuristics rather than capturing underlying world models.",
      "ai_keywords": [
        "sequence prediction",
        "foundation models",
        "inductive bias",
        "synthetic datasets",
        "world model",
        "inductive bias probe",
        "Newtonian mechanics",
        "task-specific heuristics",
        "generalization"
      ]
    },
    "publishedAt": "2025-07-09T11:36:15.000Z",
    "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for\n  World Models",
    "summary": "Foundation models are premised on the idea that sequence prediction can\nuncover deeper domain understanding, much like how Kepler's predictions of\nplanetary motion later led to the discovery of Newtonian mechanics. However,\nevaluating whether these models truly capture deeper structure remains a\nchallenge. We develop a technique for evaluating foundation models that\nexamines how they adapt to synthetic datasets generated from some postulated\nworld model. Our technique measures whether the foundation model's inductive\nbias aligns with the world model, and so we refer to it as an inductive bias\nprobe. Across multiple domains, we find that foundation models can excel at\ntheir training tasks yet fail to develop inductive biases towards the\nunderlying world model when adapted to new tasks. We particularly find that\nfoundation models trained on orbital trajectories consistently fail to apply\nNewtonian mechanics when adapted to new physics tasks. Further analysis reveals\nthat these models behave as if they develop task-specific heuristics that fail\nto generalize.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d98ef7a4839890b25eb78b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
      "fullname": "Fangyuan Yu",
      "name": "Ksgk-fy",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.08771",
      "authors": [
        {
          "_id": "68747a17257d4f04353702ef",
          "name": "Chenyang Song",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f0",
          "name": "Weilin Zhao",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f1",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f2",
          "name": "Chaojun Xiao",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f3",
          "name": "Yingfa Chen",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f4",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f5",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "68747a17257d4f04353702f6",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T17:28:56.000Z",
      "submittedOnDailyAt": "2025-07-14T03:24:41.458Z",
      "title": "BlockFFN: チャンクレベル活性化スパーシティによる終端側加速に適したミックスオブエキスポーターズ向け",
      "submittedOnDailyBy": {
        "_id": "64c09684e56520a63d35ec87",
        "avatarUrl": "/avatars/86a338e8d122a66a94143bbb9bf3ebf8.svg",
        "isPro": false,
        "fullname": "Chenyang Song",
        "user": "Raincleared",
        "type": "user"
      },
      "summary": "ラージュラング言語モデル（LLMs）の計算負担を軽減するために、活性化のスパース性を持つアーキテクチャ、特にミックスオブエキスパート（MoE）を代表するものが注目されています。しかし、ベースモデルのMoEの非微分別性と柔軟性の欠陥がモデルの性能を損なっています。また、各トークンがその少数のパラメータを活性化するだけであるのに対して、スパースに活性化されたアーキテクチャはチャンクレベルのスパース性が低いことが明らかです。これは、連続した多数のトークンの活性化パラメータの割合が大きいことを示しています。このスパース性のパターンは、低リソース条件（例：端末デバイス）での加速に不適合であり、主流の加速技術（例：推論解釈）とは不適合です。これらの課題を解決するために、私たちは新しいMoEアーキテクチャ、BlockFFNを導入し、その効率的な訓練および部署手法を提案します。特に、ReLU活性化とRMSNormを組み合わせたルータを使用して、微分可能と柔軟なルーティングを実現します。次に、トークンレベルのスパース性（TLS）とチャンクレベルのスパース性（CLS）の両方を促進するために、CLSに関する訓練オブジェクティブを設計し、BlockFFNを加速に適したものにします。最後に、活性化のスパース性と推論解釈を組み合わせた最初の有効な加速キャンバーを実装します。実験結果は、BlockFFNが他のMoEベースラインに対して上位の性能を示し、80%以上のTLSと70%以上の8トークンのCLSを達成しました。私たちのキャンバーは、実際の端末デバイスでの3.67倍のスピードアップを達成しました。すべてのコードとチェックポイントは公開に供給されています（https://github.com/thunlp/BlockFFN）。",
      "upvotes": 2,
      "discussionId": "68747a17257d4f04353702f7",
      "githubRepo": "https://github.com/thunlp/BlockFFN",
      "githubStars": 3
    },
    "publishedAt": "2025-07-11T13:28:56.000Z",
    "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity",
    "summary": "To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67times speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.08771.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c09684e56520a63d35ec87",
      "avatarUrl": "/avatars/86a338e8d122a66a94143bbb9bf3ebf8.svg",
      "fullname": "Chenyang Song",
      "name": "Raincleared",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07151",
      "authors": [
        {
          "_id": "68707d75c8391850d6097823",
          "user": {
            "_id": "66b34647a29e5c00011d34c3",
            "avatarUrl": "/avatars/ae39f9b84f9379423fa3a8509fbcc94e.svg",
            "isPro": false,
            "fullname": "Zongmeng Zhang",
            "user": "ustc-zhangzm",
            "type": "user"
          },
          "name": "Zongmeng Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:24.241Z",
          "hidden": false
        },
        {
          "_id": "68707d75c8391850d6097824",
          "name": "Wengang Zhou",
          "hidden": false
        },
        {
          "_id": "68707d75c8391850d6097825",
          "name": "Jie Zhao",
          "hidden": false
        },
        {
          "_id": "68707d75c8391850d6097826",
          "name": "Houqiang Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66b34647a29e5c00011d34c3/Zj-2S7r3SlJnKipepoQxm.png"
      ],
      "publishedAt": "2025-07-09T11:18:38.000Z",
      "submittedOnDailyAt": "2025-07-14T01:45:01.699Z",
      "title": "モデルフリックトレッド対策の強固な多モーダル大語言モデル",
      "submittedOnDailyBy": {
        "_id": "66b34647a29e5c00011d34c3",
        "avatarUrl": "/avatars/ae39f9b84f9379423fa3a8509fbcc94e.svg",
        "isPro": false,
        "fullname": "Zongmeng Zhang",
        "user": "ustc-zhangzm",
        "type": "user"
      },
      "summary": "多模态大語言モデル（MLLM）は、視覚言語タスクでは優れた能力を示しますが、実世界の場合にはホライゼーションが発生しやすい。本論文は、モデルの回答と入力との間の衝突に焦点を当てて、MLLMのホライゼーション現象を検討します。既存の研究は、モデルの回答と入力との衝突を焦点にしていますが、私たちは、モデルにホライゼーションを直接招くモデルの入力の固有の衝突を研究します。モデル衝突を正式的に定義し、ビジョン言語タスクでのこの現象をシミュレートするために、Multimodal Modality Conflict (MMMC) データセットを構築しました。プロンプトエンジニアリング、規範的調整、強化学習に基づく3つの方法を提案し、モデル衝突によるホライゼーションを軽減することを目指します。MMMCデータセット上で拡大した実験を行い、これらの方法の長所と短所を分析しました。私たちの結果は、強化学習方法がモデル衝突の下でのホライゼーションを軽減する最良の性能を達成し、規範的調整方法は望ましいおさましい性能を示しました。私たちの研究は、ホライゼーションを招く未注意のモデル衝突を明らかにし、MLLMの強固性についてより深い理解を提供します。",
      "upvotes": 2,
      "discussionId": "68707d76c8391850d6097827",
      "projectPage": "https://github.com/zmzhang2000/MMMC",
      "githubRepo": "https://github.com/zmzhang2000/MMMC",
      "ai_summary": "Investigation of modality conflict in multimodal large language models reveals its role in causing hallucinations, with reinforcement learning emerging as the most effective mitigation strategy.",
      "ai_keywords": [
        "multimodal large language models",
        "vision-language tasks",
        "hallucinations",
        "modality conflict",
        "prompt engineering",
        "supervised fine-tuning",
        "reinforcement learning",
        "Multimodal Modality Conflict (MMMC)"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-09T07:18:38.000Z",
    "title": "Robust Multimodal Large Language Models Against Modality Conflict",
    "summary": "Despite the impressive capabilities of multimodal large language models\n(MLLMs) in vision-language tasks, they are prone to hallucinations in\nreal-world scenarios. This paper investigates the hallucination phenomenon in\nMLLMs from the perspective of modality conflict. Unlike existing works focusing\non the conflicts between model responses and inputs, we study the inherent\nconflicts in inputs from different modalities that place MLLMs in a dilemma and\ndirectly lead to hallucinations. We formally define the modality conflict and\nconstruct a dataset named Multimodal Modality Conflict (MMMC) to simulate this\nphenomenon in vision-language tasks. Three methods based on prompt engineering,\nsupervised fine-tuning, and reinforcement learning are proposed to alleviate\nthe hallucination caused by modality conflict. Extensive experiments are\nconducted on the MMMC dataset to analyze the merits and demerits of these\nmethods. Our results show that the reinforcement learning method achieves the\nbest performance in mitigating the hallucination under modality conflict, while\nthe supervised fine-tuning method shows promising and stable performance. Our\nwork sheds light on the unnoticed modality conflict that leads to\nhallucinations and provides more insights into the robustness of MLLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b34647a29e5c00011d34c3/Zj-2S7r3SlJnKipepoQxm.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07151.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b34647a29e5c00011d34c3",
      "avatarUrl": "/avatars/ae39f9b84f9379423fa3a8509fbcc94e.svg",
      "fullname": "Zongmeng Zhang",
      "name": "ustc-zhangzm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]