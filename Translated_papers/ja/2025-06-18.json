[
  {
    "paper": {
      "id": "2506.14429",
      "authors": [
        {
          "_id": "68521a9a0164cd131671045c",
          "name": "Xiaoran Liu",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd131671045d",
          "name": "Zhigeng Liu",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd131671045e",
          "name": "Zengfeng Huang",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd131671045f",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd1316710460",
          "name": "Ziwei He",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd1316710461",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T11:45:37.000Z",
      "submittedOnDailyAt": "2025-06-18T00:18:23.135Z",
      "title": "LongLLaDA: 長文脈能力を解放するディフュージョンLLM",
      "submittedOnDailyBy": {
        "_id": "64f033ef82c6eea604c4da8b",
        "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
        "isPro": false,
        "fullname": "Liu Xiaoran",
        "user": "LiuXR",
        "type": "user"
      },
      "summary": "大語言拡散モデル（Large Language Diffusion Models, LLMs）は、NLP研究の重要な焦点として登場し、スケーラビリティと下流タスクの性能に向けて大幅な努力が向けられています。しかし、それらの長文脈能力は調査されていません、システマティックな分析や文脈拡張の方法が欠けています。本研究では、長文脈性能の比較を行う最初のシステマティックな調査を行います。長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較するために、長文脈性能を比較する",
      "upvotes": 27,
      "discussionId": "68521a9a0164cd1316710462",
      "ai_summary": "This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.",
      "ai_keywords": [
        "diffusion LLMs",
        "auto-regressive LLMs",
        "stable perplexity",
        "local perception",
        "Rotary Position Embedding (RoPE) scaling theory",
        "LongLLaDA",
        "NTK-based RoPE extrapolation",
        "context extrapolation scaling laws",
        "long-context tasks"
      ]
    },
    "publishedAt": "2025-06-17T07:45:37.000Z",
    "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs",
    "summary": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a\nsignificant focus in NLP research, with substantial effort directed toward\nunderstanding their scalability and downstream task performance. However, their\nlong-context capabilities remain unexplored, lacking systematic analysis or\nmethods for context extension. In this work, we present the first systematic\ninvestigation comparing the long-context performance of diffusion LLMs and\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\n\\textit{stable perplexity} during direct context extrapolation.\nFurthermore, where auto-regressive models fail outright during the\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\ndiscover diffusion LLMs exhibit a distinct \\textit{local perception}\nphenomenon, enabling successful retrieval from recent context segments. We\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\nscaling theory. Building on these observations, we propose LongLLaDA, a\ntraining-free method that integrates LLaDA with the NTK-based RoPE\nextrapolation. Our results validate that established extrapolation scaling laws\nremain effective for extending the context windows of diffusion LLMs.\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\nauto-regressive LLMs and others where they fall short. Consequently, this study\nestablishes the first context extrapolation method for diffusion LLMs while\nproviding essential theoretical insights and empirical benchmarks critical for\nadvancing future research on long-context diffusion LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14429.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f033ef82c6eea604c4da8b",
      "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
      "fullname": "Liu Xiaoran",
      "name": "LiuXR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.12928",
      "authors": [
        {
          "_id": "6851dd060164cd13167103d7",
          "name": "King Zhu",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103d8",
          "name": "Hanhao Li",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103d9",
          "name": "Siwei Wu",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103da",
          "name": "Tianshun Xing",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103db",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103dc",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103dd",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103de",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103df",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103e0",
          "name": "Yuchen Eleanor Jiang",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103e1",
          "name": "Changwang Zhang",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103e2",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103e3",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103e4",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103e5",
          "name": "Wangchunshu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-15T17:59:47.000Z",
      "submittedOnDailyAt": "2025-06-18T04:21:02.464Z",
      "title": "スケーリングテストタイムコンピュートのLLMアガント",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "測定時スケーリングの計算量を拡大することが、大規模言語モデル（LLMs）の推理能力を向上させるために非常に成功していることが示されています。本研究では、言語アガイントに対して測定時スケーリング方法の応用を最初の系統的な調査を行い、その効果を評価することを目的としています。特に、以下のような測定時スケーリング戦略について調査しています：1. 并列サンプリングアルゴリズム；2. 順次修正戦略；3. バリデータと結果の統合方法；4. ロードアウトの多様化戦略。測定時スケーリングを言語アガイントに適用する際の異なる設計戦略の影響を詳細に分析し、以下の結果が得られました：1. 測定時スケーリングの計算量の拡大はアガイントの性能を向上させることができます。2. どのタイミングで反省するかはアガイントにとって重要です。3. 異なるバリデーションと結果の統合手法の中では、リストウィスエーションモジュールが最も効果的です。4. ロードアウトの多様化はアガイントのタスク性能に正面的な影響を及ぼします。",
      "upvotes": 26,
      "discussionId": "6851dd060164cd13167103e6",
      "ai_summary": "Systematic exploration of test-time scaling methods in large language agents reveals that computational scaling improves performance, especially through parallel sampling, sequential revision, effective verification, and increased rollout diversity.",
      "ai_keywords": [
        "parallel sampling algorithms",
        "sequential revision strategies",
        "verifiers",
        "merging methods",
        "diversified rollouts",
        "test-time scaling",
        "large language models"
      ]
    },
    "publishedAt": "2025-06-15T13:59:47.000Z",
    "title": "Scaling Test-time Compute for LLM Agents",
    "summary": "Scaling test time compute has shown remarkable success in improving the\nreasoning abilities of large language models (LLMs). In this work, we conduct\nthe first systematic exploration of applying test-time scaling methods to\nlanguage agents and investigate the extent to which it improves their\neffectiveness. Specifically, we explore different test-time scaling strategies,\nincluding: (1) parallel sampling algorithms; (2) sequential revision\nstrategies; (3) verifiers and merging methods; (4)strategies for diversifying\nrollouts.We carefully analyze and ablate the impact of different design\nstrategies on applying test-time scaling on language agents, and have follow\nfindings: 1. Scaling test time compute could improve the performance of agents.\n2. Knowing when to reflect is important for agents. 3. Among different\nverification and result merging approaches, the list-wise method performs best.\n4. Increasing diversified rollouts exerts a positive effect on the agent's task\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12928.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 49
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13642",
      "authors": [
        {
          "_id": "685129448a68fee7f6ba4c04",
          "name": "Shaolei Zhang",
          "hidden": false
        },
        {
          "_id": "685129448a68fee7f6ba4c05",
          "name": "Shoutao Guo",
          "hidden": false
        },
        {
          "_id": "685129448a68fee7f6ba4c06",
          "name": "Qingkai Fang",
          "hidden": false
        },
        {
          "_id": "685129448a68fee7f6ba4c07",
          "name": "Yan Zhou",
          "hidden": false
        },
        {
          "_id": "685129448a68fee7f6ba4c08",
          "name": "Yang Feng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/MBm95m2RAX6iKKBaKTma8.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/PoLupV32gI1iLxILZccQS.mp4"
      ],
      "publishedAt": "2025-06-16T16:06:45.000Z",
      "submittedOnDailyAt": "2025-06-18T00:16:02.465Z",
      "title": "ストリーム・オムニ：同時多モーダルインタラクションと大規模な言語・視覚・声学モデル",
      "submittedOnDailyBy": {
        "_id": "64803e5dc57f629056c601f1",
        "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
        "isPro": false,
        "fullname": "Shaolei Zhang",
        "user": "zhangshaolei",
        "type": "user"
      },
      "summary": "GPT-4oみたいな大規模多モダルモデル（LMMs）の出現は、テキスト、ビジョン、スピーチモデルの統合をサポートするために、より柔軟な多モダルインタラクションの検討を促進しています。現在のLMMsは、通常、モデルのシーケンス次元にモデルの表現を連結し、大規模言語モデル（LLM）のバックボーンに入力します。シーケンス次元の連結は、モデルの統合には簡単ですが、モデルの対位を学習するためには、大規模なデータが必要です。本論文では、モデルの関係をより目的的にモデル化し、より効率的かつ柔軟なモデルの対位を実現することを目指しています。そのために、Stream-Omniという、効率的なモデルの対位を持つ大規模言語-ビジョン-スピーチモデルを提案します。Stream-Omniは、LLMをバックボーンとして使用し、ビジョンとスピーチをテキストに基づいて対位します。ビジョンがテキストと語意的に補間的である場合、Stream-Omniはシーケンス次元の連結を使用し、ビジョン-テキストの対位を実現します。スピーチがテキストと語意的に一致する場合、Stream-OmniはCTCベースのレイヤー次元のマッピングを使用し、スピーチ-テキストの対位を実現します。このように、Stream-Omniはデータ量（特にスピーチ）を少なくてもモデルの対位を実現することができ、テキストの機能をモデルの他のモデルに転移することができます。多様なベンチマークでの実験は、Stream-Omniが視覚理解、スピーチインタラクション、ビジョンベースのスピーチインタラクションタスクに強力な性能を示しています。レイヤー次元のマッピングにより、Stream-Omniは、スピーチインタラクション中に、ASR訳読やモデルの応答などの中间テキスト出力を同時に提供することができ、ユーザーにより詳細な多モデル体験を提供します。",
      "upvotes": 18,
      "discussionId": "685129458a68fee7f6ba4c09",
      "projectPage": "https://github.com/ictnlp/Stream-Omni",
      "githubRepo": "https://github.com/ictnlp/Stream-Omni",
      "ai_summary": "Stream-Omni, a large multimodal model, integrates text, vision, and speech by efficiently aligning modalities using sequence-dimension concatenation for vision and layer-dimension mapping for speech, achieving strong performance with less data.",
      "ai_keywords": [
        "GPT-4o-like",
        "large multimodal models",
        "LLM backbone",
        "modality alignments",
        "sequence-dimension concatenation",
        "CTC-based layer-dimension mapping",
        "visual understanding",
        "speech interaction",
        "vision-grounded speech interaction",
        "ASR transcriptions",
        "model responses"
      ]
    },
    "publishedAt": "2025-06-16T12:06:45.000Z",
    "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model",
    "summary": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/MBm95m2RAX6iKKBaKTma8.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/PoLupV32gI1iLxILZccQS.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13642.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64803e5dc57f629056c601f1",
      "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
      "fullname": "Shaolei Zhang",
      "name": "zhangshaolei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14234",
      "authors": [
        {
          "_id": "68522d7f0164cd13167104ee",
          "name": "Md Tanzib Hosain",
          "hidden": false
        },
        {
          "_id": "68522d7f0164cd13167104ef",
          "name": "Salman Rahman",
          "hidden": false
        },
        {
          "_id": "68522d7f0164cd13167104f0",
          "name": "Md Kishor Morol",
          "hidden": false
        },
        {
          "_id": "68522d7f0164cd13167104f1",
          "name": "Md Rizwan Parvez",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T06:47:19.000Z",
      "submittedOnDailyAt": "2025-06-18T01:39:00.207Z",
      "title": "ソルバー: ホリスティックな経験学習を用いた多効果的な理由論として、オリンピックチームのように協力して解決する",
      "submittedOnDailyBy": {
        "_id": "65ae1c4468139e3c42973fe4",
        "avatarUrl": "/avatars/b065a857dd763410caadea37a2dc01c4.svg",
        "isPro": false,
        "fullname": "Md Rizwan Parvez",
        "user": "mparvez",
        "type": "user"
      },
      "summary": "現在の複雑な理由論理における驚異的な進歩を踏まえても、現在の大規模な言語モデル（LLMs）は通常独自に動作し、問題を独立した試みとして扱い、経験的知識を蓄積または統合しない。対照的に、オリンピックやプログラミングコンテストチームのような専門家の問題解決者は、豊富な経験の組織を活用している：教練からの指導を受け取り、過去の問題から直感を鍛錬し、ツールの使用とライブラリ機能の知識を活用し、同僚の専門家と共にスキルと経験を基に戦略を適切に変更し、試行錯誤のように理由論理を繰り返し改善し、コンペション中でも他の関連した問題から学ぶ。Xolverというトレーニング無しの多エージェントの理由論理フレームワークを紹介します。Xolverは、ブラックボックスのLLMに持続的的、進化的な経験の全体像的なメモリーを付与します。Xolverは、外部と自分の検索、ツールの使用、共同処理、エージェント駆動の評価、イテレーション的な改善を含む多様な経験のモデルを統合しています。Xolverは、推論時に関連する戦略、コードフレーム、抽象的な理由論理パターンを学習し、解決策を最初から生成することを避け、孤立した推論から経験に基づく言語アガントへの転移を示します。Xolverは、開放ウェイトモデルとプロプライティモデルの両方をベースに構築されているが、専門的な理由論理アガントを超える経験的な性能を維持しています。Xolverは、軽量バックボーン（例：QWQ-32B）でも、Qwen3-235B、Gemini 2.5 Pro、o3、o4-mini-highなどの先進モデルを超えることが多くあります。Xolverは、o3-mini-highを使用して、GSM8K（98.1%）、AIME'24（94.4%）、AIME'25（93.7%）、Math-500（99.8%）、LiveCodeBench-V5（91.6%）で新たな最善結果を収め、経験的な学習をキーとする一般的なアガントの開発に向けて進むことを示します。コードとデータは、https://kagnlp.github.io/xolver.github.io/から利用可能です。",
      "upvotes": 17,
      "discussionId": "68522d7f0164cd13167104f2",
      "ai_summary": "Xolver, a multi-agent reasoning framework, enhances large language models with persistent memory and diverse experience modalities, improving performance on complex reasoning tasks by avoiding generating solutions from scratch.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "multi-agent reasoning framework",
        "persistent memory",
        "experience-aware language agents",
        "external and self-retrieval",
        "tool use",
        "collaborative interactions",
        "agent-driven evaluation",
        "iterative refinement",
        "GSM8K",
        "AIME'24",
        "AIME'25",
        "Math-500",
        "LiveCodeBench-V5",
        "generalist agents",
        "expert-level reasoning"
      ]
    },
    "publishedAt": "2025-06-17T02:47:19.000Z",
    "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n  Like an Olympiad Team",
    "summary": "Despite impressive progress on complex reasoning, current large language\nmodels (LLMs) typically operate in isolation - treating each problem as an\nindependent attempt, without accumulating or integrating experiential\nknowledge. In contrast, expert problem solvers - such as Olympiad or\nprogramming contest teams - leverage a rich tapestry of experiences: absorbing\nmentorship from coaches, developing intuition from past problems, leveraging\nknowledge of tool usage and library functionality, adapting strategies based on\nthe expertise and experiences of peers, continuously refining their reasoning\nthrough trial and error, and learning from other related problems even during\ncompetition. We introduce Xolver, a training-free multi-agent reasoning\nframework that equips a black-box LLM with a persistent, evolving memory of\nholistic experience. Xolver integrates diverse experience modalities, including\nexternal and self-retrieval, tool use, collaborative interactions, agent-driven\nevaluation, and iterative refinement. By learning from relevant strategies,\ncode fragments, and abstract reasoning patterns at inference time, Xolver\navoids generating solutions from scratch - marking a transition from isolated\ninference toward experience-aware language agents. Built on both open-weight\nand proprietary models, Xolver consistently outperforms specialized reasoning\nagents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses\nadvanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high.\nWith o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24\n(94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) -\nhighlighting holistic experience learning as a key step toward generalist\nagents capable of expert-level reasoning. Code and data are available at\nhttps://kagnlp.github.io/xolver.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14234.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ae1c4468139e3c42973fe4",
      "avatarUrl": "/avatars/b065a857dd763410caadea37a2dc01c4.svg",
      "fullname": "Md Rizwan Parvez",
      "name": "mparvez",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13363",
      "authors": [
        {
          "_id": "685234100164cd1316710508",
          "name": "Lijun Liu",
          "hidden": false
        },
        {
          "_id": "685234100164cd1316710509",
          "name": "Ruiyang Li",
          "hidden": false
        },
        {
          "_id": "685234100164cd131671050a",
          "name": "Zhaocheng Liu",
          "hidden": false
        },
        {
          "_id": "685234100164cd131671050b",
          "name": "Chenglin Zhu",
          "hidden": false
        },
        {
          "_id": "685234100164cd131671050c",
          "name": "Chong Li",
          "hidden": false
        },
        {
          "_id": "685234100164cd131671050d",
          "name": "Jiehan Cheng",
          "hidden": false
        },
        {
          "_id": "685234100164cd131671050e",
          "name": "Qiang Ju",
          "hidden": false
        },
        {
          "_id": "685234100164cd131671050f",
          "name": "Jian Xie",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/633e570be7d5ce7bfe037a53/-7W-jpcvZwQ046FQHgVdY.qt"
      ],
      "publishedAt": "2025-06-16T11:10:25.000Z",
      "submittedOnDailyAt": "2025-06-18T02:12:29.534Z",
      "title": "Efficient Medical VIE via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "633e570be7d5ce7bfe037a53",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633e570be7d5ce7bfe037a53/zV8ULv4Mu7YIGZ8D3JtmK.jpeg",
        "isPro": false,
        "fullname": "Zhaocheng Liu",
        "user": "zhaocheng",
        "type": "user"
      },
      "summary": "Visual Information Extraction (VIE)は、医療應用のようなレポート分析やオンライン諮問に必要なJSONなどの構造化フォーマットに無構造化ドキュメント画像を変換する。従来の方法はOCRと言語モデルを基にしていましたが、端末からの多模構造モデルは直接JSONを生成することができます。しかし、領域特有のスキーマと高額のアノテーションコストが医療VIEの効果性を限定しています。私たちのアプローチは、Reinforcement Learning with Verifiable Rewards (RLVR)フレームワークを基にして、100件のアノテーションサンプルを使ってこれらの課題を解決します。私たちのアプローチはデータセットの多様性を確保し、精度と再現率のバランスを取る証拠可能な報酬機構を使用してヘラミニングを減らし、領域カバーを改善し、理由論の強化を促す新しいサンプリング戦略を提案しています。Qwen2.5-VL-7BをRLVR方法で微調節して、医療VIEタスクで最先端の性能を達成し、F1、精度、再現率を大幅に向上させました。私たちのモデルは医療データセットに似たタスクで優れていますが、違うタスクでは性能が低下し、領域特有の最適化の必要性を強調しています。ケーススタディは、VIEの訓練および推論時の理由論の価値を進一歩に示しています。",
      "upvotes": 17,
      "discussionId": "685234100164cd1316710510",
      "ai_summary": "An RLVR framework using fine-tuned Qwen2.5-VL-7B achieves state-of-the-art performance in medical VIE with limited annotated samples, enhancing reasoning and balance between precision and recall.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "JSON generation",
        "multimodal models",
        "dataset diversity",
        "precision-recall reward mechanism",
        "hallucinations",
        "field coverage",
        "sampling strategies",
        "fine-tuning",
        "Qwen2.5-VL-7B",
        "F1 score",
        "case studies"
      ]
    },
    "publishedAt": "2025-06-16T07:10:25.000Z",
    "title": "Efficient Medical VIE via Reinforcement Learning",
    "summary": "Visual Information Extraction (VIE) converts unstructured document images\ninto structured formats like JSON, critical for medical applications such as\nreport analysis and online consultations. Traditional methods rely on OCR and\nlanguage models, while end-to-end multimodal models offer direct JSON\ngeneration. However, domain-specific schemas and high annotation costs limit\ntheir effectiveness in medical VIE. We base our approach on the Reinforcement\nLearning with Verifiable Rewards (RLVR) framework to address these challenges\nusing only 100 annotated samples. Our approach ensures dataset diversity, a\nbalanced precision-recall reward mechanism to reduce hallucinations and improve\nfield coverage, and innovative sampling strategies to enhance reasoning\ncapabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve\nstate-of-the-art performance on medical VIE tasks, significantly improving F1,\nprecision, and recall. While our models excel on tasks similar to medical\ndatasets, performance drops on dissimilar tasks, highlighting the need for\ndomain-specific optimization. Case studies further demonstrate the value of\nreasoning during training and inference for VIE.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/633e570be7d5ce7bfe037a53/-7W-jpcvZwQ046FQHgVdY.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13363.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633e570be7d5ce7bfe037a53",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633e570be7d5ce7bfe037a53/zV8ULv4Mu7YIGZ8D3JtmK.jpeg",
      "fullname": "Zhaocheng Liu",
      "name": "zhaocheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14758",
      "authors": [
        {
          "_id": "685239610164cd1316710553",
          "user": {
            "_id": "649e6761f9134a06ed1e0cea",
            "avatarUrl": "/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg",
            "isPro": false,
            "fullname": "Daixuan Cheng",
            "user": "daixuancheng",
            "type": "user"
          },
          "name": "Daixuan Cheng",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-18T06:57:21.864Z",
          "hidden": false
        },
        {
          "_id": "685239610164cd1316710554",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "685239610164cd1316710555",
          "name": "Xuekai Zhu",
          "hidden": false
        },
        {
          "_id": "685239610164cd1316710556",
          "name": "Bo Dai",
          "hidden": false
        },
        {
          "_id": "685239610164cd1316710557",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "685239610164cd1316710558",
          "name": "Zhenliang Zhang",
          "hidden": false
        },
        {
          "_id": "685239610164cd1316710559",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UekvaawzSgcb5I120mngD.png",
        "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UbAwRMdcT31OV6ORPZG52.png"
      ],
      "publishedAt": "2025-06-17T17:54:03.000Z",
      "submittedOnDailyAt": "2025-06-18T02:33:04.259Z",
      "title": "探索の理由：エントロピーからの視点",
      "submittedOnDailyBy": {
        "_id": "649e6761f9134a06ed1e0cea",
        "avatarUrl": "/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg",
        "isPro": false,
        "fullname": "Daixuan Cheng",
        "user": "daixuancheng",
        "type": "user"
      },
      "summary": "ボリング探索と採掘のバランスは、強化学習（RL）の中心的な目標です。最近の言語モデル（LM）の理由論の向上にもとって、多くの方法は採掘に偏っていて、性能のスライドによく遭遇します。本稿では、RLでの探索の信号としてのエントロピーを再評価し、LMでの探索的な理由論との関係を調べます。実験的な分析を通じて、高エントロピー領域と探索的な理由論行動の3つの種類との強い正の相関関係を明らかにしました：（1）ロジックステップを決定または結ぶポイントトークン、（2）自覚証と補正などの反省的な行動、（3）基礎LMが軽く探索されていない稀少な行動。これによって、標準的なRLに1行のコードだけを追加する最小限の変更を提案します：エントロピーベースの項を優位関数に追加します。傳統的な最大エントロピー法は、不確実性を促して探索を促していたり、これは、長いもしくは深い理由論連鎖を促して探索を促しています。特に、我々の方法は、LMの理由論能力の上限の推定としてのPass@Kメトリックによって显著な収益を達成し、極端に大きなK値で評価されても、LMの理由論の境界を超えることができます。",
      "upvotes": 16,
      "discussionId": "685239610164cd131671055a",
      "ai_summary": "Introducing an entropy-based term to the advantage function in reinforcement learning enhances exploratory reasoning in language models, leading to improved performance on complex reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "entropy",
        "exploratory reasoning",
        "pivotal tokens",
        "reflective actions",
        "rare behaviors",
        "advantage function",
        "Pass@K"
      ]
    },
    "publishedAt": "2025-06-17T13:54:03.000Z",
    "title": "Reasoning with Exploration: An Entropy Perspective",
    "summary": "Balancing exploration and exploitation is a central goal in reinforcement\nlearning (RL). Despite recent advances in enhancing language model (LM)\nreasoning, most methods lean toward exploitation, and increasingly encounter\nperformance plateaus. In this work, we revisit entropy -- a signal of\nexploration in RL -- and examine its relationship to exploratory reasoning in\nLMs. Through empirical analysis, we uncover strong positive correlations\nbetween high-entropy regions and three types of exploratory reasoning actions:\n(1) pivotal tokens that determine or connect logical steps, (2) reflective\nactions such as self-verification and correction, and (3) rare behaviors\nunder-explored by the base LMs. Motivated by this, we introduce a minimal\nmodification to standard RL with only one line of code: augmenting the\nadvantage function with an entropy-based term. Unlike traditional\nmaximum-entropy methods which encourage exploration by promoting uncertainty,\nwe encourage exploration by promoting longer and deeper reasoning chains.\nNotably, our method achieves significant gains on the Pass@K metric -- an\nupper-bound estimator of LM reasoning capabilities -- even when evaluated with\nextremely large K values, pushing the boundaries of LM reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UekvaawzSgcb5I120mngD.png",
      "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UbAwRMdcT31OV6ORPZG52.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14758.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "649e6761f9134a06ed1e0cea",
      "avatarUrl": "/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg",
      "fullname": "Daixuan Cheng",
      "name": "daixuancheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.14245",
      "authors": [
        {
          "_id": "68521c2a0164cd131671046b",
          "name": "Xumeng Wen",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd131671046c",
          "name": "Zihan Liu",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd131671046d",
          "user": {
            "_id": "64a7a2bad001860e0c34f7f2",
            "avatarUrl": "/avatars/2433104071e4ae1c3e2d755d81d7964b.svg",
            "isPro": false,
            "fullname": "Shun Zheng",
            "user": "shun-zheng",
            "type": "user"
          },
          "name": "Shun Zheng",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-18T02:37:06.379Z",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd131671046e",
          "name": "Zhijian Xu",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd131671046f",
          "name": "Shengyu Ye",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd1316710470",
          "name": "Zhirong Wu",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd1316710471",
          "name": "Xiao Liang",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd1316710472",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd1316710473",
          "name": "Junjie Li",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd1316710474",
          "name": "Ziming Miao",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd1316710475",
          "name": "Jiang Bian",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd1316710476",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a7a2bad001860e0c34f7f2/zpklFaaRznQyEa0t9Ji70.png"
      ],
      "publishedAt": "2025-06-17T07:06:56.000Z",
      "submittedOnDailyAt": "2025-06-18T01:24:22.712Z",
      "title": "実験的リワードを証明可能にした強化学習は、基盤LLMで正しい推理を実質的に奨励します。",
      "submittedOnDailyBy": {
        "_id": "64a7a2bad001860e0c34f7f2",
        "avatarUrl": "/avatars/2433104071e4ae1c3e2d755d81d7964b.svg",
        "isPro": false,
        "fullname": "Shun Zheng",
        "user": "shun-zheng",
        "type": "user"
      },
      "summary": "RLVR（Verifiable Rewards を用いた Reinforcement Learning）は、Large Language Models (LLMs) の論理能力を進化させるための有望的なパラダイムとして登場しました。しかし、その効果性には重要な矛盾があります：RLVR で調整されたモデルは、解決策の探索に関する Pass@K メトリックで基礎モデルよりも劣り、RLVR は論理多様性を失わずに現有の論理パスを再権衡するだけであるという仮説が生まれました。本研究では、この矛盾を解決するために、問題の原因を特定しました：Pass@K メトリック自身は、論理を正確に評価することができないことを示しています。これは、正しい最終的な答えを与えることによって認められるため、不正確なコンティンューションや不完全なコンティンューション（CoTs）から生まれることがあるからです。これに対して、我々は、論理パスと最終的な答えの両方が正しいことを求めるより正確な評価メトリック、CoT-Pass@K を導入しました。我々は、RLVR は、単なる伝統的な RL と異なり、論理の整合性を奨励することを特に機能させることを形式化した新しい理論的基盤を提供します。我々の実験結果は、CoT-Pass@K を用いて、RLVR はすべての K の値に対して正しい論理の一般化を奨励することを観察しました。また、学習ダイナミクスを分析することにより、この向上さえは学習過程の初期段階で出現し、平滑に一般化しています。我々の研究は、RLVR の役割を明確にし、その評価の信頼性の高い方法を提供し、その真の実力を認識することを確認しました。",
      "upvotes": 12,
      "discussionId": "68521c2a0164cd1316710477",
      "ai_summary": "RLVR advances machine reasoning by incentivizing correct and logical thought chains, addressing limitations identified by a more precise evaluation metric, $CoT$-$Pass@K$.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "LLMS",
        "Pass@K",
        "chains of thought",
        "CoT-Pass@K",
        "logical integrity",
        "machine reasoning",
        "training dynamics"
      ]
    },
    "publishedAt": "2025-06-17T03:06:56.000Z",
    "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npromising paradigm for advancing the reasoning capabilities of Large Language\nModels (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned\nmodels often underperform their base models on the Pass@K metric for\nsolution-finding, leading to the hypothesis that RLVR merely re-weights\nexisting reasoning paths at the cost of reasoning diversity. In this work, we\nresolve this contradiction by identifying the source of the problem: the\nPass@K metric itself is a flawed measure of reasoning, as it credits correct\nfinal answers that probably arise from inaccurate or incomplete chains of\nthought (CoTs). To address this, we introduce a more precise evaluation metric,\nCoT-Pass@K, which mandates that both the reasoning path and the final\nanswer be correct. We provide a new theoretical foundation that formalizes how\nRLVR, unlike traditional RL, is uniquely structured to incentivize logical\nintegrity. Our empirical results are supportive: using CoT-Pass@K, we\nobserve that RLVR can incentivize the generalization of correct reasoning for\nall values of K. Furthermore, by analyzing the training dynamics, we find\nthat this enhanced reasoning capability emerges early in the training process\nand smoothly generalizes. Our work provides a clear perspective on the role of\nRLVR, offers a more reliable method for its evaluation, and confirms its\npotential to genuinely advance machine reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a7a2bad001860e0c34f7f2/zpklFaaRznQyEa0t9Ji70.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14245.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a7a2bad001860e0c34f7f2",
      "avatarUrl": "/avatars/2433104071e4ae1c3e2d755d81d7964b.svg",
      "fullname": "Shun Zheng",
      "name": "shun-zheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.12860",
      "authors": [
        {
          "_id": "685226a40164cd13167104bd",
          "name": "Wanlong Liu",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104be",
          "name": "Junxiao Xu",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104bf",
          "name": "Fei Yu",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104c0",
          "name": "Yukang Lin",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104c1",
          "name": "Ke Ji",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104c2",
          "name": "Wenyu Chen",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104c3",
          "name": "Yan Xu",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104c4",
          "name": "Yasheng Wang",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104c5",
          "name": "Lifeng Shang",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104c6",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-15T14:21:28.000Z",
      "submittedOnDailyAt": "2025-06-18T02:06:38.533Z",
      "title": "QFFT、理由のない問題のない微調校での適応的理由",
      "submittedOnDailyBy": {
        "_id": "64eb333e6878d90b031fa5c5",
        "avatarUrl": "/avatars/a0d875b49d1c56be88f34854647306da.svg",
        "isPro": false,
        "fullname": "Wanlong Liu",
        "user": "lwl-uestc",
        "type": "user"
      },
      "summary": "最近の長いコンティンュースの思考（CoT）モデルの進展は、複雑なタスクに対する性能を向上させたが、過度な思考により、冗長な理由のステップが生成され、特に簡単な質問に対してこの問題がある。本論文は、長いと短いCoTモデルの理由のパターンを再調査し、短いCoTのパターンが効率的に簡潔な理由を提供し、長いCoTのパターンが短いCoTのパターンが苦戦する難しいシナリオで優れていることを観察した。モデルが両方のパターンを活用することを可能にするために、問題無しの微調校（QFFT）を提案します。QFFTは、トレーニング時に入力の質問を除去し、長いCoTのレスポンスからだけ学習する微調校アプローチです。このアプローチは、モデルが両方の理由のパターンを適応的に使用することを可能にします：短いCoTのパターンを優先し、必要に応じて長いCoTのパターンを活性化します。数学データセット上での様々な実験により、QFFTは平均のレスポンスの長さを50％以上削減し、規範的な性能を達成します。また、QFFTは、SFTよりもノイズのある、ドメイン外の、そしてリソースの低いシナリオでも上位の性能を示します。",
      "upvotes": 12,
      "discussionId": "685226a40164cd13167104c7",
      "githubRepo": "https://github.com/LWL-cpu/Question-Free-Fine-Tuning",
      "ai_summary": "Question-Free Fine-Tuning (QFFT) improves efficiency and adaptability in cognitive models by leveraging both short and long chain-of-thought patterns, reducing response length while maintaining performance across various scenarios.",
      "ai_keywords": [
        "Long Chain-of-Thought",
        "Short Chain-of-Thought",
        "Question-Free Fine-Tuning",
        "fine-tuning",
        "Supervised Fine-Tuning"
      ]
    },
    "publishedAt": "2025-06-15T10:21:28.000Z",
    "title": "QFFT, Question-Free Fine-Tuning for Adaptive Reasoning",
    "summary": "Recent advancements in Long Chain-of-Thought (CoT) reasoning models have\nimproved performance on complex tasks, but they suffer from overthinking, which\ngenerates redundant reasoning steps, especially for simple questions. This\npaper revisits the reasoning patterns of Long and Short CoT models, observing\nthat the Short CoT patterns offer concise reasoning efficiently, while the Long\nCoT patterns excel in challenging scenarios where the Short CoT patterns\nstruggle. To enable models to leverage both patterns, we propose Question-Free\nFine-Tuning (QFFT), a fine-tuning approach that removes the input question\nduring training and learns exclusively from Long CoT responses. This approach\nenables the model to adaptively employ both reasoning patterns: it prioritizes\nthe Short CoT patterns and activates the Long CoT patterns only when necessary.\nExperiments on various mathematical datasets demonstrate that QFFT reduces\naverage response length by more than 50\\%, while achieving performance\ncomparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits\nsuperior performance compared to SFT in noisy, out-of-domain, and low-resource\nscenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12860.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64eb333e6878d90b031fa5c5",
      "avatarUrl": "/avatars/a0d875b49d1c56be88f34854647306da.svg",
      "fullname": "Wanlong Liu",
      "name": "lwl-uestc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.12278",
      "authors": [
        {
          "_id": "685234030164cd1316710502",
          "name": "Zheyuan Yang",
          "hidden": false
        },
        {
          "_id": "685234030164cd1316710503",
          "name": "Zexi Kuang",
          "hidden": false
        },
        {
          "_id": "685234030164cd1316710504",
          "name": "Xue Xia",
          "hidden": false
        },
        {
          "_id": "685234030164cd1316710505",
          "name": "Yilun Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-13T23:56:17.000Z",
      "submittedOnDailyAt": "2025-06-18T02:08:35.444Z",
      "title": "LLMsはアルゴリズム問題の高品質なテストケースを生成できるか？  \nTestCase-Eval: ファウルチャーカバレーションとエクスポーションのシステム的な評価",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "TestCase-Evalは、テストケース生成におけるLLMのシステマティック評価のための新しいベンチマークです。TestCase-Evalは、Codeforcesプラットフォームからの500問のアルゴリズム問題と100,000件の人間が作成した解決策を含みます。2つの中心的なタスクに焦点を当てています：1) フェアドキャビディング、LLM生成されたテストセットが多様な入力スケーナrioを検出し、潜在的な失敗モードの広い範囲を被覆することを評価します。2) フェアドキャビディング、LLMが特定の不正なコード実装を示すためのタイラードテスト入力を作成することができるかを評価します。TestCase-Evalで19つの最先端の開放ソースおよび所有権LLMの効果的なテストケース生成の強みと限界について、詳細な評価を提供します。",
      "upvotes": 12,
      "discussionId": "685234030164cd1316710506",
      "githubRepo": "https://github.com/FlowRays/TestCase-Eval",
      "ai_summary": "TestCase-Eval is a benchmark for evaluating LLMs in generating comprehensive and targeted test cases for algorithm problems.",
      "ai_keywords": [
        "test-case generation",
        "Fault Coverage",
        "Fault Exposure",
        "LLMs",
        "algorithm problems",
        "human-crafted solutions",
        "Codeforces",
        "test sets",
        "failure modes",
        "incorrect code implementation"
      ]
    },
    "publishedAt": "2025-06-13T19:56:17.000Z",
    "title": "Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure",
    "summary": "We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs\nin test-case generation. TestCase-Eval includes 500 algorithm problems and\n100,000 human-crafted solutions from the Codeforces platform. It focuses on two\npivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test\nsets probe diverse input scenarios and cover a wide range of potential failure\nmodes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored\ntest input that reveals a specific incorrect code implementation. We provide a\ncomprehensive assessment of 19 state-of-the-art open-source and proprietary\nLLMs on TestCase-Eval, offering insights into their strengths and limitations\nin generating effective test cases for algorithm problems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14606",
      "authors": [
        {
          "_id": "68521f240164cd131671047a",
          "name": "Ahmed Heakl",
          "hidden": false
        },
        {
          "_id": "68521f240164cd131671047b",
          "name": "Sarim Hashmi",
          "hidden": false
        },
        {
          "_id": "68521f240164cd131671047c",
          "name": "Chaimaa Abi",
          "hidden": false
        },
        {
          "_id": "68521f240164cd131671047d",
          "name": "Celine Lee",
          "hidden": false
        },
        {
          "_id": "68521f240164cd131671047e",
          "name": "Abdulrahman Mahmoud",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G_EGzMfb1C6fX_o-yLFbl.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G2UhoU9mbZ8tQ0VzmKLV5.png"
      ],
      "publishedAt": "2025-06-17T15:06:54.000Z",
      "submittedOnDailyAt": "2025-06-18T00:38:14.336Z",
      "title": "確保予想：CISCからRISCへの言語モデリングアプローチ\nテストガランティーズ付きトレンスパイル",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "ハードウェアエコシステムは急速に進化しており、低レベルプログラムを異なるインストラクションセットアーキテクチャ（ISA）間に迅速かつ柔軟な方法で翻訳し、既存コードのポータビリティと長期間の耐久性を向上させることに興味が高まっています。この翻訳問題の特に難しいクラスとして、複雑な（CISC）と減少された（RISC）ハードウェアアーキテクチャ間の翻訳があります。これは、インストラクションの複雑性、メモリモデル、実行パラダイムの基本的な違いにより難しいことになっています。本研究では、LLM（大規模訓練された言語モデル）の翻訳力と設定されたソフトウェアテスト構造の厳密性を統合したISA中心的な翻訳パイプラインGG（Guaranteed Guess）を介して、この問題を解決しています。我々の方法は、LLMを用いて一つのISAからもう一つのISAへの候補翻訳を生成し、これらの翻訳をソフトウェアテストフレームワーク内に埋め込み、翻訳の信頼度を定量的に評価することです。我々のGGアプローチは、2つの多様なデータセットを通じて評価され、ユニットテストで高いコードカバレッジ（>98%）を実現し、HumanEvalプログラムの機能/セマンティック正確性が99%、BringupBenchプログラムの場合は49%を達成しました。また、Apple Siliconでの最先端のRosetta 2フレームワークと比較し、我々のアプローチは、実行時性能が1.73倍、エネルギー効率が1.47倍、メモリ使用量が2.41倍改善され、CISC-to-RISCの実世界的な翻訳タスクに対してGGの効果を示しました。我々は、コード、データ、モデル、ベンチマークを公開し、ISAレベルのコード翻訳研究の共通の基盤を築くことを目指します。",
      "upvotes": 10,
      "discussionId": "68521f240164cd131671047f",
      "projectPage": "https://ahmedheakl.github.io/Guaranteed-Guess/",
      "githubRepo": "https://github.com/ahmedheakl/Guaranteed-Guess",
      "ai_summary": "A novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.",
      "ai_keywords": [
        "pre-trained large language models",
        "software testing constructs",
        "ISA-centric transpilation",
        "complex-instruction set computing (CISC)",
        "reduced-instruction set computing (RISC)",
        "instruction set architecture (ISA)",
        "HumanEval",
        "BringupBench",
        "Rosetta 2 framework",
        "functional/semantic correctness",
        "real-world CISC-to-RISC translation",
        "memory models",
        "execution paradigms",
        "transpilation",
        "hardware ecosystem",
        "low-level programs",
        "code portability",
        "longevity"
      ]
    },
    "publishedAt": "2025-06-17T11:06:54.000Z",
    "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
    "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G_EGzMfb1C6fX_o-yLFbl.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G2UhoU9mbZ8tQ0VzmKLV5.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14606.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 41
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14603",
      "authors": [
        {
          "_id": "68521f8a0164cd1316710481",
          "name": "Amirmojtaba Sabour",
          "hidden": false
        },
        {
          "_id": "68521f8a0164cd1316710482",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "68521f8a0164cd1316710483",
          "name": "Karsten Kreis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/_yuUKzjzngNZzaUJCVwjR.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/2FmUhOtaDCtAzFkNEx6Di.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/MC_ZLBqT81dOpANroMLrm.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/degPI6Z5IEf4v_y2_UL9V.jpeg"
      ],
      "publishedAt": "2025-06-17T15:06:07.000Z",
      "submittedOnDailyAt": "2025-06-18T00:42:20.168Z",
      "title": "Align Your Flow: 時間連続フローマップの拡大と経験\n\n**注**: この翻訳は、オリジナルの英語テキストの意味を忠実に伝えるために、タイトルと内容を最適な日本語に調整しました。",
      "submittedOnDailyBy": {
        "_id": "656015f28138827138c70858",
        "avatarUrl": "/avatars/b8471ae4d80f078c7c928fc3d8f49126.svg",
        "isPro": false,
        "fullname": "Amirmojtaba Sabour",
        "user": "amsabour",
        "type": "user"
      },
      "summary": "ディフュージョンおよび流れベースモデルは最先端の生成モデリングアプローチとして現れてきましたが、それらは複数のサンプリングステップが必要となります。一致モデルはこれらのモデルを効率的な一ステップジェネレーターに収納することができますが、流れベースおよびディフュージョンベースの方法に比べてステップ数を増やすと性能が必然的に低下します。これは分析的および実験的にも示されています。流マップは、どのような2つのノイズレベルを連結することを可能にし、全ステップ数でも効果的であることを示します。本論文では、流マップの訓練に向けて2つの新しい連続時間の目標を介し、既存の一致モデルと流マッチング目標を拡張した新しい訓練手法を紹介します。また、自導航を使用して低品質モデルをガイドしたディスタイル化で性能向上を示し、相手対的な最終調整でもサンプル多様性の最小限の損失で追加のブーストを実現できることを示します。我々は、難しい画像生成ベンチマークで強力な流マップモデル「フローマップを調整」を構築し、ImageNet 64x64と512x512でも少ないステップで最先端の性能を収めました。最後に、文から画像への流マップモデルを紹介し、すべての現在の非相手対的訓練された少ステップサンプライザーを超える性能を示します。",
      "upvotes": 8,
      "discussionId": "68521f8a0164cd1316710484",
      "projectPage": "https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/",
      "ai_summary": "Flow maps, introduced with new continuous-time objectives and training techniques, achieve state-of-the-art performance in few-step image and text-to-image generation.",
      "ai_keywords": [
        "diffusion models",
        "flow-based models",
        "consistency models",
        "flow maps",
        "noise levels",
        "autoguidance",
        "adversarial finetuning",
        "Align Your Flow",
        "ImageNet",
        "text-to-image synthesis"
      ]
    },
    "publishedAt": "2025-06-17T11:06:07.000Z",
    "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation",
    "summary": "Diffusion- and flow-based models have emerged as state-of-the-art generative\nmodeling approaches, but they require many sampling steps. Consistency models\ncan distill these models into efficient one-step generators; however, unlike\nflow- and diffusion-based methods, their performance inevitably degrades when\nincreasing the number of steps, which we show both analytically and\nempirically. Flow maps generalize these approaches by connecting any two noise\nlevels in a single step and remain effective across all step counts. In this\npaper, we introduce two new continuous-time objectives for training flow maps,\nalong with additional novel training techniques, generalizing existing\nconsistency and flow matching objectives. We further demonstrate that\nautoguidance can improve performance, using a low-quality model for guidance\nduring distillation, and an additional boost can be achieved by adversarial\nfinetuning, with minimal loss in sample diversity. We extensively validate our\nflow map models, called Align Your Flow, on challenging image generation\nbenchmarks and achieve state-of-the-art few-step generation performance on both\nImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,\nwe show text-to-image flow map models that outperform all existing\nnon-adversarially trained few-step samplers in text-conditioned synthesis.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/_yuUKzjzngNZzaUJCVwjR.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/2FmUhOtaDCtAzFkNEx6Di.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/MC_ZLBqT81dOpANroMLrm.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/degPI6Z5IEf4v_y2_UL9V.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14603.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "656015f28138827138c70858",
      "avatarUrl": "/avatars/b8471ae4d80f078c7c928fc3d8f49126.svg",
      "fullname": "Amirmojtaba Sabour",
      "name": "amsabour",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13977",
      "authors": [
        {
          "_id": "68524ff90164cd13167105aa",
          "name": "Shiting Huang",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105ab",
          "name": "Zhen Fang",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105ac",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105ad",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105ae",
          "name": "Junjie Ye",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105af",
          "name": "Yu Zeng",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105b0",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105b1",
          "name": "Qi Mao",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105b2",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:59:18.000Z",
      "submittedOnDailyAt": "2025-06-18T04:10:42.444Z",
      "title": "CRITICTOOL: ツール呼び出しエラーのスケーラーでの大規模言語モデルの自己批判能力の評価",
      "submittedOnDailyBy": {
        "_id": "64b0a5037a475fba70a7260d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg",
        "isPro": false,
        "fullname": "Zhen Fang",
        "user": "CostaliyA",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）が外部ツールを利用する能力は、多様なタスクを手がかりにすることができました。しかし、タスクが複雑になり、長期的なものになると、複雑なツール利用プロセスが意図したエラーを引き起こすことがあります。そこで、このようなエラーを有効に対処する方法、特に認識、診断、そしてそれらからリカバリする方法が、ツール学習の進歩の重要な研究方向として現れました。本稿では、数々の競争的なツール評価ベンチマークでファンクション呼び出しプロセス中に発生するエラーの種類を検討します。その基に、CRITICTOOLという、ツール学習に特化した詳細な批判評価ベンチマークを介しています。新しいデータセット構築の進化策略をベースに、CRITICTOOLは多様なツール使用エラーを持ち、その複雑性を変えています。CRITICTOOL上で拡大的な実験を行い、構築されたベンチマーク戦略の一般化と効果性を証明します。また、各LLMのツール反省能力について深い分析を行い、LLMのツール学習領域に新しい視点を提供します。コードは、https://github.com/Shellorley0513/CriticTool{https://github.com/Shellorley0513/CriticTool}にアクセスできます。",
      "upvotes": 7,
      "discussionId": "68524ff90164cd13167105b3",
      "githubRepo": "https://github.com/Shellorley0513/CriticTool",
      "ai_summary": "A comprehensive benchmark, CRITICTOOL, evaluates and enhances the robustness of large language models in handling errors during tool usage.",
      "ai_keywords": [
        "large language models",
        "tool learning",
        "function-calling process",
        "error identification",
        "error diagnosis",
        "error recovery",
        "evolutionary strategy",
        "dataset construction",
        "tool reflection ability",
        "critique evaluation benchmark"
      ]
    },
    "publishedAt": "2025-06-11T13:59:18.000Z",
    "title": "CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language\n  Models in Tool-Calling Error Scenarios",
    "summary": "The ability of large language models (LLMs) to utilize external tools has\nenabled them to tackle an increasingly diverse range of tasks. However, as the\ntasks become more complex and long-horizon, the intricate tool utilization\nprocess may trigger various unexpected errors. Therefore, how to effectively\nhandle such errors, including identifying, diagnosing, and recovering from\nthem, has emerged as a key research direction for advancing tool learning. In\nthis work, we first extensively analyze the types of errors encountered during\nthe function-calling process on several competitive tool evaluation benchmarks.\nBased on it, we introduce CRITICTOOL, a comprehensive critique evaluation\nbenchmark specialized for tool learning. Building upon a novel evolutionary\nstrategy for dataset construction, CRITICTOOL holds diverse tool-use errors\nwith varying complexities, which better reflects real-world scenarios. We\nconduct extensive experiments on CRITICTOOL, and validate the generalization\nand effectiveness of our constructed benchmark strategy. We also provide an\nin-depth analysis of the tool reflection ability on various LLMs, offering a\nnew perspective on the field of tool learning in LLMs. The code is available at\nhttps://github.com/Shellorley0513/CriticTool{https://github.com/Shellorley0513/CriticTool}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b0a5037a475fba70a7260d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg",
      "fullname": "Zhen Fang",
      "name": "CostaliyA",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13651",
      "authors": [
        {
          "_id": "6850cf555e07650ecce88fe2",
          "name": "Kaiyuan Chen",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fe3",
          "name": "Yixin Ren",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fe4",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fe5",
          "name": "Xiaobo Hu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fe6",
          "name": "Haotong Tian",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fe7",
          "name": "Tianbao Xie",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fe8",
          "name": "Fangfu Liu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fe9",
          "name": "Haoye Zhang",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fea",
          "name": "Hongzhang Liu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88feb",
          "name": "Yuan Gong",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fec",
          "name": "Chen Sun",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fed",
          "name": "Han Hou",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fee",
          "name": "Hui Yang",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fef",
          "name": "James Pan",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff0",
          "name": "Jianan Lou",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff1",
          "name": "Jiayi Mao",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff2",
          "name": "Jizheng Liu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff3",
          "name": "Jinpeng Li",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff4",
          "name": "Kangyi Liu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff5",
          "name": "Kenkun Liu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff6",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff7",
          "name": "Run Li",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff8",
          "name": "Tong Niu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff9",
          "name": "Wenlong Zhang",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ffa",
          "name": "Wenqi Yan",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ffb",
          "name": "Xuanzheng Wang",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ffc",
          "name": "Yuchen Zhang",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ffd",
          "name": "Yi-Hsin Hung",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ffe",
          "name": "Yuan Jiang",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fff",
          "name": "Zexuan Liu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce89000",
          "name": "Zihan Yin",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce89001",
          "name": "Zijian Ma",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce89002",
          "name": "Zhiwen Mo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-16T16:16:14.000Z",
      "submittedOnDailyAt": "2025-06-18T04:38:47.336Z",
      "title": "xbench: 専門家に合わせた実世界的評価によるAgentsの生産性スケーリングの追跡",
      "submittedOnDailyBy": {
        "_id": "6505a02f9310ce8c400edc63",
        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
        "isPro": false,
        "fullname": "Fangfu Liu",
        "user": "Liuff23",
        "type": "user"
      },
      "summary": "ディナイム、職業に合わせた評価システム「xbench」を紹介します。これは、AIアグエントの能力と実世界的な生産性の間の隙間を埋めるために設計されました。現在の評価ベースは、孤立した技術スキルを中心に焦点を当てていることが多いが、それらはアジェントが専門家環境で提供する経済価値を正確に反映していません。この問題に対処するため、「xbench」は、業界専門家が定めた評価タスクを対象とした商業的に重要な領域をターゲットにしています。我々のフレームワークは、生産性価値と強烈に関連付けられたメトリックを作成し、Technology-Market Fit (TMF) の予測を可能にし、時間過ぎにおけるプロダクト能力の軌跡を追跡することを促進します。最初の実装として、我々は2つのベースマークを提供します：「Recruitment」と「Marketing」。「Recruitment」では、実世界的なハイプロティングビジネスシナリオから50タスクを集め、アジェントの企業マッピング、情報検索、および才能採用の能力を評価します。「Marketing」では、インフルエンサーを広告業者の需要に合わせる能力を評価し、836名の候補インフルエンサーのカレードを使用して50ビジネスの広告業者の要求を満たす性能を評価します。我々は、先進的な現代アジェントの初期評価結果を提供し、これらの専門家領域における基準を確立します。我々は、https://xbench.org から、連続的に更新される評価セットと評価を提供しています。",
      "upvotes": 5,
      "discussionId": "6850cf555e07650ecce89003",
      "projectPage": "https://xbench.org/",
      "githubRepo": "https://github.com/xbench-ai/xbench-evals"
    },
    "publishedAt": "2025-06-16T12:16:14.000Z",
    "title": "xbench: Tracking Agents Productivity Scaling with Profession-Aligned\n  Real-World Evaluations",
    "summary": "We introduce xbench, a dynamic, profession-aligned evaluation suite designed\nto bridge the gap between AI agent capabilities and real-world productivity.\nWhile existing benchmarks often focus on isolated technical skills, they may\nnot accurately reflect the economic value agents deliver in professional\nsettings. To address this, xbench targets commercially significant domains with\nevaluation tasks defined by industry professionals. Our framework creates\nmetrics that strongly correlate with productivity value, enables prediction of\nTechnology-Market Fit (TMF), and facilitates tracking of product capabilities\nover time. As our initial implementations, we present two benchmarks:\nRecruitment and Marketing. For Recruitment, we collect 50 tasks from real-world\nheadhunting business scenarios to evaluate agents' abilities in company\nmapping, information retrieval, and talent sourcing. For Marketing, we assess\nagents' ability to match influencers with advertiser needs, evaluating their\nperformance across 50 advertiser requirements using a curated pool of 836\ncandidate influencers. We present initial evaluation results for leading\ncontemporary agents, establishing a baseline for these professional domains.\nOur continuously updated evalsets and evaluations are available at\nhttps://xbench.org.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13651.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6505a02f9310ce8c400edc63",
      "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
      "fullname": "Fangfu Liu",
      "name": "Liuff23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14002",
      "authors": [
        {
          "_id": "685210eb0164cd131671043e",
          "name": "Siyu Chen",
          "hidden": false
        },
        {
          "_id": "685210eb0164cd131671043f",
          "name": "Heejune Sheen",
          "hidden": false
        },
        {
          "_id": "685210eb0164cd1316710440",
          "name": "Xuyuan Xiong",
          "hidden": false
        },
        {
          "_id": "685210eb0164cd1316710441",
          "name": "Tianhao Wang",
          "hidden": false
        },
        {
          "_id": "685210eb0164cd1316710442",
          "name": "Zhuoran Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-16T20:58:05.000Z",
      "submittedOnDailyAt": "2025-06-18T00:09:08.222Z",
      "title": "LLMの多意味性を制御するためのSparse Autoencoderによる可証明的的特徴量復元法",
      "submittedOnDailyBy": {
        "_id": "683229900411a9d65cd410c0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VqwvpUYF8CQAKPHMNfLyw.png",
        "isPro": false,
        "fullname": "Siyu Chen",
        "user": "Siyuc",
        "type": "user"
      },
      "summary": "私たちは、大語言モデルの解釈においてスパースオートエンコーダー（SAE）を用いて理論的に根拠のある特徴量の復元を実現する課題を研究しています。現在のSAEの訓練アルゴリズムは、厳密な数学的な保証を欠くことが多いですし、ハイパーパラメーターの敏感性や不穩定などの実用的な制限を受けています。これらの問題に対して、私たちは最初に特徴量の復元問題に対する新しい統計的フレームワークを提案します。このフレームワークは、多意味的な特徴量を潜在的な一意味的な概念のスパースな混和としてモデル化した新しい特徴量の特定性の概念を含みます。このフレームワークに基づいて、私たちは「バイアスアダプタション」という技術に基づく新しいSAEの訓練アルゴリズムを提案します。この技術は、神経ネットワークのバイアスパラメーターを適切な活性のスパースさを確保するために適応的に調整することを目的としています。理論的にこのアルゴリズムは、入力データが我々の提案した統計的モデルからサンプリングされた場合にすべての一意味的な特徴量を正しく復元することを証明しています。また、私たちは、このアルゴリズムの実験的な改善版、グループバイアスアダプタション（GBA）を開発し、150億パラメーター程度のLLMに対してベンチマーク方法と比較して上位の性能を示しました。この研究は、SAEの訓練を解明する基盤的なステップであり、理論的な復元保証を持つ最初のSAEアルゴリズムを提供し、機構的な解釈性を高めることで、透明で信頼できるAIシステムの開発に貢献します。",
      "upvotes": 4,
      "discussionId": "685210ec0164cd1316710443",
      "ai_summary": "A new statistical framework and training algorithm, Group Bias Adaptation, enhance Sparse Autoencoders for recovering monosemantic features in Large Language Models, offering theoretical guarantees and superior performance.",
      "ai_keywords": [
        "Sparse Autoencoders",
        "feature recovery",
        "statistical framework",
        "feature identifiability",
        "polysemantic features",
        "monosemantic concepts",
        "bias adaptation",
        "Group Bias Adaptation",
        "Large Language Models",
        "theoretical recovery guarantees",
        "mechnistic interpretability"
      ]
    },
    "publishedAt": "2025-06-16T16:58:05.000Z",
    "title": "Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n  Autoencoders",
    "summary": "We study the challenge of achieving theoretically grounded feature recovery\nusing Sparse Autoencoders (SAEs) for the interpretation of Large Language\nModels. Existing SAE training algorithms often lack rigorous mathematical\nguarantees and suffer from practical limitations such as hyperparameter\nsensitivity and instability. To address these issues, we first propose a novel\nstatistical framework for the feature recovery problem, which includes a new\nnotion of feature identifiability by modeling polysemantic features as sparse\nmixtures of underlying monosemantic concepts. Building on this framework, we\nintroduce a new SAE training algorithm based on ``bias adaptation'', a\ntechnique that adaptively adjusts neural network bias parameters to ensure\nappropriate activation sparsity. We theoretically prove that this\nalgorithm correctly recovers all monosemantic features when input data is\nsampled from our proposed statistical model. Furthermore, we develop an\nimproved empirical variant, Group Bias Adaptation (GBA), and\ndemonstrate its superior performance against benchmark methods when\napplied to LLMs with up to 1.5 billion parameters. This work represents a\nfoundational step in demystifying SAE training by providing the first SAE\nalgorithm with theoretical recovery guarantees, thereby advancing the\ndevelopment of more transparent and trustworthy AI systems through enhanced\nmechanistic interpretability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14002.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "683229900411a9d65cd410c0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VqwvpUYF8CQAKPHMNfLyw.png",
      "fullname": "Siyu Chen",
      "name": "Siyuc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10100",
      "authors": [
        {
          "_id": "68522b190164cd13167104d9",
          "name": "Yantai Yang",
          "hidden": false
        },
        {
          "_id": "68522b190164cd13167104da",
          "name": "Yuhao Wang",
          "hidden": false
        },
        {
          "_id": "68522b190164cd13167104db",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "68522b190164cd13167104dc",
          "name": "Luo Zhongwei",
          "hidden": false
        },
        {
          "_id": "68522b190164cd13167104dd",
          "name": "Chang Zou",
          "hidden": false
        },
        {
          "_id": "68522b190164cd13167104de",
          "name": "Zhipeng Zhang",
          "hidden": false
        },
        {
          "_id": "68522b190164cd13167104df",
          "name": "Chuan Wen",
          "hidden": false
        },
        {
          "_id": "68522b190164cd13167104e0",
          "name": "Linfeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T18:34:57.000Z",
      "submittedOnDailyAt": "2025-06-18T01:30:42.708Z",
      "title": "EfficientVLA: 訓練無し加速と圧縮のための\n  視覚言語アクションモデル",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA)モデル、特にディフュージョンベースのアーキテクチャが構体化した知能を示すための変換的な可能性を持つが、固有のと推論時の冗領による高い計算量とメモリ要求により厳しい制限を受けています。現在の加速効果を目指した応用により、孤立した不適切さを解決しようとしているが、これらの片方づつの解決策は、VLAプイプライン全体の様々な計算やメモリボトルネックを全体的に解決することはできなく、実用的な機能性を制限しています。EfficientVLAという構造化された、学習不要な推論加速フレームワークを紹介します。これは、多面性の冗領を一連的に利用してこれらの壁をシステマティックに排除することで、実用的な機能性を増強します。EfficientVLAは、3つの目標策をシンプレックス的に統合しています：言語モジュールから機能的に無駄なレイヤーを削除する（レイヤ間冗領の分析によるガイド）、視覚処理パスウェイをタスクに適した戦略で最適化する（タスクキシャリティと情報カバーをバランスにする簡潔な、多様な視覚トークンの選択）、イテレーション的なディフュージョンベースのアクションヘッド内の時間的な計算冗領を減らす（戦略的にキャッシュと再利用する鍵の中間特徴量）。標準のVLAモデルCogACTにこの方法を適用した結果、推論速度が1.93倍になり、FLOPsを28.9%に抑え、SIMPLERベンチマークでの成功率が0.6%だけ減少しました。",
      "upvotes": 4,
      "discussionId": "68522b190164cd13167104e1",
      "ai_summary": "EfficientVLA accelerates Vision-Language-Action models by pruning language layers, optimizing visual token selection, and caching intermediate features in the diffusion-based action head.",
      "ai_keywords": [
        "diffusion-based architectures",
        "inference acceleration framework",
        "pruning",
        "inter-layer redundancies",
        "visual tokens",
        "task-aware strategy",
        "iterative diffusion-based action head",
        "caching",
        "FLOPs",
        "SIMPLER benchmark"
      ]
    },
    "publishedAt": "2025-06-11T14:34:57.000Z",
    "title": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models",
    "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10100.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10038",
      "authors": [
        {
          "_id": "68523b4a0164cd131671055d",
          "name": "Giannis Daras",
          "hidden": false
        },
        {
          "_id": "68523b4a0164cd131671055e",
          "user": {
            "_id": "67d204c05422de5644126f0b",
            "avatarUrl": "/avatars/8a9ac73d93785f48e63184d612b9fff1.svg",
            "isPro": false,
            "fullname": "Adrian Rodriguez Munoz",
            "user": "adrianrm",
            "type": "user"
          },
          "name": "Adrian Rodriguez-Munoz",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-18T04:06:37.569Z",
          "hidden": false
        },
        {
          "_id": "68523b4a0164cd131671055f",
          "name": "Adam Klivans",
          "hidden": false
        },
        {
          "_id": "68523b4a0164cd1316710560",
          "name": "Antonio Torralba",
          "hidden": false
        },
        {
          "_id": "68523b4a0164cd1316710561",
          "name": "Constantinos Daskalakis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T22:37:39.000Z",
      "submittedOnDailyAt": "2025-06-18T02:40:58.837Z",
      "title": "Ambient Diffusion Omni: 悪いデータでも良いモデルを学習する",
      "submittedOnDailyBy": {
        "_id": "5f45f44b79c1ba4c353d1035",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f45f44b79c1ba4c353d1035/6piqagYr7RNCy6XwJGWCG.jpeg",
        "isPro": false,
        "fullname": "Giannis Daras",
        "user": "giannisdaras",
        "type": "user"
      },
      "summary": "私たちは、低品質、合成された、そして分布外の画像を使用して、ディフュージョンモデルの品質を向上させる方法を示します。通常、ディフュージョンモデルは、Webや他のソースからの高度にフィルターされたデータポールからのカレーティングされたデータセットでトレーニングされます。私たちは、通常に捨てられる低品質の画像にあまりの価値があることを示します。私たちは、Ambient Diffusion Omniという簡単で原則的なフレームワークを提出し、トレーニング中に全ての画像から信号を抽出できるディフュージョンモデルを学習させる方法を示します。我々のフレームワークは、自然の画像の2つの特性を利用しています -- スペクトルパワーラーの衰減と局在性。まず、私たちは、ガウスブラー、JPEG圧縮、そして動作ブラーによって合成的に悪化された画像を使用して、ディフュージョンモデルを成功にトレーニングできることを証明します。次に、私たちは、フレームワークを使用して、最先端のImageNet FIDを達成し、文から画像を生成する生成モデリングの画像の品質と多様性において显著な向上を示します。核心のアイデアは、ノイズが、望ましい高品質の分布と実際に観察される混在分布の間の初期の偏りをダンプすることです。私たちは、学習されたデータの偏りと有限な無偏だけのデータの間の転換を分析し、我々のアプローチに厳密な理論的な証明を提供します。",
      "upvotes": 3,
      "discussionId": "68523b4a0164cd1316710562",
      "projectPage": "https://giannisdaras.github.io/publication/ambient_omni",
      "githubRepo": "https://github.com/giannisdaras/ambient-omni",
      "ai_summary": "Ambient Diffusion Omni framework leverages low-quality images to enhance diffusion models by utilizing properties of natural images and shows improvements in ImageNet FID and text-to-image quality.",
      "ai_keywords": [
        "diffusion models",
        "synthetic images",
        "out-of-distribution images",
        "Ambient Diffusion Omni",
        "spectral power law decay",
        "locality",
        "Gaussian blur",
        "JPEG compression",
        "motion blur",
        "ImageNet FID",
        "text-to-image generative modeling",
        "noise dampening",
        "biased data",
        "limited unbiased data"
      ]
    },
    "publishedAt": "2025-06-10T18:37:39.000Z",
    "title": "Ambient Diffusion Omni: Training Good Models with Bad Data",
    "summary": "We show how to use low-quality, synthetic, and out-of-distribution images to\nimprove the quality of a diffusion model. Typically, diffusion models are\ntrained on curated datasets that emerge from highly filtered data pools from\nthe Web and other sources. We show that there is immense value in the\nlower-quality images that are often discarded. We present Ambient Diffusion\nOmni, a simple, principled framework to train diffusion models that can extract\nsignal from all available images during training. Our framework exploits two\nproperties of natural images -- spectral power law decay and locality. We first\nvalidate our framework by successfully training diffusion models with images\nsynthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We\nthen use our framework to achieve state-of-the-art ImageNet FID, and we show\nsignificant improvements in both image quality and diversity for text-to-image\ngenerative modeling. The core insight is that noise dampens the initial skew\nbetween the desired high-quality distribution and the mixed distribution we\nactually observe. We provide rigorous theoretical justification for our\napproach by analyzing the trade-off between learning from biased data versus\nlimited unbiased data across diffusion times.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10038.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f45f44b79c1ba4c353d1035",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f45f44b79c1ba4c353d1035/6piqagYr7RNCy6XwJGWCG.jpeg",
      "fullname": "Giannis Daras",
      "name": "giannisdaras",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05336",
      "authors": [
        {
          "_id": "68425a7ab63271ff41652734",
          "name": "Ghazi Shazan Ahmad",
          "hidden": false
        },
        {
          "_id": "68425a7ab63271ff41652735",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-07T05:48:53.401Z",
          "hidden": false
        },
        {
          "_id": "68425a7ab63271ff41652736",
          "name": "Hanan Gani",
          "hidden": false
        },
        {
          "_id": "68425a7ab63271ff41652737",
          "name": "Abdelrahman Shaker",
          "hidden": false
        },
        {
          "_id": "68425a7ab63271ff41652738",
          "name": "Zhiqiang Shen",
          "hidden": false
        },
        {
          "_id": "68425a7ab63271ff41652739",
          "name": "Ranjay Krishna",
          "hidden": false
        },
        {
          "_id": "68425a7ab63271ff4165273a",
          "name": "Fahad Shahbaz Khan",
          "hidden": false
        },
        {
          "_id": "68425a7ab63271ff4165273b",
          "name": "Salman Khan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/7r2x8C0UYFCrqk95QpBGr.mp4"
      ],
      "publishedAt": "2025-06-05T17:59:29.000Z",
      "submittedOnDailyAt": "2025-06-18T01:30:20.599Z",
      "title": "VideoMolmo: スペクトラル-タイムグラウンディングと指し手",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "時間空間定位は、多様な領域での精密な相互作用に重要です。生物学研究から自動運転へ、相互作用インターフェースへと幅広く範囲が広い。現在の映像ベースのアプローチは、追跡に優れていますが、大規模な言語モデルの複雑な理由論能力を持っていないため、コンテキスト的理解と一般化に制限があります。VideoMolmoという大規模な多様的モデルを紹介します。これは、文脈を基にした細かい時間空間の指し示しを目的として設計されています。Molmoアーキテクチャを基に、VideoMolmoは時間モジュールを採用し、注意機能を利用して、各フレームを前のフレームに条件付け、時間的な一貫性を確保します。また、新しい時間マスク融合プロセスを導入し、SAM2を用いて双方向的な点の伝播を実現し、映像シーケンスの連続性を大幅に向上させます。この2段階分解、LLMを用いて先に正確な指し示し座標を生成し、後に順序的なマスク融合モジュールを依頼して、コンテキストの理解を簡単にし、解釈性を向上させます。適切なデータセットがないため、72kの映像-キャプチャペアを含む詳細なデータセットをカレーレードしました。VideoMolmoの一般化を評価するために、VPoS-Benchという難しい分布外ベンチマークを導入し、5つの実世界的スケーナーを挙げます：セルトラッキング、ビークコンピュータビジョン、自動運転、映像-GUI相互作用、ロボティクス。また、Referring Video Object Segmentation (Refer-VOS)とReasoning VOSタスクにも評価を行います。既存のモデルと比較して、VideoMolmoは時間空間の指し示し精度と理由論能力を大幅に向上させます。コードとモデルは、https://github.com/mbzuai-oryx/VideoMolmo で公開しています。",
      "upvotes": 3,
      "discussionId": "68425a80b63271ff416528f2",
      "projectPage": "https://mbzuai-oryx.github.io/VideoMolmo/",
      "githubRepo": "https://github.com/mbzuai-oryx/VideoMolmo",
      "ai_summary": "VideoMolmo, a multimodal model incorporating a temporal attention mechanism and SAM2 for mask fusion, enhances spatio-temporal pointing accuracy and reasoning capabilities in diverse real-world scenarios.",
      "ai_keywords": [
        "Molmo",
        "attention mechanism",
        "temporal mask fusion",
        "SAM2",
        "bidirectional point propagation",
        "VideoMolmo",
        "LLM",
        "sequential mask-fusion module",
        "VPoS-Bench",
        "Referring Video Object Segmentation",
        "Reasoning VOS"
      ]
    },
    "publishedAt": "2025-06-05T13:59:29.000Z",
    "title": "VideoMolmo: Spatio-Temporal Grounding Meets Pointing",
    "summary": "Spatio-temporal localization is vital for precise interactions across diverse\ndomains, from biological research to autonomous navigation and interactive\ninterfaces. Current video-based approaches, while proficient in tracking, lack\nthe sophisticated reasoning capabilities of large language models, limiting\ntheir contextual understanding and generalization. We introduce VideoMolmo, a\nlarge multimodal model tailored for fine-grained spatio-temporal pointing\nconditioned on textual descriptions. Building upon the Molmo architecture,\nVideoMolmo incorporates a temporal module utilizing an attention mechanism to\ncondition each frame on preceding frames, ensuring temporal consistency.\nAdditionally, our novel temporal mask fusion pipeline employs SAM2 for\nbidirectional point propagation, significantly enhancing coherence across video\nsequences. This two-step decomposition, i.e., first using the LLM to generate\nprecise pointing coordinates, then relying on a sequential mask-fusion module\nto produce coherent segmentation, not only simplifies the task for the language\nmodel but also enhances interpretability. Due to the lack of suitable datasets,\nwe curate a comprehensive dataset comprising 72k video-caption pairs annotated\nwith 100k object points. To evaluate the generalization of VideoMolmo, we\nintroduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five\nreal-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving,\nVideo-GUI Interaction, and Robotics. We also evaluate our model on Referring\nVideo Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to\nexisting models, VideoMolmo substantially improves spatio-temporal pointing\naccuracy and reasoning capability. Our code and models are publicly available\nat https://github.com/mbzuai-oryx/VideoMolmo.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/7r2x8C0UYFCrqk95QpBGr.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05336.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 41
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.14755",
      "authors": [
        {
          "_id": "685225250164cd13167104aa",
          "name": "Zhengxiang Cheng",
          "hidden": false
        },
        {
          "_id": "685225250164cd13167104ab",
          "name": "Dongping Chen",
          "hidden": false
        },
        {
          "_id": "685225250164cd13167104ac",
          "name": "Mingyang Fu",
          "hidden": false
        },
        {
          "_id": "685225250164cd13167104ad",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T17:50:16.000Z",
      "submittedOnDailyAt": "2025-06-18T01:05:46.554Z",
      "title": "最適化長さ圧縮在大規模計算モデル",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "大論理モデル（LRMs）は驚異的な成功を達成していますが、よく必要なさえないや詳しすぎる論理鍵を生成する問題に苦戦します。この問題の核心として「無効な思い」というものを識別しました。モデルは正しい答えを得た後に、その作業を再確認することを好みます。この具体的な不適切さを解決するために、一般的な効果性と効率性の原則を超えて、2つの新しい、細かな原則を提案します：Brevity（簡潔性），これは冗長を除去することを主張し、Sufficiency（十分性），これは重要な論理ステップを保存することを確認します。これらの原則によって、Group Relative Policy Optimization（GRPO）に基づく後学習方法であるLC-R1を紹介します。LC-R1は、全体の簡潔性を確保するLength Rewardと、思考過程の無効な部分を除去するために特に設計されたCompress Rewardの新しい組み合わせを使用しています。複数の論理ベンチマークでの拡大的な実験は、LC-R1は長さ（約50%）を大幅に減少させ、精度のみ少し（約2%）のドロップで、高圧縮を優先したポアロット境界上の有利な転換点を達成することを示します。さらに、LC-R1の強固性を評価し、効率的な計算モデルの開発において有價値なヒントを提供します。コードは、https://github.com/zxiangx/LC-R1にリリースされています。",
      "upvotes": 1,
      "discussionId": "685225250164cd13167104ae",
      "githubRepo": "https://github.com/zxiangx/LC-R1",
      "ai_summary": "LC-R1, a post-training method guided by Brevity and Sufficiency principles, reduces unnecessary reasoning in Large Reasoning Models with minimal accuracy loss.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRM",
        "post-training method",
        "Group Relative Policy Optimization",
        "GRPO",
        "Length Reward",
        "Compress Reward",
        "reasoning benchmarks",
        "Pareto frontier"
      ]
    },
    "publishedAt": "2025-06-17T13:50:16.000Z",
    "title": "Optimizing Length Compression in Large Reasoning Models",
    "summary": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they\noften suffer from producing unnecessary and verbose reasoning chains. We\nidentify a core aspect of this issue as \"invalid thinking\" -- models tend to\nrepeatedly double-check their work after having derived the correct answer. To\naddress this specific inefficiency, we move beyond the general principles of\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\ncritical reasoning steps are preserved. Guided by these principles, we\nintroduce LC-R1, a post-training method based on Group Relative Policy\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\noverall conciseness and a Compress Reward that is specifically designed to\nremove the invalid portion of the thinking process. Extensive experiments on\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\nprioritizes high compression. Our analysis further validates the robustness of\nLC-R1 and provides valuable insights for developing more powerful yet\ncomputationally efficient LRMs. Our code is released at\nhttps://github.com/zxiangx/LC-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14755.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14731",
      "authors": [
        {
          "_id": "685236ac0164cd1316710512",
          "name": "Ring Team",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710513",
          "name": "Bin Hu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710514",
          "name": "Cai Chen",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710515",
          "name": "Deng Zhao",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710516",
          "name": "Ding Liu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710517",
          "name": "Dingnan Jin",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710518",
          "name": "Feng Zhu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710519",
          "name": "Hao Dai",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671051a",
          "name": "Hongzhi Luan",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671051b",
          "name": "Jia Guo",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671051c",
          "name": "Jiaming Liu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671051d",
          "name": "Jiewei Wu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671051e",
          "name": "Jun Mei",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671051f",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710520",
          "name": "Junbo Zhao",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710521",
          "name": "Junwu Xiong",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710522",
          "name": "Kaihong Zhang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710523",
          "name": "Kuan Xu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710524",
          "name": "Lei Liang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710525",
          "name": "Liang Jiang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710526",
          "name": "Liangcheng Fu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710527",
          "name": "Longfei Zheng",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710528",
          "name": "Qiang Gao",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710529",
          "name": "Qing Cui",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671052a",
          "name": "Quan Wan",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671052b",
          "name": "Shaomian Zheng",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671052c",
          "name": "Shuaicheng Li",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671052d",
          "name": "Tongkai Yang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671052e",
          "name": "Wang Ren",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671052f",
          "name": "Xiaodong Yan",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710530",
          "name": "Xiaopei Wan",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710531",
          "name": "Xiaoyun Feng",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710532",
          "name": "Xin Zhao",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710533",
          "name": "Xinxing Yang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710534",
          "name": "Xinyu Kong",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710535",
          "name": "Xuemin Yang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710536",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710537",
          "name": "Yingting Wu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710538",
          "name": "Yongkang Liu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710539",
          "name": "Zhankai Xu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671053a",
          "name": "Zhenduo Zhang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671053b",
          "name": "Zhenglei Zhou",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671053c",
          "name": "Zhenyu Huang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671053d",
          "name": "Zhiqiang Zhang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671053e",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671053f",
          "name": "Zujie Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T17:12:34.000Z",
      "submittedOnDailyAt": "2025-06-18T03:14:38.830Z",
      "title": "リングライト: C3PO で安定化された強化学習によるスケーラブルな理由論理を用いた LLM 向け",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Ring-liteを紹介します。これは、強化学習（RL）によって最適化されたMixture-of-Experts（MoE）ベースの大規模な言語モデルで、効率的かつ強固な理由論の能力を達成することができます。Ling-liteモデルを基盤に、168億パラメータ、27億5000万個アクティブパラメータを持つ公開モデルによって構築されています。このアプローチは、比較的モデルが必要とするパラメータの1/3しか活性化しないように、難しいベンチマーク（例：AIME、LiveCodeBench、GPQA-Diamond）で最先端（SOTA）の小規模の理由論モデルの性能を匹敵します。これを達成するために、MoEのRL学習における未記録の課題を明らかにするために、統合学習プイルオフィンを導入しました。まず、RL学習中の最適化の不穩定さを識別し、Constrained Contextual Computation Policy Optimization（C3PO）を提案しました。これはアルゴリズムとシステムの共創設計手法を用いて、学習の安定性を高め、計算のスロットを向上させる新しいアプローチです。次に、テンプレート損失に基づいた経験的な証拠を示し、それによってRL学習の後続プロセスでの性能-効率の転換を改善することを証明しました。最後に、2段階の学習パラダイムを開発し、多様データの統合を協調し、混合データセットでの学習による領域の衝突を解決しました。モデル、データセット、コードを公開します。",
      "upvotes": 1,
      "discussionId": "685236ad0164cd1316710540",
      "ai_summary": "Ring-lite uses a MoE architecture and reinforcement learning to efficiently match SOTA reasoning models while activating fewer parameters and addressing challenges specific to MoE training.",
      "ai_keywords": [
        "Mixture-of-Experts (MoE)",
        "reinforcement learning (RL)",
        "Ling-lite",
        "AIME",
        "LiveCodeBench",
        "GPQA-Diamond",
        "Constrained Contextual Computation Policy Optimization(C3PO)",
        "entropy loss",
        "two-stage training paradigm"
      ]
    },
    "publishedAt": "2025-06-17T13:12:34.000Z",
    "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs",
    "summary": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14731.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7130
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14702",
      "authors": [
        {
          "_id": "6852331e0164cd13167104fb",
          "name": "Daniel D'souza",
          "hidden": false
        },
        {
          "_id": "6852331e0164cd13167104fc",
          "name": "Julia Kreutzer",
          "hidden": false
        },
        {
          "_id": "6852331e0164cd13167104fd",
          "name": "Adrien Morisot",
          "hidden": false
        },
        {
          "_id": "6852331e0164cd13167104fe",
          "name": "Ahmet Üstün",
          "hidden": false
        },
        {
          "_id": "6852331e0164cd13167104ff",
          "name": "Sara Hooker",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6658011eaba105a066e37e1b/RBQEHm8CJA9fKsg4bupgs.png"
      ],
      "publishedAt": "2025-06-17T16:40:42.000Z",
      "submittedOnDailyAt": "2025-06-18T02:55:26.808Z",
      "title": "テキストの翻訳結果：\n\n「宝塚狩り：学習時のマーカーを用いた長尾の実時時点ターゲット化」",
      "submittedOnDailyBy": {
        "_id": "6658011eaba105a066e37e1b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6658011eaba105a066e37e1b/VPwyTv1bnVMQbVMoMQzcf.jpeg",
        "isPro": false,
        "fullname": "Daniel D'souza",
        "user": "dsouzadaniel",
        "type": "user"
      },
      "summary": "現代機械学習の最も深刻な課題の一つは、稀少で表現不足した特徴の長尾部分での優秀な性能を維持することです。大規模な一般用モデルは複数のタスクによって訓練されていますが、高頻度の場合に最適です。訓練後、訓練コーパスに欠落した特徴の特定のタスクにモデルを適用することが難しいです。特定のテストケースの出力品質を最大化するために、プロンプトエンジニアリングやフィーチャーショットエグゼンスを依存させることは、モデルが小さな変化に高度に敏感、予想外の反応をするか、固定システムプロンプトを依存して性能を維持することによって、挫折しやすいです。本研究では、「訓練プロトコルを最適化し、推論時に欠落したタスクの制御性と性能を両方向上させることができるか？」と質問しています。訓練と推論の技術の間の区別を再評価し、長尾性能を向上させながら、ユーザーにモデルが適切に応答する制御機能を提供することを目指しています。データの特徴とタスクの起源の詳細なタクロロジーを作成し、推論時に生成属性を明記的に制御し、暗黙的に生成を条件付けることを試みています。基盤モデルを微調校して、これらのマーカーを自動的に推論することで、推論時ではそれらは選択肢として存在します。この原則的で柔軟なアプローチは、特に訓練分布の長尾部分からの例では明らかに性能向上を示し、平均で5.7%の優位率の上昇を見ています。マーカーを使用して開放的な生成品質の平均優位率は5.7%の上昇を示し、欠落した領域では9.1%以上の増加を見ています。また、CodeRepairや欠落したタスクに対しては相対的な優位率の上昇が14.1%を超え、長さの指示に従う評価では絶対的な改善が35.3%を超えています。",
      "upvotes": 1,
      "discussionId": "6852331e0164cd1316710500",
      "ai_summary": "A principled approach to fine-tuning models for better performance and controllability on underrepresented use cases is developed through automatic inference of generation attributes.",
      "ai_keywords": [
        "prompt engineering",
        "few-shot examples",
        "controllability",
        "performance",
        "long-tail",
        "training protocols",
        "inference techniques",
        "taxonomy",
        "data characteristics",
        "task provenance",
        "fine-tuning",
        "generation attributes",
        "markers",
        "underrepresented domains",
        "CodeRepair",
        "length instruction following"
      ]
    },
    "publishedAt": "2025-06-17T12:40:42.000Z",
    "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n  Markers",
    "summary": "One of the most profound challenges of modern machine learning is performing\nwell on the long-tail of rare and underrepresented features. Large\ngeneral-purpose models are trained for many tasks, but work best on\nhigh-frequency use cases. After training, it is hard to adapt a model to\nperform well on specific use cases underrepresented in the training corpus.\nRelying on prompt engineering or few-shot examples to maximize the output\nquality on a particular test case can be frustrating, as models can be highly\nsensitive to small changes, react in unpredicted ways or rely on a fixed system\nprompt for maintaining performance. In this work, we ask: \"Can we optimize our\ntraining protocols to both improve controllability and performance on\nunderrepresented use cases at inference time?\" We revisit the divide between\ntraining and inference techniques to improve long-tail performance while\nproviding users with a set of control levers the model is trained to be\nresponsive to. We create a detailed taxonomy of data characteristics and task\nprovenance to explicitly control generation attributes and implicitly condition\ngenerations at inference time. We fine-tune a base model to infer these markers\nautomatically, which makes them optional at inference time. This principled and\nflexible approach yields pronounced improvements in performance, especially on\nexamples from the long tail of the training distribution. While we observe an\naverage lift of 5.7% win rates in open-ended generation quality with our\nmarkers, we see over 9.1% gains in underrepresented domains. We also observe\nrelative lifts of up to 14.1% on underrepresented tasks like CodeRepair and\nabsolute improvements of 35.3% on length instruction following evaluations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6658011eaba105a066e37e1b/RBQEHm8CJA9fKsg4bupgs.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14702.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6658011eaba105a066e37e1b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6658011eaba105a066e37e1b/VPwyTv1bnVMQbVMoMQzcf.jpeg",
      "fullname": "Daniel D'souza",
      "name": "dsouzadaniel",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13901",
      "authors": [
        {
          "_id": "685252860164cd13167105c7",
          "name": "Abhilekh Borah",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105c8",
          "name": "Chhavi Sharma",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105c9",
          "name": "Danush Khanna",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105ca",
          "name": "Utkarsh Bhatt",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105cb",
          "name": "Gurpreet Singh",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105cc",
          "name": "Hasnat Md Abdullah",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105cd",
          "name": "Raghav Kaushik Ravi",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105ce",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105cf",
          "name": "Jyoti Patel",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105d0",
          "name": "Shubham Singh",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105d1",
          "name": "Vasu Sharma",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105d2",
          "name": "Arpita Vats",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105d3",
          "name": "Rahul Raja",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105d4",
          "name": "Aman Chadha",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105d5",
          "name": "Amitava Das",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-16T18:22:28.000Z",
      "submittedOnDailyAt": "2025-06-18T04:17:43.427Z",
      "title": "アラインメントジャンクスキャリングインデックス（AQI）: 拒否よりも遠く: 潜在的な幾何学、クラスターの離散性、レイヤワイズによるパウンド表現でのAQIの固有なアラインメントダイアジャスト",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "アラインメントは、ユーモアになりません、必要となります。大型言語モデル（LLMs）は教育、医療、ジョーニャー、法律など高関係領域に入り、その行動は人間のアラインメントの価値と安全制約に確実に反映しなければなりません。しかし、現在の評価は拒否率、G-Evalスコア、毒性クラスファイザーなどの行動の代理を重く依存していますが、これらは重要な欠点を持っています。アラインメントモデルは、ジャイルブレイキング、生成の乱数性、アラインメントのファクシングに脆弱です。\n\nこの問題を解決するために、私たちはアラインメント品質インデックス（AQI）を紹介します。この新しい幾何学的なプロンプト不変なメトリックは、潜在空間で安全なと不安全なアクティベーションの分離を分析してLLMのアラインメントを実験的に評価します。DBS（Davies-Bouldin Score）、DI（Dunn Index）、XBI（Xie-Beni Index）、CHI（Calinski-Harabasz Index）などの測定を結合して、AQIはクラスタリングの品質を捉え、隠された不適切なアラインメントやジャイルブレイキングリスクを検出できます。AQIは、アラインメントのファクシングの早期警告信号としても役立ち、行動無関係的安全性エラートラインディングの堅固な、解码不変なツールです。\n\nまた、私たちはLITMUSデータセットを提案し、これらの難しい条件での強固な評価を促進します。DPO、GRPO、RLHFの条件で訓練された異なるモデルに対してLITMUSでの実験は、AQIと外部の判断者の関連性と拒否メトリックによって見落とされた脆弱性を明らかにします。私たちの実装は公開的に利用可能にして、この領域の将来の研究を促進します。",
      "upvotes": 1,
      "discussionId": "685252870164cd13167105d6",
      "ai_summary": "A new evaluation metric called Alignment Quality Index (AQI) assesses the alignment of large language models by analyzing latent space activations, capturing clustering quality to detect misalignments and fake alignment, and complementing existing behavioral proxies.",
      "ai_keywords": [
        "Alignment Quality Index (AQI)",
        "latent space",
        "Davies-Bouldin Score (DBS)",
        "Dunn Index (DI)",
        "Xie-Beni Index (XBI)",
        "Calinski-Harabasz Index (CHI)",
        "LITMUS dataset",
        "DPO",
        "GRPO",
        "RLHF",
        "alignment faking",
        "external judges"
      ]
    },
    "publishedAt": "2025-06-16T14:22:28.000Z",
    "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\n  Pooled Representations",
    "summary": "Alignment is no longer a luxury, it is a necessity. As large language models\n(LLMs) enter high-stakes domains like education, healthcare, governance, and\nlaw, their behavior must reliably reflect human-aligned values and safety\nconstraints. Yet current evaluations rely heavily on behavioral proxies such as\nrefusal rates, G-Eval scores, and toxicity classifiers, all of which have\ncritical blind spots. Aligned models are often vulnerable to jailbreaking,\nstochasticity of generation, and alignment faking.\n  To address this issue, we introduce the Alignment Quality Index (AQI). This\nnovel geometric and prompt-invariant metric empirically assesses LLM alignment\nby analyzing the separation of safe and unsafe activations in latent space. By\ncombining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI),\nXie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various\nformulations, AQI captures clustering quality to detect hidden misalignments\nand jailbreak risks, even when outputs appear compliant. AQI also serves as an\nearly warning signal for alignment faking, offering a robust, decoding\ninvariant tool for behavior agnostic safety auditing.\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation\nunder these challenging conditions. Empirical tests on LITMUS across different\nmodels trained under DPO, GRPO, and RLHF conditions demonstrate AQI's\ncorrelation with external judges and ability to reveal vulnerabilities missed\nby refusal metrics. We make our implementation publicly available to foster\nfuture research in this area.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13901.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13599",
      "authors": [
        {
          "_id": "685220810164cd131671048e",
          "name": "Yuwei Du",
          "hidden": false
        },
        {
          "_id": "685220810164cd131671048f",
          "name": "Jie Feng",
          "hidden": false
        },
        {
          "_id": "685220810164cd1316710490",
          "name": "Jian Yuan",
          "hidden": false
        },
        {
          "_id": "685220810164cd1316710491",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/t_3QiPaJ54vNcgeYZKMgy.jpeg"
      ],
      "publishedAt": "2025-06-16T15:24:07.000Z",
      "submittedOnDailyAt": "2025-06-18T01:27:33.225Z",
      "title": "CAMS: 都市GPTをポートフォリエーションする都市人モビリティアガンシストフレームワークのシミュレーション",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "人間移動シミュレーションは、多くの実世界のアプリケーションに重要な役割を果たしています。最近、データ駆動アプローチの制限を解決するために、研究者たちは、大規模な言語モデル（LLMs）の共通知識と推論能力を活用して、人間移動シミュレーションを加速する方法を探しています。しかし、これらの方法は、都市空間の不十分なモデリングと個人移動パターンと集団移動分布との不良な統合を含む、数多くの重要な欠点を見守っています。これらの挑戦に対処するために、私たちは、言語ベースの都市基盤モデルを活用して都市空間での人間移動をシミュレートするアガントフレームワーク「CAMS」を提案します。CAMSは、MobExtractor、GeoGenerator、TrajEnhancerの3つの核心モジュールを構成しており、ユーザープロフィールに基づいてテンプレートモデルの移動パターンを抽出し、新しいパターンを合成し、集団的知識を検討してアンカーポイントを生成し、都市空間の候補キューンスキルを生成し、実際の移動パターンに基づいてDPOを通じてトラジェクトリーを生成します。実世界的データセットに対する実験結果から、CAMSは外部的に提供された地理的エリア情報を依存しないものの、上位の性能を収めています。また、個人移動パターンと集団移動制約を全体的にモデル化することで、CAMSは更に写実的で適切なトラジェクトリーを生成します。一般的に、CAMSは、アガントフレームワークと都市知識を持つLLMsを統合する新しいパラダイムを構築し、人間移動シミュレーションにおいて新たな可能性を開拓します。",
      "upvotes": 1,
      "discussionId": "685220820164cd1316710492",
      "ai_summary": "CAMS integrates an agentic framework with urban-knowledgeable large language models to simulate human mobility more realistically by modeling individual and collective patterns.",
      "ai_keywords": [
        "large language models",
        "CityGPT",
        "agentic framework",
        "human mobility simulation",
        "urban spaces",
        "individual mobility patterns",
        "collective mobility distributions",
        "MobExtractor",
        "GeoGenerator",
        "TrajEnhancer",
        "DPO",
        "trajectory preference alignment",
        "real-world datasets"
      ]
    },
    "publishedAt": "2025-06-16T11:24:07.000Z",
    "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation",
    "summary": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose CityGPT-Powered\nAgentic framework for Mobility Simulation\n(CAMS), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. CAMS\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that CAMS achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, CAMS generates more realistic and\nplausible trajectories. In general, CAMS establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/t_3QiPaJ54vNcgeYZKMgy.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13599.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13387",
      "authors": [
        {
          "_id": "6852239f0164cd13167104a4",
          "name": "Beilei Cui",
          "hidden": false
        },
        {
          "_id": "6852239f0164cd13167104a5",
          "name": "Yiming Huang",
          "hidden": false
        },
        {
          "_id": "6852239f0164cd13167104a6",
          "name": "Long Bai",
          "hidden": false
        },
        {
          "_id": "6852239f0164cd13167104a7",
          "name": "Hongliang Ren",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-16T11:50:00.000Z",
      "submittedOnDailyAt": "2025-06-18T00:57:15.579Z",
      "title": "TR2M: ラングアウトとスケール取向的な比較を用いたモノカメラの相対的深さをメトリック深さに転送",
      "submittedOnDailyBy": {
        "_id": "68518fb45452a74491857c5b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SOYIB8T0LsNZM3ORcHXlr.png",
        "isPro": false,
        "fullname": "Beilei Cui",
        "user": "BeileiCui",
        "type": "user"
      },
      "summary": "この研究では、相対的深さを標準的な深さに変換する一般的なフレームワークを提案します。現在のモノライトデプス推定手法は主に標準的な深さ推定（MMDE）と相対的な深さ推定（MRDE）に分かれています。MMDEは標準的なスケールで深さを推定しますが、特定の領域に限定されています。MRDEは違う領域でも一般化できますが、不明確なスケールが下流のアプリケーションに妨げています。この点に対して、我々はスケールの不確実性を解決し、相対的深さを標準的な深さに変換するフレームワークを構築することを目指しています。以前の方法は言語を入力として使用し、リサイズを行うための2つの因子を推定しました。我々のアプローチでは、TR2Mはテキスト説明と画像を入力として使用し、相対的深さを標準的な深さに変換するためにピクセルレベルで2つのリサイズマップを推定します。2つのモディュールからの特徴量は、交差モディュールアテンションモジュールを用いてよりよくスケール情報を捉えます。コンフィデンスの高いファルシー標準的な深さを構築し、フィルタリングすることで、より詳細なスーパーバイザーを提供するための戦略を設計しました。また、スケールに向けた対比的学習を開発し、深さ分布をガイドとして、スケール分布に合わせた固有の知識を学習させることを強制します。TR2Mは、訓練データセットを多様な領域で訓練するために、少数の学習パラメータを使用します。実験は、見たことのあるデータセットでの卓越した性能を示し、5つの見たことのないデータセットでの超卓なゼロショット能力を明らかにしました。言語の助けを受けて相対的な深さを標準的な深さに変換するピクセルレベルでの巨大な可能性を示します。コードは以下のURLから利用できます：https://github.com/BeileiCui/TR2M",
      "upvotes": 0,
      "discussionId": "6852239f0164cd13167104a8",
      "ai_summary": "A framework, TR2M, uses multimodal inputs to rescale relative depth to metric depth, enhancing performance across various datasets through cross-modality attention and contrastive learning.",
      "ai_keywords": [
        "relative depth estimation",
        "metric depth estimation",
        "cross-modality attention",
        "contrastive learning",
        "rescale maps",
        "pseudo metric depth",
        "intrinsically aligned scale distribution"
      ]
    },
    "publishedAt": "2025-06-16T07:50:00.000Z",
    "title": "TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast",
    "summary": "This work presents a generalizable framework to transfer relative depth to\nmetric depth. Current monocular depth estimation methods are mainly divided\ninto metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs\nestimate depth in metric scale but are often limited to a specific domain.\nMRDEs generalize well across different domains, but with uncertain scales which\nhinders downstream applications. To this end, we aim to build up a framework to\nsolve scale uncertainty and transfer relative depth to metric depth. Previous\nmethods used language as input and estimated two factors for conducting\nrescaling. Our approach, TR2M, utilizes both text description and image as\ninputs and estimates two rescale maps to transfer relative depth to metric\ndepth at pixel level. Features from two modalities are fused with a\ncross-modality attention module to better capture scale information. A strategy\nis designed to construct and filter confident pseudo metric depth for more\ncomprehensive supervision. We also develop scale-oriented contrastive learning\nto utilize depth distribution as guidance to enforce the model learning about\nintrinsic knowledge aligning with the scale distribution. TR2M only exploits a\nsmall number of trainable parameters to train on datasets in various domains\nand experiments not only demonstrate TR2M's great performance in seen datasets\nbut also reveal superior zero-shot capabilities on five unseen datasets. We\nshow the huge potential in pixel-wise transferring relative depth to metric\ndepth with language assistance. (Code is available at:\nhttps://github.com/BeileiCui/TR2M)",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68518fb45452a74491857c5b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SOYIB8T0LsNZM3ORcHXlr.png",
      "fullname": "Beilei Cui",
      "name": "BeileiCui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]