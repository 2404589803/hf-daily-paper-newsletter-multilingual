[
  {
    "paper": {
      "id": "2505.02567",
      "authors": [
        {
          "_id": "681c7895c7211b7efbc49f17",
          "name": "Xinjie Zhang",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f18",
          "name": "Jintao Guo",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f19",
          "user": {
            "_id": "66ab4c8a1703f12f49583c6d",
            "avatarUrl": "/avatars/59c77d4556edc049bb410e180813d5e3.svg",
            "isPro": false,
            "fullname": "zss",
            "user": "Suikong",
            "type": "user"
          },
          "name": "Shanshan Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T10:07:03.107Z",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1a",
          "name": "Minghao Fu",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1b",
          "name": "Lunhao Duan",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1c",
          "user": {
            "_id": "636f4c6b5d2050767e4a1491",
            "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
            "isPro": false,
            "fullname": "Guo-Hua Wang",
            "user": "Flourish",
            "type": "user"
          },
          "name": "Guo-Hua Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:04.757Z",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1d",
          "name": "Qing-Guo Chen",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1e",
          "name": "Zhao Xu",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f1f",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "681c7895c7211b7efbc49f20",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658a8a837959448ef5500ce5/DwtDeTTjSrZoe2r4sL-a4.png"
      ],
      "publishedAt": "2025-05-05T11:18:03.000Z",
      "submittedOnDailyAt": "2025-05-08T07:57:47.854Z",
      "title": "統合多モデル理解と生成モデルの進歩、課題と機会",
      "submittedOnDailyBy": {
        "_id": "658a8a837959448ef5500ce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
        "isPro": false,
        "fullname": "Shiyin Lu",
        "user": "runninglsy",
        "type": "user"
      },
      "summary": "近年、多モデル理解モデルと画像生成モデルの両方に関して、驚異的な進歩が見られています。それぞれの成功に加えて、これらの2つの領域は独立して進化し、異なるアーキテクチャルパラダイムを形成しています：自動回帰ベースのアーキテクチャが多モデル理解において主導的であり、ディフューションベースのモデルが画像生成の基礎となっています。最近、これらの仕事を統合するフレームワークの開発に対する興味が増加しています。GPT-4oの新しい機能の発見はこの潮流を示し、統合の可能性を明らかにしています。しかし、これらの2つの領域のアーキテクチャルの違いは大きな課題をさらっています。現在の努力の一覧を明確にし、将来の研究をガイドするために、これらの努力の一覧を提供します。まず、多モデル理解とテキストから画像生成モデルの基礎的な概念と最近の進歩について紹介します。次に、既存の統合モデルを3つの主要なアーキテクチャルパラダイムに分類し、それぞれのカテゴリーについて構造設計と関連研究による革新を分析します。また、統合モデルに適したデータセットとベンチマークをコンパイルし、将来の検討のためのリソースを提供します。最後に、この新しい分野における主な課題を議論します。トーキナイゼーション戦略、クロスモーダルアテンション、データなどが含まれます。この領域はまだ初期段階であるため、急速な進歩を予測し、この調査を定期的に更新します。私たちの目標は、進める研究を励まし、コミュニティに有効なリソースを提供することです。この調査に関連した参考文献はGitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models)にアクセスできます。",
      "upvotes": 33,
      "discussionId": "681c7896c7211b7efbc49f76",
      "githubRepo": "https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models",
      "ai_keywords": [
        "autoregressive-based architectures",
        "diffusion-based models",
        "unified frameworks",
        "GPT-4o",
        "multimodal understanding",
        "text-to-image generation models",
        "diffusion-based",
        "autoregressive-based",
        "hybrid approaches",
        "cross-modal attention"
      ]
    },
    "publishedAt": "2025-05-05T07:18:03.000Z",
    "title": "Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities",
    "summary": "Recent years have seen remarkable progress in both multimodal understanding\nmodels and image generation models. Despite their respective successes, these\ntwo domains have evolved independently, leading to distinct architectural\nparadigms: While autoregressive-based architectures have dominated multimodal\nunderstanding, diffusion-based models have become the cornerstone of image\ngeneration. Recently, there has been growing interest in developing unified\nframeworks that integrate these tasks. The emergence of GPT-4o's new\ncapabilities exemplifies this trend, highlighting the potential for\nunification. However, the architectural differences between the two domains\npose significant challenges. To provide a clear overview of current efforts\ntoward unification, we present a comprehensive survey aimed at guiding future\nresearch. First, we introduce the foundational concepts and recent advancements\nin multimodal understanding and text-to-image generation models. Next, we\nreview existing unified models, categorizing them into three main architectural\nparadigms: diffusion-based, autoregressive-based, and hybrid approaches that\nfuse autoregressive and diffusion mechanisms. For each category, we analyze the\nstructural designs and innovations introduced by related works. Additionally,\nwe compile datasets and benchmarks tailored for unified models, offering\nresources for future exploration. Finally, we discuss the key challenges facing\nthis nascent field, including tokenization strategy, cross-modal attention, and\ndata. As this area is still in its early stages, we anticipate rapid\nadvancements and will regularly update this survey. Our goal is to inspire\nfurther research and provide a valuable reference for the community. The\nreferences associated with this survey are available on GitHub\n(https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658a8a837959448ef5500ce5/DwtDeTTjSrZoe2r4sL-a4.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02567.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "658a8a837959448ef5500ce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
      "fullname": "Shiyin Lu",
      "name": "runninglsy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04588",
      "authors": [
        {
          "_id": "681c15ab84d0a008fcdb1ee8",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1ee9",
          "name": "Zile Qiao",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eea",
          "user": {
            "_id": "66224557c61c7fbd98099079",
            "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
            "isPro": false,
            "fullname": "GJ",
            "user": "SpaceProduct",
            "type": "user"
          },
          "name": "Jiayan Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:01.834Z",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eeb",
          "name": "Xuanbo Fan",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eec",
          "name": "Yingyan Hou",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eed",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eee",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1eef",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "681c15ab84d0a008fcdb1ef0",
          "name": "Yan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T17:30:22.000Z",
      "submittedOnDailyAt": "2025-05-08T00:54:07.103Z",
      "title": "ZeroSearch: 検索能力を奨励するための検索を不要にする",
      "submittedOnDailyBy": {
        "_id": "66224557c61c7fbd98099079",
        "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
        "isPro": false,
        "fullname": "GJ",
        "user": "SpaceProduct",
        "type": "user"
      },
      "summary": "有效な情報検索は、大規模言語モデル（LLMs）の理由論と生成能力を向上させるために重要です。最近の研究は、実世界環境でのリアルタイムサーチエンジンとの相互作用を利用して、LLMsの検索能力を改善するための強化認知学習（RL）を検討しています。これらのアプローチは有望な結果を示しているが、2つの大きな課題があります：（1）ドキュメントの質の制御ができない：サーチエンジンが返すドキュメントの質は予測できないことが多い、トレーニングプロセスにノイズと不穩定を引き込みます。（2）APIコストの高騰：RLトレーニングは頻繁なロールアウトを必要とし、数百万のサーチリクエストを含む可能性があり、APIの費用が大きくなり、スケーラビリティを厳しく制限します。これらの課題を解決するために、ZeroSearchという強化認知学習フレームワークを紹介します。このアプローチは、実世界のサーチエンジンとの相互作用を避けてLLMsの検索能力を奨励することを目的としています。私たちのアプローチは、軽量監督付きの微調校を始め、LLMを検索モジュールとして機能するように変形させることで始まります。この後、RLトレーニングの際には、カレクリウムベースのロールアウト戦略を用い、生成されるドキュメントの質を進段的に低下させ、モデルの理由論能力を進歩的に引き出すために、モデルにより難しい検索シナリオに暴露させます。広範な実験は、ZeroSearchは3BのLLMを検索モジュールとして、LLMsの検索能力を奨励することを効果的に実現していることを示しています。驚くべきに、7Bの検索モジュールは実際のサーチエンジンとの同等の性能を達成し、14Bの検索モジュールはそれを超えています。また、様々なパラメーターサイズの基盤モデルとインストラクションチューニングモデルの両方でよく拡張でき、広範なRLアルゴリズムとの相容性があります。",
      "upvotes": 23,
      "discussionId": "681c15ac84d0a008fcdb1f21",
      "projectPage": "https://alibaba-nlp.github.io/ZeroSearch/",
      "githubRepo": "https://github.com/Alibaba-nlp/ZeroSearch",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "large language models (LLMs)",
        "search capabilities",
        "live search engines",
        "real-world environments",
        "document quality",
        "noise",
        "instability",
        "training process",
        "API costs",
        "rollouts",
        "search requests",
        "ZeroSearch",
        "lightweight supervised fine-tuning",
        "retrieval module",
        "relevant documents",
        "noisy documents",
        "query",
        "curriculum-based rollout strategy",
        "reasoning ability",
        "retrieval scenarios",
        "base models",
        "instruction-tuned models",
        "parameter sizes",
        "RL algorithms"
      ]
    },
    "publishedAt": "2025-05-07T13:30:22.000Z",
    "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
    "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04588.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66224557c61c7fbd98099079",
      "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg",
      "fullname": "GJ",
      "name": "SpaceProduct",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04512",
      "authors": [
        {
          "_id": "681c546817fc8222efed5318",
          "name": "Teng Hu",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed5319",
          "name": "Zhentao Yu",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531a",
          "name": "Zhengguang Zhou",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531b",
          "name": "Sen Liang",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531c",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531d",
          "name": "Qin Lin",
          "hidden": false
        },
        {
          "_id": "681c546817fc8222efed531e",
          "name": "Qinglin Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T15:33:18.000Z",
      "submittedOnDailyAt": "2025-05-08T05:21:39.978Z",
      "title": "フンユウンカスタム：カスタマイズされたビデオの多様性駆動アーキテクチャ",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "カスタマイズされたビデオ生成は、特定の主題を含むビデオを、ユーザーが柔軟に定義した条件の下で生成することを目指しています。しかし、現在の方法は、アイデンティティの一貫性と入力モデルの制限に悩みます。本論文では、HunyuanCustomという多モデルバースのカスタマイズされたビデオ生成フレームワークを提案します。これは、主題の一貫性を重視しながら、画像、音声、ビデオ、およびテキスト条件をサポートすることを目的としています。HunyuanVideoにビルドされたモデルは、LLaVAに基づくテキスト画像融合モジュールと画像ID強化モジュールを導入し、画像テキスト条件付き生成タスクを解決します。さらに、音声モデルとビデオ条件付き生成を可能にするために、モデルバース条件注入機構を提案します。AudioNetモジュールは空間クロスアテンションを用いて階層的なアライメントを実現し、ビデオ駆動インジェクションモジュールはパチィファイズベースの特徴対応ネットワークを用いて潜在圧縮された条件ビデオを統合します。単一主題と複数主題の場合の実験は、IDの一貫性、リアリティ、テキストビデオのアライメントにおいて、現在の最先端の開放サイスとクローズドサイスの方法を大幅に超えることを示します。また、ダウンストリームタスクの連携性を検証し、音声ビデオ駆動のカスタマイズされたビデオ生成においても強固であることを示します。結果から、多モデル条件付き生成とアイデンティティ保持策の効果性を高めることが、制御可能なビデオ生成の進歩における重要性を示しています。すべてのコードとモデルは、https://hunyuancustom.github.io から利用可能です。",
      "upvotes": 8,
      "discussionId": "681c546e17fc8222efed54ce",
      "ai_keywords": [
        "LLaVA",
        "text-image fusion module",
        "image ID enhancement module",
        "temporal concatenation",
        "modality-specific condition injection mechanisms",
        "AudioNet module",
        "spatial cross-attention",
        "video-driven injection module",
        "latent-compressed conditional video",
        "patchify-based feature-alignment network",
        "ID consistency",
        "text-video alignment",
        "controllable video generation"
      ]
    },
    "publishedAt": "2025-05-07T11:33:18.000Z",
    "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation",
    "summary": "Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04512.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 50
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04622",
      "authors": [
        {
          "_id": "681c03418ff29a163ef5f370",
          "name": "Jingwen Ye",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f371",
          "user": {
            "_id": "64c903957b4d0d947ce86bc6",
            "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
            "isPro": false,
            "fullname": "Yuze He",
            "user": "hyz317",
            "type": "user"
          },
          "name": "Yuze He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:10.350Z",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f372",
          "name": "Yanning Zhou",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f373",
          "name": "Yiqin Zhu",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f374",
          "user": {
            "_id": "6441491c5d600fb0951cd872",
            "avatarUrl": "/avatars/d98892f3b52d87c2328201efa9897110.svg",
            "isPro": false,
            "fullname": "Kaiwen Xiao",
            "user": "loktarxiao",
            "type": "user"
          },
          "name": "Kaiwen Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:12.445Z",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f375",
          "name": "Yong-Jin Liu",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f376",
          "name": "Wei Yang",
          "hidden": false
        },
        {
          "_id": "681c03418ff29a163ef5f377",
          "name": "Xiao Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T17:59:46.000Z",
      "submittedOnDailyAt": "2025-05-08T05:41:14.360Z",
      "title": "プリミティブアニキティ: 人間作りの3次元プリミティブアセンブリ生成における自動回帰タンスフォーマー",
      "submittedOnDailyBy": {
        "_id": "64c903957b4d0d947ce86bc6",
        "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
        "isPro": false,
        "fullname": "Yuze He",
        "user": "hyz317",
        "type": "user"
      },
      "summary": "形状素抽象化は、複雑な3D形状を簡単な幾何要素に分解することで、人間の視覚認知に重要な役割を果たし、コンピュータビジョンとグラフィックに広い応用を持つ。最近の3Dコンテンツ生成の進歩は驚異的な進展を示しているが、現在の素性抽象化手法は、幾何的な最適化に依存し、限定的な語意的理解を持ち、また小規模でクラス別のデータセットから学習し、多様な形状クラス間での一般化を難しく思われる。我々は、PrimitiveAnythingという新しいフレームワークを紹介します。これは、素性抽象化を素性の組み立て生成のタスクとして再定式化し、形状条件付きの素性トランジャーを含む。PrimitiveAnythingは、組み立て生成を自動的に行うための素性トランジャーと、複数の素性の種類を一貫的に表現する無誤差パラメタ化スキームを含む。提案されたフレームワークは、大規模な人間が作った抽象化から素性の組み立ての過程を直接学習し、人間が複雑な形状を素性要素に分解する方法を捉えることができる。拡大的な実験を通じて、PrimitiveAnythingは、多様な形状クラスでもジオメトリーの忠実性を維持する同時に、人間の観察により良い素性の組み立てを生成できることを示し、3Dエンドプロジェクトの多様な応用にバナーを立て、ゲームでの素性ベースのユーザーコンテンツ（UGC）の可能性を示している。プロジェクトページ：https://primitiveanything.github.io",
      "upvotes": 7,
      "discussionId": "681c03468ff29a163ef5f4d7",
      "projectPage": "https://primitiveanything.github.io/",
      "githubRepo": "https://github.com/PrimitiveAnything/PrimitiveAnything",
      "ai_keywords": [
        "shape primitive abstraction",
        "geometric elements",
        "human visual cognition",
        "computer vision",
        "graphics",
        "3D content generation",
        "geometric optimization",
        "semantic understanding",
        "category-specific datasets",
        "primitive assembly generation task",
        "shape-conditioned primitive transformer",
        "auto-regressive generation",
        "ambiguity-free parameterization scheme",
        "human-crafted abstractions",
        "high-quality primitive assemblies",
        "human perception",
        "geometric fidelity",
        "3D applications",
        "user-generated content (UGC)"
      ]
    },
    "publishedAt": "2025-05-07T13:59:46.000Z",
    "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer",
    "summary": "Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04622.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c903957b4d0d947ce86bc6",
      "avatarUrl": "/avatars/61d70a3ba00c83a5950f5c909a1a06f8.svg",
      "fullname": "Yuze He",
      "name": "hyz317",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.04364",
      "authors": [
        {
          "_id": "681c189c791c72783efe5a94",
          "user": {
            "_id": "6205fefd3f1dc8a642d70b10",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
            "isPro": false,
            "fullname": "Kai Ruan",
            "user": "6cf",
            "type": "user"
          },
          "name": "Kai Ruan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:58:58.134Z",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a95",
          "name": "Mowen Huang",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a96",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "681c189c791c72783efe5a97",
          "name": "Hao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T12:32:01.000Z",
      "submittedOnDailyAt": "2025-05-08T01:06:26.256Z",
      "title": "LLMの群集知能のベンチマーク",
      "submittedOnDailyBy": {
        "_id": "6205fefd3f1dc8a642d70b10",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
        "isPro": false,
        "fullname": "Kai Ruan",
        "user": "6cf",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は複雑な理由論の可能性を示していますが、自然の群集の特徴として限られた制限（例えば、限られた局所的知覚とコミュニケーション）の下でのマルチアグエントシステム（MAS）でのエピードム的協調の機能を主に調べていません。現在のベンチマークは、アグエントが不完全な空間時間情報を持って動作した際に発生する分散的協調の特徴的な課題を完全に捉えていません。この隙を埋めるために、SwarmBenchという新しいベンチマークを導入します。SwarmBenchは、分散的アグエントとしてのLLMsの群集知識能力をシステマ的に評価するために設計されています。SwarmBenchは、設定可能な2Dグリッド環境内に5つの基盤的なMAS協調タスクを挙げ、アグエントが主に局所的感覚入力（k×kの視界）と局所的コミュニケーションを依存させるように設計されています。協調の効果性に関する指標を提案し、現れるエピードム的群集ダイナミクスを分析します。ゼロショット設定で数々の先進的なLLMsを評価し、各タスクでは显著な性能の差異が見出され、局所的な情報制約による課題を明らかにします。エピードム的協調は現れますが、これらの分散的シナリオでの不確実性による強固な計画と戦略形成においては制限があります。LLMsの性能を群集的な条件において評価することは、将来の分散的システムでのプロポズションを実現するために重要です。SwarmBenchは、決められた機械的性質を持つ可調整的でスケーラブルな物理システムによって構築され、環境、プロンプト、評価スクリプト、詳細な実験データセットを提供し、LLMベースのMAS協調の再現可能な研究およびEmbodied MASの理論的基盤に貢献します。コードリポジトリは、https://github.com/x66ccff/swarmbenchに公開されています。",
      "upvotes": 7,
      "discussionId": "681c189e791c72783efe5b2d",
      "githubRepo": "https://github.com/x66ccff/swarmbench",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Multi-Agent Systems (MAS)",
        "swarm intelligence",
        "decentralized coordination",
        "spatio-temporal information",
        "SwarmBench",
        "foundational MAS coordination tasks",
        "2D grid environment",
        "local sensory input",
        "local communication",
        "coordination effectiveness",
        "emergent group dynamics",
        "zero-shot setting",
        "robust planning",
        "strategy formation",
        "uncertainty",
        "decentralized scenarios",
        "Embodied MAS"
      ]
    },
    "publishedAt": "2025-05-07T08:32:01.000Z",
    "title": "Benchmarking LLMs' Swarm intelligence",
    "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04364.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6205fefd3f1dc8a642d70b10",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673948194938-6205fefd3f1dc8a642d70b10.jpeg",
      "fullname": "Kai Ruan",
      "name": "6cf",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04528",
      "authors": [
        {
          "_id": "681c5152c7211b7efbba4b73",
          "user": {
            "_id": "641aef7b1911d3be67425338",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641aef7b1911d3be67425338/CmCbWWB6NxkAaus59q31w.jpeg",
            "isPro": false,
            "fullname": "Qi Liu",
            "user": "purewhite42",
            "type": "user"
          },
          "name": "Qi Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:58:55.624Z",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b74",
          "name": "Xinhao Zheng",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b75",
          "name": "Renqiu Xia",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b76",
          "name": "Xingzhi Qi",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b77",
          "name": "Qinxiang Cao",
          "hidden": false
        },
        {
          "_id": "681c5152c7211b7efbba4b78",
          "name": "Junchi Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-07T16:02:14.000Z",
      "submittedOnDailyAt": "2025-05-08T05:14:50.449Z",
      "title": "正式な問題解決のための公式化、フレームワークとベンチマーク",
      "submittedOnDailyBy": {
        "_id": "65b7ae76768464877cdb2e39",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
        "isPro": false,
        "fullname": "Renqiu Xia",
        "user": "renqiux0302",
        "type": "user"
      },
      "summary": "問題解決は、その自明なタスクとして、科学と工学の重要な構成部分であり、これは関係がある。しかし、問題解決の一般的で具体的な定式化は欠落している。AIベースの問題解決アガントの最近の開発に伴い、プロセスレベルの確認可能性の要求は急速に増加し、まだ調査が不足している。これらの欠点を補うために、我々は、確定的なマルコフ決定過程としての問題解決の原則的な定式化を、FPS（Formal Problem-Solving）として提案します。FPSは、現存するFTP（正式定理証明）環境を利用して、プロセス確認された問題解決を行う新しいフレームワークです。また、D-FPS（Deductive FPS）では、解決と答えの確認を分離して、より人間の意向に合わせた解決を行うことを目指しています。これらのフレームワークの表現力、正確性と完全性は証明されています。問題解決に関する3つのベンチマークを構築しました：FormalMath500は、MATH500ベンチマークの一部の正式化です。MiniF2F-SolvingとPutnamBench-Solvingは、FTPベンチマークMiniF2FとPutnamBenchの改良版です。忠実的、解釈可能で、人間の意向に合わせた評価のために、RPE（Restricted Propositional Equivalence）という、形式証明を用いた符号的アプローチを提案します。RPEは、答えの正確性を決定するための正式証明による方法です。4つの一般的なFTPモデルと2つのプロンプティング方法を基準として、FormalMath500の最大23.77%、MiniF2F-Solvingの最大27.47%、PutnamBench-Solvingの最大0.31%を解くことができました。",
      "upvotes": 5,
      "discussionId": "681c5153c7211b7efbba4bb4",
      "githubRepo": "https://github.com/Purewhite2019/formal_problem_solving_main",
      "ai_keywords": [
        "Markov decision process",
        "FPS (Formal Problem-Solving)",
        "FTP (formal theorem proving)",
        "D-FPS (Deductive FPS)",
        "FormalMath500",
        "MiniF2F-Solving",
        "PutnamBench-Solving",
        "RPE (Restricted Propositional Equivalence)"
      ]
    },
    "publishedAt": "2025-05-07T12:02:14.000Z",
    "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving",
    "summary": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04528.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b7ae76768464877cdb2e39",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b7ae76768464877cdb2e39/uAWPo4tpkqbZoDeEkc7y0.jpeg",
      "fullname": "Renqiu Xia",
      "name": "renqiux0302",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03912",
      "authors": [
        {
          "_id": "681c549cb322a2fe864c8b0d",
          "name": "Can Cui",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b0e",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b0f",
          "name": "Wenxuan Song",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b10",
          "name": "Shuanghao Bai",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b11",
          "name": "Xinyang Tong",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b12",
          "name": "Zirui Ge",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b13",
          "name": "Runze Suo",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b14",
          "name": "Wanqi Zhou",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b15",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b16",
          "name": "Bofang Jia",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b17",
          "name": "Han Zhao",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b18",
          "name": "Siteng Huang",
          "hidden": false
        },
        {
          "_id": "681c549cb322a2fe864c8b19",
          "name": "Donglin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T18:35:07.000Z",
      "submittedOnDailyAt": "2025-05-08T05:23:33.004Z",
      "title": "OpenHelix: ショートサービス、実験的解析、オープンソース\n  ロボット操作のダブルシステムVLAモデル",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "ダブルシステムVLA（Vision-Language-Action）アーキテクチャは、具象的知能研究のホットトピックとなりましたが、進歩的な性能分析および最適化に関する十分な開放ソースのワークが不足しています。この問題に対処するため、本論文では、既存のダブルシステムアーキテクチャの構造設計を要約し、その核心設計要素について系統的な実験的評価を行います。最終的には、進歩的な探索に向けた低コスト開放ソースモデルを提供します。当然、このプロジェクトは、様々な実験結果と性能向上の開放ソースモデルを継続的に追加し、みんなが選択肢として利用できるようにします。プロジェクトページ：https://openhelix-robot.github.io/。",
      "upvotes": 3,
      "discussionId": "681c549eb322a2fe864c8b6e"
    },
    "publishedAt": "2025-05-06T14:35:07.000Z",
    "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation",
    "summary": "Dual-system VLA (Vision-Language-Action) architectures have become a hot\ntopic in embodied intelligence research, but there is a lack of sufficient\nopen-source work for further performance analysis and optimization. To address\nthis problem, this paper will summarize and compare the structural designs of\nexisting dual-system architectures, and conduct systematic empirical\nevaluations on the core design elements of existing dual-system architectures.\nUltimately, it will provide a low-cost open-source model for further\nexploration. Of course, this project will continue to update with more\nexperimental conclusions and open-source models with improved performance for\neveryone to choose from. Project page: https://openhelix-robot.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03912.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03418",
      "authors": [
        {
          "_id": "681c4d5b5971460af345032a",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032b",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032c",
          "name": "Junwei Su",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032d",
          "name": "Yuchen Tian",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032e",
          "name": "Yuqi Zhu",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af345032f",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450330",
          "name": "Lanning Wei",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450331",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "681c4d5b5971460af3450332",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T10:53:58.000Z",
      "submittedOnDailyAt": "2025-05-08T04:51:36.213Z",
      "title": "知識加算の複雑な問題解決法を大規模言語モデルによって実現する：\n  調査",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "問題解決は、複数の領域で人類の進歩の基本的な動機として歴史上よく認識されています。人工知能の進歩に伴い、大規模言語モデル（LLMs）が複雑な問題を多様な領域で解決する強力なツールとして登場しました。傳統的な計算システムと異なり、LLMsは剰余計算力と人類の理由論の近似を組み合わせ、解決策を生成し、推論を行い、外部の計算ツールを利用することができます。しかし、実世界的な問題解決にLLMsを適用すると、多段階的な理由論、領域知識の統合、結果の検証など、重大な課題があります。この調査は、複雑な問題解決におけるLLMsの能力と限界を調べ、Chain-of-Thought（CoT）理由論、知識の拡張、LLMsやツールベースの検証手法などの手法を検討しています。また、ソフトウェア開発、数学的な理由論と証明、データ分析とモデリング、科学研究などの領域に特化した課題を明らかにしています。この論文は、現在のLLMsソリューションの基本的な限界と、多段階的な理由論、領域知識の統合、結果の検証の視点からLLMsベースの複雑な問題解決の将来の方向を更に議論しています。",
      "upvotes": 2,
      "discussionId": "681c4d5f5971460af3450465",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Chain-of-Thought (CoT) reasoning",
        "knowledge augmentation",
        "verification techniques",
        "software engineering",
        "mathematical reasoning and proving",
        "data analysis and modeling",
        "scientific research",
        "multi-step reasoning",
        "domain knowledge integration",
        "result verification"
      ]
    },
    "publishedAt": "2025-05-06T06:53:58.000Z",
    "title": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey",
    "summary": "Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03418.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03821",
      "authors": [
        {
          "_id": "681c7a3829ba66a745217db5",
          "user": {
            "_id": "63caf7ce9f78909f9f81eb72",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
            "isPro": true,
            "fullname": "Gracjan Goral",
            "user": "Gracjan",
            "type": "user"
          },
          "name": "Gracjan Góral",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:02.558Z",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db6",
          "name": "Alicja Ziarko",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db7",
          "name": "Piotr Miłoś",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db8",
          "name": "Michał Nauman",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217db9",
          "name": "Maciej Wołczyk",
          "hidden": false
        },
        {
          "_id": "681c7a3829ba66a745217dba",
          "name": "Michał Kosiński",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63caf7ce9f78909f9f81eb72/b8OMQONPXPyBtcitD1Yg7.jpeg"
      ],
      "publishedAt": "2025-05-03T00:10:41.000Z",
      "submittedOnDailyAt": "2025-05-08T08:19:59.040Z",
      "title": "超認識：視覚モデルの視覚的立場理解評価",
      "submittedOnDailyBy": {
        "_id": "63caf7ce9f78909f9f81eb72",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
        "isPro": true,
        "fullname": "Gracjan Goral",
        "user": "Gracjan",
        "type": "user"
      },
      "summary": "ビジョン・ラングラウジングモデル（VLMs）の視覚的なポートレイビリング能力を調査し、既存の人間のテストにもっとも類似した新しいビジョンタスクを使用しています。我々のアプローチは、シングルの人形ミニフィギュアとシングルのオブジェクトを組み合わせたように調節されたシーンを利用しています。オブジェクトの位置と人形ミニフィギュアの向きをシステマティックに変更し、鳥の眼からの視点と表面レベルの視点を両方使用して、144種類の独自のビジョンタスクを作成しました。各ビジョンタスクは、シーン理解、空間理由、ビジョンのポートレイビリングを評価するために設計された7つの診断質問のシリーズと組み合わされています。我々は、GPT-4-Turbo、GPT-4o、Llama-3.2-11B-Vision-Instruct、Claude Sonnetのバージョンのいくつかの最先端のモデルの評価を行い、それらがシーン理解で優れているが、空間理由においてはその性能が显著に低下し、ポートレイビリングにおいてはさらに悪化していることを明らかにしました。我々の分析は、表面レベルのオブジェクト認識と複雑なビジョンタスクに必要な深い空間的およびポートレイビリングの理由の間にあるギャップを示し、将来のVLM開発において明確な幾何学的表現とチューニングプロトコルの統合が必要としています。",
      "upvotes": 2,
      "discussionId": "681c7a3e29ba66a745217f0c"
    },
    "publishedAt": "2025-05-02T20:10:41.000Z",
    "title": "Beyond Recognition: Evaluating Visual Perspective Taking in Vision\n  Language Models",
    "summary": "We investigate the ability of Vision Language Models (VLMs) to perform visual\nperspective taking using a novel set of visual tasks inspired by established\nhuman tests. Our approach leverages carefully controlled scenes, in which a\nsingle humanoid minifigure is paired with a single object. By systematically\nvarying spatial configurations - such as object position relative to the\nhumanoid minifigure and the humanoid minifigure's orientation - and using both\nbird's-eye and surface-level views, we created 144 unique visual tasks. Each\nvisual task is paired with a series of 7 diagnostic questions designed to\nassess three levels of visual cognition: scene understanding, spatial\nreasoning, and visual perspective taking. Our evaluation of several\nstate-of-the-art models, including GPT-4-Turbo, GPT-4o,\nLlama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that\nwhile they excel in scene understanding, the performance declines significantly\non spatial reasoning and further deteriorates on perspective-taking. Our\nanalysis suggests a gap between surface-level object recognition and the deeper\nspatial and perspective reasoning required for complex visual tasks, pointing\nto the need for integrating explicit geometric representations and tailored\ntraining protocols in future VLM development.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63caf7ce9f78909f9f81eb72/b8OMQONPXPyBtcitD1Yg7.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63caf7ce9f78909f9f81eb72",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63caf7ce9f78909f9f81eb72/2mLUr_DdEom28DjodWimv.jpeg",
      "fullname": "Gracjan Goral",
      "name": "Gracjan",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.00358",
      "authors": [
        {
          "_id": "68154d77c8ab88a66b8d81a7",
          "name": "Albert Ge",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81a8",
          "name": "Tzu-Heng Huang",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81a9",
          "name": "John Cooper",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81aa",
          "name": "Avi Trost",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ab",
          "name": "Ziyi Chu",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ac",
          "name": "Satya Sai Srinath Namburi GNVV",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ad",
          "name": "Ziyang Cai",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81ae",
          "name": "Kendall Park",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81af",
          "name": "Nicholas Roberts",
          "hidden": false
        },
        {
          "_id": "68154d77c8ab88a66b8d81b0",
          "name": "Frederic Sala",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T07:08:19.000Z",
      "submittedOnDailyAt": "2025-05-08T05:38:42.650Z",
      "title": "R&B: 領域の再組織化とデータの混雑バランスによる効率的な基盤モデルの訓練",
      "submittedOnDailyBy": {
        "_id": "650263c89a612aa33a018383",
        "avatarUrl": "/avatars/a1415a4696da98ffebc1ac798cbab052.svg",
        "isPro": false,
        "fullname": "Albert Ge",
        "user": "albertge",
        "type": "user"
      },
      "summary": "データ混合システムは、言語モデルの訓練における費用を削減することができました。これらの方法は、2つの欠点を持っています。最初に、データの領域（例：データソース、タスクの種類）が事前に決定されていることを依存し、これは重要な語義的なニュアンスを捉えられない可能性があり、性能の向上につながりません。二つ目に、これらの方法は計算的に難しいように、領域の数に比例してスケールすることにより、計算量が増えてしまいます。これらの挑戦に対処するために、R&Bフレームワークを使用して、語義的な類似性に基づいて訓練データを再分割し、より細かな領域を作成し、訓練全過程で得られる領域勾配によって引き起こされるGram行列を利用して、データの構成を効率的に最適化します。先行研究と異なり、評価情報のような損失や勾配の取得に必要な追加計算を必要としません。標準的な正規性条件の下でこの技術を分析し、R&Bの効果性を非適応混合アプローチに比べて正当化する理論的なコンプライアンスを提供します。実験的には、自然言語、推理、多タイプデータタスクの5つの異なるデータセットでR&Bの効果性を示しました。R&Bは、状態の最先端のデータ混合システムの性能を追い越すまたは匹敵することができるように、0.01%の追加計算オーバーヘッドでも実現できます。",
      "upvotes": 2,
      "discussionId": "68154d78c8ab88a66b8d820c",
      "ai_keywords": [
        "semantic similarity",
        "Gram matrix",
        "domain gradients"
      ]
    },
    "publishedAt": "2025-05-01T03:08:19.000Z",
    "title": "R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training",
    "summary": "Data mixing strategies have successfully reduced the costs involved in\ntraining language models. While promising, such methods suffer from two flaws.\nFirst, they rely on predetermined data domains (e.g., data sources, task\ntypes), which may fail to capture critical semantic nuances, leaving\nperformance on the table. Second, these methods scale with the number of\ndomains in a computationally prohibitive way. We address these challenges via\nR&B, a framework that re-partitions training data based on semantic similarity\n(Regroup) to create finer-grained domains, and efficiently optimizes the data\ncomposition (Balance) by leveraging a Gram matrix induced by domain gradients\nobtained throughout training. Unlike prior works, it removes the need for\nadditional compute to obtain evaluation information such as losses or\ngradients. We analyze this technique under standard regularity conditions and\nprovide theoretical insights that justify R&B's effectiveness compared to\nnon-adaptive mixing approaches. Empirically, we demonstrate the effectiveness\nof R&B on five diverse datasets ranging from natural language to reasoning and\nmultimodal tasks. With as little as 0.01% additional compute overhead, R&B\nmatches or exceeds the performance of state-of-the-art data mixing strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00358.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650263c89a612aa33a018383",
      "avatarUrl": "/avatars/a1415a4696da98ffebc1ac798cbab052.svg",
      "fullname": "Albert Ge",
      "name": "albertge",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03570",
      "authors": [
        {
          "_id": "681b518bf497fd5e45b55eeb",
          "user": {
            "_id": "667ed2bf12e48bee0e972ccc",
            "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
            "isPro": false,
            "fullname": "Mariya Davydova",
            "user": "mariya-davydova",
            "type": "user"
          },
          "name": "Mariya Davydova",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T08:59:24.254Z",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eec",
          "name": "Daniel Jeffries",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eed",
          "name": "Patrick Barker",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eee",
          "name": "Arturo Márquez Flores",
          "hidden": false
        },
        {
          "_id": "681b518bf497fd5e45b55eef",
          "name": "Sinéad Ryan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/fe49DABHt5h8Uw-_GCzaB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/yfxA_jM6W5khwa_hHL_mF.png"
      ],
      "publishedAt": "2025-05-06T14:29:47.000Z",
      "submittedOnDailyAt": "2025-05-08T07:36:52.978Z",
      "title": "OSUniverse: 多モーダルGUIナビゲーションAIアグェントのベンチマーク",
      "submittedOnDailyBy": {
        "_id": "667ed2bf12e48bee0e972ccc",
        "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
        "isPro": false,
        "fullname": "Mariya Davydova",
        "user": "mariya-davydova",
        "type": "user"
      },
      "summary": "この論文では、OSUniverse: 専門的なGUI-navigation AIアグェントに向けた複雑な多様性を持つデスクトップ向けタスクのベンチマークを介して紹介します。このベンチマークは、使いやすさ、拡張性、テストケースの完全な覆縁、自動化バリデーションを焦点としています。タスクは、基本的な精度ピククリングから、多ステップ、多アプリケーションテストまで、アグェントに必要なディケシビリティ、精度、そして明確な思考を求めます。ベンチマークのバージョン1では、ここで紹介しています。テストケースの複雑さを調整し、発表時のSOTAアグェントが50%以上の結果を得ることを防ぎ、平均の白領がこれらのタスクを完全な正確性で行うことを確認します。ベンチマークは手動でスコアされることができますが、平均誤差率が2%以下の自動化バリデーション機構も紹介しています。このベンチマークは、短期および中期の視野での完全自動化された進歩、能力、およびGUI-navigation AIアグェントの効果性を測定するための堅固な基盤を提供します。ベンチマークのソースコードは、https://github.com/agentsea/osuniverse から利用できます。",
      "upvotes": 1,
      "discussionId": "681b518cf497fd5e45b55f0f",
      "projectPage": "https://agentsea.github.io/osuniverse/",
      "githubRepo": "https://github.com/agentsea/osuniverse"
    },
    "publishedAt": "2025-05-06T10:29:47.000Z",
    "title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents",
    "summary": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\nease of use, extensibility, comprehensive coverage of test cases, and automated\nvalidation. We divide the tasks in increasing levels of complexity, from basic\nprecision clicking to multistep, multiapplication tests requiring dexterity,\nprecision, and clear thinking from the agent. In version one of the benchmark,\npresented here, we have calibrated the complexity of the benchmark test cases\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\ndo not achieve results higher than 50%, while the average white collar worker\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\nmanually, but we also introduce an automated validation mechanism that has an\naverage error rate less than 2%. Therefore, this benchmark presents solid\nground for fully automated measuring of progress, capabilities and the\neffectiveness of GUI-navigation AI agents over the short and medium-term\nhorizon. The source code of the benchmark is available at\nhttps://github.com/agentsea/osuniverse.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/fe49DABHt5h8Uw-_GCzaB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/667ed2bf12e48bee0e972ccc/yfxA_jM6W5khwa_hHL_mF.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03570.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667ed2bf12e48bee0e972ccc",
      "avatarUrl": "/avatars/9489dba960a63127397f4ca13e507078.svg",
      "fullname": "Mariya Davydova",
      "name": "mariya-davydova",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02393",
      "authors": [
        {
          "_id": "681c423f198e1dea5c26f2f4",
          "user": {
            "_id": "6445e9bd1cfc9ae6bb40985c",
            "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
            "isPro": false,
            "fullname": "Evan Jeong",
            "user": "Eavn",
            "type": "user"
          },
          "name": "Sungheon Jeong",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-08T09:38:37.142Z",
          "hidden": false
        },
        {
          "_id": "681c423f198e1dea5c26f2f5",
          "user": {
            "_id": "646b57c6e5abcbf6709fabf6",
            "avatarUrl": "/avatars/e9749acf7866eeaf017f0a43351794fc.svg",
            "isPro": false,
            "fullname": "Jihong Park",
            "user": "Paper9795",
            "type": "user"
          },
          "name": "Jihong Park",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-08T05:36:09.797Z",
          "hidden": false
        },
        {
          "_id": "681c423f198e1dea5c26f2f6",
          "name": "Mohsen Imani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:33:20.000Z",
      "submittedOnDailyAt": "2025-05-08T04:12:55.976Z",
      "title": "不確かさを重み付けした画像・イベントの多様的な融合を用いたビデオアノマリー検出",
      "submittedOnDailyBy": {
        "_id": "6445e9bd1cfc9ae6bb40985c",
        "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
        "isPro": false,
        "fullname": "Evan Jeong",
        "user": "Eavn",
        "type": "user"
      },
      "summary": "多くの現在のビデオ異常検出システムは、RGBフレームのみを依存していますが、これは、急激なまたは瞬間的な移動コードを捉えるために必要な時系列解析能力を欠くことにより、異常事象の指標となるものです。この制限を解決するために、私たちは、RGBビデオからのイベント表現の合成と画像特徴量との融合を行う「イメージイベント融合ビデオ異常検出（IEF-VAD）」フレームワークを提案します。このフレームワークは、原則的な、不確実性に関する過程でイメージフレームを直接合成し、画像特徴量と融合します。このシステムは、（i）Student's-t 適性で重尾のセンサーノイズをモデル化し、Laplace近似を用いて値レベルの逆分散重みを得ます；（ii）Kalman風格のフレームごとの更新を適用し、時系列におけるモデルのバランスを調整します；（iii）融合された潜在状態を反復的に精確化し、残りのクロスモードノイズを削除します。IEF-VADは、専門的なイベントセンサーやフレームレベルのラベルを必要としないもので、複数の実世界的な異常検出ベンチマークで最先端の状態を設定します。これらの発見は、RGBフレームで通常表現されないような移動コードを強調するための合成イベント表現の有用性を示し、多様なアプリケーションで正確かつ強固なビデオ理解を実現することを可能にします。コードとモデルは、https://github.com/EavnJeong/IEF-VAD にアクセスできます。",
      "upvotes": 1,
      "discussionId": "681c4243198e1dea5c26f3cd",
      "githubRepo": "https://github.com/EavnJeong/IEF-VAD",
      "ai_keywords": [
        "Image-Event Fusion",
        "Video Anomaly Detection",
        "event representations",
        "Student`s-t likelihood",
        "Laplace approximation",
        "Kalman-style frame-wise updates",
        "fused latent state",
        "cross-modal noise"
      ]
    },
    "publishedAt": "2025-05-05T02:33:20.000Z",
    "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection",
    "summary": "Most existing video anomaly detectors rely solely on RGB frames, which lack\nthe temporal resolution needed to capture abrupt or transient motion cues, key\nindicators of anomalous events. To address this limitation, we propose\nImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that\nsynthesizes event representations directly from RGB videos and fuses them with\nimage features through a principled, uncertainty-aware process. The system (i)\nmodels heavy-tailed sensor noise with a Student`s-t likelihood, deriving\nvalue-level inverse-variance weights via a Laplace approximation; (ii) applies\nKalman-style frame-wise updates to balance modalities over time; and (iii)\niteratively refines the fused latent state to erase residual cross-modal noise.\nWithout any dedicated event sensor or frame-level labels, IEF-VAD sets a new\nstate of the art across multiple real-world anomaly detection benchmarks. These\nfindings highlight the utility of synthetic event representations in\nemphasizing motion cues that are often underrepresented in RGB frames, enabling\naccurate and robust video understanding across diverse applications without\nrequiring dedicated event sensors. Code and models are available at\nhttps://github.com/EavnJeong/IEF-VAD.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02393.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6445e9bd1cfc9ae6bb40985c",
      "avatarUrl": "/avatars/33f48c1c820e592cbf4ea4af479dd378.svg",
      "fullname": "Evan Jeong",
      "name": "Eavn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]