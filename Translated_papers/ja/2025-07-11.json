[
  {
    "paper": {
      "id": "2507.07966",
      "authors": [
        {
          "_id": "68706bdcc8391850d60977eb",
          "name": "Yukang Chen",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977ec",
          "name": "Wei Huang",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977ed",
          "name": "Baifeng Shi",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977ee",
          "name": "Qinghao Hu",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977ef",
          "name": "Hanrong Ye",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f0",
          "name": "Ligeng Zhu",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f1",
          "name": "Zhijian Liu",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f2",
          "name": "Pavlo Molchanov",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f3",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f4",
          "name": "Xiaojuan Qi",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f5",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f6",
          "name": "Hongxu Yin",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f7",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "68706bdcc8391850d60977f8",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62919485a29097b211bc7b83/Tu__SuWZeWyCBPoK8YUyh.mp4"
      ],
      "publishedAt": "2025-07-10T17:47:40.000Z",
      "submittedOnDailyAt": "2025-07-11T00:13:53.988Z",
      "title": "スケーリングRLロングビデオに",
      "submittedOnDailyBy": {
        "_id": "62919485a29097b211bc7b83",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653710384819-62919485a29097b211bc7b83.png",
        "isPro": false,
        "fullname": "YukangChen",
        "user": "Yukang",
        "type": "user"
      },
      "summary": "We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 52K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves strong performance on long video QA benchmarks such as VideoMME. It also outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent performance gains as the number of input video frames scales. LongVILA-R1 marks a firm step towards long video reasoning in VLMs. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames / around 256k tokens).",
      "upvotes": 69,
      "discussionId": "68706bdcc8391850d60977f9",
      "projectPage": "https://github.com/NVlabs/Long-RL",
      "githubRepo": "https://github.com/NVlabs/Long-RL",
      "ai_summary": "A framework for scaling vision-language models to long videos using reinforcement learning, achieving strong performance on various reasoning tasks with a specialized training infrastructure.",
      "ai_keywords": [
        "vision-language models",
        "reinforcement learning",
        "chain-of-thought supervised fine-tuning",
        "CoT-SFT",
        "Multi-modal Reinforcement Sequence Parallelism",
        "MR-SP",
        "sequence parallelism",
        "vLLM",
        "long video QA",
        "VideoMME",
        "LongVideo-Reason-eval",
        "temporal reasoning",
        "goal and purpose reasoning",
        "spatial reasoning",
        "plot reasoning",
        "RL training",
        "image and video generation models"
      ],
      "githubStars": 216
    },
    "publishedAt": "2025-07-10T13:47:40.000Z",
    "title": "Scaling RL to Long Videos",
    "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62919485a29097b211bc7b83/Tu__SuWZeWyCBPoK8YUyh.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07966.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62919485a29097b211bc7b83",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653710384819-62919485a29097b211bc7b83.png",
      "fullname": "YukangChen",
      "name": "Yukang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 61
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05964",
      "authors": [
        {
          "_id": "6870b8b5c8391850d60978e0",
          "name": "Vera Soboleva",
          "hidden": false
        },
        {
          "_id": "6870b8b5c8391850d60978e1",
          "user": {
            "_id": "66680c6451545a8b46c6fd21",
            "avatarUrl": "/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg",
            "isPro": false,
            "fullname": "Aibek Alanov",
            "user": "ai-alanov",
            "type": "user"
          },
          "name": "Aibek Alanov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T07:59:56.813Z",
          "hidden": false
        },
        {
          "_id": "6870b8b5c8391850d60978e2",
          "name": "Andrey Kuznetsov",
          "hidden": false
        },
        {
          "_id": "6870b8b5c8391850d60978e3",
          "name": "Konstantin Sobolev",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T13:14:10.000Z",
      "submittedOnDailyAt": "2025-07-11T05:42:42.057Z",
      "title": "T-LoRA: 1枚の画像のディフフェーションモデルのカスタマイズメントによる過学習を防ぐ方法",
      "submittedOnDailyBy": {
        "_id": "66680c6451545a8b46c6fd21",
        "avatarUrl": "/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg",
        "isPro": false,
        "fullname": "Aibek Alanov",
        "user": "ai-alanov",
        "type": "user"
      },
      "summary": "While diffusion model fine-tuning offers a powerful approach for customizing pre-trained models to generate specific objects, it frequently suffers from overfitting when training samples are limited, compromising both generalization capability and output diversity. This paper tackles the challenging yet most impactful task of adapting a diffusion model using just a single concept image, as single-image customization holds the greatest practical potential. We introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework specifically designed for diffusion model personalization. In our work, we show that higher diffusion timesteps are more prone to overfitting than lower ones, necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates two key innovations: (1) a dynamic fine-tuning strategy that adjusts rank-constrained updates based on diffusion timesteps, and (2) a weight parametrization technique that ensures independence between adapter components through orthogonal initialization. Extensive experiments show that T-LoRA and its individual components outperform standard LoRA and other diffusion model personalization techniques. They achieve a superior balance between concept fidelity and text alignment, highlighting the potential of T-LoRA in data-limited and resource-constrained scenarios. Code is available at https://github.com/ControlGenAI/T-LoRA.",
      "upvotes": 53,
      "discussionId": "6870b8b5c8391850d60978e4",
      "githubRepo": "https://github.com/ControlGenAI/T-LoRA",
      "ai_summary": "T-LoRA, a timestep-dependent low-rank adaptation framework, enhances diffusion model personalization with a dynamic fine-tuning strategy and orthogonal initialization, achieving better concept fidelity and text alignment in data-limited settings.",
      "ai_keywords": [
        "diffusion model fine-tuning",
        "overfitting",
        "generalization capability",
        "output diversity",
        "single-image customization",
        "T-LoRA",
        "timestep-dependent low-rank adaptation",
        "dynamic fine-tuning strategy",
        "rank-constrained updates",
        "diffusion timesteps",
        "weight parametrization",
        "orthogonal initialization",
        "concept fidelity",
        "text alignment",
        "data-limited scenarios"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-07-08T09:14:10.000Z",
    "title": "T-LoRA: Single Image Diffusion Model Customization Without Overfitting",
    "summary": "While diffusion model fine-tuning offers a powerful approach for customizing\npre-trained models to generate specific objects, it frequently suffers from\noverfitting when training samples are limited, compromising both generalization\ncapability and output diversity. This paper tackles the challenging yet most\nimpactful task of adapting a diffusion model using just a single concept image,\nas single-image customization holds the greatest practical potential. We\nintroduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework\nspecifically designed for diffusion model personalization. In our work we show\nthat higher diffusion timesteps are more prone to overfitting than lower ones,\nnecessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates\ntwo key innovations: (1) a dynamic fine-tuning strategy that adjusts\nrank-constrained updates based on diffusion timesteps, and (2) a weight\nparametrization technique that ensures independence between adapter components\nthrough orthogonal initialization. Extensive experiments show that T-LoRA and\nits individual components outperform standard LoRA and other diffusion model\npersonalization techniques. They achieve a superior balance between concept\nfidelity and text alignment, highlighting the potential of T-LoRA in\ndata-limited and resource-constrained scenarios. Code is available at\nhttps://github.com/ControlGenAI/T-LoRA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05964.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66680c6451545a8b46c6fd21",
      "avatarUrl": "/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg",
      "fullname": "Aibek Alanov",
      "name": "ai-alanov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07999",
      "authors": [
        {
          "_id": "68706dcdc8391850d60977fb",
          "user": {
            "_id": "6499809cf19fc795e7724e43",
            "avatarUrl": "/avatars/4b3adce8c85e2f3ef05318ded6c89c3e.svg",
            "isPro": false,
            "fullname": "HaochenWang",
            "user": "HaochenWang",
            "type": "user"
          },
          "name": "Haochen Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:30.986Z",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d60977fc",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:28.856Z",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d60977fd",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d60977fe",
          "name": "Anran Wang",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d60977ff",
          "user": {
            "_id": "64d201b1c2bd235422fb1d14",
            "avatarUrl": "/avatars/e50581aa66391cedae94e116e759b9ec.svg",
            "isPro": false,
            "fullname": "wang",
            "user": "stormthunder",
            "type": "user"
          },
          "name": "Jiacong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:26.581Z",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d6097800",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d6097801",
          "user": {
            "_id": "64531f631a57e1179c203e6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64531f631a57e1179c203e6b/C_J7pXFLqoJoHYPPhK3J9.jpeg",
            "isPro": false,
            "fullname": "zjn",
            "user": "garlicisnotmyfavor",
            "type": "user"
          },
          "name": "Jiani Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:33.749Z",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d6097802",
          "name": "Sule Bai",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d6097803",
          "name": "Zijian Kang",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d6097804",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d6097805",
          "name": "Zhuochen Wang",
          "hidden": false
        },
        {
          "_id": "68706dcdc8391850d6097806",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T17:59:58.000Z",
      "submittedOnDailyAt": "2025-07-11T00:29:18.339Z",
      "title": "跡のある証拠を強化した可視化ガラフライング推論: 評価と方法学",
      "submittedOnDailyBy": {
        "_id": "6499809cf19fc795e7724e43",
        "avatarUrl": "/avatars/4b3adce8c85e2f3ef05318ded6c89c3e.svg",
        "isPro": false,
        "fullname": "HaochenWang",
        "user": "HaochenWang",
        "type": "user"
      },
      "summary": "モデルがOpenAI-o3のように、視覚領域を動的に参照して視覚に基づく理由論を先駆するようなものを創出しています。しかし、これらの能力を全体的に評価するベンチマークは存在しません。この隙を埋めるために、私たちはTraceable Evidence Evaluation Benchmarkという名前のTreeBenchを提案します。これは3つの原則に基づいて構築されています: (1) 複雑なスケーンでの視覚的な観察、(2) バウンディングボックス評価による証拠の跡の証明、(3) 2次元の理由論を用いて物体の相互作用と空間の階層を検証することです。複雑な物体が密集に配置された画像を優先して、SA-1Bから1K枚の高品質の画像を最初にサンプリングし、8人のLMMエクスパートを通じて各画像に対する質問、候補の選択肢、答えを手動で注釈します。3つのステージの質量管理を通じて、TreeBenchは405個の難しい視覚的な質問回答ペアを構成し、最先端のモデルもこのベンチマークに難しいです。そのうち、OpenAI-o3のスコアは54.87であり、そのうちのどれも60%の精度を達成しません。また、私たちはTreeVGR（証拠の跡を強化学習と統合した視覚的な理由論）を導入します。これは、記述的な理由論のパスウェイを可能にし、正確な記述と理由論のパスウェイを可能にします。Qwen2.5-VL-7Bから初期化され、V* Bench (+16.8)、MME-RealWorld (+12.6)、TreeBench (+13.4)を改善し、証拠の跡が視覚的な理由論を進めるキーとなることを示します。コードはhttps://github.com/Haochen-Wang409/TreeVGRにアクセスできます。",
      "upvotes": 32,
      "discussionId": "68706dcec8391850d6097807",
      "projectPage": "https://github.com/Haochen-Wang409/TreeVGR",
      "githubRepo": "https://github.com/Haochen-Wang409/TreeVGR",
      "ai_summary": "TreeBench evaluates visual grounded reasoning through subtle target detection, traceable evidence, and second-order reasoning, while TreeVGR enhances this with joint localization and reasoning using reinforcement learning.",
      "ai_keywords": [
        "visual grounded reasoning",
        "bounding box evaluation",
        "second-order reasoning",
        "TreeBench",
        "TreeVGR",
        "reinforcement learning",
        "localization",
        "reasoning pathways",
        "V* Bench",
        "MME-RealWorld"
      ],
      "githubStars": 19
    },
    "publishedAt": "2025-07-10T13:59:58.000Z",
    "title": "Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and\n  Methodology",
    "summary": "Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically\nreferencing visual regions, just like human \"thinking with images\". However, no\nbenchmark exists to evaluate these capabilities holistically. To bridge this\ngap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a\ndiagnostic benchmark built on three principles: (1) focused visual perception\nof subtle targets in complex scenes, (2) traceable evidence via bounding box\nevaluation, and (3) second-order reasoning to test object interactions and\nspatial hierarchies beyond simple object localization. Prioritizing images with\ndense objects, we initially sample 1K high-quality images from SA-1B, and\nincorporate eight LMM experts to manually annotate questions, candidate\noptions, and answers for each image. After three stages of quality control,\nTreeBench consists of 405 challenging visual question-answering pairs, even the\nmost advanced models struggle with this benchmark, where none of them reach 60%\naccuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR\n(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to\nsupervise localization and reasoning jointly with reinforcement learning,\nenabling accurate localizations and explainable reasoning pathways. Initialized\nfrom Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and\nTreeBench (+13.4), proving traceability is key to advancing vision-grounded\nreasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07999.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6499809cf19fc795e7724e43",
      "avatarUrl": "/avatars/4b3adce8c85e2f3ef05318ded6c89c3e.svg",
      "fullname": "HaochenWang",
      "name": "HaochenWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07984",
      "authors": [
        {
          "_id": "687088a6c8391850d6097874",
          "name": "JingLi Lin",
          "hidden": false
        },
        {
          "_id": "687088a6c8391850d6097875",
          "name": "Chenming Zhu",
          "hidden": false
        },
        {
          "_id": "687088a6c8391850d6097876",
          "name": "Runsen Xu",
          "hidden": false
        },
        {
          "_id": "687088a6c8391850d6097877",
          "name": "Xiaohan Mao",
          "hidden": false
        },
        {
          "_id": "687088a6c8391850d6097878",
          "name": "Xihui Liu",
          "hidden": false
        },
        {
          "_id": "687088a6c8391850d6097879",
          "name": "Tai Wang",
          "hidden": false
        },
        {
          "_id": "687088a6c8391850d609787a",
          "name": "Jiangmiao Pang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6433aba4546e16f17a0f19f6/pNjo9cepo_BTSUgc_ZLsg.mp4"
      ],
      "publishedAt": "2025-07-10T17:56:07.000Z",
      "submittedOnDailyAt": "2025-07-11T04:12:08.963Z",
      "title": "OST-Bench: オンラインスペクトラル・タイムスペース場所理解におけるMLLMの能力評価",
      "submittedOnDailyBy": {
        "_id": "6433aba4546e16f17a0f19f6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433aba4546e16f17a0f19f6/3hlNy4_Suy0bO__Qa9amL.jpeg",
        "isPro": false,
        "fullname": "Chenming Zhu",
        "user": "ChaimZhu",
        "type": "user"
      },
      "summary": "最近の多モデル大語言モデル（MLLM）の進展は、複雑な理由における視覚と言語の統合において驚異的な能力を示しています。既存のベンチマークでは、固定の予め録画された入力のオフライン設定でモデルを評価していますが、我々は、アガントが場面を活動的に探索しているプールの視覚と時系列的な理解を評価するためのベンチマークを紹介します。オンラインの面では、増分して得られる観測を処理し、理由を行う必要があることを強調し、時系列的な成分では、現在の視覚入力と歴史のメモリを統合して動的な空間的な理由を支える必要があります。OST-Benchは、実世界的な具象的な認識の課題をより正確に反映しています。効率的なデータの収集プイプラインに基づいて構築され、OST-Benchは、ScanNet、Matterport3D、ARKitScenesから1.4kの場面と10kの質問・答えペアを収集しています。我々は、数々の先進的なMLLMをOST-Benchで評価し、複雑な時系列的な理由に必要なタスクでは、それらのモデルが不足していることを見出しました。オンライン設定では、探索の境界が拡大し、メモリが増大するにつれて、精度が低下します。さらなる実験的な分析を通じて、モデル間で共有されるエラーパターンを特定し、複雑なシークルベースの空間的な理由の要求と長期的なメモリの取り出し要求がモデルの性能を大幅に低下させ、2つの異なる軸に沿って、改善のために必要な核心的な課題を明らかにしました。フィードバックのための進める研究と開発のために、我々のコード、データセット、ベンチマークは利用可能です。我々のプロジェクトページは、https://rbler1234.github.io/OSTBench.github.io/ です。",
      "upvotes": 24,
      "discussionId": "687088a6c8391850d609787b",
      "projectPage": "https://rbler1234.github.io/OSTBench.github.io/",
      "githubRepo": "https://github.com/OpenRobotLab/OST-Bench",
      "ai_summary": "OST-Bench evaluates multimodal large language models in online spatio-temporal reasoning tasks, revealing challenges in handling complex spatial cues and long-term memory in real-world scenarios.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "OST-Bench",
        "Online Spatio-Temporal understanding",
        "ScanNet",
        "Matterport3D",
        "ARKitScenes",
        "complex spatio-temporal reasoning",
        "long-term memory retrieval"
      ],
      "githubStars": 36
    },
    "publishedAt": "2025-07-10T13:56:07.000Z",
    "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online\n  Spatio-temporal Scene Understanding",
    "summary": "Recent advances in multimodal large language models (MLLMs) have shown\nremarkable capabilities in integrating vision and language for complex\nreasoning. While most existing benchmarks evaluate models under offline\nsettings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\nbenchmark designed to evaluate Online Spatio-Temporal understanding from the\nperspective of an agent actively exploring a scene. The Online aspect\nemphasizes the need to process and reason over incrementally acquired\nobservations, while the Spatio-Temporal component requires integrating current\nvisual inputs with historical memory to support dynamic spatial reasoning.\nOST-Bench better reflects the challenges of real-world embodied perception.\nBuilt on an efficient data collection pipeline, OST-Bench consists of 1.4k\nscenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\nARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\nthey fall short on tasks requiring complex spatio-temporal reasoning. Under the\nonline setting, their accuracy declines as the exploration horizon extends and\nthe memory grows. Through further experimental analysis, we identify common\nerror patterns across models and find that both complex clue-based spatial\nreasoning demands and long-term memory retrieval requirements significantly\ndrop model performance along two separate axes, highlighting the core\nchallenges that must be addressed to improve online embodied reasoning. To\nfoster further research and development in the field, our codes, dataset, and\nbenchmark are available. Our project page is:\nhttps://rbler1234.github.io/OSTBench.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6433aba4546e16f17a0f19f6/pNjo9cepo_BTSUgc_ZLsg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6433aba4546e16f17a0f19f6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433aba4546e16f17a0f19f6/3hlNy4_Suy0bO__Qa9amL.jpeg",
      "fullname": "Chenming Zhu",
      "name": "ChaimZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07990",
      "authors": [
        {
          "_id": "68708156c8391850d6097869",
          "user": {
            "_id": "6513030fb3a463e17df56edd",
            "avatarUrl": "/avatars/867bd4316b2de758654ad3a84ea868c1.svg",
            "isPro": false,
            "fullname": "Hyun, Jeongseok",
            "user": "js-hyun",
            "type": "user"
          },
          "name": "Jeongseok Hyun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:01.307Z",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d609786a",
          "name": "Sukjun Hwang",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d609786b",
          "name": "Su Ho Han",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d609786c",
          "name": "Taeoh Kim",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d609786d",
          "name": "Inwoong Lee",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d609786e",
          "name": "Dongyoon Wee",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d609786f",
          "name": "Joon-Young Lee",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d6097870",
          "name": "Seon Joo Kim",
          "hidden": false
        },
        {
          "_id": "68708156c8391850d6097871",
          "name": "Minho Shim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T17:59:02.000Z",
      "submittedOnDailyAt": "2025-07-11T05:04:15.008Z",
      "title": "多粒度空时标记合并用于无需训练的视频LLM加速",
      "submittedOnDailyBy": {
        "_id": "6513030fb3a463e17df56edd",
        "avatarUrl": "/avatars/867bd4316b2de758654ad3a84ea868c1.svg",
        "isPro": false,
        "fullname": "Hyun, Jeongseok",
        "user": "js-hyun",
        "type": "user"
      },
      "summary": "Video large language models (LLMs) achieve strong video understanding by leveraging a large number of spatio-temporal tokens, but suffer from quadratic computational scaling with token count. To address this, we propose a training-free spatio-temporal token merging method, named STTM. Our key insight is to exploit local spatial and temporal redundancy in video data which has been overlooked in prior work. STTM first transforms each frame into multi-granular spatial tokens using a coarse-to-fine search over a quadtree structure, then performs directed pairwise merging across the temporal dimension. This decomposed merging approach outperforms existing token reduction methods across six video QA benchmarks. Notably, STTM achieves a 2times speed-up with only a 0.5% accuracy drop under a 50% token budget, and a 3times speed-up with just a 2% drop under a 30% budget. Moreover, STTM is query-agnostic, allowing KV cache reuse across different questions for the same video. The project page is available at https://www.jshyun.me/projects/sttm.",
      "upvotes": 19,
      "discussionId": "68708157c8391850d6097872",
      "projectPage": "https://www.jshyun.me/projects/sttm",
      "githubRepo": "https://github.com/HYUNJS/STTM",
      "ai_summary": "A spatio-temporal token merging method improves video LLM efficiency by exploiting redundancy, achieving significant speed-ups with minimal accuracy loss.",
      "ai_keywords": [
        "spatio-temporal tokens",
        "quadratic computational scaling",
        "token merging method",
        "STTM",
        "multi-granular spatial tokens",
        "quadtree structure",
        "directed pairwise merging",
        "video QA benchmarks",
        "token budget",
        "query-agnostic",
        "KV cache reuse"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-07-10T13:59:02.000Z",
    "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
    "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2times speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3times speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07990.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6513030fb3a463e17df56edd",
      "avatarUrl": "/avatars/867bd4316b2de758654ad3a84ea868c1.svg",
      "fullname": "Hyun, Jeongseok",
      "name": "js-hyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07998",
      "authors": [
        {
          "_id": "687068dec8391850d60977e2",
          "user": {
            "_id": "62c66504031996c36c86976a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
            "isPro": true,
            "fullname": "steve z",
            "user": "stzhao",
            "type": "user"
          },
          "name": "Shitian Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:02:29.862Z",
          "hidden": false
        },
        {
          "_id": "687068dec8391850d60977e3",
          "user": {
            "_id": "67ff7f687351095d4b606b84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67ff7f687351095d4b606b84/KhNPmbBC3zghuP5h1MK-c.png",
            "isPro": false,
            "fullname": "Haoquan Zhang",
            "user": "haoquan03",
            "type": "user"
          },
          "name": "Haoquan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:02:26.572Z",
          "hidden": false
        },
        {
          "_id": "687068dec8391850d60977e4",
          "name": "Shaoheng Lin",
          "hidden": false
        },
        {
          "_id": "687068dec8391850d60977e5",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "687068dec8391850d60977e6",
          "name": "Qilong Wu",
          "hidden": false
        },
        {
          "_id": "687068dec8391850d60977e7",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "687068dec8391850d60977e8",
          "name": "Chen Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T17:59:55.000Z",
      "submittedOnDailyAt": "2025-07-11T00:33:22.135Z",
      "title": "PyVision: 動的ツールを持つアガント的な視覚",
      "submittedOnDailyBy": {
        "_id": "62c66504031996c36c86976a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
        "isPro": true,
        "fullname": "steve z",
        "user": "stzhao",
        "type": "user"
      },
      "summary": "LLMsは、計画、理由、外部ツールの動的な呼び出しを可能にするアガントとして増加しています。しかし、可視的理由では、先行のアプローチは主に特定のワークフローと静的なツールセットに制限されています。本報告書では、PyVisionというインタラクティブな、多段階フレームワークを提案します。これは、MLLMが手順に合わせたPythonベースのツールを自動的に生成、実行、改善することを可能にし、柔軟かつ解釈可能な問題解決を解放します。PyVisionが作成したツールのタクロロジーを開発し、多様なベンチマークでの使用を分析します。定量的には、PyVisionは穩定的な性能向上を達成し、V*でGPT-4.1を+7.8%向上させ、VLMsAreBlind-miniでClaude-4.0-Sonnetを+31.1%向上させます。これらの結果は、広い変革を示しています：動的なツール設計はモデルがツールを使うだけではなく、ツールを発明することを可能にし、よりアガント的な可視的理由へと進むことを示しています。",
      "upvotes": 17,
      "discussionId": "687068dec8391850d60977e9",
      "projectPage": "https://agent-x.space/pyvision/",
      "githubRepo": "https://github.com/agents-x-project/PyVision",
      "ai_summary": "PyVision, an interactive framework, enables LLMs to autonomously create and refine Python-based tools for visual reasoning, achieving significant performance improvements across benchmarks.",
      "ai_keywords": [
        "LLMs",
        "agents",
        "visual reasoning",
        "predefined workflows",
        "static toolsets",
        "interactive framework",
        "multi-turn framework",
        "autonomously generate",
        "execute",
        "refine",
        "Python-based tools",
        "taxonomy",
        "benchmarks",
        "GPT-4.1",
        "Claude-4.0-Sonnet",
        "V*",
        "VLMsAreBlind-mini",
        "dynamic tooling",
        "agentic visual reasoning"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-07-10T13:59:55.000Z",
    "title": "PyVision: Agentic Vision with Dynamic Tooling",
    "summary": "LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07998.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c66504031996c36c86976a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
      "fullname": "steve z",
      "name": "stzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07982",
      "authors": [
        {
          "_id": "68707ef2c8391850d6097860",
          "user": {
            "_id": "6590f7880c993129053a2344",
            "avatarUrl": "/avatars/d08049493234edb8e23f1c1531e386d3.svg",
            "isPro": false,
            "fullname": "Haoyu wu",
            "user": "Haoyuwu",
            "type": "user"
          },
          "name": "Haoyu Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:05.153Z",
          "hidden": false
        },
        {
          "_id": "68707ef2c8391850d6097861",
          "name": "Diankun Wu",
          "hidden": false
        },
        {
          "_id": "68707ef2c8391850d6097862",
          "user": {
            "_id": "619b7b1cab4c7b7f16a7d59e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619b7b1cab4c7b7f16a7d59e/6TvXaAqBghAMYO1-j5l4v.jpeg",
            "isPro": false,
            "fullname": "Tianyu He",
            "user": "deeptimhe",
            "type": "user"
          },
          "name": "Tianyu He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:03.023Z",
          "hidden": false
        },
        {
          "_id": "68707ef2c8391850d6097863",
          "name": "Junliang Guo",
          "hidden": false
        },
        {
          "_id": "68707ef2c8391850d6097864",
          "name": "Yang Ye",
          "hidden": false
        },
        {
          "_id": "68707ef2c8391850d6097865",
          "name": "Yueqi Duan",
          "hidden": false
        },
        {
          "_id": "68707ef2c8391850d6097866",
          "name": "Jiang Bian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T17:55:08.000Z",
      "submittedOnDailyAt": "2025-07-11T02:06:14.101Z",
      "title": "ジェオメトリーフォーカス：映像ディフュージョンと3D表現の結婚による一致的な世界モデリング",
      "submittedOnDailyBy": {
        "_id": "6577fba2eb02736add6377f5",
        "avatarUrl": "/avatars/3e486dd36021feaa4a0259cd89c1eee9.svg",
        "isPro": false,
        "fullname": "Wu",
        "user": "Diankun",
        "type": "user"
      },
      "summary": "動画は、動的な3次元世界の2次元投下で表現される。しかし、我々の分析によると、単に裸の動画データを用いて訓練された動画拡散モデルは、学習された表現に意味のあるジェオメトリ的な構造を捉えられないことが多い。この動画拡散モデルと物理的な世界の潜在的な3次元性質の間の隙を埋めるために、我々はGeometry Forcingを提案します。これは簡単で効果的な方法で、動画拡散モデルが潜在的な3次元表現を内視するように奨励するものです。我々の主な見解は、予ったジェオメトリ的な基盤モデルの特徴を用いてモデルの中間表現をジェオメトリ的な構造に向けて調整することです。ここで、我々は2つの補間する調整の目標を導入します：Angular Alignmentはコサイン類似度を用いて方向一致性を強制し、Scale Alignmentは正規化された拡散表現からの非正規化ジェオメトリ的な特徴量の回帰を通じてスケール関連の情報を保存するものです。Geometry Forcingはカメラの視点条件付きおよび行動条件付きの動画生成タスクにおいて評価されます。実験結果は、ベースラインモデルに比べて視覚的な質と3次元一致性が大幅に向上したことを示します。プロジェクトページ：https://GeometryForcing.github.io.",
      "upvotes": 16,
      "discussionId": "68707ef2c8391850d6097867"
    },
    "publishedAt": "2025-07-10T13:55:08.000Z",
    "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for\n  Consistent World Modeling",
    "summary": "Videos inherently represent 2D projections of a dynamic 3D world. However,\nour analysis suggests that video diffusion models trained solely on raw video\ndata often fail to capture meaningful geometric-aware structure in their\nlearned representations. To bridge this gap between video diffusion models and\nthe underlying 3D nature of the physical world, we propose Geometry Forcing, a\nsimple yet effective method that encourages video diffusion models to\ninternalize latent 3D representations. Our key insight is to guide the model's\nintermediate representations toward geometry-aware structure by aligning them\nwith features from a pretrained geometric foundation model. To this end, we\nintroduce two complementary alignment objectives: Angular Alignment, which\nenforces directional consistency via cosine similarity, and Scale Alignment,\nwhich preserves scale-related information by regressing unnormalized geometric\nfeatures from normalized diffusion representation. We evaluate Geometry Forcing\non both camera view-conditioned and action-conditioned video generation tasks.\nExperimental results demonstrate that our method substantially improves visual\nquality and 3D consistency over the baseline methods. Project page:\nhttps://GeometryForcing.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07982.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6577fba2eb02736add6377f5",
      "avatarUrl": "/avatars/3e486dd36021feaa4a0259cd89c1eee9.svg",
      "fullname": "Wu",
      "name": "Diankun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07136",
      "authors": [
        {
          "_id": "68708cb9c8391850d609788f",
          "name": "Wanhua Li",
          "hidden": false
        },
        {
          "_id": "68708cb9c8391850d6097890",
          "name": "Yujie Zhao",
          "hidden": false
        },
        {
          "_id": "68708cb9c8391850d6097891",
          "name": "Minghan Qin",
          "hidden": false
        },
        {
          "_id": "68708cb9c8391850d6097892",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "68708cb9c8391850d6097893",
          "name": "Yuanhao Cai",
          "hidden": false
        },
        {
          "_id": "68708cb9c8391850d6097894",
          "name": "Chuang Gan",
          "hidden": false
        },
        {
          "_id": "68708cb9c8391850d6097895",
          "name": "Hanspeter Pfister",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/tfzmPoFtikY2cpVZVd5Rt.mp4"
      ],
      "publishedAt": "2025-07-09T00:19:58.000Z",
      "submittedOnDailyAt": "2025-07-11T02:43:05.954Z",
      "title": "LangSplatV2: 高次元3D言語ガウススプレッドテクニックで450点以上のFPSを実現",
      "submittedOnDailyBy": {
        "_id": "658bb7e47459b6e471b9d2e6",
        "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
        "isPro": false,
        "fullname": "Wanhua Li",
        "user": "EthanTaylor",
        "type": "user"
      },
      "summary": "この論文では、LangSplatV2を介して、高次元の特徴スプレッティングが476.2 FPSで実現され、高解像度画像に対する3D開語彙テキスト検索が384.6 FPSで実現され、LangSplatに比べて42倍のスピードアップと47倍の性能向上を収め、さらにクエリ精度を向上させることができます。LangSplatは、2D CLIP言語特徴を3Dに埋め込み、SAMの語義を用いて正確な3D言語フィールドを学習するためにガウススプレッティングを使用しています。このような3D言語フィールドの進展は、複雑なスケーン内での言語相互作用を必要とするアプリケーションにおいて重要です。しかし、LangSplatは、進捗したA100GPUを使っても実時間推論性能（8.2 FPS）を達成していないため、広範囲的な応用につながりません。この論文では、LangSplatの時間分析を詳細に行い、重み付きデコーダーが主なスピードボトルネックとして識別します。私たちの解決策は、LangSplatV2では、各ガウスがグローバル辞書内のスパースコードとして機能することを前提とし、3Dスパース係数フィールドを学習することで、重み付きデコーダーの必要を完全に排除します。このスパースさを活用し、CUDA最適化を用いた効率的なスパース係数スプレッティング方法を提案し、高品質で高次元の特徴マップを描画することを可能にし、スプレッティングの超低次元特徴に相当する時間コストしか課せるようにします。実験結果によると、LangSplatV2は、より良いまたは相競合したクエリ精度を実現し、もっとも速くなります。コードとデモは、プロジェクトページ：https://langsplat-v2.github.ioで提供されています。",
      "upvotes": 12,
      "discussionId": "68708cbac8391850d6097896",
      "ai_summary": "LangSplatV2 enhances 3D text querying speed and accuracy by replacing the heavyweight decoder with a sparse coefficient field and efficient CUDA optimization.",
      "ai_keywords": [
        "Gaussian Splatting",
        "CLIP language features",
        "3D language field",
        "SAM semantics",
        "sparse code",
        "sparse coefficient field",
        "sparse coefficient splatting",
        "CUDA optimization"
      ]
    },
    "publishedAt": "2025-07-08T20:19:58.000Z",
    "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+\n  FPS",
    "summary": "In this paper, we introduce LangSplatV2, which achieves high-dimensional\nfeature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6\nFPS for high-resolution images, providing a 42 times speedup and a 47\ntimes boost over LangSplat respectively, along with improved query accuracy.\nLangSplat employs Gaussian Splatting to embed 2D CLIP language features into\n3D, significantly enhancing speed and learning a precise 3D language field with\nSAM semantics. Such advancements in 3D language fields are crucial for\napplications that require language interaction within complex scenes. However,\nLangSplat does not yet achieve real-time inference performance (8.2 FPS), even\nwith advanced A100 GPUs, severely limiting its broader application. In this\npaper, we first conduct a detailed time analysis of LangSplat, identifying the\nheavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2\nassumes that each Gaussian acts as a sparse code within a global dictionary,\nleading to the learning of a 3D sparse coefficient field that entirely\neliminates the need for a heavyweight decoder. By leveraging this sparsity, we\nfurther propose an efficient sparse coefficient splatting method with CUDA\noptimization, rendering high-dimensional feature maps at high quality while\nincurring only the time cost of splatting an ultra-low-dimensional feature. Our\nexperimental results demonstrate that LangSplatV2 not only achieves better or\ncompetitive query accuracy but is also significantly faster. Codes and demos\nare available at our project page: https://langsplat-v2.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/tfzmPoFtikY2cpVZVd5Rt.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07136.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658bb7e47459b6e471b9d2e6",
      "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
      "fullname": "Wanhua Li",
      "name": "EthanTaylor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07202",
      "authors": [
        {
          "_id": "68707dfbc8391850d6097841",
          "user": {
            "_id": "659d164f4b29e5948c66b9f6",
            "avatarUrl": "/avatars/6b4531673a76f7d93b84402ecac74cbe.svg",
            "isPro": false,
            "fullname": "Mohamed Elmoghany",
            "user": "elmoghany",
            "type": "user"
          },
          "name": "Mohamed Elmoghany",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:12.603Z",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097842",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097843",
          "name": "Seunghyun Yoon",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097844",
          "name": "Subhojyoti Mukherjee",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097845",
          "name": "Eslam Bakr",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097846",
          "name": "Puneet Mathur",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097847",
          "name": "Gang Wu",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097848",
          "name": "Viet Dac Lai",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097849",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609784a",
          "name": "Ruiyi Zhang",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609784b",
          "name": "Varun Manjunatha",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609784c",
          "name": "Chien Nguyen",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609784d",
          "name": "Daksh Dangi",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609784e",
          "name": "Abel Salinas",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609784f",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Taesiri",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:08.440Z",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097850",
          "name": "Hongjie Chen",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097851",
          "name": "Xiaolei Huang",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097852",
          "name": "Joe Barrow",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097853",
          "name": "Nesreen Ahmed",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097854",
          "name": "Hoda Eldardiry",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097855",
          "name": "Namyong Park",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097856",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097857",
          "name": "Jaemin Cho",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097858",
          "name": "Anh Totti Nguyen",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d6097859",
          "name": "Zhengzhong Tu",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609785a",
          "name": "Thien Nguyen",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609785b",
          "name": "Dinesh Manocha",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609785c",
          "name": "Mohamed Elhoseiny",
          "hidden": false
        },
        {
          "_id": "68707dfbc8391850d609785d",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:00:10.400Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T18:20:33.000Z",
      "submittedOnDailyAt": "2025-07-11T01:30:02.686Z",
      "title": "長ビデオの物語生成に関する調査：アーキテクチャ、一致性と映画質量",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "電子ビデオ生成モデルにおいて適切な進展が達成されているにも関わらず、現在の最先端の方法では、長めの電子ビデオ（5-16秒間）のみを生成できることが多く、これらを「長めの電子ビデオ」としてよく扱われています。また、16秒以上の電子ビデオは、ビデオの全てのノートで一致した人物の外見とスペースの配置を維持することが難しいです。特に、多エピソードの長めの電子ビデオは、人物の一致性と動作の一貫性を維持することが難しいです。一方で、一部の方法は150秒間の長めの電子ビデオを生成することができるものがありますが、これらは通常、フレームの冗長さと時間的な多様性の低さに悪影響を受けます。最近の研究は、多エピソードの長めの電子ビデオを生成し、説明の一貫性と高品質な詳細を含むものを作成することを試みています。私たちは、32篇の電子ビデオ生成に関する論文を検討し、これらの質素を一貫して得られるキー的なアーキテクチャルコンポーネントとトレーニング戦略を特定しました。また、私たちは、現在の方法の詳細な新しいタクノロジーを構築し、アーキテクチャルデザインと性能の特徴をもとに論文を分類した比較テーブルを提供します。",
      "upvotes": 9,
      "discussionId": "68707dfcc8391850d609785e"
    },
    "publishedAt": "2025-07-09T14:20:33.000Z",
    "title": "A Survey on Long-Video Storytelling Generation: Architectures,\n  Consistency, and Cinematic Quality",
    "summary": "Despite the significant progress that has been made in video generative\nmodels, existing state-of-the-art methods can only produce videos lasting 5-16\nseconds, often labeled \"long-form videos\". Furthermore, videos exceeding 16\nseconds struggle to maintain consistent character appearances and scene layouts\nthroughout the narrative. In particular, multi-subject long videos still fail\nto preserve character consistency and motion coherence. While some methods can\ngenerate videos up to 150 seconds long, they often suffer from frame redundancy\nand low temporal diversity. Recent work has attempted to produce long-form\nvideos featuring multiple characters, narrative coherence, and high-fidelity\ndetail. We comprehensively studied 32 papers on video generation to identify\nkey architectural components and training strategies that consistently yield\nthese qualities. We also construct a comprehensive novel taxonomy of existing\nmethods and present comparative tables that categorize papers by their\narchitectural designs and performance characteristics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07202.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06543",
      "authors": [
        {
          "_id": "686f9aad706a6ea465418a08",
          "user": {
            "_id": "67c6a1e75e2443d7d5f85cb3",
            "avatarUrl": "/avatars/0569b368520411ab828d46725bc3896a.svg",
            "isPro": false,
            "fullname": "Taekyung Kim",
            "user": "taekyung-k",
            "type": "user"
          },
          "name": "Taekyung Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:03:22.401Z",
          "hidden": false
        },
        {
          "_id": "686f9aad706a6ea465418a09",
          "user": {
            "_id": "660f8cc1a61244f3df3d4426",
            "avatarUrl": "/avatars/45d59766122bb3482f6dd7f9d98aa87a.svg",
            "isPro": false,
            "fullname": "Dongyoon Han",
            "user": "calintz",
            "type": "user"
          },
          "name": "Dongyoon Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:03:14.509Z",
          "hidden": false
        },
        {
          "_id": "686f9aad706a6ea465418a0a",
          "user": {
            "_id": "64b9feed96676e40d0fa89a7",
            "avatarUrl": "/avatars/2154a6ceb87677ad2c9d9620de5b18ec.svg",
            "isPro": false,
            "fullname": "Byeongho Heo",
            "user": "bhheo",
            "type": "user"
          },
          "name": "Byeongho Heo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T08:03:16.434Z",
          "hidden": false
        },
        {
          "_id": "686f9aad706a6ea465418a0b",
          "name": "Jeongeun Park",
          "hidden": false
        },
        {
          "_id": "686f9aad706a6ea465418a0c",
          "name": "Sangdoo Yun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T04:57:29.000Z",
      "submittedOnDailyAt": "2025-07-11T04:21:34.394Z",
      "title": "トークンボトルンキング：1トークンでメモリーダイナミクスを覚える",
      "submittedOnDailyBy": {
        "_id": "64b9feed96676e40d0fa89a7",
        "avatarUrl": "/avatars/2154a6ceb87677ad2c9d9620de5b18ec.svg",
        "isPro": false,
        "fullname": "Byeongho Heo",
        "user": "bhheo",
        "type": "user"
      },
      "summary": "動的シーンからコンパクトで時系列に関心のある視覚的表現を抽出することは、視覚追跡やロボット操作などの順次なシーン理解タスクの成功実行に重要です。本論文では、簡単で直感的な自動軽学習パイプライン「Token Bottleneck (ToBo)」を紹介します。このパイプラインは、シーンをボトルネックトークンに圧縮し、最小限のパッチをヒントとして後続のシーンを予測します。ToBoパイプラインは、参考シーンをコンパクトなボトルネックトークンに圧縮し、順次なシーン表現の学習を促進します。拡大ステップでは、ボトルネックトークンと少数のターゲットパッチをヒントとして、モデルを時系列的な動作を捉えるようにガイドします。この設計は、視覚バックボーンに時系列的依存関係を埋め込むように促し、シーン間の動的なトランジションを理解することができます。多様な順次タスクの拡張実験、ビデオラベル伝播やロボット操作の記録環境での実験で、ToBoの優れた性能を示しました。また、物理的なロボットにプレインドラインモデルを実装し、実世界環境での強固性と有効性を確認しました。また、ToBoの拡張性を異なるモデルサイズの幅に及ぼすことを進一度に評価しました。",
      "upvotes": 9,
      "discussionId": "686f9aae706a6ea465418a0d",
      "projectPage": "https://token-bottleneck.github.io",
      "githubRepo": "https://github.com/naver-ai/tobo",
      "ai_summary": "ToBo is a self-supervised learning method that creates compact, temporally aware visual representations for sequential scene understanding tasks, outperforming baselines in both simulated and real-world environments.",
      "ai_keywords": [
        "Token Bottleneck",
        "self-supervised learning",
        "bottleneck token",
        "sequential scene representations",
        "temporal dependencies",
        "video label propagation",
        "robot manipulation"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-07-09T00:57:29.000Z",
    "title": "Token Bottleneck: One Token to Remember Dynamics",
    "summary": "Deriving compact and temporally aware visual representations from dynamic\nscenes is essential for successful execution of sequential scene understanding\ntasks such as visual tracking and robotic manipulation. In this paper, we\nintroduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised\nlearning pipeline that squeezes a scene into a bottleneck token and predicts\nthe subsequent scene using minimal patches as hints. The ToBo pipeline\nfacilitates the learning of sequential scene representations by conservatively\nencoding the reference scene into a compact bottleneck token during the squeeze\nstep. In the expansion step, we guide the model to capture temporal dynamics by\npredicting the target scene using the bottleneck token along with few target\npatches as hints. This design encourages the vision backbone to embed temporal\ndependencies, thereby enabling understanding of dynamic transitions across\nscenes. Extensive experiments in diverse sequential tasks, including video\nlabel propagation and robot manipulation in simulated environments demonstrate\nthe superiority of ToBo over baselines. Moreover, deploying our pre-trained\nmodel on physical robots confirms its robustness and effectiveness in\nreal-world environments. We further validate the scalability of ToBo across\ndifferent model scales.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06543.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64b9feed96676e40d0fa89a7",
      "avatarUrl": "/avatars/2154a6ceb87677ad2c9d9620de5b18ec.svg",
      "fullname": "Byeongho Heo",
      "name": "bhheo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07996",
      "authors": [
        {
          "_id": "68709572c8391850d60978b7",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "68709572c8391850d60978b8",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "68709572c8391850d60978b9",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T07:59:58.984Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T17:59:53.000Z",
      "submittedOnDailyAt": "2025-07-11T03:54:10.527Z",
      "title": "スキップレイヤーまたはループそのまま？ テストタイムの深さアダプタプレトレーンドLLMs",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "プレトレーンドニューラルネットワークが経験後の訓練により入力の違いに適応できるかどうかを調べることができますか？簡単なタスクではすべてのレイヤーが必要であり、難しいタスクでは十分であるかどうかを調べる必要がありますか？私たちは、プレトレーンド大規模言語モデル（LLM）のレイヤーを別々のモジュールとして操作し、各テストサンプルにカスタマイズされたより良いおおきなモデルを構築できることを見出しました。特に、プレトレーンドモデルの各レイヤーは、循環ニューラルネットワーク（RNN）としてスキップされるか削除されるか、または複数回繰り返されることができ、任意の順番で他のレイヤーとスタックされ、それぞれのサンプルについて連鎖のレイヤー（CoLa）を生成することができます。この構成空間は、循環プレトレーンドモジュール、レイヤー削除、または早期エクスイットネットワークに関する既存の研究の範囲を大幅に拡張します。MCTS（モンテカルロ木探索）プロトコルを開発し、数学と一般知識論理ベンチマークからの各サンプルに対して最適なCoLaを探索し、識別します。固定深さの静的モデルに比べ、CoLaは短絡パス（速い考え方）、同じレイヤー（サンス）の再現（遅い考え方）、そして両方の組み合わせで、入力に応じた柔軟な、動的なアーキテクチャを提供します。MCTS最適化されたCoLaについて検討し、2つの主要な発見が得られました：（1）原則LLMが正しく予測したサンプルの75%以上に対して、短絡パスを持つCoLaが見出され、推論の効率化の大きな空間があることが示唆されます；（2）原則LLMが不正しく予測したサンプルの60%以上に対して、正しい予測を達成するCoLaが見出され、性能向上の大きな空間が示唆されます。これらの結果は、固定アーキテクチャのプレトレーンドLLMを異なるサンプルに対して推論する際の欠点を明らかにし、測定時の深さ変更の一般化能力を解放することにつながります。",
      "upvotes": 8,
      "discussionId": "68709573c8391850d60978ba",
      "ai_summary": "A method using chain-of-layers (CoLa) derived from a pretrained large language model allows for dynamic architecture adaptation, improving efficiency and accuracy across diverse tasks through selective layer manipulation and Monte Carlo Tree Search optimization.",
      "ai_keywords": [
        "pretrained large language model (LLM)",
        "chain-of-layers (CoLa)",
        "Monte Carlo Tree Search (MCTS)",
        "looped/recurrent pretrained modules",
        "layer pruning",
        "early-exit networks",
        "test-time depth adaptation"
      ]
    },
    "publishedAt": "2025-07-10T13:59:53.000Z",
    "title": "Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs",
    "summary": "Can a pretrained neural network adapt its architecture to different inputs\nwithout any finetuning? Do we need all layers for simple tasks, and are they\nadequate for challenging tasks? We found that the layers of a pretrained large\nlanguage model (LLM) can be manipulated as separate modules to build a better\nand even shallower model customized for each test sample. In particular, each\nlayer from the pretrained model can be skipped/pruned or repeated multiple\ntimes as recurrent neural networks (RNN), and stacked with others in arbitrary\norders, yielding a chain-of-layers (CoLa) per sample. This compositional space\ngreatly expands the scope of existing works on looped/recurrent pretrained\nmodules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree\nSearch (MCTS) protocol to explore and identify the optimal CoLa for each sample\nfrom math and commonsense reasoning benchmarks. Compared to a static model of a\nfixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same\nlayer(s) (slow thinking), and combining both, offering more flexible, dynamic\narchitectures for different inputs. We conduct an extensive analysis of the\nMCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples\nwith correct predictions by the original LLM, we can find shorter CoLa,\nsuggesting a large space for improving inference efficiency; (2) For >60% of\nsamples with originally incorrect predictions, we can identify CoLa achieving\ncorrect predictions, suggesting a large space of performance enhancement. Our\nresults highlight the shortcomings of using a fixed architecture of pre-trained\nLLMs for inference on different samples and pave the way to unlock the\ngeneralization power of test-time depth adaptation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07996.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07484",
      "authors": [
        {
          "_id": "68708d7ac8391850d609789f",
          "name": "Kaiqu Liang",
          "hidden": false
        },
        {
          "_id": "68708d7ac8391850d60978a0",
          "name": "Haimin Hu",
          "hidden": false
        },
        {
          "_id": "68708d7ac8391850d60978a1",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "68708d7ac8391850d60978a2",
          "name": "Dawn Song",
          "hidden": false
        },
        {
          "_id": "68708d7ac8391850d60978a3",
          "name": "Thomas L. Griffiths",
          "hidden": false
        },
        {
          "_id": "68708d7ac8391850d60978a4",
          "name": "Jaime Fernández Fisac",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T07:11:57.000Z",
      "submittedOnDailyAt": "2025-07-11T02:36:07.699Z",
      "title": "マシンバジャーショット：大規模言語モデルでの真理の無視の特徴化",
      "submittedOnDailyBy": {
        "_id": "6275a465597c70eb8949fce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
        "isPro": false,
        "fullname": "Xuandong Zhao",
        "user": "Xuandong",
        "type": "user"
      },
      "summary": "バウスヒット、フォーストランドの哲学者ハリー・フランキングによって概念化されるものは、真実の価値に関して考慮せずに述べられるものを指します。先行の研究は、大規模な言語モデル（LLM）のハウリネーションとサププリエムショペについて調査していましたが、私たちは、マシンバウスヒットという概念的なフレームワークを提案し、研究者がLLMの真実性の損失を特徴化することができるものとして、その基盤的な機構を明らかにすることを目指しています。私たちは、バウスヒットインデックスという新しいメトリックを紹介し、LLMの真実性に対する不変性を定量化し、空の演説、プラテリング、ワーセルワード、バランスのない主張という四つの定性的なバウスヒットの形式を分析するタクショナミーを提案します。市場プレースデータセット、政治中立データセット、および私たちの新しいバウスヒットエバルベンチマーク（100つのAIアシスタントを経験する2,400つのシナリオ）において実験的評価を実施しました。これらの結果は、RLHFによるモデルの微調校はバウスヒットを顕著に拡大し、推論時のチャインオフサインオプティングは特に空の演説とプラテリングの特定のバウスヒットの形式を顕著に拡大します。また、政治的なコンテキストでは、ワーセルワードが主な戦略となっています。これらの発見は、AIのアラインメントの体系的な課題を明らかにし、真実性のあるLLMの行動を目指した新しい視点を提供します。",
      "upvotes": 3,
      "discussionId": "68708d7ac8391850d60978a5",
      "ai_summary": "Machine bullshit, characterized by LLMs' indifference to truth, is quantified and analyzed through a new framework, revealing that RLHF and CoT prompting exacerbate certain bullshit forms.",
      "ai_keywords": [
        "Bullshit Index",
        "reinforcement learning from human feedback (RLHF)",
        "chain-of-thought (CoT) prompting",
        "empty rhetoric",
        "paltering",
        "weasel words",
        "unverified claims",
        "AI alignment"
      ]
    },
    "publishedAt": "2025-07-10T03:11:57.000Z",
    "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in\n  Large Language Models",
    "summary": "Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to\nstatements made without regard to their truth value. While previous work has\nexplored large language model (LLM) hallucination and sycophancy, we propose\nmachine bullshit as an overarching conceptual framework that can allow\nresearchers to characterize the broader phenomenon of emergent loss of\ntruthfulness in LLMs and shed light on its underlying mechanisms. We introduce\nthe Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and\npropose a complementary taxonomy analyzing four qualitative forms of bullshit:\nempty rhetoric, paltering, weasel words, and unverified claims. We conduct\nempirical evaluations on the Marketplace dataset, the Political Neutrality\ndataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI\nassistants) explicitly designed to evaluate machine bullshit. Our results\ndemonstrate that model fine-tuning with reinforcement learning from human\nfeedback (RLHF) significantly exacerbates bullshit and inference-time\nchain-of-thought (CoT) prompting notably amplify specific bullshit forms,\nparticularly empty rhetoric and paltering. We also observe prevalent machine\nbullshit in political contexts, with weasel words as the dominant strategy. Our\nfindings highlight systematic challenges in AI alignment and provide new\ninsights toward more truthful LLM behavior.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07484.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6275a465597c70eb8949fce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
      "fullname": "Xuandong Zhao",
      "name": "Xuandong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07574",
      "authors": [
        {
          "_id": "6870c2d3c8391850d60978e6",
          "user": {
            "_id": "63aadfe9a4bdd629b7ea7692",
            "avatarUrl": "/avatars/9cc4eb5d4090ce84a590ec195b70e545.svg",
            "isPro": false,
            "fullname": "Enrico",
            "user": "envomp",
            "type": "user"
          },
          "name": "Enrico Vompa",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-11T07:59:54.331Z",
          "hidden": false
        },
        {
          "_id": "6870c2d3c8391850d60978e7",
          "name": "Tanel Tammet",
          "hidden": false
        },
        {
          "_id": "6870c2d3c8391850d60978e8",
          "name": "Mohit Vaishnav",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T09:23:32.000Z",
      "submittedOnDailyAt": "2025-07-11T06:38:11.205Z",
      "title": "ライナー・セパラティビリティの天井を超える",
      "submittedOnDailyBy": {
        "_id": "63aadfe9a4bdd629b7ea7692",
        "avatarUrl": "/avatars/9cc4eb5d4090ce84a590ec195b70e545.svg",
        "isPro": false,
        "fullname": "Enrico",
        "user": "envomp",
        "type": "user"
      },
      "summary": "最先進なVisual-Language Models（VLMs）は、抽象的な理由論の仕組みでの視覚的な埋め込みの線形可分性に制限されそうです。本研究は、この「線形理由のボトルネック」を調査し、VLMの視覚的な埋め込みに対する簡単な線形分類器の性能であるLinear Separability Ceiling（LSC）を引き出します。私たちは、このボトルネックが広く存在し、視覚的な認識の悪いことではなく、言語モデルの理由の経路の失敗から原因です。私たちは、これは解決可能なアラインメント問題であることを示します。しかし、必要なインテビションは、タスクに依存します：セマンティックな概念に対しては、既存の経路を活性化するだけで十分であり、複雑な関係的な理由論は、モデルの核心の重みの調整が必要です。後付け調整を方法学的な制御として使用して、VLMの中に強力な、休眠している理由の経路があることを強い証拠を持ちます。しかし、複雑な関係的なタスクに必要な深い調整を求める場合、明示的に表現の質を向上させることは、新しいプロンプトフォーマットでモデルが失敗することによって、モデルの埋め込みがよく区別できていることに反することになります。最終的に、本研究はVLMの分析に新しいレンズを提供し、強固な理由論は、目標的なアラインメントであり、単に表現学習の向上ではないことを示します。",
      "upvotes": 2,
      "discussionId": "6870c2d4c8391850d60978e9",
      "githubRepo": "https://github.com/envomp/Beyond-the-Linear-Separability-Ceiling",
      "ai_summary": "The study identifies a linear reasoning bottleneck in Visual-Language Models and proposes the Linear Separability Ceiling as a metric to evaluate it, suggesting targeted alignment rather than improved representation learning as a solution.",
      "ai_keywords": [
        "Visual-Language Models",
        "VLMs",
        "linear separability",
        "Linear Separability Ceiling",
        "linear classifier",
        "visual embeddings",
        "abstract reasoning tasks",
        "reasoning pathways",
        "postfix tuning",
        "representation quality",
        "prompt formats"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-07-10T05:23:32.000Z",
    "title": "Beyond the Linear Separability Ceiling",
    "summary": "Most state-of-the-art Visual-Language Models (VLMs) are seemingly limited by\nthe linear separabilty of their visual embeddings on abstract reasoning tasks.\nThis work investigates this \"linear reasoning bottleneck\" by introducing the\nLinear Separability Ceiling (LSC), the performance of a simple linear\nclassifier on a VLM's visual embeddings. We find this bottleneck is widespread\nand stems not from poor perception, but from failures in the language model's\nreasoning pathways. We demonstrate this is a solvable alignment issue. The\nrequired intervention, however, is task-dependent: activating existing pathways\nsuffices for semantic concepts, while complex relational reasoning requires\nadapting core model weights. Using postfix tuning as a methodological control,\nwe find strong evidence for powerful, dormant reasoning pathways within VLMs.\nHowever, for complex relational tasks requiring deeper adaptation, explicitly\nimproving representation quality causes the model to fail on new prompt formats\ndespite its embeddings remaining well separated. Ultimately, this work provides\na new lens for VLM analysis, showing that robust reasoning is a matter of\ntargeted alignment, not simply improved representation learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07574.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63aadfe9a4bdd629b7ea7692",
      "avatarUrl": "/avatars/9cc4eb5d4090ce84a590ec195b70e545.svg",
      "fullname": "Enrico",
      "name": "envomp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05241",
      "authors": [
        {
          "_id": "6870a7c6c8391850d60978ca",
          "name": "Jingyi Chai",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978cb",
          "name": "Shuo Tang",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978cc",
          "name": "Rui Ye",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978cd",
          "name": "Yuwen Du",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978ce",
          "name": "Xinyu Zhu",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978cf",
          "name": "Mengcheng Zhou",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978d0",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978d1",
          "name": "Weinan E",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978d2",
          "name": "Yuzhi Zhang",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978d3",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "6870a7c6c8391850d60978d4",
          "name": "Siheng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T17:50:52.000Z",
      "submittedOnDailyAt": "2025-07-11T04:29:26.187Z",
      "title": "SciMaster: 科学AIアガントへの向け方、第1部。\nX-Masterを基礎として：人類の最後の試験では私たちが先駆ですか？",
      "submittedOnDailyBy": {
        "_id": "62d22496c58f969c152bcefd",
        "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
        "isPro": false,
        "fullname": "Tiezhen WANG",
        "user": "xianbao",
        "type": "user"
      },
      "summary": "AIアガントの急速な進歩は、科学の発見を加速するための長く抱かれていた野望を燃やし出した。この目標を達成するには、人類の知識の先鋒に深く理解する必要がある。そのため、「人類の最終試験」（HLE）は、科学のAIアガントの評価に特に難しい指標として提供される。この研究では、一般用アガントの基盤的アーキテクチャを構築し、HLEでの先駆的な実績を通じてその能力を証明することを目指している。これを達成するために、X-Master、外部ツールと柔軟に相互作用することで人類の研究者を模倣するタイプのロジックアガントを介している。このアガントは、コードをインタラクティブ言語として考える概念化により、構築されたPythonライブラリとカスタマイズされたツールを柔軟に利用できるように設計されている。また、X-Masters、スカラーされたアガントワークフローを通じて、理由の幅と深さをシステマティックに向上させ、その能力を拡大する。私たちのオープンソースソリューション、X-Mastersは、32.1%のスコアでHLEで新たな最先端レベルを設定し、OpenAIとGoogleのDeep Research（26.6%と26.9%）を超え、30%のシュリードを超えることを初めて実現した。この研究は、複雑なタスク解決の理解を深め、将来の進歩に役立つ有價な経験を集め、その後のモデルトレーニングにも指導を与えることができる。",
      "upvotes": 1,
      "discussionId": "6870a7c6c8391850d60978d5"
    },
    "publishedAt": "2025-07-07T13:50:52.000Z",
    "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I.\n  X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
    "summary": "The rapid advancements of AI agents have ignited the long-held ambition of\nleveraging them to accelerate scientific discovery. Achieving this goal\nrequires a deep understanding of the frontiers of human knowledge. As such,\nHumanity's Last Exam (HLE) provides an exceptionally challenging touchstone for\nevaluating scientific AI agents. In this work, we aim to construct the\nfoundational architecture for general-purpose agents and validate the\ncapabilities through leading performance on HLE. To achieve this, we introduce\nX-Master, a tool-augmented reasoning agent designed to emulate human\nresearchers by interacting flexibly with external tools during its reasoning\nprocess. This agent, guided by the conceptualization of code as an interaction\nlanguage, can flexibly leverage built-in Python libraries and our customized\ntools to augment the reasoning. We further scale its capabilities through\nX-Masters, a scattered-and-stacked agentic workflow that systematically\nenhances breadth and depth of reasoning. Our open-source solution, X-Masters,\nsets a new state-of-the-art record on HLE with a score of 32.1%, surpassing\nOpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to\nexceed the 30% threshold. This work allows us to gain a deeper understanding of\ncomplex task-solving and accumulates valuable experience that can inform future\nadvancements, guiding subsequent model training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05241.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d22496c58f969c152bcefd",
      "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
      "fullname": "Tiezhen WANG",
      "name": "xianbao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 120
    },
    "isAuthorParticipating": false
  }
]