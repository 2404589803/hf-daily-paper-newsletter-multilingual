[
  {
    "paper": {
      "id": "2505.04410",
      "authors": [
        {
          "_id": "681d615fbd89ba9ceb5e94bc",
          "user": {
            "_id": "64a385281cbf675203fbb7df",
            "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
            "isPro": false,
            "fullname": "Junjie Wang",
            "user": "xiaomoguhzz",
            "type": "user"
          },
          "name": "Junjie Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:48.630Z",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94bd",
          "name": "Bin Chen",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94be",
          "name": "Yulin Li",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94bf",
          "name": "Bin Kang",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94c0",
          "name": "Yichi Chen",
          "hidden": false
        },
        {
          "_id": "681d615fbd89ba9ceb5e94c1",
          "name": "Zhuotao Tian",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a385281cbf675203fbb7df/MpXgXINxg1RmSFnfeqXqU.mp4"
      ],
      "publishedAt": "2025-05-07T13:46:34.000Z",
      "submittedOnDailyAt": "2025-05-15T06:24:33.810Z",
      "title": "DeCLIP: 開放ボックスの離れた学習による密度観測",
      "submittedOnDailyBy": {
        "_id": "64a385281cbf675203fbb7df",
        "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
        "isPro": false,
        "fullname": "Junjie Wang",
        "user": "xiaomoguhzz",
        "type": "user"
      },
      "summary": "Dense ビジュアル予測タスクは、予約定義カテゴリーに依存していることにより、写真概念が無制限である実世界のスケーナリオでの適用範囲が狭まり、実践的な利用が困難となっている。CLIP などの視覚言語モデル（VLMs）は、開放ボックス言語タスクにおいて優れた性能を示しているが、デンス予測に直接適用すると、局所特徴表現の制限により、より優れた性能を示すことが難しくなる。本稿では、CLIP の画像トークンが空間的または語意的に関連付けられた領域からの情報を有効に集約できないことを観察し、この問題に対処するために、DeCLIP フレームワークを提案しています。DeCLIP は、自動注意モジュールを独立化して、「内容」と「コンテキスト」の特徴量を得ることで CLIP を強化しています。「内容」の特徴量は、画像クロップの表現に一致し、局所の識別性を向上させ、「コンテキスト」の特徴量は、DINO などの視覚ファンダメンタルモデルの指導の下で、空間的関係を保持することを学習しています。拡張的な実験は、DeCLIP がオブジェクト検出や語意的分割などの複数の開放ボックス言語タスクで、現在の方法を大幅に超えることを示しています。コードは、https://github.com/xiaomoguhz/DeCLIP にアクセスできます。",
      "upvotes": 28,
      "discussionId": "681d6161bd89ba9ceb5e9571",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "CLIP",
        "dense prediction",
        "predefined categories",
        "open-vocabulary tasks",
        "spatially related regions",
        "semantically related regions",
        "local discriminability",
        "spatial consistency",
        "self-attention module",
        "content features",
        "context features",
        "image crop representations",
        "vision foundation models",
        "DINO",
        "object detection",
        "semantic segmentation"
      ]
    },
    "publishedAt": "2025-05-07T09:46:34.000Z",
    "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
    "summary": "Dense visual prediction tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense prediction often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. The ``content'' features are aligned with image crop\nrepresentations to improve local discriminability, while ``context'' features\nlearn to retain the spatial correlations under the guidance of vision\nfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIP\nsignificantly outperforms existing methods across multiple open-vocabulary\ndense prediction tasks, including object detection and semantic segmentation.\nCode is available at magenta{https://github.com/xiaomoguhz/DeCLIP}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a385281cbf675203fbb7df/MpXgXINxg1RmSFnfeqXqU.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a385281cbf675203fbb7df",
      "avatarUrl": "/avatars/f259d080d3127c45bcf564a8d1fafcc6.svg",
      "fullname": "Junjie Wang",
      "name": "xiaomoguhzz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09568",
      "authors": [
        {
          "_id": "68254419181d43c25d829239",
          "name": "Jiuhai Chen",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923a",
          "name": "Zhiyang Xu",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923b",
          "name": "Xichen Pan",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923c",
          "name": "Yushi Hu",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923d",
          "name": "Can Qin",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923e",
          "name": "Tom Goldstein",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d82923f",
          "name": "Lifu Huang",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829240",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829241",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829242",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829243",
          "name": "Le Xue",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829244",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "68254419181d43c25d829245",
          "name": "Ran Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T17:11:07.000Z",
      "submittedOnDailyAt": "2025-05-15T00:07:05.564Z",
      "title": "BLIP3-o: フルオープン・ユニフォーム・マルチモーダルモデルの家族 - アーキテクチャ、訓練、データセット",
      "submittedOnDailyBy": {
        "_id": "6393847e3e30234ae798b7be",
        "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
        "isPro": true,
        "fullname": "JiuhaiChen",
        "user": "jiuhai",
        "type": "user"
      },
      "summary": "最近の多モーダルモデルの研究で、画像理解と生成の統合が注目を集めています。画像理解のデザイン選択は広範囲で研究されていますが、画像生成と統合フレームワークの最適なモデル構造と訓練ドリブではまだ調査が不足しています。自動復元モデルとディフュージョンモデルが高品質な生成とスケーラビリティに強い可能性を示していることをモチベーションに基づき、これらのモデルの統合モデル設定での使用について詳細な研究を行いました。これらの研究に基づき、新しいアプローチを提案しました。このアプローチは、単純なVAEベースの表現と異なり、ディフュージョントランスフォーマーを用いて意味的に豊富なCLIP画像特徴量を生成します。この設計は、学習の効率と生成の品質の向上を実現します。また、統合モデルの順番的な予ち練習戦略を示し、画像理解の能力を保っながら画像生成の能力を強化する実用的な利点を示します。最後に、GPT-4oに多様なカプチャーを用いて、ビデオオブジェクト、人間の手の動きなどを構成する異なるスペースにわたるデータセットを用いて、高品質なインストラクションチューニングデータセットBLIP3o-60kを精選しました。この新しいモデル設計、訓練ドリブ、データセットを基に、BLIP3-oという最先端の統合モデルシステムを開発しました。BLIP3-oは、画像理解と生成の両方のテストで最先端の性能を収めます。将来の研究を促進するために、モデルのコード、モデル重み、訓練スクリプト、予ち練習データセット、インストラクションチューニングデータセットを完全に公開します。",
      "upvotes": 13,
      "discussionId": "6825441a181d43c25d82927a",
      "ai_keywords": [
        "autoregressive models",
        "diffusion models",
        "semantically rich CLIP image features",
        "diffusion transformer",
        "VAE-based representations",
        "sequential pretraining strategy",
        "image understanding",
        "image generation",
        "instruction-tuning dataset",
        "GPT-4o",
        "state-of-the-art unified multimodal models"
      ]
    },
    "publishedAt": "2025-05-14T13:11:07.000Z",
    "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset",
    "summary": "Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09568.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6393847e3e30234ae798b7be",
      "avatarUrl": "/avatars/daeb8c37dff4432d837a69b87c196521.svg",
      "fullname": "JiuhaiChen",
      "name": "jiuhai",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 27
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09343",
      "authors": [
        {
          "_id": "682578ca1b93095c061429ff",
          "name": "Chenggang Zhao",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a00",
          "name": "Chengqi Deng",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a01",
          "name": "Chong Ruan",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a02",
          "name": "Damai Dai",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a03",
          "name": "Huazuo Gao",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a04",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a05",
          "name": "Liyue Zhang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a06",
          "name": "Panpan Huang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a07",
          "name": "Shangyan Zhou",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a08",
          "name": "Shirong Ma",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a09",
          "name": "Wenfeng Liang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0a",
          "name": "Ying He",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0b",
          "name": "Yuqing Wang",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0c",
          "name": "Yuxuan Liu",
          "hidden": false
        },
        {
          "_id": "682578ca1b93095c06142a0d",
          "name": "Y. X. Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T12:39:03.000Z",
      "submittedOnDailyAt": "2025-05-15T05:22:39.526Z",
      "title": "DeepSeek-V3の見解：AIアーキテクチャのスケーリング課題とハードウェアの反省",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "大規模言語モデル（LLMs）の急速なスケーリングは、現在のハードウェアアーキテクチャにおける重要な制限を明らかにしています。これらの制限は、メモリ容量、計算効率、およびインターコネクションバンドワイフの制限を含みます。DeepSeek-V3は、2,048台のNVIDIA H800 GPUで訓練されたことを証明し、ハードウェアに関心を持つモデル共創設計がこれらの挑戦を効果的に解決できることを示しています。この論文では、DeepSeek-V3/R1モデルアーキテクチャとそのAIインフラストラクチャについての詳細な分析を提供し、Multi-head Latent Attention（MLA）でメモリ効率を向上させること、Mixture of Experts（MoE）アーキテクチャで計算-通信のトレードオフを最適化すること、FP8混合精度訓練でハードウェアの機能を最大限に発揮すること、およびMulti-Planeネットワークトポロジーでクラスタレベルのネットワークオーバーヘッドを最小化することなど、主なイノベーションを重点的に説明しています。DeepSeek-V3の開発中に遭遇したハードウェアボトルネックをベースに、学術界と業界の仲間と一冊になって、精度の高い低精度計算ユニット、スケールアップとスケールアウトの収束、および低ラットコミュニケーションファブリックのイノベーションについてのブラウザー的な議論を行います。これらのインサイトは、AIワークロードの増加する要求に対応するためのハードウェアとモデルの共創設計の重要な役割を強調し、次世代AIシステムのイノベーションの実用的なプランを提供しています。",
      "upvotes": 10,
      "discussionId": "682578cb1b93095c06142a55",
      "ai_keywords": [
        "Multi-head Latent Attention (MLA)",
        "Mixture of Experts (MoE)",
        "FP8 mixed-precision training",
        "Multi-Plane Network Topology",
        "low-precision computation units",
        "scale-up and scale-out convergence",
        "low-latency communication fabrics"
      ]
    },
    "publishedAt": "2025-05-14T08:39:03.000Z",
    "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures",
    "summary": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09343.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 862
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.08455",
      "authors": [
        {
          "_id": "6824176351679cbc704daa88",
          "user": {
            "_id": "6483b3d52193a1768c00c5ff",
            "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
            "isPro": false,
            "fullname": "Pritam Sarkar",
            "user": "pritamqu",
            "type": "user"
          },
          "name": "Pritam Sarkar",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-14T04:09:09.283Z",
          "hidden": false
        },
        {
          "_id": "6824176351679cbc704daa89",
          "name": "Ali Etemad",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-13T11:35:58.000Z",
      "submittedOnDailyAt": "2025-05-15T04:58:35.875Z",
      "title": "VCRBench: 大規模ビデオ言語モデルの長文脈因果推理能力の検討",
      "submittedOnDailyBy": {
        "_id": "6483b3d52193a1768c00c5ff",
        "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
        "isPro": false,
        "fullname": "Pritam Sarkar",
        "user": "pritamqu",
        "type": "user"
      },
      "summary": "最近の映像理解の進展にもかかわらず、大規模な映像言語モデル（LVLMs）が映像ベースの原因的推理を行う能力は、視覚的に基づいたゴール駆動セットアップでの原因的推理の評価に関する関連するベンチマークのないことにより、調査が不足しています。この欠点を補うために、私たちは新しいベンチマーク「Video-based long-form Causal Reasoning（VCRBench）」を紹介します。VCRBenchは、簡単な日常的な活動の手順を適意にシャッフルした手順映像を用いて作成され、各ショットが鍵の原因的イベントを撮影し、LVLMsが特定のゴールを達成するために必要なイベントを識別、理由を与え、正しい順序を調べることができるかをテストします。また、ベンチマークは、複数選択や二値QAフォーマットで見られる言語的なスロットを利用しないように謹め、開放的なQAの評価に関する課題を避けるように謹めて設計されています。最先端のLVLMsに対するVCRBenchでの評価は、これらのモデルが映像ベースの長文原因的推理に苦戦していることを示し、その原因は、直接の視覚的観察から長距離の原因的依存関係をモデル化することの難易度にあることです。この能力を設定するための簡単なステップとして、私たちはRecognition-Reasoning Decomposition（RRD）を提案します。RRDは、映像ベースの原因的推理を映像識別と原因的推理の2つのサブタスクに分解するモジュール化アプローチです。VCRBenchでの実験は、RRDがVCRBenchの精度を大幅に向上させ、最高で25.2%の収益を示しました。最後に、詳細な分析は、例えば、LVLMsが複雑な映像ベースの長文原因的推理タスクで主に言語知識を依存していることなど、興味深いヒントを示しました。",
      "upvotes": 2,
      "discussionId": "6824176551679cbc704daafb",
      "projectPage": "https://pritamsarkar.com/VCRBench/",
      "githubRepo": "https://github.com/pritamqu/VCRBench",
      "ai_keywords": [
        "Large Video Language Models (LVLMs)",
        "causal reasoning",
        "benchmarks",
        "procedural videos",
        "causal dependencies",
        "video-based long-form causal reasoning",
        "VCRBench",
        "video recognition",
        "Recognition-Reasoning Decomposition (RRD)"
      ]
    },
    "publishedAt": "2025-05-13T07:35:58.000Z",
    "title": "VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large\n  Video Language Models",
    "summary": "Despite recent advances in video understanding, the capabilities of Large\nVideo Language Models (LVLMs) to perform video-based causal reasoning remains\nunderexplored, largely due to the absence of relevant and dedicated benchmarks\nfor evaluating causal reasoning in visually grounded and goal-driven settings.\nTo fill this gap, we introduce a novel benchmark named Video-based long-form\nCausal Reasoning (VCRBench). We create VCRBench using procedural videos of\nsimple everyday activities, where the steps are deliberately shuffled with each\nclip capturing a key causal event, to test whether LVLMs can identify, reason\nabout, and correctly sequence the events needed to accomplish a specific goal.\nMoreover, the benchmark is carefully designed to prevent LVLMs from exploiting\nlinguistic shortcuts, as seen in multiple-choice or binary QA formats, while\nalso avoiding the challenges associated with evaluating open-ended QA. Our\nevaluation of state-of-the-art LVLMs on VCRBench suggests that these models\nstruggle with video-based long-form causal reasoning, primarily due to their\ndifficulty in modeling long-range causal dependencies directly from visual\nobservations. As a simple step toward enabling such capabilities, we propose\nRecognition-Reasoning Decomposition (RRD), a modular approach that breaks\nvideo-based causal reasoning into two sub-tasks of video recognition and causal\nreasoning. Our experiments on VCRBench show that RRD significantly boosts\naccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis\nreveals interesting insights, for instance, that LVLMs primarily rely on\nlanguage knowledge for complex video-based long-form causal reasoning tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08455.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6483b3d52193a1768c00c5ff",
      "avatarUrl": "/avatars/489e7bdcdac4600bd5b136c930a29cd2.svg",
      "fullname": "Pritam Sarkar",
      "name": "pritamqu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]