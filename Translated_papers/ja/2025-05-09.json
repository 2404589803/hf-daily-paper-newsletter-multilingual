[
  {
    "paper": {
      "id": "2505.04620",
      "authors": [
        {
          "_id": "681c6c1817fc8222eff39a1a",
          "user": {
            "_id": "647773a1168cb428e00e9a8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
            "isPro": false,
            "fullname": "Hao Fei",
            "user": "scofield7419",
            "type": "user"
          },
          "name": "Hao Fei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:07.591Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1b",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1c",
          "user": {
            "_id": "67bc247b593452cc18965cb1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/EA3kTYaaff0Hr7-dGiOOj.png",
            "isPro": false,
            "fullname": "JUNCHENG LI",
            "user": "JunchengLi",
            "type": "user"
          },
          "name": "Juncheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:36:52.461Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1d",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:36:59.117Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1e",
          "name": "Qingshan Xu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1f",
          "name": "Bobo Li",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a20",
          "user": {
            "_id": "64c139d867eff857ea51caa8",
            "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
            "isPro": false,
            "fullname": "Shengqiong Wu",
            "user": "ChocoWu",
            "type": "user"
          },
          "name": "Shengqiong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:39.333Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a21",
          "user": {
            "_id": "64ff369d9abcc85a5519b33e",
            "avatarUrl": "/avatars/4b99cdaf5f970d930b196eddf1e5e499.svg",
            "isPro": false,
            "fullname": "Yaoting Wang",
            "user": "Gh0stAR",
            "type": "user"
          },
          "name": "Yaoting Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:37:27.790Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a22",
          "user": {
            "_id": "67e906836c7216f5bf91f70c",
            "avatarUrl": "/avatars/9c7f34d5b1d41ad7231d2733a399abb3.svg",
            "isPro": false,
            "fullname": "junbao.zhou",
            "user": "junbaozhou",
            "type": "user"
          },
          "name": "Junbao Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:37:34.046Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a23",
          "user": {
            "_id": "65a28e129acab19980226731",
            "avatarUrl": "/avatars/abc3828f807efc4e03837b0eae063f98.svg",
            "isPro": false,
            "fullname": "Jiahao Meng",
            "user": "marinero4972",
            "type": "user"
          },
          "name": "Jiahao Meng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:37:40.200Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a24",
          "user": {
            "_id": "656724074f6ec72017754d33",
            "avatarUrl": "/avatars/e61de248f6f53719b2375077340dd033.svg",
            "isPro": false,
            "fullname": "QingyuShi",
            "user": "QingyuShi",
            "type": "user"
          },
          "name": "Qingyu Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:34.088Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a25",
          "name": "Zhiyuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a26",
          "name": "Liangtao Shi",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a27",
          "user": {
            "_id": "648ef24dc92367eecac0f4bd",
            "avatarUrl": "/avatars/38f1afd6b52efeee3aa41cc80225d788.svg",
            "isPro": false,
            "fullname": "Minghe Gao",
            "user": "gmh5811",
            "type": "user"
          },
          "name": "Minghe Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:38:16.554Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a28",
          "user": {
            "_id": "6241b95cfee9374a2598ecfe",
            "avatarUrl": "/avatars/196669df1689a5872fc18b271e80fdc1.svg",
            "isPro": false,
            "fullname": "Zhang Daoan",
            "user": "hazard",
            "type": "user"
          },
          "name": "Daoan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:38:28.567Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a29",
          "name": "Zhiqi Ge",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2a",
          "name": "Weiming Wu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2b",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2c",
          "name": "Kaihang Pan",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2d",
          "user": {
            "_id": "662917afda1cae6cbb50cd00",
            "avatarUrl": "/avatars/aa66de6cef6665c5d67071d82bac35c4.svg",
            "isPro": false,
            "fullname": "Yaobo Ye",
            "user": "superyyb",
            "type": "user"
          },
          "name": "Yaobo Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:55:06.463Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2e",
          "user": {
            "_id": "6391e41f2e73987364e6bcb2",
            "avatarUrl": "/avatars/d09a9ee329bb8c3a9e2929d67d24e97d.svg",
            "isPro": false,
            "fullname": "Haobo Yuan",
            "user": "HarborYuan",
            "type": "user"
          },
          "name": "Haobo Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:39:11.094Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2f",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a30",
          "user": {
            "_id": "6816d98fc075e49c1b15928e",
            "avatarUrl": "/avatars/6b24d047fc25075bedb3e74f78981bc0.svg",
            "isPro": false,
            "fullname": "Tianjie Ju",
            "user": "jometeorieNUS",
            "type": "user"
          },
          "name": "Tianjie Ju",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:53:06.930Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a31",
          "name": "Zixiang Meng",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a32",
          "name": "Shilin Xu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a33",
          "name": "Liyu Jia",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a34",
          "name": "Wentao Hu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a35",
          "user": {
            "_id": "64ad1c0bad6218d51a07b54e",
            "avatarUrl": "/avatars/0f84d9a51c6ca9bcef44de2d7c707d9b.svg",
            "isPro": false,
            "fullname": "LUO MENG",
            "user": "Eureka-Leo",
            "type": "user"
          },
          "name": "Meng Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:37.235Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a36",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a37",
          "name": "Tat-Seng Chua",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a38",
          "user": {
            "_id": "67eaa070b9fa8908e151fd7d",
            "avatarUrl": "/avatars/1fe2fd678d2e71099a83a9bcb9ab517e.svg",
            "isPro": false,
            "fullname": "shuicheng yan",
            "user": "shuicheng",
            "type": "user"
          },
          "name": "Shuicheng Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:52:30.205Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a39",
          "name": "Hanwang Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/TqzNcdmo0rwc-0JkEc0-i.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HBVHtWBiagedRLXD-lWZc.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/X02B6xk6CZDywtG8tGliK.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Xnc89DOmtr5j2hST5Um17.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Awl6jj9MX38cMRkgXpxHp.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HT_9Y1ponvvqUREjMTMBB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/sG3FBiQCx3EONF25cEbED.png"
      ],
      "publishedAt": "2025-05-07T17:59:32.000Z",
      "submittedOnDailyAt": "2025-05-09T01:19:17.510Z",
      "title": "マルチモーダルジャンプラン：一般レベルと一般ベンチマーク",
      "submittedOnDailyBy": {
        "_id": "647773a1168cb428e00e9a8f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
        "isPro": false,
        "fullname": "Hao Fei",
        "user": "scofield7419",
        "type": "user"
      },
      "summary": "多モデル大語言モデル（MLLM）は、LLMの先進的な能力によって急速に成長しています。先週の専門家と異なり、現在のMLLMは多モデルジャンプパラダイムに向けて進化しています。初期的には、複数のモデルを理解することに限られていましたが、これらのモデルは理解しながらも、モデルを生成することも可能になり、その能力は粗粒度からフィンエグリード多モデル理解に、限定的なモデルから任意のモデルに拡張しました。モデルの性能を評価するためには、複数のベンチマークが存在しますが、重要な問題があります：各タスクでの高い性能がMLLMの強力な能力を示すことを仮定できるか？これらのモデルが真のAIに近づくかどうかを調べるかどうか。このプロジェクトでは、General-Levelという評価フレームワークを紹介し、MLLMの性能と一般性を5スケールで定義し、MLLMの比較と現在のシステムの進歩を評価する方法を提供します。このフレームワークの核心は、シンプレックスという概念です。これは、モデルが理解と生成、または複数のモデルでの能力の一貫性を測定します。この評価を支援するために、General-Benchというものを提供し、技能、モデル、フォーマット、能力の広い範囲を含むものです。100以上の現在の最先端のMLLMを含む評価結果は、一般的なモデルの能力順位を示し、真のAIに至るための挑戦を明らかにします。このプロジェクトは、次世代の多モデル基盤モデルの研究に道を開け、AGIの実現を促進する強固なインフラを提供します。プロジェクトページ：https://generalist.top/",
      "upvotes": 42,
      "discussionId": "681c6c1d17fc8222eff39b45",
      "projectPage": "https://generalist.top/",
      "githubRepo": "https://github.com/path2generalist/General-Level",
      "ai_keywords": [
        "Multimodal Large Language Model (MLLM)",
        "Multimodal Generalist",
        "multimodal understanding",
        "comprehension",
        "generation",
        "General-Level",
        "Synergy",
        "General-Bench",
        "AGI (Artificial General Intelligence)"
      ]
    },
    "publishedAt": "2025-05-07T13:59:32.000Z",
    "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
    "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/TqzNcdmo0rwc-0JkEc0-i.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HBVHtWBiagedRLXD-lWZc.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/X02B6xk6CZDywtG8tGliK.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Xnc89DOmtr5j2hST5Um17.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Awl6jj9MX38cMRkgXpxHp.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HT_9Y1ponvvqUREjMTMBB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/sG3FBiQCx3EONF25cEbED.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04620.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "647773a1168cb428e00e9a8f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
      "fullname": "Hao Fei",
      "name": "scofield7419",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05470",
      "authors": [
        {
          "_id": "681d9829edf34a77aab565eb",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ec",
          "user": {
            "_id": "6553316bf151de82f6a23e1d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6553316bf151de82f6a23e1d/GTBkSj4Fa3OoyM6Muz_Sc.jpeg",
            "isPro": false,
            "fullname": "Gongye Liu",
            "user": "liuhuohuo",
            "type": "user"
          },
          "name": "Gongye Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:41:47.403Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ed",
          "name": "Jiajun Liang",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ee",
          "user": {
            "_id": "64d71083a787c9bc7b9f1238",
            "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
            "isPro": false,
            "fullname": "Yangguang Li",
            "user": "Lp256",
            "type": "user"
          },
          "name": "Yangguang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:43:44.697Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ef",
          "user": {
            "_id": "65377c30e48353201e6fdda0",
            "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
            "isPro": false,
            "fullname": "Jiaheng Liu",
            "user": "CheeryLJH",
            "type": "user"
          },
          "name": "Jiaheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:45:07.297Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f0",
          "user": {
            "_id": "60e272ca6c78a8c122b12127",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
            "isPro": false,
            "fullname": "Xintao Wang",
            "user": "Xintao",
            "type": "user"
          },
          "name": "Xintao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:42:27.977Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f1",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f2",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:43:01.366Z",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f3",
          "name": "Wanli Ouyang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T17:58:45.000Z",
      "submittedOnDailyAt": "2025-05-09T05:45:53.355Z",
      "title": "Flow-GRPO: オンラインRLによるフローマッチングモデルの訓練",
      "submittedOnDailyBy": {
        "_id": "64d71083a787c9bc7b9f1238",
        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
        "isPro": false,
        "fullname": "Yangguang Li",
        "user": "Lp256",
        "type": "user"
      },
      "summary": "フロー-GRPOを提案します。これは、フローマッチングモデルにオンライン強化学習（RL）を統合する最初の方法です。私たちのアプローチは、2つの鍵の戦略を使用しています。\n\n(1) ODE-to-SDE変換：確定的な普通微分方程式（ODE）を、時間ステップ全てでオリジナルモデルの境界分布をマッチする等価の乱数微分方程式（SDE）に変換します。これにより、RL検索の統計的サンプリングが可能になります。\n\n(2) Denoising Reduction戦略：デノイズ軽減ステップを減らしながら、元の推論時間ステップ数を維持し、性能低下を避けながらサンプリング効率を大幅に向上させます。\n\n実験的に、Flow-GRPOは複雑な組織にも効果的です。複雑な組織では、RL調整されたSD3.5は近似になるほどの物体数、空間関係、細かい属性を生成し、GenEvalの精度を63%から95%に上げます。可視化のテキストレンディングでは、精度は59%から92%に上げられ、テキスト生成を大幅に向上させます。\n\nFlow-GRPOは人間の好みの一致にも大幅な効果を発揮します。特に、少なくとも報酬ハッキングは発生しませんでした。これは、画像の品質や多様性を損なわずに報酬が増加しなかったことを意味し、両方は実験で安定しました。",
      "upvotes": 26,
      "discussionId": "681d982aedf34a77aab56635",
      "ai_keywords": [
        "Flow-GRPO",
        "reinforcement learning (RL)",
        "flow matching models",
        "ODE-to-SDE conversion",
        "Ordinary Differential Equation (ODE)",
        "Stochastic Differential Equation (SDE)",
        "Denoising Reduction strategy",
        "GenEval accuracy",
        "text-to-image tasks",
        "SD3.5",
        "visual text rendering",
        "human preference alignment",
        "reward hacking"
      ]
    },
    "publishedAt": "2025-05-08T13:58:45.000Z",
    "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
    "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05470.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64d71083a787c9bc7b9f1238",
      "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
      "fullname": "Yangguang Li",
      "name": "Lp256",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.04921",
      "authors": [
        {
          "_id": "681dbb9988ca86d430f1d0d2",
          "user": {
            "_id": "62fdb01bc1588e1d4c6c1a7c",
            "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
            "isPro": false,
            "fullname": "Yunxin Li",
            "user": "YunxinLi",
            "type": "user"
          },
          "name": "Yunxin Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:05:42.761Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d3",
          "user": {
            "_id": "64380ae1819f3ab20d17431b",
            "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
            "isPro": false,
            "fullname": "ZhenyuLiu",
            "user": "foggyforest",
            "type": "user"
          },
          "name": "Zhenyu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T10:08:42.493Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d4",
          "user": {
            "_id": "67ecc6a08647cfa1775a9fda",
            "avatarUrl": "/avatars/bb15abd7a3d2c51380b0b1f819ef76e2.svg",
            "isPro": false,
            "fullname": "Zitao Li",
            "user": "TerenceL-TL",
            "type": "user"
          },
          "name": "Zitao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T08:33:26.702Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d5",
          "name": "Xuanyu Zhang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d6",
          "user": {
            "_id": "639c379cdb7c5f35004066cb",
            "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
            "isPro": false,
            "fullname": "Zhenran Xu",
            "user": "imryanxu",
            "type": "user"
          },
          "name": "Zhenran Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T10:08:40.339Z",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d7",
          "name": "Xinyu Chen",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d8",
          "name": "Haoyuan Shi",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0d9",
          "name": "Shenyuan Jiang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0da",
          "name": "Xintong Wang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0db",
          "name": "Jifang Wang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0dc",
          "name": "Shouzheng Huang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0dd",
          "name": "Xinping Zhao",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0de",
          "name": "Borui Jiang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0df",
          "name": "Lanqing Hong",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e0",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e1",
          "name": "Zhuotao Tian",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e2",
          "name": "Baoxing Huai",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e3",
          "name": "Wenhan Luo",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e4",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e5",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e6",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "681dbb9988ca86d430f1d0e7",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T03:35:23.000Z",
      "submittedOnDailyAt": "2025-05-09T06:54:36.013Z",
      "title": "知覚、理由、思い、計画：大規模多モデルの理由論についての調査",
      "submittedOnDailyBy": {
        "_id": "64380ae1819f3ab20d17431b",
        "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
        "isPro": false,
        "fullname": "ZhenyuLiu",
        "user": "foggyforest",
        "type": "user"
      },
      "summary": "理由が智能の中心にあり、決策の能力、結論の引き出し、そして領域を超える一般化の能力を形成しています。人工知能において、システムが開放的、不確実な、多タイプの環境で運転するようになるにつれて、理由論がロバストなとして適応的な行動を可能にするために重要になります。大規模多タイプ理由論モデル（LMRMs）は、テキスト、画像、音声、映像などのモデルを統合して複雑な理由論能力をサポートし、全体的な認識、精密な理解、深い理由論を目指して岀現しました。研究が進むにつれて、多タイプ理由論は、モジュール化された、認識を駆動するパイプラインから、一連の、言語中心的なフレームワークへと急速に進化しました。この過程で、指示調整と強化学習がモデルの理由論を改善しましたが、全タイプの一般化、理由論の深さ、そしてアウトプットの行動については、大きな課題が残っています。これらの問題を解決するために、私たちは、フィールドの変化した設計哲学と新たな能力を反映した4段階の開発ローマップをもとに、一つの詳細な構造化された多タイプ理由論研究の調査を提供します。まず、特定のタスクに基づくモジュールの早期の努力を調査し、理由論が表現、アラインメント、融合のステージで隠れているように挟まれていました。次に、最近のアプローチを見て、理由論を多タイプLLMsに統一し、Multimodal Chain-of-Thought（MCoT）や多タイプ強化学習などの進歩が複雑なさらに構造化された理由論チェーンを可能にします。最後に、OpenAI O3とO4-miniの難しいベンチマークと実験ケースの実験的なデータからの経験的なインサイプトを元に、本質的な大規模多タイプ理由論モデル（N-LMRMs）の概念的な方向を議論し、複雑な、実世界的な環境でスケーラブルな、アウトプットの、適応的な理由論と計画を支援することを目指しています。",
      "upvotes": 21,
      "discussionId": "681dbb9b88ca86d430f1d183",
      "projectPage": "https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models",
      "githubRepo": "https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models",
      "ai_keywords": [
        "Large Multimodal Reasoning Models (LMRMs)",
        "multimodal reasoning",
        "Cross-modal understanding",
        "task-specific modules",
        "representation",
        "alignment",
        "fusion",
        "Multimodal Chain-of-Thought (MCoT)",
        "multimodal reinforcement learning",
        "native large multimodal reasoning models (N-LMRMs)",
        "scalable",
        "agentic",
        "adaptive reasoning",
        "planning"
      ]
    },
    "publishedAt": "2025-05-07T23:35:23.000Z",
    "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models",
    "summary": "Reasoning lies at the heart of intelligence, shaping the ability to make\ndecisions, draw conclusions, and generalize across domains. In artificial\nintelligence, as systems increasingly operate in open, uncertain, and\nmultimodal environments, reasoning becomes essential for enabling robust and\nadaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a\npromising paradigm, integrating modalities such as text, images, audio, and\nvideo to support complex reasoning capabilities and aiming to achieve\ncomprehensive perception, precise understanding, and deep reasoning. As\nresearch advances, multimodal reasoning has rapidly evolved from modular,\nperception-driven pipelines to unified, language-centric frameworks that offer\nmore coherent cross-modal understanding. While instruction tuning and\nreinforcement learning have improved model reasoning, significant challenges\nremain in omni-modal generalization, reasoning depth, and agentic behavior. To\naddress these issues, we present a comprehensive and structured survey of\nmultimodal reasoning research, organized around a four-stage developmental\nroadmap that reflects the field's shifting design philosophies and emerging\ncapabilities. First, we review early efforts based on task-specific modules,\nwhere reasoning was implicitly embedded across stages of representation,\nalignment, and fusion. Next, we examine recent approaches that unify reasoning\ninto multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)\nand multimodal reinforcement learning enabling richer and more structured\nreasoning chains. Finally, drawing on empirical insights from challenging\nbenchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the\nconceptual direction of native large multimodal reasoning models (N-LMRMs),\nwhich aim to support scalable, agentic, and adaptive reasoning and planning in\ncomplex, real-world environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64380ae1819f3ab20d17431b",
      "avatarUrl": "/avatars/a36b073c1c783102ddb455204fd816bd.svg",
      "fullname": "ZhenyuLiu",
      "name": "foggyforest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02847",
      "authors": [
        {
          "_id": "681d7031e9969eecfcb4eb81",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb82",
          "user": {
            "_id": "648294b2eb4befee378951c1",
            "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
            "isPro": false,
            "fullname": "Ruotian Ma",
            "user": "vvibt",
            "type": "user"
          },
          "name": "Ruotian Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:30.886Z",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb83",
          "name": "Qingxuan Jiang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb84",
          "name": "Peisong Wang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb85",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb86",
          "name": "Zheng Xie",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb87",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb88",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb89",
          "name": "Fanghua Ye",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8a",
          "name": "Jian Li",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8b",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8c",
          "user": {
            "_id": "67485743561b1e6f9579389f",
            "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
            "isPro": false,
            "fullname": "Zhaopeng Tu",
            "user": "zptu",
            "type": "user"
          },
          "name": "Zhaopeng Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:28.468Z",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8d",
          "name": "Xiaolong Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T19:06:10.000Z",
      "submittedOnDailyAt": "2025-05-09T01:37:10.548Z",
      "title": "シンテント・アジェントを裁判官として：大規模言語モデルの高階社会認知を評価する",
      "submittedOnDailyBy": {
        "_id": "648294b2eb4befee378951c1",
        "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
        "isPro": false,
        "fullname": "Ruotian Ma",
        "user": "vvibt",
        "type": "user"
      },
      "summary": "大語言モデル（LLM）が人間を理解することで、それだけではなく文章を理解することでどれだけ理解しているかを評価することは開放的な挑戦です。この隙を橋渡しするために、私たちは「Sentient Agent as a Judge」（SAGE）という自動評価フレームワークを紹介します。これはLLMの高次ソシャル認知を測定するためのものです。SAGEは、人間のような感情変化と内面の思いを模倣し、多ターンコンバーションで試したモデルを更に写実的に評価することを目的としています。それぞれのターンで、このアガントは（i）感情がどのように変化しているか、（ii）どのように感じているか、（iii）どのように返信するかを理由にして、数値的な感情軌跡と解釈可能な内面の思いを提供します。100ヵバイサポートダイアログのスケーナーでの実験は、最終的なSentient感情スコアがBarrett-Lennard Relationship Inventory（BLRI）のレーティングとユータンレベルのエマチーミーツを強く関連づけることを示し、心理学的な忠実性を証明します。また、18つの商業モデルとオープンソースモデルをカバーする公開サンチェントラボラトリも構築し、最先端のシステム（GPT-4o-Latest、Gemini2.5-Pro）と早いベースラインの間に大きな間違い（最高で4倍）を明らかにします。これは、通常のラボラトリ（例：Arena）に反映されていない間違いです。SAGEは、本当にエマチーフや社会的に優れた言語アガントの進歩を追跡するための原則的的、スケーラブルで解釈可能なツールとして提供します。",
      "upvotes": 15,
      "discussionId": "681d7033e9969eecfcb4ec2d",
      "githubRepo": "https://github.com/Tencent/digitalhuman/tree/main/SAGE",
      "ai_keywords": [
        "Sentient Agent as a Judge (SAGE)",
        "higher-order social cognition",
        "emotional changes",
        "inner thoughts",
        "multi-turn conversations",
        "numerical emotion trajectory",
        "Barrett-Lennard Relationship Inventory (BLRI)",
        "utterance-level empathy metrics",
        "psychological fidelity",
        "Sentient Leaderboard",
        "empathetic",
        "socially adept language agents"
      ]
    },
    "publishedAt": "2025-05-01T15:06:10.000Z",
    "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models",
    "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02847.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "648294b2eb4befee378951c1",
      "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
      "fullname": "Ruotian Ma",
      "name": "vvibt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05315",
      "authors": [
        {
          "_id": "681d7ccb572e742b3f42d1f3",
          "user": {
            "_id": "6602869253a0518b2a98cafd",
            "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
            "isPro": false,
            "fullname": "Yuhui Xu",
            "user": "yuhuixu",
            "type": "user"
          },
          "name": "Yuhui Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:04.644Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f4",
          "user": {
            "_id": "63a3ff69f91ad3ea5703841d",
            "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
            "isPro": false,
            "fullname": "Hanze Dong",
            "user": "hendrydong",
            "type": "user"
          },
          "name": "Hanze Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:10.829Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f5",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f6",
          "user": {
            "_id": "65f84fd980481173afd91233",
            "avatarUrl": "/avatars/6ac7bd6beba24d1476c5179b88c9e3fa.svg",
            "isPro": false,
            "fullname": "Doyen",
            "user": "doyensahoo",
            "type": "user"
          },
          "name": "Doyen Sahoo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:18.676Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f7",
          "user": {
            "_id": "61f9d3b54ac99e8a1bae85f4",
            "avatarUrl": "/avatars/ac47d13204dd22452e4bc46e280842d5.svg",
            "isPro": false,
            "fullname": "JunnanLi",
            "user": "JunnanLi",
            "type": "user"
          },
          "name": "Junnan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:32.272Z",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f8",
          "user": {
            "_id": "649dbcc4e0fff1ed099dc80a",
            "avatarUrl": "/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg",
            "isPro": false,
            "fullname": "Caiming Xiong",
            "user": "cxiong",
            "type": "user"
          },
          "name": "Caiming Xiong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:38.430Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T15:01:06.000Z",
      "submittedOnDailyAt": "2025-05-09T02:31:21.542Z",
      "title": "スケーラブルなコンティンュウ オブ スコープス バイ エレスポンド リジニング",
      "submittedOnDailyBy": {
        "_id": "6602869253a0518b2a98cafd",
        "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
        "isPro": false,
        "fullname": "Yuhui Xu",
        "user": "yuhuixu",
        "type": "user"
      },
      "summary": "大論理モデル（LRMs）は、複雑なタスクにおいて、長い思い出チェーン（CoT）を生成することで驚異的な進歩を達成しました。しかし、その制限なしな出力長さは、実世界的な実装では、トークン、ラテンシー、またはコンピュートの推論時のマナーが厳格に制限されている場合に、大きな課題を抱えています。私たちは、スケーラブルな長い思い出の新しいフレームワークを提案します。これは、理由を独立して割り当てられたマナーで、「考える」と「解決策」の２つのステップに明記して理由を分けます。テスト時には、Elastic Reasoningは、解決策セグメントの完全性を優先し、緊張したリソース制約下での信頼性を大幅に向上させます。トランクされた考えるプロセスに対して強固なモデルを訓練するために、GRPOに統合された軽量マナー制約ローウォークスティックスティジェクトを導入します。これは、考えるプロセスが短縮されるともっとも適切に理由をするようにモデルを教え、新しいマナー制約に対しても効果的に一般化し、追加の訓練を必要としません。数学（AIME、MATH500）とプログラミング（LiveCodeBench、Codeforces）のベンチマークでの実験結果は、Elastic Reasoningは厳格なマナー制約下でも強固に動作し、ベースラインメソッドより大幅に低い訓練コストを課します。特に、我々のアプローチは、無制限の設定でもより簡潔で効率的な理由を生成します。Elastic Reasoningは、スケーラブルな制御可能な理由の急迫な課題に対して原理的で実用的な解決策を提供します。",
      "upvotes": 13,
      "discussionId": "681d7ccc572e742b3f42d21a",
      "ai_keywords": [
        "Large reasoning models (LRMs)",
        "chain of thought (CoT)",
        "inference-time budgets",
        "tokens",
        "latency",
        "compute",
        "Elastic Reasoning",
        "scalable chain of thoughts",
        "thinking phase",
        "solution phase",
        "independently allocated budgets",
        "completeness of solution segments",
        "reliability",
        "resource constraints",
        "lightweight budget-constrained rollout strategy",
        "GRPO",
        "adaptive reasoning",
        "unseen budget constraints",
        "mathematical benchmarks (AIME, MATH500)",
        "programming benchmarks (LiveCodeBench, Codeforces)",
        "unconstrained settings",
        "principled solution"
      ]
    },
    "publishedAt": "2025-05-08T11:01:06.000Z",
    "title": "Scalable Chain of Thoughts via Elastic Reasoning",
    "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602869253a0518b2a98cafd",
      "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
      "fullname": "Yuhui Xu",
      "name": "yuhuixu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05071",
      "authors": [
        {
          "_id": "681da6375f701833274a0d21",
          "user": {
            "_id": "6621e591c50869c1e91a1639",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621e591c50869c1e91a1639/L_PoEn2BRAJcnWZX-JebR.jpeg",
            "isPro": false,
            "fullname": "Chunyu Xie",
            "user": "xiechunyu",
            "type": "user"
          },
          "name": "Chunyu Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:03.296Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d22",
          "user": {
            "_id": "5e49e8cf37cb5b49818287ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e49e8cf37cb5b49818287ae/IV9b5Z70NhgmBNfAlc_co.jpeg",
            "isPro": false,
            "fullname": "Bin Wang",
            "user": "binwang",
            "type": "user"
          },
          "name": "Bin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:33.968Z",
          "hidden": true
        },
        {
          "_id": "681da6375f701833274a0d23",
          "user": {
            "_id": "632c098b456c31252774e7c5",
            "avatarUrl": "/avatars/e3720d2fcb69d93c8d5aa5f50aab5f0e.svg",
            "isPro": false,
            "fullname": "kong",
            "user": "fanjing",
            "type": "user"
          },
          "name": "Fanjing Kong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:42.881Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d24",
          "user": {
            "_id": "65b793b374698ba5a815bf4f",
            "avatarUrl": "/avatars/44a7e694a5089dbc773018111270ac26.svg",
            "isPro": false,
            "fullname": "Jincheng Li",
            "user": "jinchenglijc",
            "type": "user"
          },
          "name": "Jincheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:50.538Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d25",
          "user": {
            "_id": "659b8576999b82db2ad8a398",
            "avatarUrl": "/avatars/2ec7663e25e4a0238819818e69d9a5bd.svg",
            "isPro": false,
            "fullname": "Liang",
            "user": "DaweiLiang",
            "type": "user"
          },
          "name": "Dawei Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:58:56.445Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d26",
          "name": "Gengshen Zhang",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d27",
          "user": {
            "_id": "649935abbe8fd92c27ab1ed8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
            "isPro": false,
            "fullname": "David Leon",
            "user": "DavidLeon",
            "type": "user"
          },
          "name": "Dawei Leng",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-09T07:00:10.249Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d28",
          "name": "Yuhui Yin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T09:06:53.000Z",
      "submittedOnDailyAt": "2025-05-09T05:27:38.509Z",
      "title": "FG-CLIP: 細分化の視覚的および文字的なアラインメント",
      "submittedOnDailyBy": {
        "_id": "649935abbe8fd92c27ab1ed8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
        "isPro": false,
        "fullname": "David Leon",
        "user": "DavidLeon",
        "type": "user"
      },
      "summary": "Contrastive Language-Image Pre-training (CLIP)は、画像-テキスト検索とゼロショット分類などの多様化タスクに優れていますが、コアグラインの短いキャプションに焦点を当てて、細かい理解に適していません。この問題を解決するために、私たちはFine-Grained CLIP (FG-CLIP)を提案します。FG-CLIPは、3つのキーイノベーションを通じて細かい理解を強化します。まず、大規模な多様化モデルを活用して、16億の長キャプション-画像ペアを生成し、グローバルレベルの語意的詳細を捉えることができます。次に、1200万枚の画像と4000万ペアのエリア特異的なボウンディングボックスを含む高品質データセットを構築し、詳細なキャプションとコンテキストをもつ精密な表現を確保します。最後に、1000万ペアの難しい細かい負例サンプルを追加し、モデルの語意的微妙な違いを区別する能力を向上させます。これらのデータに対応する訓練方法も細かく設計されています。拡大した実験は、FG-CLIPが細かい理解、オープンボックス対象物検出、画像-テキスト検索、および一般的な多様化ベンチマークで、元のCLIPや他の最先端の方法を超える性能を示しました。これらの結果は、FG-CLIPが細かい画像詳細を捉え、全体のモデル性能を向上させる効果を明らかにしています。関連するデータ、コード、モデルは、https://github.com/360CVGroup/FG-CLIP に提供されています。",
      "upvotes": 8,
      "discussionId": "681da6385f701833274a0d8a",
      "githubRepo": "https://github.com/360CVGroup/FG-CLIP",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training (CLIP)",
        "image-text retrieval",
        "zero-shot classification",
        "fine-grained understanding",
        "coarse-grained short captions",
        "multimodal models",
        "1.6 billion long caption-image pairs",
        "high-quality dataset",
        "12 million images",
        "40 million region-specific bounding boxes",
        "detailed captions",
        "10 million hard fine-grained negative samples",
        "fine-grained understanding",
        "open-vocabulary object detection",
        "general multimodal benchmarks",
        "FG-CLIP"
      ]
    },
    "publishedAt": "2025-05-08T05:06:53.000Z",
    "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment",
    "summary": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05071.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649935abbe8fd92c27ab1ed8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
      "fullname": "David Leon",
      "name": "DavidLeon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05474",
      "authors": [
        {
          "_id": "681d7b5ae27a030c96a28bde",
          "user": {
            "_id": "672392c4a4c4381cefc06416",
            "avatarUrl": "/avatars/8ee84a7e3e91e5d13074bc3c407ff75d.svg",
            "isPro": false,
            "fullname": "Wen Beichen",
            "user": "wenbc21",
            "type": "user"
          },
          "name": "Beichen Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:56:55.900Z",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28bdf",
          "user": {
            "_id": "63f47b5321eb234ab739e91a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
            "isPro": false,
            "fullname": "Haozhe Xie",
            "user": "hzxie",
            "type": "user"
          },
          "name": "Haozhe Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:02.341Z",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be0",
          "user": {
            "_id": "62fc8cf7ee999004b5a8b982",
            "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
            "isPro": false,
            "fullname": "Zhaoxi Chen",
            "user": "FrozenBurning",
            "type": "user"
          },
          "name": "Zhaoxi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:08.543Z",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be1",
          "name": "Fangzhou Hong",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be2",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:27.473Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/PqIosa0nVWamlNxCmd6w1.webp"
      ],
      "publishedAt": "2025-05-08T17:59:54.000Z",
      "submittedOnDailyAt": "2025-05-09T02:21:54.023Z",
      "title": "3Dスキーム生成: 概観",
      "submittedOnDailyBy": {
        "_id": "63f47b5321eb234ab739e91a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
        "isPro": false,
        "fullname": "Haozhe Xie",
        "user": "hzxie",
        "type": "user"
      },
      "summary": "3Dシーン生成は、満喫メディア、ロボット工学、自動運転、具象AIなどのアプリケーションにおける空間的に構造付き、語意的に意味を持つ、写実的な環境の合成を目的としています。早期のプロセス順序に基づく方法はスケーラブルでありましたが、多様性が限られていました。深層生成モデル（例：GAN、ディフュージョンモデル）と3D表現（例：NeRF、3Dガウス）の最近の進展は、実世界のシーン分布の学習を可能にし、忠実度、多様性、視点の一致性を向上させました。最近の進歩では、ディフュージョンモデルなどの機能を通じて、3Dシーン合成と写実性を結びつけるために、生成を画像またはビデオ合成の問題として再構成しました。この調査は、最先端のアプローチのシステマティックな概要を提供し、プロセス順序生成、ニューラル3Dベース生成、画像ベース生成、ビデオベース生成の4つのパラダイムに組み立てています。技術的な基礎、トレードオフ、代表的な結果を分析し、通常のデータセット、評価プロトコル、ダウンストリームアプリケーションを紹介します。最後に、生成能力、3D表現、データと注釈、評価の関連する主要な課題を議論し、高い忠実度、物理的知識を持つおよび相互作用生成、ユニットポーション認識生成モデルなどの有望な方向を示します。このレビューは、生成AI、3Dビジョン、具象知能の交差点での最近の進歩を組み立て、生成AI、3Dビジョン、具象知能の交差点での有望な方向を明らかにします。進歩を追跡するために、最新のプロジェクトページを維持しています：\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.",
      "upvotes": 7,
      "discussionId": "681d7b5be27a030c96a28c29",
      "githubRepo": "https://github.com/hzxie/Awesome-3D-Scene-Generation",
      "ai_keywords": [
        "deep generative models",
        "GANs",
        "diffusion models",
        "NeRF",
        "3D Gaussians",
        "procedural generation",
        "neural 3D-based generation",
        "image-based generation",
        "video-based generation"
      ]
    },
    "publishedAt": "2025-05-08T13:59:54.000Z",
    "title": "3D Scene Generation: A Survey",
    "summary": "3D scene generation seeks to synthesize spatially structured, semantically\nmeaningful, and photorealistic environments for applications such as immersive\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\nprocedural rules offered scalability but limited diversity. Recent advances in\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\ndistributions, improving fidelity, diversity, and view consistency. Recent\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\nreframing generation as image or video synthesis problems. This survey provides\na systematic overview of state-of-the-art approaches, organizing them into four\nparadigms: procedural generation, neural 3D-based generation, image-based\ngeneration, and video-based generation. We analyze their technical foundations,\ntrade-offs, and representative results, and review commonly used datasets,\nevaluation protocols, and downstream applications. We conclude by discussing\nkey challenges in generation capacity, 3D representation, data and annotations,\nand evaluation, and outline promising directions including higher fidelity,\nphysics-aware and interactive generation, and unified perception-generation\nmodels. This review organizes recent advances in 3D scene generation and\nhighlights promising directions at the intersection of generative AI, 3D\nvision, and embodied intelligence. To track ongoing developments, we maintain\nan up-to-date project page:\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/PqIosa0nVWamlNxCmd6w1.webp"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05474.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f47b5321eb234ab739e91a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
      "fullname": "Haozhe Xie",
      "name": "hzxie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05327",
      "authors": [
        {
          "_id": "681d95bc11abe59dc97e4c5a",
          "user": {
            "_id": "647e99d9becb41a272970ca4",
            "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
            "isPro": false,
            "fullname": "Ann",
            "user": "yyxsghx",
            "type": "user"
          },
          "name": "Yixin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:18.753Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5b",
          "user": {
            "_id": "670740744341dcee459fb990",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/66UkZvrAk7fQr5YCylEFk.png",
            "isPro": false,
            "fullname": "Qingxiu Dong",
            "user": "Rsy24",
            "type": "user"
          },
          "name": "Qingxiu Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:38.844Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5c",
          "user": {
            "_id": "655ca347f426a304c6b393a1",
            "avatarUrl": "/avatars/67f0310d59c5912d38c2ad8e6448614d.svg",
            "isPro": false,
            "fullname": "Linli Yao",
            "user": "yaolily",
            "type": "user"
          },
          "name": "Linli Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:47.561Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5d",
          "user": {
            "_id": "654cca3fe1b4cd6d40d5a7ae",
            "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
            "isPro": false,
            "fullname": "Fangwei Zhu",
            "user": "soliz1998",
            "type": "user"
          },
          "name": "Fangwei Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:57:53.796Z",
          "hidden": false
        },
        {
          "_id": "681d95bc11abe59dc97e4c5e",
          "name": "Zhifang Sui",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T15:17:37.000Z",
      "submittedOnDailyAt": "2025-05-09T07:13:58.961Z",
      "title": "ICon: コンテキスト内の自動データ選択の貢献",
      "submittedOnDailyBy": {
        "_id": "647e99d9becb41a272970ca4",
        "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
        "isPro": false,
        "fullname": "Ann",
        "user": "yyxsghx",
        "type": "user"
      },
      "summary": "データ選択は、大規模言語モデル（LLMs）の性能向上と訓練コスト削減において重要です。しかし、現在の自動化された選択方法は、計算的に高額な勾配ベースの測定や手動設計されたヒューリスティックに依存していますが、これらはデータの固有の属性を完全に機能させることができません。本論文では、In-context Learning for Contribution Measurement（ICon）という新しい勾配無し方法を提案します。これは、In-context Learning（ICL）の潜在的な微調節の性質を活用し、勾配計算や手動プロジェクトマニュアルの設計を必要としないように、サンプルの貢献度を評価することができます。IConは勾配ベースの方法に対して計算的に効率的な代替となり、ヒューリスティックベースのアプローチにおける人間の推論バイアスを減少します。IConは3つの構成要素を構成し、ICLによる潜在的な学習を通じて性能の変化を評価して高貢献度のデータを特定します。3つのLLMsで12ベンチマークと5ペアウィスセットを採用した拡張的な実験でIConの効果性を示しました。特に、LLaMA3.1-8Bでは、IConで選択されたデータの15%で訓練されたモデルは全体データセットより5.42%点上位に輝き、一般的に使用される選択方法の最良性能を2.06%点上位に超えました。また、IConで選択された高貢献度のサンプルについて進ける分析を行い、それらはディバーセなタスクと適切な難易度レベルを持っていることを示し、それらのみが最難関であるものではないことを示しました。",
      "upvotes": 7,
      "discussionId": "681d95bd11abe59dc97e4c87",
      "ai_keywords": [
        "in-context learning (ICL)",
        "implicit fine-tuning",
        "In-context Learning for Contribution Measurement (ICon)"
      ]
    },
    "publishedAt": "2025-05-08T11:17:37.000Z",
    "title": "ICon: In-Context Contribution for Automatic Data Selection",
    "summary": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05327.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647e99d9becb41a272970ca4",
      "avatarUrl": "/avatars/291831643937a298c5903c6dc037b950.svg",
      "fullname": "Ann",
      "name": "yyxsghx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03981",
      "authors": [
        {
          "_id": "681d7ee755699177c7fb636a",
          "user": {
            "_id": "617e7729129c9e67703ffe61",
            "avatarUrl": "/avatars/f47ee9f2f0e2b1075bebf3682ee2f817.svg",
            "isPro": false,
            "fullname": "qianchu liu",
            "user": "qianchu",
            "type": "user"
          },
          "name": "Qianchu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:18.501Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636b",
          "user": {
            "_id": "6234c11b7d5de9839bc44163",
            "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
            "isPro": false,
            "fullname": "Sheng Zhang",
            "user": "shengz",
            "type": "user"
          },
          "name": "Sheng Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:22.974Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636c",
          "user": {
            "_id": "64b8e41d52b7353d8c6dd38f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IAItP4FvD6JX9s1jwnQwF.png",
            "isPro": false,
            "fullname": "Guanghui Qin",
            "user": "hiaoxui",
            "type": "user"
          },
          "name": "Guanghui Qin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:24.320Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636d",
          "name": "Timothy Ossowski",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636e",
          "name": "Yu Gu",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636f",
          "name": "Ying Jin",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6370",
          "user": {
            "_id": "627bd86f7e62b4bf5c367108",
            "avatarUrl": "/avatars/4e87eea02d51680ebac7992dfe527e07.svg",
            "isPro": false,
            "fullname": "Sid Kiblawi",
            "user": "sidkiblawi",
            "type": "user"
          },
          "name": "Sid Kiblawi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:37.798Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6371",
          "user": {
            "_id": "65a13da85dce70a3025b7534",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zSPDBEGIULEYN4P7JCdyC.png",
            "isPro": false,
            "fullname": "Sam Preston",
            "user": "RustyArchimedes",
            "type": "user"
          },
          "name": "Sam Preston",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T08:59:43.654Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6372",
          "name": "Mu Wei",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6373",
          "user": {
            "_id": "6797f24ded1557b14d708541",
            "avatarUrl": "/avatars/d69ac80a9a500764766ce9ac7d549cc2.svg",
            "isPro": false,
            "fullname": "Paul Vozila",
            "user": "Paulvozila",
            "type": "user"
          },
          "name": "Paul Vozila",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:01.673Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6374",
          "user": {
            "_id": "5e5870466bc35159a08ca572",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e5870466bc35159a08ca572/pT6gEEs8RLRJGeM-faNWj.jpeg",
            "isPro": false,
            "fullname": "Tristan Naumann",
            "user": "tnaumann",
            "type": "user"
          },
          "name": "Tristan Naumann",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:15.476Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6375",
          "user": {
            "_id": "664d07456083f276c4feb1a4",
            "avatarUrl": "/avatars/1bfa6d8f82e9223b47630cefd79d7d0e.svg",
            "isPro": false,
            "fullname": "Hoifung Poon",
            "user": "hoifung",
            "type": "user"
          },
          "name": "Hoifung Poon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:22.468Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T21:08:27.000Z",
      "submittedOnDailyAt": "2025-05-09T02:41:23.534Z",
      "title": "X-Reasoner: 様々なモデルと領域での一般的な理由論への向け",
      "submittedOnDailyBy": {
        "_id": "6234c11b7d5de9839bc44163",
        "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
        "isPro": false,
        "fullname": "Sheng Zhang",
        "user": "shengz",
        "type": "user"
      },
      "summary": "最近のプロプライティモデル（例：o3）は、強力な多モデル論理能力を示すようになっています。しかし、現在の多くの開放ソース研究は、文のみを対象とした論理モデルの訓練に集中し、評価は主に数学的および一般領域のタスクに限られています。そのため、文の入力と一般領域を超えた論理能力を有効に拡大する方法が明らかになっていません。本論文では、基本的な研究問題を調査しています：論理はモデルと領域の両方で一般化可能ですか？我々の発見は肯定的な答えをサポートしています：一般領域の文ベースの後プログラミングは、そのような強力な一般化可能な論理を可能にします。この発見を活用し、我々はX-Reasonerを紹介しています。X-Reasonerは、一般領域の文をだけに対象とした後プログラミングされた視覚言語モデルで、2段階アプローチを使用して一般化可能な論理を実現しています：最初のサブジェクト経験学習ファイナルチューニングフェーズと、確認可能な報酬をもつ強化学習ファイナルチューニングフェーズです。実験は、X-Reasonerは視覚多モデルと領域外の設定での論理能力を成功に転移し、一般的および医学的ベンチマークで現在の最先端モデルを超えていることを示しています（図1）。また、X-Reasonerの特殊領域の性能は、領域特有の文データによる継続的な訓練でさらに向上することがわかりました。これに基づいて、我々はX-Reasoner-Medを紹介しています。X-Reasoner-Medは、訓練された一般領域モデルと視覚多モデルを超えて、訓練された訓練データを使用して新たな最先端モデルを達成しています。",
      "upvotes": 5,
      "discussionId": "681d7ee855699177c7fb63b7",
      "projectPage": "https://github.com/microsoft/x-reasoner",
      "ai_keywords": [
        "multimodal reasoning",
        "vision-language model",
        "post-training",
        "long chain-of-thoughts",
        "reinforcement learning",
        "verifiable rewards",
        "X-Reasoner",
        "out-of-domain settings",
        "X-Reasoner-Med"
      ]
    },
    "publishedAt": "2025-05-06T17:08:27.000Z",
    "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains",
    "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03981.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6234c11b7d5de9839bc44163",
      "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
      "fullname": "Sheng Zhang",
      "name": "shengz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05467",
      "authors": [
        {
          "_id": "681d95b5c7ae5f65b0e55ff9",
          "user": {
            "_id": "63fee47352441fe3e87b5088",
            "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
            "isPro": false,
            "fullname": "WANG HAIBO",
            "user": "WHB139426",
            "type": "user"
          },
          "name": "Haibo Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:20.829Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffa",
          "name": "Bo Feng",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffb",
          "user": {
            "_id": "66b5295f83425904fa7a1a6a",
            "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
            "isPro": false,
            "fullname": "Zhengfeng Lai",
            "user": "jefflai",
            "type": "user"
          },
          "name": "Zhengfeng Lai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:00:45.264Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffc",
          "name": "Mingze Xu",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffd",
          "user": {
            "_id": "67fa856547b40f55b7ff3ce5",
            "avatarUrl": "/avatars/745937497772e9b533ba7940d758d30d.svg",
            "isPro": false,
            "fullname": "Shiyu Li",
            "user": "ShiyuLi",
            "type": "user"
          },
          "name": "Shiyu Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:01:08.366Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffe",
          "name": "Weifeng Ge",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55fff",
          "user": {
            "_id": "66fc2377516eaf950d4b8209",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mcUxUxXy18Gv9KvCW23s0.png",
            "isPro": false,
            "fullname": "Afshin Dehghan",
            "user": "afshindn",
            "type": "user"
          },
          "name": "Afshin Dehghan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:01:21.716Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e56000",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e56001",
          "name": "Ping Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T17:57:40.000Z",
      "submittedOnDailyAt": "2025-05-09T04:13:07.003Z",
      "title": "ストリームブリッジ：ローカルビデオ大語言モデルを主動的なストリーミングアシスタントに変換する",
      "submittedOnDailyBy": {
        "_id": "63fee47352441fe3e87b5088",
        "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
        "isPro": false,
        "fullname": "WANG HAIBO",
        "user": "WHB139426",
        "type": "user"
      },
      "summary": "StreamBridgeは、簡単で効果的なフレームワークです。これは、オフラインのVideo-LLMsを流れ対応モデルに無難に変換することを実現します。StreamBridgeは、現有モデルをオンラインシナリオに適用する際に発生する2つの基本的な課題を解決しています。(1) 複数のターンの時間単位での理解能力の制限と(2) 動的な応答機構の欠如。特に、StreamBridgeは(1) メモリバッファと円周衰減コンピレーション戦略を組み合わせたものを採用し、長ケースの複数のターンの相互作用をサポートします。(2) 分離された軽量の活性化モデルを採用し、既存のVideo-LLMsに容易に統合できるように設計されており、継続的な動的な応答を可能にします。また、StreamBridgeのより強力なサポートを提供するために、Stream-ITという、流れ対応ビデオ理解に適した大規模データセットを構築しました。このデータセットは、間雑なビデオ-テキストシーケンスと多様な指示フォーマットを特徴としています。拡大した実験により、StreamBridgeは、オフラインのVideo-LLMsの流れ対応理解能力を大幅に向上させ、GPT-4oやGemini 1.5 Proといったプロプライティモデルを超えるような性能を示しました。同時に、標準的なビデオ理解ベンチマークでは、相対的または上位の性能を収めました。",
      "upvotes": 4,
      "discussionId": "681d95b6c7ae5f65b0e5606c",
      "ai_keywords": [
        "Video-LLMs",
        "streaming-capable models",
        "multi-turn real-time understanding",
        "proactive response mechanisms",
        "memory buffer",
        "round-decayed compression strategy",
        "long-context multi-turn interactions",
        "decoupled activation model",
        "Stream-IT",
        "interleaved video-text sequences",
        "standard video understanding benchmarks",
        "GPT-4o",
        "Gemini 1.5 Pro"
      ]
    },
    "publishedAt": "2025-05-08T13:57:40.000Z",
    "title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant",
    "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05467.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fee47352441fe3e87b5088",
      "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
      "fullname": "WANG HAIBO",
      "name": "WHB139426",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05469",
      "authors": [
        {
          "_id": "681db3a8a9286b53a51dc77b",
          "user": {
            "_id": "672403d5f328a3e6638331ee",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/TXr0SKWI-z-6FvUXTNWXT.jpeg",
            "isPro": false,
            "fullname": "Ava Pun",
            "user": "AvaLovelace",
            "type": "user"
          },
          "name": "Ava Pun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:33.422Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77c",
          "user": {
            "_id": "645d34ecce72244df7b29317",
            "avatarUrl": "/avatars/1248933d9f89a15e67086325a8322d5e.svg",
            "isPro": false,
            "fullname": "Kangle Deng",
            "user": "kangled",
            "type": "user"
          },
          "name": "Kangle Deng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:39.736Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77d",
          "user": {
            "_id": "658b307b75ddc76f9dc747ca",
            "avatarUrl": "/avatars/fc5393dc0bb33a8c0fea3a6f79640386.svg",
            "isPro": false,
            "fullname": "Ruixuan Liu",
            "user": "RLCMU",
            "type": "user"
          },
          "name": "Ruixuan Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:45.442Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77e",
          "user": {
            "_id": "6337151b0267ebcf02640eb6",
            "avatarUrl": "/avatars/14a723cafc5587043bdfb19304fc202d.svg",
            "isPro": false,
            "fullname": "Deva Ramanan",
            "user": "devakramanan",
            "type": "user"
          },
          "name": "Deva Ramanan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:06:52.637Z",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc77f",
          "name": "Changliu Liu",
          "hidden": false
        },
        {
          "_id": "681db3a8a9286b53a51dc780",
          "user": {
            "_id": "63a0acc32fabbbb899952a2b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671474335794-noauth.jpeg",
            "isPro": false,
            "fullname": "Jun-Yan Zhu",
            "user": "junyanz",
            "type": "user"
          },
          "name": "Jun-Yan Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:07:07.095Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T17:58:18.000Z",
      "submittedOnDailyAt": "2025-05-09T06:30:13.597Z",
      "title": "テキストから生成される物理的に安定し、構築可能なレギオデザイン",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "レゴGPT、文提示から生成できる物理的に安定したレゴブラックモデルを生成する最初のアプローチを紹介します。これを達成するために、レゴデザインの大規模な、物理的に安定したデータセットを構築し、関連付けされたキャプチャとともに、次のトークン予測を通じて自動復元サイズの大規模な言語モデルを訓練します。結果のデザインの安定性を向上させるために、自動復元推論時に効率的な有効性チェックと物理的知識に基づくローバックを用い、物理法則と装配制約を用いて不可行なトークン予測を削除します。実験により、レゴGPTは入力の文提示により近くなる安定した、多様な、美しいレゴデザインを生成します。また、レゴデザインを手動で人間やロボットアームで自動的に装配できるようなテクスチャ生成方法を開発します。また、新しいデータセット「StableText2Lego」をリリースします。このデータセットは、28,000種類以上の3Dオブジェクトを含む47,000以上のレゴ構造を含み、詳細なキャプチャとともに、プロジェクトウェブサイトでコードとモデルを公開します。",
      "upvotes": 3,
      "discussionId": "681db3aca9286b53a51dc875",
      "ai_keywords": [
        "autoregressive large language model",
        "next-token prediction",
        "validity check",
        "physics-aware rollback",
        "autoregressive inference",
        "physics laws",
        "assembly constraints",
        "text-based LEGO texturing method",
        "automatic assembly",
        "robotic arms"
      ]
    },
    "publishedAt": "2025-05-08T13:58:18.000Z",
    "title": "Generating Physically Stable and Buildable LEGO Designs from Text",
    "summary": "We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05469.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6796
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.05408",
      "authors": [
        {
          "_id": "681daba2e3775056736651ce",
          "user": {
            "_id": "61424bf4f0d914a5f606a823",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
            "isPro": false,
            "fullname": "Yong Zheng-Xin",
            "user": "yongzx",
            "type": "user"
          },
          "name": "Zheng-Xin Yong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:20:45.630Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651cf",
          "name": "M. Farid Adilazuarda",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d0",
          "user": {
            "_id": "6509feb92257a3afbaeecfea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6509feb92257a3afbaeecfea/a_UbA-2WtZeLTf0ugVzSh.jpeg",
            "isPro": false,
            "fullname": "Jonibek Mansurov",
            "user": "MJonibek",
            "type": "user"
          },
          "name": "Jonibek Mansurov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:03:39.843Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d1",
          "name": "Ruochen Zhang",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d2",
          "user": {
            "_id": "5f1eb362eec0ad2a071ad6e2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
            "isPro": false,
            "fullname": "Niklas Muennighoff",
            "user": "Muennighoff",
            "type": "user"
          },
          "name": "Niklas Muennighoff",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:00.900Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d3",
          "name": "Carsten Eickhoff",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d4",
          "user": {
            "_id": "5f5c4b20e56d546cd6233098",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
            "isPro": false,
            "fullname": "Genta Indra Winata",
            "user": "gentaiscool",
            "type": "user"
          },
          "name": "Genta Indra Winata",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:13.878Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d5",
          "user": {
            "_id": "6544e43b12da508864c38f96",
            "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
            "isPro": false,
            "fullname": "Julia Kreutzer",
            "user": "JuliaKreutzerCohere",
            "type": "user"
          },
          "name": "Julia Kreutzer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:29.257Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d6",
          "name": "Stephen H. Bach",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d7",
          "name": "Alham Fikri Aji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T16:50:06.000Z",
      "submittedOnDailyAt": "2025-05-09T05:46:57.523Z",
      "title": "クロス言語論理化を検証時スケーリングによって実現する",
      "submittedOnDailyBy": {
        "_id": "61424bf4f0d914a5f606a823",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
        "isPro": false,
        "fullname": "Yong Zheng-Xin",
        "user": "yongzx",
        "type": "user"
      },
      "summary": "大語言モデルの論理能力は、多言語でもプレトレーニングモデルでも主に英語において研究されています。本研究では、長いChain-of-Thoughts（CoTs）で英語の論理調整を行ったものが他の言語でどれだけ一般化できるかを調査します。まず、英語中心的な論理言語モデル（RLM）の推論計算を拡大することで、多言語で数学的な論理を改善し、低リソース言語も含む複数の言語でより高い性能を示すことが見られます。また、英語中心的なRLMのCoTsは自然と英語が主な部分であるが、引用された非英語の入力について論理する際に、引用と考えるパターンを続けていることが明らかになります。また、長いCoT論理の言語を制御する有效な戦略を発見し、モデルは資源豊富な言語でより良くより効率的に論理することが見られます。最後に、特に理科・技術・数学（STEM）から文化的な一般知識への論理の一般化が不良し、英語でもそのようなことが見られます。全体として、英語の論理テスト時のスケーリングの言語間一般化の可能性、機構、制限を示し、英語中心的なRLMは資源豊富な言語で論理することを推奨し、低リソース言語および外れ場合の論理を改善するためには進める必要があることを結論します。",
      "upvotes": 3,
      "discussionId": "681daba2e3775056736651f9",
      "ai_keywords": [
        "reasoning language models (RLMs)",
        "long chain-of-thoughts (CoTs)",
        "multilingual mathematical reasoning",
        "low-resource languages",
        "quote-and-think pattern",
        "scaling up inference compute",
        "high-resource languages",
        "out-of-domain reasoning generalization",
        "STEM",
        "cultural commonsense knowledge",
        "crosslingual generalization",
        "test-time scaling"
      ]
    },
    "publishedAt": "2025-05-08T12:50:06.000Z",
    "title": "Crosslingual Reasoning through Test-Time Scaling",
    "summary": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05408.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61424bf4f0d914a5f606a823",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
      "fullname": "Yong Zheng-Xin",
      "name": "yongzx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.19314",
      "authors": [
        {
          "_id": "681d89e6d025518b321f67ce",
          "user": {
            "_id": "6673cf668d570d59b83511cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
            "isPro": false,
            "fullname": "Peilin Zhou",
            "user": "PALIN2018",
            "type": "user"
          },
          "name": "Peilin Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:01:42.666Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67cf",
          "name": "Bruce Leon",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d0",
          "user": {
            "_id": "64489ca21d52a633c8f55aba",
            "avatarUrl": "/avatars/5199a5e93161c61d14ec13f79dd8c2c5.svg",
            "isPro": false,
            "fullname": "Xiang Ying",
            "user": "MindYing",
            "type": "user"
          },
          "name": "Xiang Ying",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-09T04:51:51.507Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d1",
          "name": "Can Zhang",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d2",
          "name": "Yifan Shao",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d3",
          "user": {
            "_id": "636dfa6193d9a0c987d41b73",
            "avatarUrl": "/avatars/14396c8beb376b0d3c27a23fadaeb15e.svg",
            "isPro": false,
            "fullname": "Qichen YE",
            "user": "yeeeqichen99",
            "type": "user"
          },
          "name": "Qichen Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:02:33.873Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d4",
          "name": "Dading Chong",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d5",
          "user": {
            "_id": "66bc683f432c73a183ef787c",
            "avatarUrl": "/avatars/9505a1e6131093a91d0454e50bcbba00.svg",
            "isPro": false,
            "fullname": "Zhiling Jin",
            "user": "HawkFaust",
            "type": "user"
          },
          "name": "Zhiling Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:02:48.180Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d6",
          "name": "Chenxuan Xie",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d7",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d8",
          "name": "Yuxin Gu",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d9",
          "name": "Sixin Hong",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67da",
          "name": "Jing Ren",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67db",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67dc",
          "name": "Chao Liu",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67dd",
          "name": "Yining Hua",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/NJsFys1PlInj9W3E7pD8t.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/gJJ8b6rc6njgCz7Fepbbd.png"
      ],
      "publishedAt": "2025-04-27T17:32:43.000Z",
      "submittedOnDailyAt": "2025-05-09T03:24:22.739Z",
      "title": "ブロードウェイコンペ-ZH: 大語言モデルの中国語ブロードウェイ能力のベンチマーク",
      "submittedOnDailyBy": {
        "_id": "6673cf668d570d59b83511cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
        "isPro": false,
        "fullname": "Peilin Zhou",
        "user": "PALIN2018",
        "type": "user"
      },
      "summary": "このエンドプログラムの応答は、ユーザーの要求に従って、指定された言語に翻訳された結果を返します。この場合、英語から日本語への翻訳は以下のようになります：\n\n「LLM がツール使用アガントとして進化する際、時間別にウェブをブラウザーする能力は、その論理的および検索能力を評価するための重要な指標となっています。現在のベンチマークの例として、BrowseComp は英語を中心に焦点を当てて、その他の主要な情報エコシステムの言語的、インフラ構造的、および強制編集関係の複雑性を無視しています。この空間を填ぐために、BrowseComp-ZH を紹介します。BrowseComp-ZH は、中国のウェブ上での LLM アガントの評価を全体的に行うために特に開発された高難易度ベンチマークです。BrowseComp-ZH は、11 種類の分野を拡りだした 289 個の多段階質問からなり、各質問は短い、客観的な、そして簡単に確認できる答えから逆工程されています（例：日付、数値、固有名詞など）。二段階の品質管理プロトコルを適用し、高い質問の難易度と答えの異なりを追求しています。この提案の BrowseComp-ZH において、20 つ以上の最先端の言語モデルとアガントサーチシステムをベンチマークしました。そのほとんどのモデルは強力なコンバーショナルおよび検索能力を持っているにも関わらず、厳しい試練を受けます：多数のモデルは正確率が 10% 未満で、その少数は 20% を超えます。最も優れたシステムである OpenAI の DeepResearch は、そのまま 42.9% を達成します。これらの結果は、BrowseComp-ZH の大きな難易度を示し、成功には有効な検索戦略だけでなく、複雑な論理的および情報の整合を求める能力が必要となることを示しています。このデータセット、構築ガイドライン、ベンチマーク結果は、https://github.com/PALIN2018/BrowseComp-ZH で公開しています。」",
      "upvotes": 3,
      "discussionId": "681d89e7d025518b321f6807",
      "githubRepo": "https://github.com/PALIN2018/BrowseComp-ZH",
      "ai_keywords": [
        "tool-using agents",
        "multihop questions",
        "information ecosystems",
        "quality control protocol",
        "reasoning and information reconciliation"
      ]
    },
    "publishedAt": "2025-04-27T13:32:43.000Z",
    "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language\n  Models in Chinese",
    "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/NJsFys1PlInj9W3E7pD8t.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/gJJ8b6rc6njgCz7Fepbbd.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19314.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6673cf668d570d59b83511cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
      "fullname": "Peilin Zhou",
      "name": "PALIN2018",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05288",
      "authors": [
        {
          "_id": "681d9dd229119d666079b275",
          "user": {
            "_id": "63a3170f8c0c89dcae316858",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
            "isPro": false,
            "fullname": "Ahmed Abdelreheem",
            "user": "Samir55",
            "type": "user"
          },
          "name": "Ahmed Abdelreheem",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:06.667Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b276",
          "user": {
            "_id": "66eae6491cd3794eb4cd1992",
            "avatarUrl": "/avatars/5802de373ccc815f68b98b320aa787bf.svg",
            "isPro": false,
            "fullname": "Filippo Aleotti",
            "user": "Filippo8",
            "type": "user"
          },
          "name": "Filippo Aleotti",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:46.129Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b277",
          "user": {
            "_id": "63166685f5e32157c51fe616",
            "avatarUrl": "/avatars/5b7d8b0e54339d2dc982676af9e4f4fe.svg",
            "isPro": false,
            "fullname": "Jamie Watson",
            "user": "Aileron",
            "type": "user"
          },
          "name": "Jamie Watson",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:52.606Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b278",
          "user": {
            "_id": "6703aece547acbd64c531b72",
            "avatarUrl": "/avatars/8042f99c6eb2c9b61be8c9b950818b2f.svg",
            "isPro": false,
            "fullname": "Zawar Qureshi",
            "user": "zuluquebec",
            "type": "user"
          },
          "name": "Zawar Qureshi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:04:58.663Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b279",
          "user": {
            "_id": "6577f3bacfb2207f11e847bb",
            "avatarUrl": "/avatars/825998cfebc47d8106f633be5ad10964.svg",
            "isPro": false,
            "fullname": "Abdelrahman Eldesokey",
            "user": "abdo-eldesokey",
            "type": "user"
          },
          "name": "Abdelrahman Eldesokey",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:05:05.828Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27a",
          "name": "Peter Wonka",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27b",
          "name": "Gabriel Brostow",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27c",
          "user": {
            "_id": "67f517dcbd75b1099bba2857",
            "avatarUrl": "/avatars/a1a25d7972b1857f8bb49bc9efc02ded.svg",
            "isPro": false,
            "fullname": "Sara Vicente",
            "user": "svicente",
            "type": "user"
          },
          "name": "Sara Vicente",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:05:21.990Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27d",
          "name": "Guillermo Garcia-Hernando",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T14:29:11.000Z",
      "submittedOnDailyAt": "2025-05-09T04:50:18.526Z",
      "title": "PlaceIt3D: リアル3Dスキームでの言語ガイドされたオブジェクト配置",
      "submittedOnDailyBy": {
        "_id": "63a3170f8c0c89dcae316858",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
        "isPro": false,
        "fullname": "Ahmed Abdelreheem",
        "user": "Samir55",
        "type": "user"
      },
      "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D Scenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual prompt broadly describing where the 3D asset should be placed. The task here is to find a valid placement for the 3D asset that respects the prompt. Compared with other language-guided localization tasks in 3D scenes such as grounding, this task has specific challenges: it is ambiguous because it has multiple valid solutions, and it requires reasoning about 3D geometric relationships and free space. We inaugurate this task by proposing a new benchmark and evaluation protocol. We also introduce a new dataset for training 3D LLMs on this task, as well as the first method to serve as a non-trivial baseline. We believe that this challenging task and our new benchmark could become part of the suite of benchmarks used to evaluate and compare generalist 3D LLM models.",
      "upvotes": 2,
      "discussionId": "681d9dd529119d666079b348",
      "projectPage": "https://nianticlabs.github.io/placeit3d/",
      "githubRepo": "https://github.com/nianticlabs/placeit3d",
      "ai_keywords": [
        "point cloud",
        "3D asset",
        "textual prompt",
        "3D LLMs",
        "bounding",
        "grounding",
        "3D geometric relationships",
        "free space",
        "evaluation protocol",
        "benchmark"
      ]
    },
    "publishedAt": "2025-05-08T10:29:11.000Z",
    "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
    "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05288.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a3170f8c0c89dcae316858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
      "fullname": "Ahmed Abdelreheem",
      "name": "Samir55",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03422",
      "authors": [
        {
          "_id": "681db58b1f1c39ba8fbe0162",
          "user": {
            "_id": "681db120007a2d4056d25c70",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
            "isPro": false,
            "fullname": "yepeng liu",
            "user": "pengliu123",
            "type": "user"
          },
          "name": "Yepeng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T08:33:21.362Z",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0163",
          "name": "Wenpeng Lai",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0164",
          "name": "Zhou Zhao",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0165",
          "name": "Yuxuan Xiong",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0166",
          "name": "Jinchi Zhu",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0167",
          "name": "Jun Cheng",
          "hidden": false
        },
        {
          "_id": "681db58b1f1c39ba8fbe0168",
          "name": "Yongchao Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T10:59:23.000Z",
      "submittedOnDailyAt": "2025-05-09T08:38:34.519Z",
      "title": "LiftFeat: 3Dギィエメトリーに関係した局所特徴マッチング",
      "submittedOnDailyBy": {
        "_id": "681db120007a2d4056d25c70",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
        "isPro": false,
        "fullname": "yepeng liu",
        "user": "pengliu123",
        "type": "user"
      },
      "summary": "ロバストで効率的な局所特徴量のマッチングは、ロボットのSLAMと視覚定位などのアプリケーションに重要な役割を果たします。進歩がありますが、照明の変化が激しい、低エッジの場所、または再現するパターンがある場合には、ロバストで区別可能な視覚特徴量の抽出は非常に難しいです。本論文では、新しい軽量ネットワークを提案しています。これは、3Dジェモトリック特徴量を集約して元のデシカートのロバスト性を高めるLiftFeatと呼ばれています。特に、まずは学習済みのモノカメラの深さ推定モデルを使用して、推定された表面正規線を生成し、それによって予測された表面正規線に基づいた3Dジェモトリック特徴量の抽出をサブジェクトします。次に、3Dジェモトリックに関知した特徴量のアップローディングモジュールを設計し、表面正規線特徴量と元の2Dデシカート特徴量を融合します。このような3Dジェモトリック特徴量の統合により、極端な条件で2D特徴量の識別力を高めます。相対的な姿勢推定、ホモグラフィー推定、視覚定位の試験結果から、我々のLiftFeatは軽量な最先端の方法を超えることを示しています。コードは以下のURLで公開されます：https://github.com/lyp-deeplearning/LiftFeat。",
      "upvotes": 2,
      "discussionId": "681db58d1f1c39ba8fbe01fb",
      "githubRepo": "https://github.com/lyp-deeplearning/LiftFeat",
      "ai_keywords": [
        "monocular depth estimation model",
        "pseudo surface normal label",
        "3D geometric feature",
        "surface normal feature",
        "2D descriptor feature",
        "3D geometry-aware feature lifting module",
        "relative pose estimation",
        "homography estimation"
      ]
    },
    "publishedAt": "2025-05-06T06:59:23.000Z",
    "title": "LiftFeat: 3D Geometry-Aware Local Feature Matching",
    "summary": "Robust and efficient local feature matching plays a crucial role in\napplications such as SLAM and visual localization for robotics. Despite great\nprogress, it is still very challenging to extract robust and discriminative\nvisual features in scenarios with drastic lighting changes, low texture areas,\nor repetitive patterns. In this paper, we propose a new lightweight network\ncalled LiftFeat, which lifts the robustness of raw descriptor by\naggregating 3D geometric feature. Specifically, we first adopt a pre-trained\nmonocular depth estimation model to generate pseudo surface normal label,\nsupervising the extraction of 3D geometric feature in terms of predicted\nsurface normal. We then design a 3D geometry-aware feature lifting module to\nfuse surface normal feature with raw 2D descriptor feature. Integrating such 3D\ngeometric feature enhances the discriminative ability of 2D feature description\nin extreme conditions. Extensive experimental results on relative pose\nestimation, homography estimation, and visual localization tasks, demonstrate\nthat our LiftFeat outperforms some lightweight state-of-the-art methods. Code\nwill be released at : https://github.com/lyp-deeplearning/LiftFeat.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03422.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "681db120007a2d4056d25c70",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XyX4OSVmH0zv9qQjFSMGd.png",
      "fullname": "yepeng liu",
      "name": "pengliu123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.04955",
      "authors": [
        {
          "_id": "681dc1b7965798cccfeab83c",
          "user": {
            "_id": "654cca3fe1b4cd6d40d5a7ae",
            "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
            "isPro": false,
            "fullname": "Fangwei Zhu",
            "user": "soliz1998",
            "type": "user"
          },
          "name": "Fangwei Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:07:17.372Z",
          "hidden": false
        },
        {
          "_id": "681dc1b7965798cccfeab83d",
          "user": {
            "_id": "656873f33fd0bf1f82558695",
            "avatarUrl": "/avatars/7a085da2e2a91d7f41988501a573ebf9.svg",
            "isPro": false,
            "fullname": "PEIYI, WANG",
            "user": "peiyiwang89",
            "type": "user"
          },
          "name": "Peiyi Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-09T09:07:23.247Z",
          "hidden": false
        },
        {
          "_id": "681dc1b7965798cccfeab83e",
          "name": "Zhifang Sui",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/654cca3fe1b4cd6d40d5a7ae/omgv1F-oog_5ohzEFDbns.png"
      ],
      "publishedAt": "2025-05-08T05:32:36.000Z",
      "submittedOnDailyAt": "2025-05-09T07:21:04.391Z",
      "title": "Chain-of-Thought Tokensはコンピュータープログラムの変数です。",
      "submittedOnDailyBy": {
        "_id": "654cca3fe1b4cd6d40d5a7ae",
        "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
        "isPro": false,
        "fullname": "Fangwei Zhu",
        "user": "soliz1998",
        "type": "user"
      },
      "summary": "Chain-of-thoughts (CoT)は、最終的な答えに達する前に中間的なステップを生成するための大規模な言語モデル（LLMs）を必要とし、複雑な理由論的仕組みの問題を解決するために効果的であることが証明されています。しかし、CoTの内側の機構はほとんど不明であります。本論文では、2つの組み合わせタスクについて、LLMsにおけるCoTトークンの役割を実験的に研究します：多桁の乗算と動的計画法。CoTはこれらの問題を解決するために不可欠であるが、中間的な結果を保存するだけで比較的性能を達成することができることを見出しました。また、中間的な結果を代替的な潜在的な形式で保存することはモデルの性能に影響を与えません。また、CoTの中で値をランダムに干渉した場合、その後のCoTトークンと最終的な答えが対応して変化することが見出されました。これらの発見は、CoTトークンがコンピュータプログラミングの変数と同様に機能する可能性を示し、無意図的なシンプルなポラードとトークン間の計算量の制限などの潜在的な欠点を含むことを示します。コードとデータは、https://github.com/solitaryzero/CoTs_are_Variables に公開されています。",
      "upvotes": 0,
      "discussionId": "681dc1b8965798cccfeab86d",
      "ai_keywords": [
        "chain-of-thought (CoT)",
        "large language models (LLMs)",
        "intermediate steps",
        "reasoning tasks",
        "inner mechanism",
        "CoT tokens",
        "compositional tasks",
        "multi-digit multiplication",
        "dynamic programming",
        "intermediate results",
        "latent form",
        "variables",
        "unintended shortcuts",
        "computational complexity limits"
      ]
    },
    "publishedAt": "2025-05-08T01:32:36.000Z",
    "title": "Chain-of-Thought Tokens are Computer Program Variables",
    "summary": "Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/654cca3fe1b4cd6d40d5a7ae/omgv1F-oog_5ohzEFDbns.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654cca3fe1b4cd6d40d5a7ae",
      "avatarUrl": "/avatars/4d09531277e16ad71474fc888c16b227.svg",
      "fullname": "Fangwei Zhu",
      "name": "soliz1998",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]