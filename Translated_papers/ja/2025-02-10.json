[
  {
    "paper": {
      "id": "2502.05173",
      "authors": [
        {
          "_id": "67a97a47174028234b74f687",
          "name": "Xilin Wei",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f688",
          "user": {
            "_id": "64f033ef82c6eea604c4da8b",
            "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
            "isPro": false,
            "fullname": "Liu Xiaoran",
            "user": "LiuXR",
            "type": "user"
          },
          "name": "Xiaoran Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:59.999Z",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f689",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:50:02.011Z",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68a",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68b",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68c",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68d",
          "name": "Jian Tong",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68e",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f68f",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f690",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f691",
          "name": "Xipeng Qiu",
          "hidden": false
        },
        {
          "_id": "67a97a47174028234b74f692",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:56:04.000Z",
      "title": "VideoRoPE: どのような条件が良いビデオロタリーポジションエンベッディングを成すか？",
      "summary": "ローテイションポジションエビデンディング（RoPE）およびその変体は、長ケースコンテキストの能力をもつために広く採用されていますが、1D RoPEを映像に拡張し、複雑な空間時系列構造を持つ映像においては、開放的な挑戦として残っています。本研究は、RoPEの効果的な応用に必要な4つのキー的な特徴を識別し、先行研究では完全に考慮されていないものを調査します。分析の一部では、V-NIAH-D（ビジュアルニードル・イン・ア・ハイスタックワイズ・ダイストラクター）の挑戦を提案します。これはV-NIAHに周期的なダイストラクターを追加しています。V-NIAH-Dの任務は、先行のRoPEの変体が適切な時系列次元割り当てを欠けているために、ダイストラクターに容易に誤って導かれることを示します。分析に基づいて、VideoRoPEを提案します。VideoRoPEは、空間時系列関係を保持するために3D構造を設計しています。VideoRoPEは、周期的な振動を抑えるために低周波数の時系列割り当て、空間対称性を維持するための斜め配置、時系列と空間インデックスを離れるための可変な時系列スペースを特徴としています。VideoRoPEは、長ケースビデオ検索、ビデオ理解、ビデオハネラティブなどの多様な下流タスクで、先行のRoPEの変体を一致しています。コードは、https://github.com/Wiselnn570/VideoRoPE{https://github.com/Wiselnn570/VideoRoPE}で提供されます。",
      "upvotes": 32,
      "discussionId": "67a97a4a174028234b74f707"
    },
    "publishedAt": "2025-02-09T23:03:21.947Z",
    "title": "VideoRoPE: What Makes for Good Video Rotary Position Embedding?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05173.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04507",
      "authors": [
        {
          "_id": "67a98cd1b8b21202c9004628",
          "name": "Peiyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c9004629",
          "user": {
            "_id": "65416817271d3bc4d70f6745",
            "avatarUrl": "/avatars/55cc24918c62ab39540c4df813b026ef.svg",
            "isPro": false,
            "fullname": "Yongqi Chen",
            "user": "BrianChen1129",
            "type": "user"
          },
          "name": "Yongqi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:48.410Z",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462a",
          "name": "Runlong Su",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462b",
          "name": "Hangliang Ding",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462c",
          "name": "Ion Stoica",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462d",
          "name": "Zhenghong Liu",
          "hidden": false
        },
        {
          "_id": "67a98cd1b8b21202c900462e",
          "name": "Hao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T21:17:09.000Z",
      "title": "スライディングタイルアテンションを用いた高速ビデオ生成",
      "summary": "Diffusion Transformers (DiTs) は、3D 全注意力を使用して最先端のビデオ生成を実現しますが、計算コストが高得ています。5秒間の720Pビデオを生成するとき、全注意力のみが945秒の計算時間の800秒を占めます。本論文では、スライディングタイルアテンション（STA）を紹介し、この課題を解決します。STAは、事前学習されたビデオディフュージョンモデルでは、アテンションスコアが主に局在的な3Dウィンドウ内に集中していることを利用します。スライディングして局在的スペクトラル時間領域にアテンションを適用することで、全注意力からの冗長を削減します。従来のトークンごとのスライディングウィンドウアテンション（SWA）に違い、STAは新しいハードウェア認識のスライディングウィンドウ設計を用いてティリーごとに動作し、表現力を保持しながらハードウェアエフィシェントです。認識キャンバスレベルの調整を行い、STAは最初の効率的な2D/3Dスライディングウィンドウようなアテンション実装を提供し、58.79%のMFUを達成します。具けては、STAはFlashAttention-2（FA2）を2.8～17倍、FlashAttention-3（FA3）を1.6～10倍高速化します。先進的なビデオDiTでは、STAは終わりから終わりまでのラテンティーを945秒から685秒に抑え、質変化なしで実行されます。訓練せずに実行できます。ファイナルチューニングを行うことで、ラテンティーを268秒に抑え、VBenchのディフュージョンのみ0.09%のディフレースを許容します。",
      "upvotes": 30,
      "discussionId": "67a98cd7b8b21202c90047c5"
    },
    "publishedAt": "2025-02-10T00:22:26.568Z",
    "title": "Fast Video Generation with Sliding Tile Attention",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63565cc56d7fcf1bedb7d347",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63565cc56d7fcf1bedb7d347/XGcHP4VkO_oieA1gZ4IAX.jpeg",
      "fullname": "Zhang Peiyuan",
      "name": "PY007",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 80
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05176",
      "authors": [
        {
          "_id": "67a9889dc1fbde5146aba8b1",
          "name": "Chung-Ho Wu",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b2",
          "name": "Yang-Jung Chen",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b3",
          "name": "Ying-Huan Chen",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b4",
          "name": "Jie-Ying Lee",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b5",
          "name": "Bo-Hsu Ke",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b6",
          "name": "Chun-Wei Tuan Mu",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b7",
          "name": "Yi-Chuan Huang",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b8",
          "name": "Chin-Yang Lin",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8b9",
          "user": {
            "_id": "64ae22dd1aee69ece065cdcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
            "isPro": false,
            "fullname": "Min-Hung Chen",
            "user": "cmhungsteve",
            "type": "user"
          },
          "name": "Min-Hung Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:50.370Z",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8ba",
          "name": "Yen-Yu Lin",
          "hidden": false
        },
        {
          "_id": "67a9889dc1fbde5146aba8bb",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:59:55.000Z",
      "title": "アウラフュージョン360: 参考に基づく360°無制限スケーンインプレーションの見えない領域のアライメント",
      "summary": "3Dスケーンインパインティングは、バーチュアルリアリティから建築可視化までの様々なアプリケーションに重要ですが、現在の方法は360度無制限スケーンでの視点一致性と幾何精度に苦戦しています。私たちは、グアシャンスプラッティングで表現される3Dスケーンでの高品質なオブジェクト削除と穴埋めを可能にする新しい参照ベースの方法、AuraFusion360を紹介します。私たちのアプローチは、(1) 測定した隠れ部分の生成、正確な遮蔽識別に向けた、(2) アダプティブガイドデプスディフュージョン、追加のトレーニングを必要とさせない正確な初期点の設定のゼロショット方法、(3) SDEditに基づく詳細拡大、多視点の一致性を確保する方法を導入します。また、360-USID、360度無制限スケーンインパインティングの真実データを含む最初の詳細なデータセットを紹介します。広範囲の実験により、AuraFusion360は現在の方法を大幅に超え、視点変化に伴う場所精度を維持する同時に、上品な視覚品質を達成します。プロジェクトページにおいて、ビデオ結果とデータセットを見ることができます。",
      "upvotes": 18,
      "discussionId": "67a988a4c1fbde5146abaa3b"
    },
    "publishedAt": "2025-02-10T00:05:28.205Z",
    "title": "AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/KMKt5j_3UB0zDhxjSiyxI.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05176.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6459d5da3b6fafd9664807ab",
      "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
      "fullname": "Yu-Lun Liu",
      "name": "yulunliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04896",
      "authors": [
        {
          "_id": "67a983ea9b72585dd12587fb",
          "user": {
            "_id": "6412a33900634c4fe9873652",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
            "isPro": false,
            "fullname": "Shoufa Chen",
            "user": "ShoufaChen",
            "type": "user"
          },
          "name": "Shoufa Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:52.136Z",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd12587fc",
          "name": "Chongjian Ge",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd12587fd",
          "name": "Yuqi Zhang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd12587fe",
          "name": "Yida Zhang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd12587ff",
          "name": "Fengda Zhu",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258800",
          "name": "Hao Yang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258801",
          "name": "Hongxiang Hao",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258802",
          "name": "Hui Wu",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258803",
          "name": "Zhichao Lai",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258804",
          "name": "Yifei Hu",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258805",
          "name": "Ting-Che Lin",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258806",
          "name": "Shilong Zhang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258807",
          "name": "Fu Li",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258808",
          "name": "Chuan Li",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258809",
          "name": "Xing Wang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880a",
          "name": "Yanghua Peng",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880b",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880c",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880d",
          "name": "Yi Jiang",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880e",
          "name": "Zehuan Yuan",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd125880f",
          "name": "Bingyue Peng",
          "hidden": false
        },
        {
          "_id": "67a983ea9b72585dd1258810",
          "name": "Xiaobing Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T13:03:55.000Z",
      "title": "悟空: フローベースビデオ生成性基盤モデル",
      "summary": "この論文では、最新のフレームワークであるGokuを紹介します。Gokuは、正規化フローTransformersを使用して画像と動画の生成を共に行う最先端なモデルシリーズで、業界最高の性能を実現しています。高品質な可視化生成を可能にする基盤的な要素を詳細に説明します。その中には、データカレーティングパイプライン、モデルアーキテクチャ設計、フローの公式化、および大規模なトレーニングの効率的かつ強固なインフラを含むものです。Gokuモデルは、質的的かつ量的評価で上位の性能を示し、主なタスクの全てに新たなベンチマークを設定しています。特に、GenEvalでは0.76、DPG-Benchでは83.65、VBenchでは84.85のスコアを達成しました。私たちは、この研究において、コンビニエーション画像と動画の生成モデルの開発において有効なヒントと実用的な進展を提供していると信じています。",
      "upvotes": 14,
      "discussionId": "67a983ee9b72585dd125890f"
    },
    "publishedAt": "2025-02-09T23:43:39.239Z",
    "title": "Goku: Flow Based Video Generative Foundation Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04896.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05003",
      "authors": [
        {
          "_id": "67a9b1a69a99341e859c488d",
          "user": {
            "_id": "623753b5eddd7763adc9346a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
            "isPro": false,
            "fullname": "Andrei Panferov",
            "user": "BlackSamorez",
            "type": "user"
          },
          "name": "Andrei Panferov",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-10T08:09:18.686Z",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c488e",
          "name": "Jiale Chen",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c488f",
          "user": {
            "_id": "632a2e325f2ff1958c0103be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632a2e325f2ff1958c0103be/Tb0ql9e4LcaFktTK1hzqe.jpeg",
            "isPro": false,
            "fullname": "Soroush Tabesh",
            "user": "soroushtabesh",
            "type": "user"
          },
          "name": "Soroush Tabesh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:37.573Z",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c4890",
          "name": "Roberto L. Castro",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c4891",
          "user": {
            "_id": "6526b8ebba9a8279c139616b",
            "avatarUrl": "/avatars/09f6b677603a03be128996a0765233e6.svg",
            "isPro": false,
            "fullname": "Mahdi Nikdan",
            "user": "mnikdan97",
            "type": "user"
          },
          "name": "Mahdi Nikdan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:50:25.944Z",
          "hidden": false
        },
        {
          "_id": "67a9b1a69a99341e859c4892",
          "user": {
            "_id": "64ef52c2718f94ae8e78a5e7",
            "avatarUrl": "/avatars/d169f4ee62786a3eb4a3fa9d1fec52e9.svg",
            "isPro": false,
            "fullname": "Alistarh",
            "user": "d-alistarh",
            "type": "user"
          },
          "name": "Dan Alistarh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:35.449Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T15:23:34.000Z",
      "title": "クエスト: 1ビットの重みと活性化を用いたLLMの安定化トレーニング",
      "summary": "1つの大規模言語モデル（LLMs）の巨大な費用を減少する方法は、トレーニングまたは実装時に使用する量子化またはスパース表現の利用です。トレーニング後の圧縮メソッドは非常に人気がありますが、このような表現を直接トレーニングしてより正確な圧縮モデルを得ることができるかどうかの問題はまだ開いています：例えば、最近の研究（arXiv:2411.04330v2）は、QAT（Quantization-Aware Training）でモデルをトレーニングできる「最適」ビット幅が、標準のFP16/BF16精度と精度的な比較であるものとして8ビットの重みと活性化を使用することを示しています。\n\n私たちは、新しい方法として呼ばれるQuESTを使用してこの最先端を進めています。これはFP16とパロードード的に比較してより良い精度を提供し、モデルサイズを低くすることで、重みと活性化を4ビット以下にすることができます。また、QuESTは1ビットの重みと活性化を使用して安定したトレーニングを可能にします。QuESTは、QATメソッドの2つのキーの面で改善してこれを実現しています：（1）Hadamard正規化とMSE最適化での（続き）分布の正確かつ高速な量子化；（2）量子化状態で計算されるノイズのない勾配と「真の」（しかし未知の）全精度勾配の誤差を明示的に最小化する新しい信頼勾配評価器の基にしています。Llamaタイプのアーキテクチャに対する実験は、QuESTが実行できるモデルを効率的に実行できることを示し、これはスパース表現に拡張できます。GPUキーボードサポートを提供して、QuESTで生成されたモデルは効率的に実行できることを示します。コードはhttps://github.com/IST-DASLab/QuESTに提供されています。",
      "upvotes": 11,
      "discussionId": "67a9b1a79a99341e859c48c7"
    },
    "publishedAt": "2025-02-10T03:00:12.065Z",
    "title": "QuEST: Stable Training of LLMs with 1-Bit Weights and Activations",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05003.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ef52c2718f94ae8e78a5e7",
      "avatarUrl": "/avatars/d169f4ee62786a3eb4a3fa9d1fec52e9.svg",
      "fullname": "Alistarh",
      "name": "d-alistarh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05163",
      "authors": [
        {
          "_id": "67a9604851169a582d14c113",
          "user": {
            "_id": "642f4c789b2484d7d8551a93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
            "isPro": true,
            "fullname": "Yihe Deng",
            "user": "ydeng9",
            "type": "user"
          },
          "name": "Yihe Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:50:06.136Z",
          "hidden": false
        },
        {
          "_id": "67a9604851169a582d14c114",
          "name": "Yu Yang",
          "hidden": false
        },
        {
          "_id": "67a9604851169a582d14c115",
          "name": "Junkai Zhang",
          "hidden": false
        },
        {
          "_id": "67a9604851169a582d14c116",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67a9604851169a582d14c117",
          "name": "Bo Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:45:03.000Z",
      "title": "デュオガード：2人用RLドライバンドフレームワークの多言語LLMガードライズ",
      "summary": "大語言モデル（LLMs）の急速な進歩に伴い、責任的な使用を確保するためのガーディラインモデルの必要性が増加しています。特に、不安全なおよび非法な内容の検出においてもその重要性が高まります。英語では安全データが豊富であることは知られていますが、他の言語での開放ソース安全データの不足により、多言語ガーディラインモデルの研究はまだ見られません。この空白を填ぐために、私たちは新しい2人のプレイヤーの強化認識学習（RL）フレームワークを提案します。このフレームワークでは、生成器とガーディラインモデルが相互に対抗的に進化し、多言語ガーディライントレーニングに向けた高品質の合成データを生成することを目指しています。この相互作用を理論的に正規化し、ネーシャス均衡に収束することを証明しました。実験的な評価により、私たちのモデル \\ours は最先端のモデルを上回り、LlamaGuard3（8B）に対して近似10%の改善を収め、推論速度が4.5倍速く、モデルサイズがわずか0.5Bです。多言語安全タスクにおいては、特に収集された実際のデータでの資源豊かな言語の不平衡を解決することができました。消滅調査は、開放ソースデータの不平衡をコンクリートするための合成データの生成の重要性を強調します。これらの発見は、合成データの生成によるスケーラブルなおよび効率的なアプローチを奨励し、多言語ガーディラインモデルの改良によるLLMの安全性向上につながることを示します。コード、モデル、データは、https://github.com/yihedeng9/DuoGuard で開放ソース化されます。",
      "upvotes": 11,
      "discussionId": "67a9604951169a582d14c14d"
    },
    "publishedAt": "2025-02-10T00:43:32.191Z",
    "title": "DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05163.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642f4c789b2484d7d8551a93",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
      "fullname": "Yihe Deng",
      "name": "ydeng9",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05171",
      "authors": [
        {
          "_id": "67a97e27495b23306cd5ea56",
          "name": "Jonas Geiping",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea57",
          "name": "Sean McLeish",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea58",
          "name": "Neel Jain",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea59",
          "name": "John Kirchenbauer",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5a",
          "name": "Siddharth Singh",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5b",
          "name": "Brian R. Bartoldson",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5c",
          "name": "Bhavya Kailkhura",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5d",
          "name": "Abhinav Bhatele",
          "hidden": false
        },
        {
          "_id": "67a97e27495b23306cd5ea5e",
          "name": "Tom Goldstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:55:02.000Z",
      "title": "テストタイムコンピューティングのスケーリングを隠れ理由論で実現する：リカレントデプスアプローチ",
      "summary": "私たちは、検査時の計算を隠れた空間で実装できる新しい言語モデルアーキテクチャを研究しています。私たちのモデルは、再帰ブロックを反復し、検査時に任意の深さに展開することで動作します。これは、主流の理由モデルと比較して、トークンの生成を通じて計算を拡大することに反しています。私たちのアプローチは、チェーンオブスコートに基づくアプローチと異なり、特殊化されたトレーニングデータが必要とならない、小さなコンテキストウィンドウと共に動作でき、言葉で容易に表現できない理由の種類を捉えることができます。私たちは、証明のモデルを35億パラメータと8000億トークンに拡大しました。私たちは、このようにして得られたモデルが、理由ベンチマークの性能を改善することができ、時には驚異的に改善し、計算負荷が50億パラメータと等しいものを示します。",
      "upvotes": 7,
      "discussionId": "67a97e29495b23306cd5eae5"
    },
    "publishedAt": "2025-02-09T23:19:16.714Z",
    "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05171.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04403",
      "authors": [
        {
          "_id": "67a97c7542d4d2f92ee57d20",
          "name": "David Abel",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d21",
          "name": "André Barreto",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d22",
          "name": "Michael Bowling",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d23",
          "name": "Will Dabney",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d24",
          "name": "Shi Dong",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d25",
          "name": "Steven Hansen",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d26",
          "name": "Anna Harutyunyan",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d27",
          "name": "Khimya Khetarpal",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d28",
          "name": "Clare Lyle",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d29",
          "name": "Razvan Pascanu",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2a",
          "name": "Georgios Piliouras",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2b",
          "name": "Doina Precup",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2c",
          "name": "Jonathan Richens",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2d",
          "name": "Mark Rowland",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2e",
          "name": "Tom Schaul",
          "hidden": false
        },
        {
          "_id": "67a97c7542d4d2f92ee57d2f",
          "name": "Satinder Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T08:34:57.000Z",
      "title": "Agency Is Frame-Dependent\n\nアジェンツはフレーム依存です",
      "summary": "アジェンシーは、システムが目的に向けた結果を制御する機能で、生物学、哲学、認知科学、人工知能の各分野で中心的な研究課題である。アジェンシーを持つかどうかを判断するのは、非常に難しい問題であり、デンネット（1989）は、岩、サイバーグ、ロボットがどれぞれアジェンシーを持つかを判断する原理を決定する謎を指摘している。我々は、強化学習の観点からこの謎を解決する。アジェンシーは根本的にフレーム依存性を持つという主張を前提に、システムのアジェンシーを測定するには参照フレームに対して行う必要があると述べる。この主張を支えるために、バランダラン等（2009）とモレノ（2018）が提案したアジェンシーの基本的な性質のそれぞれがフレーム依存性を持つことを論理的に示している。アジェンシーの基本的な科学は、フレーム依存性を必要とすることを結論し、これらの主張に対する強化学習の影響を議論する。",
      "upvotes": 7,
      "discussionId": "67a97c7642d4d2f92ee57d77"
    },
    "publishedAt": "2025-02-09T23:11:57.959Z",
    "title": "Agency Is Frame-Dependent",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04403.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04520",
      "authors": [
        {
          "_id": "67a97eea96d822bc6e13a1bb",
          "name": "Letian Peng",
          "hidden": false
        },
        {
          "_id": "67a97eea96d822bc6e13a1bc",
          "name": "Chenyang An",
          "hidden": false
        },
        {
          "_id": "67a97eea96d822bc6e13a1bd",
          "name": "Shibo Hao",
          "hidden": false
        },
        {
          "_id": "67a97eea96d822bc6e13a1be",
          "name": "Chengyu Dong",
          "hidden": false
        },
        {
          "_id": "67a97eea96d822bc6e13a1bf",
          "user": {
            "_id": "660655119e3555d648f6c6b5",
            "avatarUrl": "/avatars/ae1e2c97a08be39b77a9f1a5c2a718ef.svg",
            "isPro": false,
            "fullname": "Jingbo Shang",
            "user": "shangjingbo",
            "type": "user"
          },
          "name": "Jingbo Shang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:54.200Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T21:44:30.000Z",
      "title": "LMの構成的広範性と幻想の直線的相関",
      "summary": "言語モデル（LM）の一般化は活性的な議論におり、一般的な知能の可能性と基本的な知識構成の苦戦（例：逆転/移行の黙示録）を比較しています。本論文は、LMの知識構成期の線形相関現象を明らかにします。特に、特定の関連知識間に存在する線形変換が、プロンプトの間で次のトークン予測ロジットをマッピングします。例えば、「X lives in the city of」→「X lives in the country of」というようなものです。これは、人間の知識構成の線形性に似ています。我々の見つけたものは、大規模な微調校に対しても積極的であり、実世界的関係と一致した場合は更新された知識を一般化しますが、偏離する場合はホラーショーンを引き起こします。実験結果は、線形相関がLMの一般化の潜在的な識別子として役立つことを示しています。最後に、この線形相関は単一の前向きネットワークと事前学習されたボキャブラリ表現で学習でき、LMの一般化は後者に大きく依存していることを示しています。",
      "upvotes": 6,
      "discussionId": "67a97eea96d822bc6e13a1e7"
    },
    "publishedAt": "2025-02-09T23:22:06.784Z",
    "title": "Linear Correlation in LM's Compositional Generalization and Hallucination",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04520.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05179",
      "authors": [
        {
          "_id": "67a9901cc0310368e2488929",
          "name": "Shilong Zhang",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892a",
          "name": "Wenbo Li",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892b",
          "user": {
            "_id": "6412a33900634c4fe9873652",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
            "isPro": false,
            "fullname": "Shoufa Chen",
            "user": "ShoufaChen",
            "type": "user"
          },
          "name": "Shoufa Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:46.264Z",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892c",
          "name": "Chongjian Ge",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892d",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892e",
          "name": "Yida Zhang",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e248892f",
          "name": "Yi Jiang",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e2488930",
          "name": "Zehuan Yuan",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e2488931",
          "name": "Binyue Peng",
          "hidden": false
        },
        {
          "_id": "67a9901cc0310368e2488932",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:59:59.000Z",
      "title": "FlashVideo: フローティング・フィデリティーと詳細のフィデリティーを組み合わせて効率的な高解像度ビデオ生成に向けて",
      "summary": "DiT 拡散モデルは、テキストから動画生成において大成功を収め、モデル容量とデータスケールのスケーラビリティを活用しています。しかし、テキストプロンプトに合わせた高い内容と動作の忠実性を実現するには、大きなモデルパラメータと多量的な関数評価（NFEs）が必要です。リアリスティックなおよび視覚的に愛される詳細は通常、高解像度の出力に反映され、特にシングルステージのDiTモデルにおいて計算負担が増加します。これらの課題に対処するために、私たちは新しい2ステージフレームワーク、FlashVideoを提案しています。FlashVideoは、フレームウェーブ内でモデル容量とNFEsを戦略的に割り当てて、生成の忠実性と質をバランスすることを目指しています。1ステージでは、低解像度の生成プロセスを利用して、大きなパラメータと十分なNFEsを使用して計算効率を向上させ、プロンプトの忠実性を優先します。2ステージでは、低解像度と高解像度の間でフローマッチングを設定し、最小限のNFEsで詳細な詳細を生成します。定量的なおよび視覚的な結果は、FlashVideoが最先端の高解像度動画生成を実現し、上位の計算効率を示していることを示しています。また、2ステージの設計は、ユーザーが全解像度生成について確定する前に最初の出力をプレビューできるようにし、計算コストと待ち時間を大幅に減少させ、商業的な可能性を向上させることができます。",
      "upvotes": 5,
      "discussionId": "67a9901ec0310368e24889c2"
    },
    "publishedAt": "2025-02-10T00:35:37.019Z",
    "title": "FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05179.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04363",
      "authors": [
        {
          "_id": "67a98180d0dc1ed664297368",
          "name": "Bosung Kim",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed664297369",
          "name": "Kyuhwan Lee",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed66429736a",
          "name": "Isu Jeong",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed66429736b",
          "name": "Jungmin Cheon",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed66429736c",
          "name": "Yeojin Lee",
          "hidden": false
        },
        {
          "_id": "67a98180d0dc1ed66429736d",
          "name": "Seulki Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T05:42:29.000Z",
      "title": "デバイス上のSora：モバイルデバイスでのディフュージョンベースのテキストからビデオ生成を可能にする",
      "summary": "デバイス上のSoraを紹介します。これは、スマートフォンレベルのデバイスで効率的に動作する、拡散基底のテキストからのビデオ生成の先駆的な解決策です。Open-Soraに基づいて、デバイス上のSoraは、計算とメモリに制限されたモバイルデバイスでの拡散基底のテキストからのビデオ生成の挑戦を3つの新しい技術で解決します。まず、線形比例的跳跃（LPL）は、ビデオ拡散に必要な過剰なデノイズステップを減少するために、効率的な跳跃ベースのアプローチを使用します。次に、時間次元トークンマージング（TDTM）は、注意層での連続したトークンの処理計算を最小化するために、時間次元に沿って連続したトークンをマージします。さらに、並行推論と動的なロード（CI-DL）は、大きなモデルを小さなブロックに動的に分割し、メモリにロードして並行なモデル推論を行い、デバイスメモリの制限を効果的に解決します。iPhone 15 Pro上でOn-device Soraを実装し、実験的評価により、これは高品質のビデオを生成でき、高級GPUで実行されたOpen-Soraと比較的なものを生成できることが示されました。これらの結果は、On-device Soraは資源制限されたモバイルデバイス上で効率的で高品質なビデオ生成を可能にし、アクセス性を拡大し、ユーザープライバリティを確保し、クラウドインフラの依存性を減らし、関連コストを下げることを示します。On-device Soraの提案は、最先端の生成テクノロジーを民主化する重要な最初のステップとして見識され、商品化モバイルおよびインベストドデバイス上でのビデオ生成機能を可能にします。コード実装は、GitHubリポジトリで公開されています：https://github.com/eai-lab/On-device-Sora。",
      "upvotes": 3,
      "discussionId": "67a98185d0dc1ed664297491"
    },
    "publishedAt": "2025-02-09T23:33:13.185Z",
    "title": "On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04363.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04728",
      "authors": [
        {
          "_id": "67a97d1c02da0cdf059cb0d8",
          "name": "Zhouliang Yu",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0d9",
          "name": "Yuhuan Yuan",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0da",
          "name": "Tim Z. Xiao",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0db",
          "name": "Fuxiang Frank Xia",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0dc",
          "name": "Jie Fu",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0dd",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:56.250Z",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0de",
          "name": "Ge Lin",
          "hidden": false
        },
        {
          "_id": "67a97d1c02da0cdf059cb0df",
          "name": "Weiyang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T07:52:25.000Z",
      "title": "テストタイムスケーリングによる大規模言語モデルを通じた符号的な世界モデルの生成",
      "summary": "解決複雑な計画問題には、大語言モデル（LLMs）が状態遷移を明確にモデル化し、ルール違反を避け、制約に従い、最適性を確保する必要があります。この課題は自然言語の固有の不明確性により妨害されています。この不明確性を克服するために、計画領域定義言語（PDDL）が計画抽象化として使用され、正確で形式的な状態記述を可能にします。PDDLを使って、我々は符号的な世界モデルを生成でき、A*などの古典的な探索アルゴリズムを無難に適用して最適な計画を見つけることができます。しかし、現在のLLMsが直接PDDL領域を生成することは、PDDLの訓練データの不足により開放的な課題として残されています。この課題を解決するために、我々はLLMsのテスト時の計算を拡大することを提案し、PDDLの理由能力を高めることで、高品質のPDDL領域の生成を可能にします。特に、我々は簡単で効果的なアルゴリズムを導入し、最初にNの中の最善を選択するアプローチを用いて初期解の質を改善し、口頭化された機械学習で細かく解を精進します。我々の方法は、自然言語記述からPDDL領域を生成するものとPDDL問題からPDDL領域を生成するものの両方で、o1-miniを大幅に超える成果を収め、50%以上の成功率を達成します。これは追加的な訓練が必要としていません。PDDLを状態抽象化として利用することで、我々の方法は、ほとんどすべての競技レベルの計画タスクに対して現在の最先端の方法を超えることができます。",
      "upvotes": 3,
      "discussionId": "67a97d1d02da0cdf059cb11a"
    },
    "publishedAt": "2025-02-09T23:17:42.258Z",
    "title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04728.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04404",
      "authors": [
        {
          "_id": "67a97bc5500b3bcf5babc5e8",
          "user": {
            "_id": "64bb3d1eb1a618880956da76",
            "avatarUrl": "/avatars/ec393b5eee8a3ccec61107b4aa63c4d9.svg",
            "isPro": false,
            "fullname": "Xiao-Wen Yang",
            "user": "yangxw",
            "type": "user"
          },
          "name": "Xiao-Wen Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:57.842Z",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5e9",
          "name": "Xuan-Yi Zhu",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ea",
          "name": "Wen-Da Wei",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5eb",
          "name": "Ding-Chu Zhang",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ec",
          "name": "Jie-Jing Shao",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ed",
          "name": "Zhi Zhou",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ee",
          "name": "Lan-Zhe Guo",
          "hidden": false
        },
        {
          "_id": "67a97bc5500b3bcf5babc5ef",
          "name": "Yu-Feng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T08:52:43.000Z",
      "title": "ステップバックフォローワード：自動リバックトラッキングによる言語モデルの推論力向上",
      "summary": "LLMにおけるショットステミング機構の統合は、例えばOpenAIのo1のようなシステムによって示されるように、レベル2 AGI Reasonersの達成に向けての望ましい道を提供します。しかし、数多くの重要な課題が残っています。その中には、無駄なオーバーサイドコンシューディングとアシステントリベンュードモデルの過度依存があります。私たちは、これらの制限は、LLMが探索プロセスを内部化する能力のないことから原因づけます。この問題を解決するための重要なステップとして、LLMが自動的にバックトラックする時間と場所を決定することを許可することがあります。この点で、私たちはLLMが学習と推論の両方でバックトラックする能力を持つための自動バックトラック機構を提案します。この機構は、自動改善により、ショットステミングプロセスをファストステミングに変換し、理由能力と効率を向上させます。実験的評価により、私たちの提案は、最適パスのサブジェクト調節方法に対して40％以上の性能向上を達成します。私たちは、この研究は、進歩的で強固なReasonersの開発における新しいそばけいきの道を導入していると信じています。",
      "upvotes": 2,
      "discussionId": "67a97bc7500b3bcf5babc64e"
    },
    "publishedAt": "2025-02-09T23:09:01.160Z",
    "title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04404.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04350",
      "authors": [
        {
          "_id": "67a97a77d163c9e6ea2bdb85",
          "name": "Yongchao Chen",
          "hidden": false
        },
        {
          "_id": "67a97a77d163c9e6ea2bdb86",
          "name": "Yilun Hao",
          "hidden": false
        },
        {
          "_id": "67a97a77d163c9e6ea2bdb87",
          "name": "Yueying Liu",
          "hidden": false
        },
        {
          "_id": "67a97a77d163c9e6ea2bdb88",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "67a97a77d163c9e6ea2bdb89",
          "name": "Chuchu Fan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T15:53:59.000Z",
      "title": "CodeSteer: コード/テキストガイドニングによる符号付き言語モデル",
      "summary": "現在の方法は、大規模言語モデル（LLMs）の文字論理とコード生成の間を効果的に制御できないことで、記号計算の能力が過少利用されています。私たちは、CodeSteerという効果的な方法を紹介し、LLMのコード/文字生成をガイドすることを目的としています。私たちは、37つの記号的なタスクを含む詳細なベンチマークSymBenchを構築し、12kの多段階ガイド/生成トラジェクトと5.5kのガイド比較パースを合成しました。新設計された多段階サブプロジェクト監督学習（SFT）と直接な好み最適化（DPO）を用いてLlama-3-8Bモデルを微調校しました。結果として得られたモデル、CodeSteerLLMは、提案された記号的および自動回答チェッカーを付加し、大きなモデルのコード/文字生成を効果的にガイドすることができます。GPT-4oにCodeSteerを追加したことで、平均性能スコアは53.3から86.4に上がり、すべての37タスク（28つの見た、9つの見ぬ）で現在の最良のLLM OpenAI o1（82.7）、o1-preview（74.8）、DeepSeek R1（76.8）を上回りました。GPT-4oを訓練したCodeSteerは、Claude、Mistral、GPT-3.5に対して平均41.8の性能向上を示します。CodeSteerをガイドするLLMは、高度な複雑なタスクでも強い性能を維持し、記号計算を完全に活用しています。モデル、データセット、コードは以下のURLから利用できます：https://github.com/yongchao98/CodeSteer-v1.0。",
      "upvotes": 2,
      "discussionId": "67a97a79d163c9e6ea2bdc0c"
    },
    "publishedAt": "2025-02-09T23:03:14.294Z",
    "title": "CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04350.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5997
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04689",
      "authors": [
        {
          "_id": "67a9b911b1f5eece682d7961",
          "user": {
            "_id": "64510a21f800611f94f0d9f8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lOeHK9Bvt3IXcB7Urx6jZ.jpeg",
            "isPro": false,
            "fullname": "Yuwei Yin",
            "user": "yuweiyin",
            "type": "user"
          },
          "name": "Yuwei Yin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:32.672Z",
          "hidden": false
        },
        {
          "_id": "67a9b911b1f5eece682d7962",
          "name": "Giuseppe Carenini",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T06:30:33.000Z",
      "title": "ARR: 大規模言語モデルを用いた質問回答における分析、検索、理由論理",
      "summary": "大語言モデル（LLMs）は、多選問答（QA）タスクのような構造化された難しいベンチマークで驚異的な性能を達成します。Zero-shot Chain-of-Thought（CoT）プロンプティングはLLMsの理由を強化しますが、それは「1つずつ考える」のようなバグラスと一般的な指導に限ります。本論文では、QA解決の3つのキーステップを明記した直感的で効果的なZero-shotプロンプティング方法、ARRを紹介します。これらのステップは、問題の意図を分析、関連情報を検索、ステップごとの理由を与えます。多様な難しいQAタスクの幅広い実験は、ARRがBaseline（ARRプロンプティングを含まない）を一貫的に改善し、CoTを超えることを示します。消去実験と事例研究は、分析、検索、理由の各コンポーネントの正の貢献を進一度に証明します。特に、ARRでは意図分析が重要な役割を果たします。また、モデルサイズ、LLMシリーズ、生成設定の幅広い範囲での拡張評価は、ARRの効果性、強固性、一般化能力を確立します。",
      "upvotes": 1,
      "discussionId": "67a9b911b1f5eece682d798c"
    },
    "publishedAt": "2025-02-10T03:30:51.974Z",
    "title": "ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04689.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64510a21f800611f94f0d9f8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lOeHK9Bvt3IXcB7Urx6jZ.jpeg",
      "fullname": "Yuwei Yin",
      "name": "yuweiyin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.03512",
      "authors": [
        {
          "_id": "67a9a7cb6be3ca4a7ede471e",
          "name": "Amitava Das",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede471f",
          "name": "Yaswanth Narsupalli",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4720",
          "name": "Gurpreet Singh",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4721",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4722",
          "name": "Vasu Sharma",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4723",
          "name": "Suranjana Trivedy",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4724",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:39.550Z",
          "hidden": false
        },
        {
          "_id": "67a9a7cb6be3ca4a7ede4725",
          "name": "Amit Sheth",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T18:46:20.000Z",
      "title": "インヤングアライン：矛盾する目標のベンチマークと多目的最適化に基づくDPOの提案",
      "summary": "テキストから画像への対応（T2I）システムでの極めて重要な点として、生成される画像がユーザーの意図を正確に捉え、厳格な倫理的および美学的基準に合わせることが必要です。グーグル・ジェミニのような例では、不適切な出力がポピュラーに反対されたことがあり、強力な対応機構の必要性を強調しました。対照的に、大規模言語モデル（LLMs）は対応に関して顕著な成功を収めています。これらの進歩に基づき、研究者はDirect Preference Optimization（DPO）などの類似した対応手法をT2Iシステムに適用し、画像生成の忠実性と信頼性を向上させることを望んでいます。\n\n我々は、YinYangAlignという高度なベンチマークフレームワークを提案しています。これは、T2Iシステムの対応の忠実性をシステマティックに定量化するために、6つの基本的で固有の矛盾を含む設計目標を解決しています。各ペアは、画像生成の基本的なテンションを表し、例えば、ユーザーのプロンプトに従うことと創造的な変更のバランスや、多様性と可視的な一貫性を維持することを意味します。YinYangAlignは、人間のプロンプト、選択された対応（選択）の出力、不適切（拒否）のAI生成の出力、および裏面の矛盾の説明を含む詳細な公理データセットを含みます。",
      "upvotes": 1,
      "discussionId": "67a9a7cf6be3ca4a7ede47d5"
    },
    "publishedAt": "2025-02-10T02:21:52.370Z",
    "title": "YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03512.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.05178",
      "authors": [
        {
          "_id": "67a99dfe98423dca45d8f659",
          "user": {
            "_id": "638fe91639f7e2a7f9d2a8c6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
            "isPro": false,
            "fullname": "Yue Zhao",
            "user": "zhaoyue-zephyrus",
            "type": "user"
          },
          "name": "Yue Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-10T09:49:43.493Z",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65a",
          "name": "Fuzhao Xue",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65b",
          "name": "Scott Reed",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65c",
          "name": "Linxi Fan",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65d",
          "name": "Yuke Zhu",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65e",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f65f",
          "name": "Zhiding Yu",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f660",
          "name": "Philipp Krähenbühl",
          "hidden": false
        },
        {
          "_id": "67a99dfe98423dca45d8f661",
          "name": "De-An Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:59:57.000Z",
      "title": "QLIP: 文字対応ビジュアルトークン化は、自動的な帰納的な多モーダル理解と生成を統合する",
      "summary": "QLIP（Quantized Language-Image Pretraining）を紹介します。QLIPは、最先端の再構成質量と最先端のゼロショット画像理解を統合した可視的トークナイゼーション手法です。QLIPは、再構成と言語画像のアラインメントの目的を持つ二元球面量子化に基づく自動エンコーダーを訓練します。まず、QLIPは、二つの目的が矛盾しないことを示しました。QLIPは、訓練中にその二つの損失項を動的にバランス調整し、画像言語予習の大バッチ要求と再構成目的によるメモリボトルネックをより効果的に混合するための二段階訓練プインプルを実装しました。QLIPの効果性は、多モデル理解と文書条件付き画像生成において確認されました。特に、QLIPは、LLaVAの可視的エンコーダーとLlamaGenの画像トークナイザーの代わりとして使用でき、比較的またはより良い性能を示します。最後に、QLIPは、理解と生成のための統一的な混合モーデルを実現することを示しました。",
      "upvotes": 1,
      "discussionId": "67a99dfe98423dca45d8f691"
    },
    "publishedAt": "2025-02-10T01:35:35.818Z",
    "title": "QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05178.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638fe91639f7e2a7f9d2a8c6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
      "fullname": "Yue Zhao",
      "name": "zhaoyue-zephyrus",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.04376",
      "authors": [
        {
          "_id": "67a998fe495b23306cdbf51d",
          "name": "Lingxiang Hu",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf51e",
          "name": "Shurun Yuan",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf51f",
          "name": "Xiaoting Qin",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf520",
          "name": "Jue Zhang",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf521",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf522",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf523",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "67a998fe495b23306cdbf524",
          "name": "Qi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T16:25:43.000Z",
      "title": "会議代表：代表我們進行會議的LLM標準化",
      "summary": "現代の労働場では、会議はアイデアの交換とチームの調整に不可欠ですが、時間の消費、スケジューリングの衝突、そして不適切な参加による効率低下などの課題があります。最近の大規模な言語モデル（LLMs）の進歩は、自然言語生成と推理の強力な能力を示し、「LLMsは会議の参加者を有効に委託できるでしょうか？」という問題が生じました。これを調査するために、私たちはLLMプロティングの会議委託システムを開発し、実際の会議トランスクリプトを用いて詳細なベンチマークを作成しました。評価結果によると、GPT-4/4oは活性的で慎重な参加戦略のバランスを保っています。対照的に、Gemini 1.5 Proはより慎重な傾向を示し、Gemini 1.5 FlashとLlama3-8B/70Bはより活性的な傾向を示しています。全体として、約60%の回答は真実のキーポイントからの少なくとも1つを扱うことができます。しかし、不相関または重複の内容を減らし、実世界的な設定でのコピーエラーの耐容性を高める必要があります。また、実用的な設定でシステムを実装し、デモでの実世界的なフィードバックを収集しました。私たちの発見は、LLMsを会議委託に利用することの可能性と課題を明らかにし、会議の負担を解決するための実用的なアプリケーションについての有價値なコンテンツを提供します。",
      "upvotes": 1,
      "discussionId": "67a99900495b23306cdbf57e"
    },
    "publishedAt": "2025-02-10T01:15:52.070Z",
    "title": "MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662b0bc9c709a61df8291c0f",
      "avatarUrl": "/avatars/16dd4d945e9fbef5ac889a8087101ded.svg",
      "fullname": "Xiaoting Qin",
      "name": "XiaotingQin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03738",
      "authors": [
        {
          "_id": "67a8d049406cb5a65f847eb1",
          "name": "Feng Wang",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb2",
          "name": "Yaodong Yu",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb3",
          "name": "Guoyizhe Wei",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb4",
          "name": "Wei Shao",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb5",
          "name": "Yuyin Zhou",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb6",
          "name": "Alan Yuille",
          "hidden": false
        },
        {
          "_id": "67a8d049406cb5a65f847eb7",
          "name": "Cihang Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T03:01:38.000Z",
      "title": "スケーリングラーズにおけるパチファクション：画像は50,176トークンよりも多くの価値を持つ",
      "summary": "ビジョントランスフォーマー（ViT）の導入以降、パチファクションは素朴な視覚アーキテクチャの実際的な画像トークナリゼーションアプローチとして長期にわたり認められてきました。この手法は、画像の空間サイズを圧縮することで、ViTのような素朴なアーキテクチャの計算コストを効果的に短縮できます。本稿では、このパチファクションによる圧縮エンコーディングパラダイムによる情報の損失を詳細に調査し、それが視覚理解にどのような影響を与えるかを調べることを目指しています。幅広いパチサイズスケーリング実験を行い、パチファクションにおける関心を引き起こした関心のスケーリング法則を見出しました。モデルはパチサイズを減らすことで一貫して利益を得、予測性能が向上しますが、1x1の最小パチサイズまで達した場合は、ピクセルトークナリゼーションとなります。この結論は、異なる視覚タスク、異なる入力スケール、さらにはViTや最近のMambaモデルなどの多様なアーキテクチャに広く適用可能です。また、その副産物として、パチが小さくなると、特定のタスクに対応するデコーダーヘッドは、密な予測に対して重要性が低くなります。実験では、画像シーケンスを例外的に長く50,176トークンに拡大し、ImageNet-1kベンチマークで基盤サイズのモデルで84.6%の有競争力的テスト精度を達成しました。この研究は、将来の非圧縮視覚モデルの構築におけるフィードバックと理論的基盤を提供することを望みます。コードは、https://github.com/wangf3014/Patch_Scaling に提供されています。",
      "upvotes": 0,
      "discussionId": "67a8d04a406cb5a65f847ed3"
    },
    "publishedAt": "2025-02-10T02:34:31.480Z",
    "title": "Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03738.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 754
    },
    "isAuthorParticipating": false
  }
]