[
  {
    "paper": {
      "id": "2504.05298",
      "authors": [
        {
          "_id": "67f4a0ccfefcc542f83f84e7",
          "user": {
            "_id": "646ebf2298e8f749fc60ce38",
            "avatarUrl": "/avatars/5a76c9343451c24e9697d3165cdc0af6.svg",
            "isPro": false,
            "fullname": "Karan Dalal",
            "user": "karansdalal",
            "type": "user"
          },
          "name": "Karan Dalal",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:09:24.283Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84e8",
          "user": {
            "_id": "66b01b126038fe024ae979d3",
            "avatarUrl": "/avatars/75b561be803fa28cb78c6cff9eb5ac54.svg",
            "isPro": false,
            "fullname": "Daniel Koceja",
            "user": "koceja",
            "type": "user"
          },
          "name": "Daniel Koceja",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:09:31.602Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84e9",
          "user": {
            "_id": "638835fede1fa6adc5485a05",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669871090087-noauth.jpeg",
            "isPro": false,
            "fullname": "Gashon Hussein",
            "user": "GashonHussein",
            "type": "user"
          },
          "name": "Gashon Hussein",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:09:39.483Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ea",
          "name": "Jiarui Xu",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84eb",
          "user": {
            "_id": "638fe91639f7e2a7f9d2a8c6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
            "isPro": false,
            "fullname": "Yue Zhao",
            "user": "zhaoyue-zephyrus",
            "type": "user"
          },
          "name": "Yue Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:29.995Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ec",
          "name": "Youjin Song",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ed",
          "name": "Shihao Han",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ee",
          "name": "Ka Chun Cheung",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84ef",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f0",
          "user": {
            "_id": "677c45511fbb93b90f1c6f3d",
            "avatarUrl": "/avatars/3a78fdd8d1debc8d267e80e4b7a6bf77.svg",
            "isPro": false,
            "fullname": "Carlos Guestrin",
            "user": "guestrin",
            "type": "user"
          },
          "name": "Carlos Guestrin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:10:26.609Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f1",
          "user": {
            "_id": "67ecbb7eb57a7b083182ea3f",
            "avatarUrl": "/avatars/8fb800e0729771e61ff0d2f9a05eb5b9.svg",
            "isPro": false,
            "fullname": "Tatsunori Hashimoto",
            "user": "hashtag56",
            "type": "user"
          },
          "name": "Tatsunori Hashimoto",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:10:20.291Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f2",
          "user": {
            "_id": "64931e7e2da595588288f161",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64931e7e2da595588288f161/4jOhJOFsU7RVFMgGk5kO7.jpeg",
            "isPro": false,
            "fullname": "Sanmi Koyejo",
            "user": "sanmikoyejo",
            "type": "user"
          },
          "name": "Sanmi Koyejo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:10:14.122Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f3",
          "user": {
            "_id": "64d42729f63b01b7f676b176",
            "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg",
            "isPro": false,
            "fullname": "Yejin Choi",
            "user": "yejinchoinka",
            "type": "user"
          },
          "name": "Yejin Choi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:10:07.480Z",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f4",
          "name": "Yu Sun",
          "hidden": false
        },
        {
          "_id": "67f4a0ccfefcc542f83f84f5",
          "name": "Xiaolong Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638fe91639f7e2a7f9d2a8c6/cTDthYKFDTs8NDfvfVKJI.mp4"
      ],
      "publishedAt": "2025-04-07T17:56:31.000Z",
      "submittedOnDailyAt": "2025-04-08T02:38:37.647Z",
      "title": "1分間のビデオ生成においてテスト時学習を用いる",
      "submittedOnDailyBy": {
        "_id": "638fe91639f7e2a7f9d2a8c6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
        "isPro": false,
        "fullname": "Yue Zhao",
        "user": "zhaoyue-zephyrus",
        "type": "user"
      },
      "summary": "Transformersは現在、1分の長いビデオを生成するのに難しいことに苦戦しています。これは、自動注意層が長いコンテキストでは効率的ではありません。もう一方、マンダブ層やギャット・デルタネット、スライディングウィンドウアテンション層などの代替手法は、複雑な多スペースストーリーに対しては、隠れ状態が表現力が弱いため、難しいです。私たちは、テスト時学習（TTT）層を実験しました。これらの隠れ状態自身がニューラルネットワークとして構成されているため、より表現力が強いです。テスト時学習層を事前学習されたTransformerに追加することで、テキストストーリーボードから1分の長いビデオを生成することができます。証明のために、私たちはトムとジョーのカートオンブームに基づいたデータセットをカレードしました。マンダブ~2、ギャット・デルタネット、スライディングウィンドウアテンション層などのベースラインと比較して、テスト時学習層が複雑なストーリーを伝えるよりもより連続的なビデオを生成し、100ビデオごとの人間評価で34Elo点より優れています。これは、プロモーション的ですが、結果には、事前学習された5Bモデルの限られた能力による誤差がある可能性があります。我々の実装の効率化も改善できます。リソースの制約により、1分の長いビデオだけを実験しましたが、このアプローチは長いビデオや複雑なストーリーに拡張できます。サンプルビデオ、コードと注釈は以下のURLから利用できます：https://test-time-training.github.io/video-dit",
      "upvotes": 29,
      "discussionId": "67f4a0cefefcc542f83f8592",
      "projectPage": "https://test-time-training.github.io/video-dit/",
      "githubRepo": "https://github.com/test-time-training/ttt-video-dit"
    },
    "publishedAt": "2025-04-07T13:56:31.000Z",
    "title": "One-Minute Video Generation with Test-Time Training",
    "summary": "Transformers today still struggle to generate one-minute videos because\nself-attention layers are inefficient for long context. Alternatives such as\nMamba layers struggle with complex multi-scene stories because their hidden\nstates are less expressive. We experiment with Test-Time Training (TTT) layers,\nwhose hidden states themselves can be neural networks, therefore more\nexpressive. Adding TTT layers into a pre-trained Transformer enables it to\ngenerate one-minute videos from text storyboards. For proof of concept, we\ncurate a dataset based on Tom and Jerry cartoons. Compared to baselines such as\nMamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers\ngenerate much more coherent videos that tell complex stories, leading by 34 Elo\npoints in a human evaluation of 100 videos per method. Although promising,\nresults still contain artifacts, likely due to the limited capability of the\npre-trained 5B model. The efficiency of our implementation can also be\nimproved. We have only experimented with one-minute videos due to resource\nconstraints, but the approach can be extended to longer videos and more complex\nstories. Sample videos, code and annotations are available at:\nhttps://test-time-training.github.io/video-dit",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638fe91639f7e2a7f9d2a8c6/cTDthYKFDTs8NDfvfVKJI.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05298.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "638fe91639f7e2a7f9d2a8c6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fe91639f7e2a7f9d2a8c6/hB7DMVODcdAEUdQnXxWA8.jpeg",
      "fullname": "Yue Zhao",
      "name": "zhaoyue-zephyrus",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.04718",
      "authors": [
        {
          "_id": "67f48eff463c4d1c4ae5284b",
          "user": {
            "_id": "64b74920fe6a108d03fed767",
            "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
            "isPro": false,
            "fullname": "Minki Kang",
            "user": "Nardien",
            "type": "user"
          },
          "name": "Minki Kang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:12:42.979Z",
          "hidden": false
        },
        {
          "_id": "67f48eff463c4d1c4ae5284c",
          "name": "Jongwon Jeong",
          "hidden": false
        },
        {
          "_id": "67f48eff463c4d1c4ae5284d",
          "name": "Jaewoong Cho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T04:01:17.000Z",
      "submittedOnDailyAt": "2025-04-08T06:04:55.039Z",
      "title": "T1: ツール統合されたテスト時の計算スケーリングの自動認証を行う小型言語モデル",
      "submittedOnDailyBy": {
        "_id": "64b74920fe6a108d03fed767",
        "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
        "isPro": false,
        "fullname": "Minki Kang",
        "user": "Nardien",
        "type": "user"
      },
      "summary": "最近の研究により、テスト時の計算スケーリングが小規模な言語モデル（sLMs）の性能を効果的に向上させることが示されています。しかし、先行研究では、テスト時の計算スケーリングを実行するには、より大きなモデルを追加して確認者として使用していましたが、sLMsが自ら確認すること（自確認）については調査が浅い状態でした。本研究では、sLMsがテスト時のスケーリングの下で確信的に自ら出力を確認できるかどうかを調査します。結果として、より大きな確認者からの知識の収納による場合でも、sLMsは記憶の必要があるバリデーションタスク（例：数値計算、事実確認）に難しいことが見られました。この制限を解決するために、外部ツール（例：コードインタープリタ）に記憶の重いバリデーションステップを委託する「ツール統合された自確認（T1）」を提案します。理論的な分析により、ツール統合は記憶の必要性を減らし、テスト時のスケーリング性能を向上させることが示されました。MATHベンチマークの実験では、T1を使用したLlama-3.2 1Bモデルはテスト時のスケーリングの下で、より大きなLlama-3.1 8Bモデルを大幅に超えることが見られました。また、T1は数学的な（MATH500）および多様な領域の知識密集タスク（MMLU-Pro）にも効果的に一般化します。本研究の結果は、ツール統合がsLMsの自確認能力を大幅に向上させることの可能性を示しています。",
      "upvotes": 20,
      "discussionId": "67f48f00463c4d1c4ae52882"
    },
    "publishedAt": "2025-04-07T00:01:17.000Z",
    "title": "T1: Tool-integrated Self-verification for Test-time Compute Scaling in\n  Small Language Models",
    "summary": "Recent studies have demonstrated that test-time compute scaling effectively\nimproves the performance of small language models (sLMs). However, prior\nresearch has mainly examined test-time compute scaling with an additional\nlarger model as a verifier, leaving self-verification by sLMs underexplored. In\nthis work, we investigate whether sLMs can reliably self-verify their outputs\nunder test-time scaling. We find that even with knowledge distillation from\nlarger verifiers, sLMs struggle with verification tasks requiring memorization,\nsuch as numerical calculations and fact-checking. To address this limitation,\nwe propose Tool-integrated self-verification (T1), which delegates\nmemorization-heavy verification steps to external tools, such as a code\ninterpreter. Our theoretical analysis shows that tool integration reduces\nmemorization demands and improves test-time scaling performance. Experiments on\nthe MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under\ntest-time scaling outperforms the significantly larger Llama-3.1 8B model.\nMoreover, T1 generalizes effectively to both mathematical (MATH500) and\nmulti-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the\npotential of tool integration to substantially improve the self-verification\nabilities of sLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04718.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b74920fe6a108d03fed767",
      "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
      "fullname": "Minki Kang",
      "name": "Nardien",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05299",
      "authors": [
        {
          "_id": "67f4cf5b504263bce1236d87",
          "user": {
            "_id": "65d66b494bbd0d92b641cdbb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
            "isPro": false,
            "fullname": "Andres Marafioti",
            "user": "andito",
            "type": "user"
          },
          "name": "Andrés Marafioti",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T08:46:33.366Z",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d88",
          "user": {
            "_id": "648c9605565e3a44f3c9bb7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648c9605565e3a44f3c9bb7b/W5chvk17Zol6-2QSWkFVR.jpeg",
            "isPro": true,
            "fullname": "Orr Zohar",
            "user": "orrzohar",
            "type": "user"
          },
          "name": "Orr Zohar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T09:11:26.498Z",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d89",
          "user": {
            "_id": "61ed0ff29539bc0a3bbc89f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61ed0ff29539bc0a3bbc89f4/iYWK7GParA7Ke5F6q132W.jpeg",
            "isPro": false,
            "fullname": "Miquel Farré",
            "user": "mfarre",
            "type": "user"
          },
          "name": "Miquel Farré",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T09:11:24.824Z",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8a",
          "name": "Merve Noyan",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8b",
          "name": "Elie Bakouch",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8c",
          "name": "Pedro Cuenca",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8d",
          "name": "Cyril Zakka",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8e",
          "user": {
            "_id": "61c141342aac764ce1654e43",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61c141342aac764ce1654e43/81AwoT5IQ_Xdw0OVw7TKu.jpeg",
            "isPro": false,
            "fullname": "Loubna Ben Allal",
            "user": "loubnabnl",
            "type": "user"
          },
          "name": "Loubna Ben Allal",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T09:11:22.957Z",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d8f",
          "name": "Anton Lozhkov",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d90",
          "name": "Nouamane Tazi",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d91",
          "name": "Vaibhav Srivastav",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d92",
          "name": "Joshua Lochner",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d93",
          "name": "Hugo Larcher",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d94",
          "name": "Mathieu Morlon",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d95",
          "name": "Lewis Tunstall",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d96",
          "name": "Leandro von Werra",
          "hidden": false
        },
        {
          "_id": "67f4cf5b504263bce1236d97",
          "name": "Thomas Wolf",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:58:57.000Z",
      "submittedOnDailyAt": "2025-04-08T06:10:52.675Z",
      "title": "SmolVLM: 小さければもしかして効率的な多モーダルモデルを再定義する",
      "submittedOnDailyBy": {
        "_id": "65d66b494bbd0d92b641cdbb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
        "isPro": false,
        "fullname": "Andres Marafioti",
        "user": "andito",
        "type": "user"
      },
      "summary": "大視覚言語モデル（VLMs）は、出色な性能を提供しますが、計算資源の大量の必要性により、携帯やエッジデバイス上の採用が制限されています。小さなVLMsは通常、大きなモデルの設計選択肢をモカードにして、広範囲な画像トークナイゼーションを含むことで、GPUメモリの効率的な使用とオンデバイスアプリケーションの実用性を制限します。\n\n我々は、資源効率的な推論を目的とした緊縮化のための多タイプモデルのシリーズ、SmolVLMを紹介します。我々は、低計算オーバーヘッドに最適化されたアーキテクチャの設計、トークナイゼーションの戦略、データカレーティオンによる系統的な探索を行います。これにより、画像およびビデオタスクでの性能の大幅な向上を実現するための重要な設計選択肢を特定します。\n\n我々の最も小さなモデルであるSmolVLM-256Mは、推論時に1GB以下のGPUメモリを使用し、18ヶ月の開発間隔を超えて、300倍大きなIdefics-80Bモデルを上回ります。我々の最大のモデルは22億パラメータで、GPUメモリの2倍以上を使用する最先端のVLMsとの対抗でも、優れた性能を示します。SmolVLMモデルは静的画像を超えて、強力的なビデオ理解能力を示します。\n\n我々の結果は、戦略的なアーキテクチャの最適化、強力かつ効率的なトークナイゼーション、よりカレーティドデータのより慎重な選択による多タイプ性能の大幅な向上を示し、実用的でエネルギー効率的な小規模での採用を促進します。",
      "upvotes": 17,
      "discussionId": "67f4cf5d504263bce1236dda",
      "projectPage": "https://huggingface.co/collections/HuggingFaceTB/smolvlm2-smallest-video-lm-ever-67ab6b5e84bf8aaa60cb17c7",
      "githubRepo": "https://github.com/huggingface/smollm"
    },
    "publishedAt": "2025-04-07T13:58:57.000Z",
    "title": "SmolVLM: Redefining small and efficient multimodal models",
    "summary": "Large Vision-Language Models (VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. Smaller VLMs typically mirror design choices of larger\nmodels, such as extensive image tokenization, leading to inefficient GPU memory\nusage and constrained practicality for on-device applications.\n  We introduce SmolVLM, a series of compact multimodal models specifically\nengineered for resource-efficient inference. We systematically explore\narchitectural configurations, tokenization strategies, and data curation\noptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during\ninference and outperforms the 300-times larger Idefics-80B model, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend\nbeyond static images, demonstrating robust video comprehension capabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal performance, facilitating practical, energy-efficient\ndeployments at significantly smaller scales.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05299.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d66b494bbd0d92b641cdbb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
      "fullname": "Andres Marafioti",
      "name": "andito",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 131
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05305",
      "authors": [
        {
          "_id": "67f495437e1624ebbaf2d90e",
          "user": {
            "_id": "64842d1edc475c315e41123a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64842d1edc475c315e41123a/Yqam_dy0MnQm4Rn1_1_4N.jpeg",
            "isPro": false,
            "fullname": "Sangbeom Lim",
            "user": "SammyLim",
            "type": "user"
          },
          "name": "Sangbeom Lim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:37.341Z",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d90f",
          "user": {
            "_id": "64c2c45ae818eec6128fdda3",
            "avatarUrl": "/avatars/d4399e25e6399345e263c7902789047e.svg",
            "isPro": false,
            "fullname": "Jun-Wan KIM",
            "user": "junwann",
            "type": "user"
          },
          "name": "Junwan Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:34.302Z",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d910",
          "name": "Heeji Yoon",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d911",
          "user": {
            "_id": "65d02dc017e2b305e0d7bf4f",
            "avatarUrl": "/avatars/2a50fd0541e7b0e200c577a661956696.svg",
            "isPro": false,
            "fullname": "Jaewoo Jung",
            "user": "crepejung00",
            "type": "user"
          },
          "name": "Jaewoo Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:32.321Z",
          "hidden": false
        },
        {
          "_id": "67f495437e1624ebbaf2d912",
          "user": {
            "_id": "65cf717450818a335a1d3021",
            "avatarUrl": "/avatars/382a0e0f40f661cda1b2531e3e6ea2ee.svg",
            "isPro": false,
            "fullname": "Seungryong Kim",
            "user": "seungryong",
            "type": "user"
          },
          "name": "Seungryong Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:13:34.541Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:59:44.000Z",
      "submittedOnDailyAt": "2025-04-08T01:57:21.829Z",
      "title": "URECA: 異なる領域でもそのどこかのコメントを返す\n\nURECA: 異なる領域でもそのどこかのコメントを返す",
      "submittedOnDailyBy": {
        "_id": "64842d1edc475c315e41123a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64842d1edc475c315e41123a/Yqam_dy0MnQm4Rn1_1_4N.jpeg",
        "isPro": false,
        "fullname": "Sangbeom Lim",
        "user": "SammyLim",
        "type": "user"
      },
      "summary": "領域レベルのキャピティングは、特定の画像領域に自然言語の説明を生成し、その特徴を強調することを目的としています。しかし、現在の方法は、多様性のある領域のユニークなキャピティングを生成することが難しく、実世界的な応用可能性が限定されています。この課題を解決するために、我々は、詳細な領域レベルの理解を必要とすることを考慮して、URECAデータセットを紹介します。URECAデータセットは、多様性のある物体、部分、背景要素を含めて、領域とキャピティングの対応関係を一意的で一致的に確保するために、先行のデータセットと異なります。これを中心に、ステージごとのデータカレーティブパイプラインを構築し、各ステージで領域選択とキャピティング生成を進歩的に改善します。このパイプラインでは、各ステージでMultimodal Large Language Models (MLLMs)を利用し、ユニークなコンテキスト付きのキャピティングを生成し、精度と語意的な多様性を向上させます。URECAデータセットに基づいて、我々は、多様性のある領域を有効にエンコードするための新しいキャピティングモデル、URECAを紹介します。URECAは、現在のMLLMsに対して簡単でも影響的な変更を加え、位置や形状などの空間的な特性を維持し、ファイングラインおよび語意的に豊富な領域の説明を可能にします。我々のアプローチでは、動的なマスクモデリングと高解像度のマスクエンコーダーを導入し、キャピティングのユニーク性を向上させます。実験は、URECAはURECAデータセットで最先端の性能を達成し、現在の領域レベルのキャピティングベンチマークにもより良く拡張できることを示しています。",
      "upvotes": 16,
      "discussionId": "67f495477e1624ebbaf2da2f",
      "projectPage": "https://cvlab-kaist.github.io/URECA/",
      "githubRepo": "https://github.com/cvlab-kaist/URECA"
    },
    "publishedAt": "2025-04-07T13:59:44.000Z",
    "title": "URECA: Unique Region Caption Anything",
    "summary": "Region-level captioning aims to generate natural language descriptions for\nspecific image regions while highlighting their distinguishing features.\nHowever, existing methods struggle to produce unique captions across\nmulti-granularity, limiting their real-world applicability. To address the need\nfor detailed region-level understanding, we introduce URECA dataset, a\nlarge-scale dataset tailored for multi-granularity region captioning. Unlike\nprior datasets that focus primarily on salient objects, URECA dataset ensures a\nunique and consistent mapping between regions and captions by incorporating a\ndiverse set of objects, parts, and background elements. Central to this is a\nstage-wise data curation pipeline, where each stage incrementally refines\nregion selection and caption generation. By leveraging Multimodal Large\nLanguage Models (MLLMs) at each stage, our pipeline produces distinctive and\ncontextually grounded captions with improved accuracy and semantic diversity.\nBuilding upon this dataset, we present URECA, a novel captioning model designed\nto effectively encode multi-granularity regions. URECA maintains essential\nspatial properties such as position and shape through simple yet impactful\nmodifications to existing MLLMs, enabling fine-grained and semantically rich\nregion descriptions. Our approach introduces dynamic mask modeling and a\nhigh-resolution mask encoder to enhance caption uniqueness. Experiments show\nthat URECA achieves state-of-the-art performance on URECA dataset and\ngeneralizes well to existing region-level captioning benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05305.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64842d1edc475c315e41123a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64842d1edc475c315e41123a/Yqam_dy0MnQm4Rn1_1_4N.jpeg",
      "fullname": "Sangbeom Lim",
      "name": "SammyLim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.04823",
      "authors": [
        {
          "_id": "67f4a8ead83d88e30c450332",
          "user": {
            "_id": "6411c22dd52c57f628f7c331",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678885386428-noauth.jpeg",
            "isPro": false,
            "fullname": "ruikang liu",
            "user": "ruikangliu",
            "type": "user"
          },
          "name": "Ruikang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:24.004Z",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450333",
          "name": "Yuxuan Sun",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450334",
          "name": "Manyi Zhang",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450335",
          "name": "Haoli Bai",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450336",
          "name": "Xianzhi Yu",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450337",
          "name": "Tiezheng Yu",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450338",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "67f4a8ead83d88e30c450339",
          "name": "Lu Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T08:22:45.000Z",
      "submittedOnDailyAt": "2025-04-08T03:14:58.007Z",
      "title": "キャンマイシーズドヘルツリーズニング？ キャンマイシーズドリータシンティングモデルの実験的な研究",
      "submittedOnDailyBy": {
        "_id": "6411c22dd52c57f628f7c331",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678885386428-noauth.jpeg",
        "isPro": false,
        "fullname": "ruikang liu",
        "user": "ruikangliu",
        "type": "user"
      },
      "summary": "最近の理由言語モデルの進展は複雑なタスクでの卓越した性能を示していますが、長期的なchain-of-thought reasoningプロセスは推論オーバーヘッドを増加させています。量化は大規模な言語モデルの推論コストを減らすために広く採用されていますが、理由モデルに対する影響は研究が不足していました。本研究では、最初のシステム的な研究を行い、DeepSeek-R1-Distilled QwenとLLaMAの家族（パラメータ数が1.5Bから70Bまで）とQwQ-32Bを評価しました。研究内容は、重み、KVキャッシュ、アクティベーションの量化を含み、最先端のアルゴリズムを使用して異なるビット幅で実施しました。数学（AIME、MATH-500）、科学（GPQA）、プログラミング（LiveCodeBench）の理由ベンチマークで極端な評価を実施しました。結果として、W8A8またはW4A16の無失量量化が可能であることがわかりましたが、低ビット幅は精度のリスクを引き起こします。また、モデルサイズ、モデルの起源、タスクの難易度が性能の重要な決定因子であることが明らかになりました。期待されないように、量化されたモデルは出力の長さが増加しないことがわかりました。また、モデルサイズまたは理由ステップのスケーリングを戦略的に行うことで性能を向上させることができます。すべての量化されたモデルとコードは、https://github.com/ruikangliu/Quantized-Reasoning-Modelsに開放されます。",
      "upvotes": 12,
      "discussionId": "67f4a8efd83d88e30c45043c"
    },
    "publishedAt": "2025-04-07T04:22:45.000Z",
    "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
    "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04823.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6411c22dd52c57f628f7c331",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678885386428-noauth.jpeg",
      "fullname": "ruikang liu",
      "name": "ruikangliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02828",
      "authors": [
        {
          "_id": "67f01efe81f4f7a1b43f4930",
          "user": {
            "_id": "6279a4f6812ee439d9c72d3f",
            "avatarUrl": "/avatars/a35a5674d1168d345d9fc5018485283e.svg",
            "isPro": false,
            "fullname": "Jinqi Luo",
            "user": "peterljq",
            "type": "user"
          },
          "name": "Jinqi Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-06T08:11:28.131Z",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4931",
          "name": "Tianjiao Ding",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4932",
          "user": {
            "_id": "6627043551cedbbb0b352047",
            "avatarUrl": "/avatars/8c4f80a24f8ba8761d33692c4ed28c29.svg",
            "isPro": false,
            "fullname": "Kwan Ho Ryan Chan",
            "user": "ryanckh",
            "type": "user"
          },
          "name": "Kwan Ho Ryan Chan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-08T09:13:58.769Z",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4933",
          "name": "Hancheng Min",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4934",
          "name": "Chris Callison-Burch",
          "hidden": false
        },
        {
          "_id": "67f01efe81f4f7a1b43f4935",
          "name": "René Vidal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:59:58.000Z",
      "submittedOnDailyAt": "2025-04-08T02:59:02.267Z",
      "title": "コンセプト Lancet: 組み合わせ表現を用いた画像編集\nトランスプラント",
      "submittedOnDailyBy": {
        "_id": "6279a4f6812ee439d9c72d3f",
        "avatarUrl": "/avatars/a35a5674d1168d345d9fc5018485283e.svg",
        "isPro": false,
        "fullname": "Jinqi Luo",
        "user": "peterljq",
        "type": "user"
      },
      "summary": "ディフュージョンモデルは画像編集タスクに広く使用されています。現在の編集方法は、テキスト埋め込み空間またはスコア空間で編集方向を選び、表現操作手順を設計しています。しかし、この手順には重要な課題があります：編集の強さを過剰に評価すると視覚的な一貫性を損なう一方で、評価不足では編集タスクを達成できません。特に、ソース画像は異なる編集強度を必要とし、試行錯誤で適切な強度を見つけるのは高額です。この課題に対処するために、私たちはConcept Lancet（CoLan）を提案します。CoLanは、ディフュージョンベースの画像編集中の原則的な表現操作を行うためのゼロシートプラグインプラグインフレームワークです。推論時に、ソース入力を潜在空間（テキスト埋め込み空間またはディフュージョンスコア空間）で稀疏な線形結合として分解します。これにより、各画像に概念が存在するかどうかを正確に評価でき、これは編集に役立ちます。編集タスク（置換/追加/削除）に基づいて、カスタマイズされた概念移植プロセスを実行し、対応する編集方向を課します。概念空間を十分にモデル化するために、私たちはCoLan-150Kの概念的表現データセットを製作します。このデータセットは、潜在辞書のラベルディクショナリーにおける視覚的な用語とフレーズの多様な説明とシナリオを含みます。複数のディフュージョンベースの画像編集ベースラインの実験において、CoLanを採用した方法は編集の効果性と一貫性の保存において最先端の性能を達成しました。",
      "upvotes": 11,
      "discussionId": "67f01eff81f4f7a1b43f4971",
      "projectPage": "https://peterljq.github.io/project/colan",
      "githubRepo": "https://github.com/peterljq/Concept-Lancet",
      "ai_keywords": [
        "diffusion models",
        "image editing",
        "text embedding",
        "score space",
        "latent space",
        "sparse linear combination",
        "visual concepts",
        "concept transplantation",
        "conceptual representation dataset",
        "CoLan-150K",
        "CoLan"
      ]
    },
    "publishedAt": "2025-04-03T13:59:58.000Z",
    "title": "Concept Lancet: Image Editing with Compositional Representation\n  Transplant",
    "summary": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02828.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6279a4f6812ee439d9c72d3f",
      "avatarUrl": "/avatars/a35a5674d1168d345d9fc5018485283e.svg",
      "fullname": "Jinqi Luo",
      "name": "peterljq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05288",
      "authors": [
        {
          "_id": "67f4adf70864398533373364",
          "name": "Mingyang Fu",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373365",
          "name": "Yuyang Peng",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373366",
          "name": "Benlin Liu",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373367",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67f4adf70864398533373368",
          "name": "Dongping Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:39:31.000Z",
      "submittedOnDailyAt": "2025-04-08T03:33:51.436Z",
      "title": "LiveVQA: ライブビジュアル知識探求",
      "submittedOnDailyBy": {
        "_id": "643be8879f5d314db2d9ed23",
        "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
        "isPro": false,
        "fullname": "Chen Dongping",
        "user": "shuaishuaicdp",
        "type": "user"
      },
      "summary": "LiveVQAは、最新のビジュアル知識をインターネットから自動的に収集し、合成されたVQA問題を含むデータセットです。LiveVQAは、6つのニュースウェブサイトからの14つのニュースカテゴリからの3,602件の単一および複数ステップのビジュアルクエスチョンから構成されています。高品質の画像-テキストの一致性と真実性のある情報を特徴としています。15つのMLLM（例：GPT-4o、Gemma-3、Qwen-2.5-VLファミリー）の評価により、強化されたモデルが全体的により良い性能を示し、進歩的なビジュアル推理能力が複雑な複数ステップのクエスチョンで重要であることが示唆されました。文脈的な問題に優れているモデルは、最新のビジュアル知識を必要とするビジュアルクエスチョンに対してもらうと、ターゲットエンジンやその他のツールを使用している場合にも大きな間違いが残ることが明らかになり、将来の研究の重要な分野を明らかにしています。",
      "upvotes": 8,
      "discussionId": "67f4adfb086439853337346e"
    },
    "publishedAt": "2025-04-07T13:39:31.000Z",
    "title": "LiveVQA: Live Visual Knowledge Seeking",
    "summary": "We introduce LiveVQA, an automatically collected dataset of latest visual\nknowledge from the Internet with synthesized VQA problems. LiveVQA consists of\n3,602 single- and multi-hop visual questions from 6 news websites across 14\nnews categories, featuring high-quality image-text coherence and authentic\ninformation. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and\nQwen-2.5-VL family) demonstrates that stronger models perform better overall,\nwith advanced visual reasoning capabilities proving crucial for complex\nmulti-hop questions. Despite excellent performance on textual problems, models\nwith tools like search engines still show significant gaps when addressing\nvisual questions requiring latest visual knowledge, highlighting important\nareas for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05288.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.04715",
      "authors": [
        {
          "_id": "67f4a59bf51362f606eb84e8",
          "name": "Will Cai",
          "hidden": false
        },
        {
          "_id": "67f4a59bf51362f606eb84e9",
          "name": "Tianneng Shi",
          "hidden": false
        },
        {
          "_id": "67f4a59bf51362f606eb84ea",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "67f4a59bf51362f606eb84eb",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T03:57:41.000Z",
      "submittedOnDailyAt": "2025-04-08T02:57:43.185Z",
      "title": "お金を払って得られるものがありますか？LLM APIでのモデル置換の監視",
      "submittedOnDailyBy": {
        "_id": "6275a465597c70eb8949fce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
        "isPro": false,
        "fullname": "Xuandong Zhao",
        "user": "Xuandong",
        "type": "user"
      },
      "summary": "LLM APIでのブラックボックスAPIを利用した大規模言語モデル（LLM）の普及は、信頼性の大きな課題を帯びます：ユーザは広告されたモデルの機能（例：サイズ、性能）に基づいてサービスを支払いますが、提供元は、操作コストを削減するために、指定されたモデルを軽く、質が低い代替モデルに変更することができます。この不透明性は公平性を破壊し、信頼性を削減し、信頼性のあるベンチマークを複雑化します。このような代替を検出するのは、ブラックボックスの性質により難しいですが、通常、入力出力クエリとしか相互作用が限られます。本論文では、LLM APIでのモデル代替検出の問題を形式化します。実用的な攻撃シナリオ（例：モデルのキャリブレーション、ランダムな代替、ベンチマークの逃避）のもとで、現在の検証手法（出力基の統計的テスト、ベンチマーク評価、ログ確率分析）をシステマティックに評価します。私たちの発見は、文の出力だけを依存する方法の制限を明らかにし、特に微妙なやび動きの攻撃に対しては、その限り性を示します。ログ確率分析は、そのような場合に強い保証を提供しますが、そのアクセス性は通常限られています。最終的には、Trusted Execution Environments（TEEs）のようなハードウェアベースの解決策の可能性を議論し、安全性、性能、提供元の採用のトレードオフを明らかにします。コードは、https://github.com/sunblaze-ucb/llm-api-audit にあります。",
      "upvotes": 3,
      "discussionId": "67f4a59cf51362f606eb8529",
      "githubRepo": "https://github.com/sunblaze-ucb/llm-api-audit"
    },
    "publishedAt": "2025-04-06T23:57:41.000Z",
    "title": "Are You Getting What You Pay For? Auditing Model Substitution in LLM\n  APIs",
    "summary": "The proliferation of Large Language Models (LLMs) accessed via black-box APIs\nintroduces a significant trust challenge: users pay for services based on\nadvertised model capabilities (e.g., size, performance), but providers may\ncovertly substitute the specified model with a cheaper, lower-quality\nalternative to reduce operational costs. This lack of transparency undermines\nfairness, erodes trust, and complicates reliable benchmarking. Detecting such\nsubstitutions is difficult due to the black-box nature, typically limiting\ninteraction to input-output queries. This paper formalizes the problem of model\nsubstitution detection in LLM APIs. We systematically evaluate existing\nverification techniques, including output-based statistical tests, benchmark\nevaluations, and log probability analysis, under various realistic attack\nscenarios like model quantization, randomized substitution, and benchmark\nevasion. Our findings reveal the limitations of methods relying solely on text\noutputs, especially against subtle or adaptive attacks. While log probability\nanalysis offers stronger guarantees when available, its accessibility is often\nlimited. We conclude by discussing the potential of hardware-based solutions\nlike Trusted Execution Environments (TEEs) as a pathway towards provable model\nintegrity, highlighting the trade-offs between security, performance, and\nprovider adoption. Code is available at\nhttps://github.com/sunblaze-ucb/llm-api-audit",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.04715.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6275a465597c70eb8949fce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
      "fullname": "Xuandong Zhao",
      "name": "Xuandong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.05304",
      "authors": [
        {
          "_id": "67f48c11412c65a9d4e3e552",
          "user": {
            "_id": "638067fcb334960c987fbeda",
            "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
            "isPro": false,
            "fullname": "Hansheng Chen",
            "user": "Lakonik",
            "type": "user"
          },
          "name": "Hansheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:39.675Z",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e553",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e554",
          "name": "Hao Tan",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e555",
          "name": "Zexiang Xu",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e556",
          "name": "Fujun Luan",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e557",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e558",
          "name": "Gordon Wetzstein",
          "hidden": false
        },
        {
          "_id": "67f48c11412c65a9d4e3e559",
          "name": "Sai Bi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:59:42.000Z",
      "submittedOnDailyAt": "2025-04-08T01:13:17.992Z",
      "title": "ガウス混合フローマッチングモデル",
      "submittedOnDailyBy": {
        "_id": "638067fcb334960c987fbeda",
        "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
        "isPro": false,
        "fullname": "Hansheng Chen",
        "user": "Lakonik",
        "type": "user"
      },
      "summary": "Diffusionモデルは、ノイズの分布をガウス分布と近似し、その平均を予測します。一方、flow matchingモデルは、ガウスの平均を流れ速度として再設定します。しかし、これらのモデルは、離散化誤差により少ないステップでサンプリングする場合に性能が低下し、クラス分類器自由のガイド（CFG）のためにオーバーサテライズされた色を生成する傾向があります。これらの制限に対処するために、私たちは新しいガウス混合フローマッチング（GMFlow）モデルを提案します：GMFlowは、平均を予測するのではなく、動的なガウス混合（GM）パラメータを予測し、多モーダルな流れ速度分布を捉えます。これはKL分散損失を使用して学習できます。GMFlowは、L_2ノイズ削減損失で学習された単一のガウスを学習した前のディフュージョンモデルとflow matchingモデルを一般化します。推論時には、分析的なノイズ分布と速度場を利用したGM-SDE/ODEソルバーを提案し、少ないステップで精確なサンプリングを行うことができます。また、新しい確率的なガイドディングスキームを導入し、CFGのオーバーサテライズ問題を軽減し、画像生成の質を向上させます。拡張された実験により、GMFlowは、ImageNet 256×256で6ステップのみでPrecision 0.942を達成し、生成質量でflow matchingベースラインに対して一貫して優れています。",
      "upvotes": 2,
      "discussionId": "67f48c13412c65a9d4e3e5d7",
      "githubRepo": "https://github.com/Lakonik/GMFlow"
    },
    "publishedAt": "2025-04-07T13:59:42.000Z",
    "title": "Gaussian Mixture Flow Matching Models",
    "summary": "Diffusion models approximate the denoising distribution as a Gaussian and\npredict its mean, whereas flow matching models reparameterize the Gaussian mean\nas flow velocity. However, they underperform in few-step sampling due to\ndiscretization error and tend to produce over-saturated colors under\nclassifier-free guidance (CFG). To address these limitations, we propose a\nnovel Gaussian mixture flow matching (GMFlow) model: instead of predicting the\nmean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a\nmulti-modal flow velocity distribution, which can be learned with a KL\ndivergence loss. We demonstrate that GMFlow generalizes previous diffusion and\nflow matching models where a single Gaussian is learned with an L_2 denoising\nloss. For inference, we derive GM-SDE/ODE solvers that leverage analytic\ndenoising distributions and velocity fields for precise few-step sampling.\nFurthermore, we introduce a novel probabilistic guidance scheme that mitigates\nthe over-saturation issues of CFG and improves image generation quality.\nExtensive experiments demonstrate that GMFlow consistently outperforms flow\nmatching baselines in generation quality, achieving a Precision of 0.942 with\nonly 6 sampling steps on ImageNet 256times256.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05304.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638067fcb334960c987fbeda",
      "avatarUrl": "/avatars/87f1eaaf6b3a9c0d47d6f406261ccc18.svg",
      "fullname": "Hansheng Chen",
      "name": "Lakonik",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.03193",
      "authors": [
        {
          "_id": "67f3f25019592b36b6d8b30c",
          "user": {
            "_id": "6436290e76dbfd731bcf1f55",
            "avatarUrl": "/avatars/63af84ca382d799c4a000594c994d4bb.svg",
            "isPro": false,
            "fullname": "Xin Zhang",
            "user": "XinNUS",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:52:23.735Z",
          "hidden": false
        },
        {
          "_id": "67f3f25019592b36b6d8b30d",
          "name": "Robby T. Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T05:44:45.000Z",
      "submittedOnDailyAt": "2025-04-08T06:12:20.759Z",
      "title": "マンダーバージョン：ビジョンファンダメンションモデルとビジョン言語モデルの交差点で、ドメイン一般化セマンティックセグメンテーションモデルを検討する",
      "submittedOnDailyBy": {
        "_id": "6436290e76dbfd731bcf1f55",
        "avatarUrl": "/avatars/63af84ca382d799c4a000594c994d4bb.svg",
        "isPro": false,
        "fullname": "Xin Zhang",
        "user": "XinNUS",
        "type": "user"
      },
      "summary": "Vision Foundation Models (VFMs)とVision-Language Models (VLMs)は、強い一般化能力を持ってドメイン広範囲的記号分割（DGSS）で注目を集めています。しかし、現在のDGSS手法は、VFMsやVLMsの補間的な強さを見落としています。VFMs（例：DINOv2）は、細かい特徴を捉える優れた性能を示し、VLMs（例：CLIP）は、強力的な文書のアラインメントを提供しますが、粗粒度の処理に難しいです。VFMsとVLMsの補間的な強さはあるが、注意機構を用いて効果的に統合するのは難しいです。ポッチトークンの増加が長シーケンスモデリングを複雑にしています。これに対して、私たちは、線形可拡張性を維持しながらVFMsとVLMsの強さを効率的に統合する新しいMambaベースの融合フレームワーク、MFuserを提案します。MFuserは、MVFuserとMTEnhancerの2つのキーコンポーネントから構成され、両モデルを共に微調節し、両方の順序的および空間的な動作を捉える共適用器とします。また、MTEnhancerは、画像プロファイルを挟む統合的注意-Mambaモジュールで、文書埋め込みを精確にします。私たちのアプローチは、計算量の追加が大きくならないものの、精密な特徴の局所性と強力的な文書アラインメントを実現します。拡張的な実験は、MFuserが最先端のDGSS手法を大幅に超え、合成-実際と実際-実際ベンチマークではそれぞれ68.20 mIoUと71.87 mIoUを達成しました。コードは、https://github.com/devinxzhang/MFuserに公開されています。",
      "upvotes": 2,
      "discussionId": "67f3f25219592b36b6d8b3d0",
      "projectPage": "https://devinxzhang.github.io/MFuser_ProjPage/",
      "githubRepo": "https://github.com/devinxzhang/MFuser"
    },
    "publishedAt": "2025-04-04T01:44:45.000Z",
    "title": "Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language\n  Models for Domain-Generalized Semantic Segmentation",
    "summary": "Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained\ntraction in Domain Generalized Semantic Segmentation (DGSS) due to their strong\ngeneralization capabilities. However, existing DGSS methods often rely\nexclusively on either VFMs or VLMs, overlooking their complementary strengths.\nVFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g.,\nCLIP) provide robust text alignment but struggle with coarse granularity.\nDespite their complementary strengths, effectively integrating VFMs and VLMs\nwith attention mechanisms is challenging, as the increased patch tokens\ncomplicate long-sequence modeling. To address this, we propose MFuser, a novel\nMamba-based fusion framework that efficiently combines the strengths of VFMs\nand VLMs while maintaining linear scalability in sequence length. MFuser\nconsists of two key components: MVFuser, which acts as a co-adapter to jointly\nfine-tune the two models by capturing both sequential and spatial dynamics; and\nMTEnhancer, a hybrid attention-Mamba module that refines text embeddings by\nincorporating image priors. Our approach achieves precise feature locality and\nstrong text alignment without incurring significant computational overhead.\nExtensive experiments demonstrate that MFuser significantly outperforms\nstate-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and\n71.87 mIoU on real-to-real benchmarks. The code is available at\nhttps://github.com/devinxzhang/MFuser.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03193.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6436290e76dbfd731bcf1f55",
      "avatarUrl": "/avatars/63af84ca382d799c4a000594c994d4bb.svg",
      "fullname": "Xin Zhang",
      "name": "XinNUS",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.02812",
      "authors": [
        {
          "_id": "67f4b54ddf6757586b52a4eb",
          "name": "Van Nguyen Nguyen",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ec",
          "name": "Stephen Tyree",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ed",
          "name": "Andrew Guo",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ee",
          "name": "Mederic Fourmy",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4ef",
          "name": "Anas Gouda",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f0",
          "name": "Taeyeop Lee",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f1",
          "name": "Sungphill Moon",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f2",
          "name": "Hyeontae Son",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f3",
          "name": "Lukas Ranftl",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f4",
          "name": "Jonathan Tremblay",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f5",
          "name": "Eric Brachmann",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f6",
          "name": "Bertram Drost",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f7",
          "name": "Vincent Lepetit",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f8",
          "name": "Carsten Rother",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4f9",
          "name": "Stan Birchfield",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fa",
          "name": "Jiri Matas",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fb",
          "name": "Yann Labbe",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fc",
          "name": "Martin Sundermeyer",
          "hidden": false
        },
        {
          "_id": "67f4b54ddf6757586b52a4fd",
          "name": "Tomas Hodan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T17:55:19.000Z",
      "submittedOnDailyAt": "2025-04-08T04:04:46.852Z",
      "title": "BOPチャレンジ2024 モデルベースとモデルフリー6Dオブジェクト姿勢推定",
      "submittedOnDailyBy": {
        "_id": "67400e2bf2d0992e1f373b9c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lysHv-F1aIjZXPaApGS8k.png",
        "isPro": false,
        "fullname": "hugw",
        "user": "hugw",
        "type": "user"
      },
      "summary": "BOP チャレンジ 2024 の評価手法、データセットと結果について紹介します。これは、6D オブジェクトポーズ推定および関連タスクの最新技術を捉えるために開催された公開コンペティションの第６回です。2024年には、BOP を実世界的なシナリオに移行させることを目標としました。\n\nまず、モデル無しタスクを導入しました。これらのタスクでは、3D オブジェクトモデルが利用できない状況で、方法は提供された参照ビデオからオブジェクトをオンボーディングする必要があります。次に、6D オブジェクト検出タスクを新しく定義し、テスト画像に見えるオブジェクトの識別情報が入力として提供されないようにしました。さらに、高解像度センサーとAR/VRヘッドセットを使用して新しいBOP-H3データセットを導入し、実世界的なシナリオに近いものを提供しました。BOP-H3は、モデルベースやモデル無しタスクをサポートするために3Dモデルとオンボーディングビデオを含みます。\n\n参加者は7つの挑戦トラックで競い合いました。これらのトラックは、タスク、オンボーディングセット、データセットグループによって定義されています。特に、2024年に最適なモデルベース6D位置決定の方法（FreeZeV2.1）は、2023年の最適な方法（GenFlow）よりBOP-Classic-Coreで22%の精度向上を収め、2023年の最適な方法（GPose2023）に対しては4%だけ落ちていますが、大幅に遅く（1画像当たり24.9秒）です。このタスクに対して実用的な2024年の方法は、Co-opです。Co-opは1画像当たり0.8秒で実行され、GenFlowより25倍速く、13%の精度向上を収めます。\n\n方法は6D検出と6D位置決定と同様に類似した順位を示しますが、実行時間が長いことに注意されます。モデルベースの2D検出では、2024年の最適な方法（MUSE）は2023年の最適な方法（CNOS）より21%の相対的な改善を収めました。しかし、未見のオブジェクトの2D検出精度は、見たオブジェクトの精度（GDet2023）に比べて明らかに低い（-53%）ことに気づきます。オンライン評価システムは開放しており、http://bop.felk.cvut.cz/ から利用可能です。",
      "upvotes": 2,
      "discussionId": "67f4b552df6757586b52a613"
    },
    "publishedAt": "2025-04-03T13:55:19.000Z",
    "title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose\n  Estimation",
    "summary": "We present the evaluation methodology, datasets and results of the BOP\nChallenge 2024, the sixth in a series of public competitions organized to\ncapture the state of the art in 6D object pose estimation and related tasks. In\n2024, our goal was to transition BOP from lab-like setups to real-world\nscenarios. First, we introduced new model-free tasks, where no 3D object models\nare available and methods need to onboard objects just from provided reference\nvideos. Second, we defined a new, more practical 6D object detection task where\nidentities of objects visible in a test image are not provided as input. Third,\nwe introduced new BOP-H3 datasets recorded with high-resolution sensors and\nAR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D\nmodels and onboarding videos to support both model-based and model-free tasks.\nParticipants competed on seven challenge tracks, each defined by a task, object\nonboarding setup, and dataset group. Notably, the best 2024 method for\nmodel-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher\naccuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only\n4% behind the best 2023 method for seen objects (GPose2023) although being\nsignificantly slower (24.9 vs 2.7s per image). A more practical 2024 method for\nthis task is Co-op which takes only 0.8s per image and is 25X faster and 13%\nmore accurate than GenFlow. Methods have a similar ranking on 6D detection as\non 6D localization but higher run time. On model-based 2D detection of unseen\nobjects, the best 2024 method (MUSE) achieves 21% relative improvement compared\nto the best 2023 method (CNOS). However, the 2D detection accuracy for unseen\nobjects is still noticealy (-53%) behind the accuracy for seen objects\n(GDet2023). The online evaluation system stays open and is available at\nhttp://bop.felk.cvut.cz/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02812.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67400e2bf2d0992e1f373b9c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lysHv-F1aIjZXPaApGS8k.png",
      "fullname": "hugw",
      "name": "hugw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.02882",
      "authors": [
        {
          "_id": "67f4d15a8f00d281c155880f",
          "name": "Sunghee Jung",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558810",
          "name": "Donghun Lee",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558811",
          "name": "Shinbok Lee",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558812",
          "name": "Gaeun Seo",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558813",
          "name": "Daniel Lee",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558814",
          "name": "Byeongil Ko",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558815",
          "name": "Junrae Cho",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558816",
          "name": "Kihyun Kim",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558817",
          "name": "Eunggyun Kim",
          "hidden": false
        },
        {
          "_id": "67f4d15a8f00d281c1558818",
          "name": "Myeongcheol Shin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T05:47:28.000Z",
      "submittedOnDailyAt": "2025-04-08T06:11:32.041Z",
      "title": "DiaTool-DPO: ツール付属の大規模言語モデルの多ターン直接好み最適化",
      "submittedOnDailyBy": {
        "_id": "65e30342e8b017ee1384824c",
        "avatarUrl": "/avatars/e5d07b037f611ccfaf719959d971d102.svg",
        "isPro": false,
        "fullname": "Sunghee Jung",
        "user": "hash2430",
        "type": "user"
      },
      "summary": "Tool-Augmented Large Language Models (TA-LLMs) は実世界的なアプリケーションにおいて望ましい結果を示していますが、不完全なクエリと範囲外のリクエストの処理に関しては課題があります。既存のアプローチは主に専門家のトラジェクトを基にした複数の監督学習であり、ここでは Direct Preference Optimization を用いた新しい方法 DiaTool-DPO を提案します。TA-LLM のインターフェースを 5 つの異なるダイアローグ状態としてマルコフィアンデザインプロセスとして構成し、ダイアローグ状態の遷移トラジェクトに基づいてユーザークエリを 3 つの種類に分類します。正しいおかしなダイアローグフローのペアのトラジェクトデータセットを自動的に構築し、ダイアローグ制御の特製的な目的関数損失を導入します。詳細な評価により、DiaTool-DPO は GPT-4o の性能を近似しています（情報の収集：94.8%、ツールコールの拒否：91%）、基準と比較して大幅な改善が見られます（44% と 9.6% の値）この上、コア機能を維持します。このアプローチは、追加の専門家のデモンストレーションや人間のラベリングが必要とならないような多様な実世界的なシナリオを扱う TA-LLM の開発に新しい可能性を開きます。",
      "upvotes": 2,
      "discussionId": "67f4d15c8f00d281c1558870"
    },
    "publishedAt": "2025-04-02T01:47:28.000Z",
    "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models",
    "summary": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02882.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e30342e8b017ee1384824c",
      "avatarUrl": "/avatars/e5d07b037f611ccfaf719959d971d102.svg",
      "fullname": "Sunghee Jung",
      "name": "hash2430",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03964",
      "authors": [
        {
          "_id": "67f4889c7e1624ebbaef710d",
          "user": {
            "_id": "652ee41e7b0079ff035b2269",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ee41e7b0079ff035b2269/jv-2-UPHN_e9Lhwl3Evwh.jpeg",
            "isPro": false,
            "fullname": "Simon Lee",
            "user": "Simonlee711",
            "type": "user"
          },
          "name": "Simon A. Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:44.271Z",
          "hidden": false
        },
        {
          "_id": "67f4889c7e1624ebbaef710e",
          "name": "Anthony Wu",
          "hidden": false
        },
        {
          "_id": "67f4889c7e1624ebbaef710f",
          "name": "Jeffrey N. Chiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T22:14:12.000Z",
      "submittedOnDailyAt": "2025-04-08T05:40:27.532Z",
      "title": "クリニカル・モダン・BERT: 生物医学文書の効率的な長文脈エンコーダー",
      "submittedOnDailyBy": {
        "_id": "652ee41e7b0079ff035b2269",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ee41e7b0079ff035b2269/jv-2-UPHN_e9Lhwl3Evwh.jpeg",
        "isPro": false,
        "fullname": "Simon Lee",
        "user": "Simonlee711",
        "type": "user"
      },
      "summary": "クリニカル・モダン・BERTを紹介します。これは、大規模な生物医学文献、クリニック記録、医学オントロジーに基づいて、PubMedの要約、MIMIC IVのクリニックデータ、および医学コードとその文脈的な説明を統合したTransformerベースのエンコーダです。現在の最先端の自然言語文脈エンコーダであるModernBERTに基づいて、回転位置埋め（RoPE）、Flash Attention、および8,192トークンの拡張コンテキスト長などのアーキテクチャアップグレードを採用し、特に生物医学とクリニック領域に適したこれらのイノベーションを適用しています。クリニカル・モダン・BERTは、長文脈タスクに適した語意的に豊富な表現を生成することで優れます。これを証明するために、学習済み重みの分析とクリニックNLPベンチマークの実験的な評価を行います。",
      "upvotes": 1,
      "discussionId": "67f4889d7e1624ebbaef715a",
      "githubRepo": "https://github.com/Simonlee711/Clinical_ModernBERT"
    },
    "publishedAt": "2025-04-04T18:14:12.000Z",
    "title": "Clinical ModernBERT: An efficient and long context encoder for\n  biomedical text",
    "summary": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on\nlarge scale biomedical literature, clinical notes, and medical ontologies,\nincorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with\ntheir textual descriptions. Building on ModernBERT the current state of the art\nnatural language text encoder featuring architectural upgrades such as rotary\npositional embeddings (RoPE), Flash Attention, and extended context length up\nto 8,192 tokens our model adapts these innovations specifically for biomedical\nand clinical domains. Clinical ModernBERT excels at producing semantically rich\nrepresentations tailored for long context tasks. We validate this both by\nanalyzing its pretrained weights and through empirical evaluation on a\ncomprehensive suite of clinical NLP benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03964.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652ee41e7b0079ff035b2269",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652ee41e7b0079ff035b2269/jv-2-UPHN_e9Lhwl3Evwh.jpeg",
      "fullname": "Simon Lee",
      "name": "Simonlee711",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.03770",
      "authors": [
        {
          "_id": "67f4a80a261cb1c328d9b0a2",
          "name": "Yi Nian",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a3",
          "user": {
            "_id": "6614243f67d7bfc73afc6b77",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614243f67d7bfc73afc6b77/ifdjAb_BPraEJ2eIySk_K.jpeg",
            "isPro": false,
            "fullname": "Shenzhe Zhu",
            "user": "Chouoftears",
            "type": "user"
          },
          "name": "Shenzhe Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-08T06:51:27.601Z",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a4",
          "name": "Yuehan Qin",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a5",
          "name": "Li Li",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a6",
          "name": "Ziyi Wang",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a7",
          "name": "Chaowei Xiao",
          "hidden": false
        },
        {
          "_id": "67f4a80a261cb1c328d9b0a8",
          "name": "Yue Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6614243f67d7bfc73afc6b77/z0oklTQ627SEQN42jYoat.png"
      ],
      "publishedAt": "2025-04-03T05:00:28.000Z",
      "submittedOnDailyAt": "2025-04-08T03:10:00.355Z",
      "title": "JailDAM: ビジョン-言語モデルの適応メモリによるジャイルブレイク検出",
      "submittedOnDailyBy": {
        "_id": "6614243f67d7bfc73afc6b77",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614243f67d7bfc73afc6b77/ifdjAb_BPraEJ2eIySk_K.jpeg",
        "isPro": false,
        "fullname": "Shenzhe Zhu",
        "user": "Chouoftears",
        "type": "user"
      },
      "summary": "多模态大語言モデル（MLLM）は、視覚言語タスクで優れていますが、特にジャイルブレイク攻撃による有害内容の生成にも重大なリスクがあります。ジャイルブレイク攻撃は、モデルの安全機能を通過するための意図的な操作で、適切なものでないもしくは不安全な内容の生成により、リスクが高まります。これらの攻撃を検出することは、MLLMの責任的な採用を確保するために重要です。現在のジャイルブレイク検出方法は、3つの主な課題に直面しています：（1）多くはモデルの隠れ状態または勾配を基に、白箱モデルのみに適用可能で、モデルの内部機能がアクセス可能である；（2）確率ベースの分析からの高い計算オーバーヘッドが、実時間検出に制限をかけます；（3）完全にラベル付けされた有害データセットが、実世界的な設定では稀少です。これらの問題に対処するために、私たちはテスト時に適応可能なフレームワーク「JAILDAM」を介しています。私たちの方法は、政策駆動の不安全な知識表現をガイドするメモリベースのアプローチを利用し、有害データに直接接触する必要を消去します。テスト時に不安全な知識を動的に更新することで、私たちのフレームワークは、未見のジャイルブレイクステージに対する一般化を改善し、同時に効率を維持します。多くのVLMジャイルブレイクベンチマーク上の実験は、JAILDAMが有害内容検出に最先端の性能を提供し、精度と速度の両方を向上させることを示しています。",
      "upvotes": 1,
      "discussionId": "67f4a80b261cb1c328d9b0e9",
      "githubRepo": "https://github.com/ShenzheZhu/JailDAM"
    },
    "publishedAt": "2025-04-03T01:00:28.000Z",
    "title": "JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language\n  Model",
    "summary": "Multimodal large language models (MLLMs) excel in vision-language tasks but\nalso pose significant risks of generating harmful content, particularly through\njailbreak attacks. Jailbreak attacks refer to intentional manipulations that\nbypass safety mechanisms in models, leading to the generation of inappropriate\nor unsafe content. Detecting such attacks is critical to ensuring the\nresponsible deployment of MLLMs. Existing jailbreak detection methods face\nthree primary challenges: (1) Many rely on model hidden states or gradients,\nlimiting their applicability to white-box models, where the internal workings\nof the model are accessible; (2) They involve high computational overhead from\nuncertainty-based analysis, which limits real-time detection, and (3) They\nrequire fully labeled harmful datasets, which are often scarce in real-world\nsettings. To address these issues, we introduce a test-time adaptive framework\ncalled JAILDAM. Our method leverages a memory-based approach guided by\npolicy-driven unsafe knowledge representations, eliminating the need for\nexplicit exposure to harmful data. By dynamically updating unsafe knowledge\nduring test-time, our framework improves generalization to unseen jailbreak\nstrategies while maintaining efficiency. Experiments on multiple VLM jailbreak\nbenchmarks demonstrate that JAILDAM delivers state-of-the-art performance in\nharmful content detection, improving both accuracy and speed.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6614243f67d7bfc73afc6b77/z0oklTQ627SEQN42jYoat.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03770.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6614243f67d7bfc73afc6b77",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614243f67d7bfc73afc6b77/ifdjAb_BPraEJ2eIySk_K.jpeg",
      "fullname": "Shenzhe Zhu",
      "name": "Chouoftears",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]