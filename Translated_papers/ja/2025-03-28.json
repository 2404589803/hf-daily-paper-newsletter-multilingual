[
  {
    "paper": {
      "id": "2503.21776",
      "authors": [
        {
          "_id": "67e6090248742d6df75853ae",
          "user": {
            "_id": "67079840a9bcb7459b8d2a46",
            "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
            "isPro": false,
            "fullname": "Kaituo Feng",
            "user": "KaituoFeng",
            "type": "user"
          },
          "name": "Kaituo Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:40:54.494Z",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853af",
          "user": {
            "_id": "642e427f6748dd4f8eeb2f38",
            "avatarUrl": "/avatars/07158ff6aa1803c846403594c5d55a34.svg",
            "isPro": false,
            "fullname": "Kaixiong Gong",
            "user": "kxgong",
            "type": "user"
          },
          "name": "Kaixiong Gong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:41:00.671Z",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b0",
          "user": {
            "_id": "6310b7e70a43f97f6c56191e",
            "avatarUrl": "/avatars/4a24c76e34d12c3d6230a4a081115f72.svg",
            "isPro": false,
            "fullname": "Bohao Li",
            "user": "BreakLee",
            "type": "user"
          },
          "name": "Bohao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:22.901Z",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b1",
          "user": {
            "_id": "6491af36c1741666238f3bff",
            "avatarUrl": "/avatars/0ee7d2ec1566e2cc5e8f144140e17f00.svg",
            "isPro": false,
            "fullname": "Zonghao Guo",
            "user": "guozonghao96",
            "type": "user"
          },
          "name": "Zonghao Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:41:07.167Z",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b2",
          "name": "Yibing Wang",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b3",
          "user": {
            "_id": "6538dd471ad9b3ba7c2df861",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6538dd471ad9b3ba7c2df861/MbEa7KHAK6u7PRb7WiPUC.jpeg",
            "isPro": false,
            "fullname": "Tianshuo Peng",
            "user": "Potentialts",
            "type": "user"
          },
          "name": "Tianshuo Peng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:41:29.644Z",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b4",
          "user": {
            "_id": "637c6703ca8542a0ba900ccb",
            "avatarUrl": "/avatars/288ed63a1efa566c3f01e850c6ba5dd5.svg",
            "isPro": false,
            "fullname": "Wang",
            "user": "Benyou",
            "type": "user"
          },
          "name": "Benyou Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:41:41.619Z",
          "hidden": false
        },
        {
          "_id": "67e6090248742d6df75853b5",
          "user": {
            "_id": "666a8f24e2990b0cb16b7bf9",
            "avatarUrl": "/avatars/fcbaf8f1e3e53a2a4a819b7cb2c53aa4.svg",
            "isPro": false,
            "fullname": "Xiangyu Yue",
            "user": "xyyue",
            "type": "user"
          },
          "name": "Xiangyu Yue",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:41:48.267Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:59:51.000Z",
      "submittedOnDailyAt": "2025-03-28T01:02:30.945Z",
      "title": "Video-R1: ビデオ認識の理由論を強化するMLLM",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "受DeepSeek-R1通过基于规则的强化学习（RL）激发推理能力的成功启发，我们介绍Video-R1作为首次尝试系统地探索R1范式以激发多模态大型语言模型（MLLMs）中的视频推理。然而，直接将GRPO算法的RL训练应用于视频推理面临两个主要挑战：（i）缺乏对视频推理的时间建模，（ii）高质量视频推理数据的稀缺。为了解决这些问题，我们首先提出了T-GRPO算法，鼓励模型利用视频中的时间信息进行推理。此外，我们没有仅依赖视频数据，而是将高质量的图像推理数据纳入训练过程。我们构建了两个数据集：Video-R1-COT-165k用于SFT冷启动，Video-R1-260k用于RL训练，两者均包含图像和视频数据。实验结果表明，Video-R1在视频推理基准如VideoMMMU和VSI-Bench上取得了显著的改进，以及在一般视频基准如MVBench和TempCompass等上。值得注意的是，Video-R1-7B在视频空间推理基准VSI-bench上达到了35.8%的准确率，超过了商业专有模型GPT-4o。所有代码、模型和数据均已发布。",
      "upvotes": 42,
      "discussionId": "67e6090348742d6df75853de",
      "ai_keywords": [
        "rule-based reinforcement learning (RL)",
        "GRPO algorithm",
        "temporal modeling",
        "Video-R1-COT-165k",
        "Video-R1-260k",
        "SFT cold start",
        "VideoMMMU",
        "VSI-Bench",
        "MVBench",
        "TempCompass",
        "video spatial reasoning",
        "GPT-4o",
        "T-GRPO algorithm"
      ]
    },
    "publishedAt": "2025-03-27T13:59:51.000Z",
    "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
    "summary": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for eliciting video reasoning\nwithin multimodal large language models (MLLMs). However, directly applying RL\ntraining with the GRPO algorithm to video reasoning presents two primary\nchallenges: (i) a lack of temporal modeling for video reasoning, and (ii) the\nscarcity of high-quality video-reasoning data. To address these issues, we\nfirst propose the T-GRPO algorithm, which encourages models to utilize temporal\ninformation in videos for reasoning. Additionally, instead of relying solely on\nvideo data, we incorporate high-quality image-reasoning data into the training\nprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold start\nand Video-R1-260k for RL training, both comprising image and video data.\nExperimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncodes, models, data are released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21776.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21620",
      "authors": [
        {
          "_id": "67e606fb6c44ab0376a498a1",
          "user": {
            "_id": "676127cf11b19ea602bb202a",
            "avatarUrl": "/avatars/dfd802a24bd63e509728159ebb1769f6.svg",
            "isPro": false,
            "fullname": "Zhengxi Lu",
            "user": "LZXzju",
            "type": "user"
          },
          "name": "Zhengxi Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:42:01.155Z",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a2",
          "user": {
            "_id": "6458ce236fa580137af5aa95",
            "avatarUrl": "/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg",
            "isPro": false,
            "fullname": "Yuxiang Chai",
            "user": "Yuxiang007",
            "type": "user"
          },
          "name": "Yuxiang Chai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:39.852Z",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a3",
          "user": {
            "_id": "65c237220c57a7141888363e",
            "avatarUrl": "/avatars/ce43c52f47d524c5b747523058946325.svg",
            "isPro": false,
            "fullname": "guoyaxuan",
            "user": "guoyaxuan0106",
            "type": "user"
          },
          "name": "Yaxuan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:42:12.269Z",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a4",
          "name": "Xi Yin",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a5",
          "name": "Liang Liu",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a6",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a7",
          "name": "Guanjing Xiong",
          "hidden": false
        },
        {
          "_id": "67e606fb6c44ab0376a498a8",
          "user": {
            "_id": "65c04e9c27a5fdca81abcbd9",
            "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
            "isPro": false,
            "fullname": "Hongsheng LI",
            "user": "hsli-cuhk",
            "type": "user"
          },
          "name": "Hongsheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:43:30.298Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T15:39:30.000Z",
      "submittedOnDailyAt": "2025-03-28T00:48:51.950Z",
      "title": "UI-R1: グラフィカルエージェントの行動予測を強化学習によって向上させる",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "最近のDeepSeek-R1は、ルールベースの報酬を用いた強化学習（RL）による理由論的な能力の発展を示しました。このアイデアに基づき、私たちは、ルールベースのRLがグラフィックユーザーインターフェース（GUI）アクション予測タスクの多タイプ大語言モデル（MLLMs）の理由論的な能力を向上させることができるかを最初に調査しました。このために、5種類の一般的なアクションタイプを含む136の難しいタスクの小さな高品質なデータセットをカレーレットし、また、グループ相対的なポリシー最適化（GRPO）などのポリシーベースアルゴリズムを用いたモデル最適化を可能にするために、一連のルールベースアクション報酬を導入しました。実験結果から、私たちが提案したデータエフィシェントなモデル、UI-R1-3Bは、データセット内（ID）とデータセット外（OOD）の両方で大幅な向上を収めました。特に、IDベンチマークAndroidControlでは、アクションタイプの正確性は15%上がり、基礎モデル（Qwen2.5-VL-3B）と比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比較して、基礎モデルと比",
      "upvotes": 27,
      "discussionId": "67e606fe6c44ab0376a49962",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "multimodal large language models (MLLMs)",
        "graphical user interface (GUI) action prediction tasks",
        "rule-based action reward",
        "Group Relative Policy Optimization (GRPO)",
        "in-domain (ID) tasks",
        "out-of-domain (OOD) tasks",
        "action type accuracy",
        "grounding accuracy",
        "supervised fine-tuning (SFT)",
        "GUI grounding benchmark ScreenSpot-Pro",
        "OS-Atlas-7B"
      ]
    },
    "publishedAt": "2025-03-27T11:39:30.000Z",
    "title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning",
    "summary": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities\nin LLMs through reinforcement learning (RL) with rule-based rewards. Building\non this idea, we are the first to explore how rule-based RL can enhance the\nreasoning capabilities of multimodal large language models (MLLMs) for graphic\nuser interface (GUI) action prediction tasks. To this end, we curate a small\nyet high-quality dataset of 136 challenging tasks, encompassing five common\naction types on mobile devices. We also introduce a unified rule-based action\nreward, enabling model optimization via policy-based algorithms such as Group\nRelative Policy Optimization (GRPO). Experimental results demonstrate that our\nproposed data-efficient model, UI-R1-3B, achieves substantial improvements on\nboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID\nbenchmark AndroidControl, the action type accuracy improves by 15%, while\ngrounding accuracy increases by 10.3%, compared with the base model (i.e.\nQwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model\nsurpasses the base model by 6.0% and achieves competitive performance with\nlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning\n(SFT) on 76K data. These results underscore the potential of rule-based\nreinforcement learning to advance GUI understanding and control, paving the way\nfor future research in this domain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21620.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21380",
      "authors": [
        {
          "_id": "67e5f4ad147ee85622ad0df1",
          "user": {
            "_id": "65df408822d66a997b4d5f6e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65df408822d66a997b4d5f6e/poROuCSvB39NZSiLzxLZf.jpeg",
            "isPro": false,
            "fullname": "Haoxiang Sun",
            "user": "CoderBak",
            "type": "user"
          },
          "name": "Haoxiang Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:46:46.418Z",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df2",
          "user": {
            "_id": "6703ac76ea890f0ca5b225eb",
            "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
            "isPro": false,
            "fullname": "Yingqian Min",
            "user": "EliverQ",
            "type": "user"
          },
          "name": "Yingqian Min",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:38:03.334Z",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df3",
          "user": {
            "_id": "629b765ce1af194c641fcbc6",
            "avatarUrl": "/avatars/7c53a4c2a1e528c19641a2b601731754.svg",
            "isPro": false,
            "fullname": "Zhipeng Chen",
            "user": "TimothyCzp",
            "type": "user"
          },
          "name": "Zhipeng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:38:01.177Z",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df4",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df5",
          "name": "Zheng Liu",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df6",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df7",
          "name": "Lei Fang",
          "hidden": false
        },
        {
          "_id": "67e5f4ad147ee85622ad0df8",
          "user": {
            "_id": "64b8c89052b7353d8c6a1013",
            "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
            "isPro": false,
            "fullname": "Ji-Rong Wen",
            "user": "jrwen",
            "type": "user"
          },
          "name": "Ji-Rong Wen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:47:20.525Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T11:20:17.000Z",
      "submittedOnDailyAt": "2025-03-28T01:15:16.769Z",
      "title": "理由の境界を挑む：オリンピックレベルの数学のベンチマークへの挑戦",
      "submittedOnDailyBy": {
        "_id": "648e6a4567aa8ab0e0e4c30f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648e6a4567aa8ab0e0e4c30f/b9OsE6C0a5iJE1yAlFG-_.jpeg",
        "isPro": false,
        "fullname": "Beichen Zhang",
        "user": "ToheartZhang",
        "type": "user"
      },
      "summary": "近年、大規模の推論モデルの急速な開発により、数学的推論を評価するための現在のベンチマークが飽和し、より難しく厳密な評価フレームワークの必要性が急迫的に見出されています。この空間に対処するために、オリンピックレベルの数学ベンチマーク「OlymMATH」を紹介します。このベンチマークは、LLMの複雑な推論能力を厳密に検証するために設計されています。OlymMATHは、200問の厳密に選ばれた問題を特徴とし、それぞれ手動で検証され、両語の英語と中国語の両方で利用可能です。問題は、(1)AIMEレベルの問題（容易）と(2)より難しい問題（難）の2つの異なる難易度レベルにわたります。前者は数学的推論の評価の基準を確立し、後者は現在の最先端のモデルの界限を超えるために設計されています。このベンチマークでは、4つの核心的な数学分野における問題があり、各問題には可証明的な数値解が含まれ、主観的な評価を避けた、ルールに基づく評価が可能です。実験結果は、OlymMATHによる重大な課題を明らかにし、最先端のモデルであるDeepSeek-R1とOpenAIのo3-miniは、難しいセットにおいて著しく限られた精度を示しています。また、このベンチマークは、主流の数学的推論ベンチマークでは主に解決されていない重要な次元として、数学的推論能力の詳細なバイリンガル評価を可能にします。OlymMATHベンチマークは、「STILL」プロジェクトで公開されています: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs。",
      "upvotes": 24,
      "discussionId": "67e5f4ae147ee85622ad0e27",
      "ai_keywords": [
        "DeepSeek-R1",
        "o3-mini"
      ]
    },
    "publishedAt": "2025-03-27T07:20:17.000Z",
    "title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math\n  Benchmark for Large Language Models",
    "summary": "In recent years, the rapid development of large reasoning models has resulted\nin the saturation of existing benchmarks for evaluating mathematical reasoning,\nhighlighting the urgent need for more challenging and rigorous evaluation\nframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level\nmathematical benchmark, designed to rigorously test the complex reasoning\ncapabilities of LLMs. OlymMATH features 200 meticulously curated problems, each\nmanually verified and available in parallel English and Chinese versions. The\nproblems are systematically organized into two distinct difficulty tiers: (1)\nAIME-level problems (easy) that establish a baseline for mathematical reasoning\nassessment, and (2) significantly more challenging problems (hard) designed to\npush the boundaries of current state-of-the-art models. In our benchmark, these\nproblems span four core mathematical fields, each including a verifiable\nnumerical solution to enable objective, rule-based evaluation. Empirical\nresults underscore the significant challenge presented by OlymMATH, with\nstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini\ndemonstrating notably limited accuracy on the hard subset. Furthermore, the\nbenchmark facilitates comprehensive bilingual assessment of mathematical\nreasoning abilities-a critical dimension that remains largely unaddressed in\nmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark\nat the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21380.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "648e6a4567aa8ab0e0e4c30f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648e6a4567aa8ab0e0e4c30f/b9OsE6C0a5iJE1yAlFG-_.jpeg",
      "fullname": "Beichen Zhang",
      "name": "ToheartZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21755",
      "authors": [
        {
          "_id": "67e60823284844fd3014f62b",
          "name": "Dian Zheng",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f62c",
          "user": {
            "_id": "60efe7fa0d920bc7805cada5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png",
            "isPro": false,
            "fullname": "Ziqi Huang",
            "user": "Ziqi",
            "type": "user"
          },
          "name": "Ziqi Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:48:07.968Z",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f62d",
          "user": {
            "_id": "6690dfd73bbfdee5f43ffc4d",
            "avatarUrl": "/avatars/88ff9b61663299d7751037696a75f1d7.svg",
            "isPro": false,
            "fullname": "Hongbo Liu",
            "user": "HongboLiu",
            "type": "user"
          },
          "name": "Hongbo Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:48:14.875Z",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f62e",
          "user": {
            "_id": "647993d9f966f086918da59e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647993d9f966f086918da59e/NDxz3PEpo3srZQNhwT7Qf.jpeg",
            "isPro": false,
            "fullname": "kzou",
            "user": "jackyhate",
            "type": "user"
          },
          "name": "Kai Zou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:25.760Z",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f62f",
          "user": {
            "_id": "65b9d9961fe588f824fde191",
            "avatarUrl": "/avatars/a9245958cc998a4b4b870bf2490fdaee.svg",
            "isPro": false,
            "fullname": "Yinan He",
            "user": "yinanhe",
            "type": "user"
          },
          "name": "Yinan He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:48:21.118Z",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f630",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f631",
          "name": "Yuanhan Zhang",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f632",
          "user": {
            "_id": "670749a9d827da9f37508209",
            "avatarUrl": "/avatars/f14fc05ad405f3967b9af0bcc73d4207.svg",
            "isPro": false,
            "fullname": "he jingwen",
            "user": "mimihe",
            "type": "user"
          },
          "name": "Jingwen He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:48:30.978Z",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f633",
          "name": "Wei-Shi Zheng",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f634",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67e60823284844fd3014f635",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:48:49.191Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:57:01.000Z",
      "submittedOnDailyAt": "2025-03-28T00:53:44.602Z",
      "title": "VBench-2.0: 内在忠实度に向けたビデオ生成ベンチマークセットの進展",
      "submittedOnDailyBy": {
        "_id": "60efe7fa0d920bc7805cada5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png",
        "isPro": false,
        "fullname": "Ziqi Huang",
        "user": "Ziqi",
        "type": "user"
      },
      "summary": "ビデオ生成は显著に進歩し、不実在な出力から視覚的に信頼できるようなビデオを生成するようになりました。これらのビデオ生成モデルを評価するために、VBenchなどのベンチマークが開発され、忠実度を評価し、一フレームの美術性、時系列的一致性、基本的なプロンプトの遵守度などの要素を測定します。しかし、これらの面では主に表面的な忠実度を示し、ビデオが視覚的に信頼できるかどうかを焦点にしていますが、実世界の原則に忠実であるかどうかに焦点を当てていません。最近のモデルはこれらのメトリックに対して逐次に良い性能を示していますが、ビデオが視覚的に信頼できるだけではなく、本質的に実際的であることを難しく感じています。実際の「ワールドモデル」を通じてビデオ生成で達成したい場合、次のフロンティアは内在的な忠実性の確保にあります。生成されたビデオが物理的法則、常識的推理、解剖学的正確性、構成の整合性に忠実であることを確認することが重要です。このレベルの写実性を達成することは、AIディレクトフィルムマKINGや記録世界モデリングなどのアプリケーションにおいて重要です。このギャップをカバーするために、VBench-2.0という次世代ベンチマークを紹介します。VBench-2.0は、ビデオ生成モデルの内在的な忠実性を自動的に評価するために設計されています。VBench-2.0は、Human Fidelity、Controllability、Creativity、Physics、Commonsenseの5つのキーディメンションを評価し、それぞれについてより細かくなる能力を含むものです。各キーディメンションに合わせて設計された評価フレームワークでは、最先端のVLMsとLLMsなどのジャンル別のジャンル専門家と、ビデオ生成に向けた異常検出手法などの専門家を組み合わせています。人間の判断との一致性を確保するために、拡張的な注釈を行います。表面的な忠実性から内在的な忠実性へと進むVBench-2.0は、次世代のビデオ生成モデルの開発に新たな標準を設定しようとしています。",
      "upvotes": 21,
      "discussionId": "67e60824284844fd3014f68e",
      "ai_keywords": [
        "VBench",
        "VBench-2.0",
        "visual generation",
        "per-frame aesthetics",
        "temporal consistency",
        "prompt adherence",
        "intrinsic faithfulness",
        "physical laws",
        "commonsense reasoning",
        "anatomical correctness",
        "compositional integrity",
        "AI-assisted filmmaking",
        "simulated world modeling",
        "VLMs",
        "LLMs",
        "anomaly detection"
      ]
    },
    "publishedAt": "2025-03-27T13:57:01.000Z",
    "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness",
    "summary": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21755.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60efe7fa0d920bc7805cada5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png",
      "fullname": "Ziqi Huang",
      "name": "Ziqi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21749",
      "authors": [
        {
          "_id": "67e6041d9a97e46f3102f7cc",
          "user": {
            "_id": "62c66504031996c36c86976a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
            "isPro": true,
            "fullname": "steve z",
            "user": "stzhao",
            "type": "user"
          },
          "name": "Shitian Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:47.519Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7cd",
          "user": {
            "_id": "64379d79fac5ea753f1c10f3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png",
            "isPro": false,
            "fullname": "Jerry Wu",
            "user": "QJerry",
            "type": "user"
          },
          "name": "Qilong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:55.109Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7ce",
          "user": {
            "_id": "66aba287b0f0b7411f511a47",
            "avatarUrl": "/avatars/1450f182c38e80066ae5ea5df4fa218f.svg",
            "isPro": false,
            "fullname": "Xinyue Li",
            "user": "Xxxy13",
            "type": "user"
          },
          "name": "Xinyue Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:52.434Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7cf",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d0",
          "user": {
            "_id": "6794cd79b72b1721ea69f4f2",
            "avatarUrl": "/avatars/4e4fb9e9e127a0c031131ace705687cd.svg",
            "isPro": false,
            "fullname": "Ming Li",
            "user": "afdsafas",
            "type": "user"
          },
          "name": "Ming Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:49.525Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d1",
          "user": {
            "_id": "66bb136002fd8eb58bc84ffb",
            "avatarUrl": "/avatars/122cb8f59c502392768099b3c2afe043.svg",
            "isPro": false,
            "fullname": "qinqi",
            "user": "Dakerqi",
            "type": "user"
          },
          "name": "Qi Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:58:27.757Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d2",
          "user": {
            "_id": "646f1bef075e11ca78da3bb7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f1bef075e11ca78da3bb7/gNS-ikyZXYeMrf4a7HTQE.jpeg",
            "isPro": false,
            "fullname": "Dongyang Liu (Chris Liu)",
            "user": "Cxxs",
            "type": "user"
          },
          "name": "Dongyang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:49:23.924Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d3",
          "user": {
            "_id": "63527f4e7d071f23d085ad45",
            "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
            "isPro": false,
            "fullname": "KAIPENG ZHANG",
            "user": "kpzhang",
            "type": "user"
          },
          "name": "Kaipeng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:49:30.453Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d4",
          "user": {
            "_id": "65c04e9c27a5fdca81abcbd9",
            "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
            "isPro": false,
            "fullname": "Hongsheng LI",
            "user": "hsli-cuhk",
            "type": "user"
          },
          "name": "Hongsheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:49:37.542Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d5",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d6",
          "user": {
            "_id": "67b299cc6f6dc4376d9e6c76",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UniMpmfOUlyiSOrf47wuT.png",
            "isPro": false,
            "fullname": "Peng Gao",
            "user": "cosumosu25",
            "type": "user"
          },
          "name": "Peng Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:49:44.411Z",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d7",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "67e6041d9a97e46f3102f7d8",
          "user": {
            "_id": "6285a9133ab6642179158944",
            "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
            "isPro": false,
            "fullname": "Zhen Li",
            "user": "Paper99",
            "type": "user"
          },
          "name": "Zhen Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:58:29.550Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:56:15.000Z",
      "submittedOnDailyAt": "2025-03-28T00:54:19.590Z",
      "title": "LeX-Art: スケーラブルな高品質データの合成を通じての文生成の再考",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "LeX-Artは、プロンプトの表現力と文字描画の忠実度の間の隙間をシステマティックに埋め合わせるための高品質なテキスト・イメージ合成の一構成です。我々のアプローチはデータセンター的パラダイムに基づき、Deepseek-R1に基づいた高品質なデータ合成パイプラインを構築して、LeX-10Kという10K枚の高解像度、美術的に整飾された1024×1024画像のデータセットを編集します。データセットの構築を超えて、我々はLeX-Enhancerという強力なプロンプト増強モデルを開発し、LeX-FLUXとLeX-Luminaという2つのテキスト・イメージモデルを訓練し、最先端のテキスト描画性能を達成します。ビジュアルテキスト生成のシステマティックな評価に向けて、Pairwise Normalized Edit Distance (PNED)という新しいメトリックを用いた、忠実度、美術性、一致性を評価するLeX-Benchというベンチマークを導入します。実験は、LeX-LuminaがCreateBenchで79.81%のPNED収益を達成し、LeX-FLUXは色(+3.18%)、位置(+4.45%)、フォント精度(+3.81%)でベースラインを超えた効果を示しました。我々のコード、モデル、データセット、デモは公開的に利用可能です。",
      "upvotes": 15,
      "discussionId": "67e6041f9a97e46f3102f89b",
      "projectPage": "https://zhaoshitian.github.io/lexart/",
      "githubRepo": "https://github.com/zhaoshitian/LeX-Art/",
      "ai_keywords": [
        "Deepseek-R1",
        "LeX-Enhancer",
        "LeX-FLUX",
        "LeX-Lumina",
        "LeX-Bench",
        "Pairwise Normalized Edit Distance (PNED)"
      ]
    },
    "publishedAt": "2025-03-27T13:56:15.000Z",
    "title": "LeX-Art: Rethinking Text Generation via Scalable High-Quality Data\n  Synthesis",
    "summary": "We introduce LeX-Art, a comprehensive suite for high-quality text-image\nsynthesis that systematically bridges the gap between prompt expressiveness and\ntext rendering fidelity. Our approach follows a data-centric paradigm,\nconstructing a high-quality data synthesis pipeline based on Deepseek-R1 to\ncurate LeX-10K, a dataset of 10K high-resolution, aesthetically refined\n1024times1024 images. Beyond dataset construction, we develop LeX-Enhancer,\na robust prompt enrichment model, and train two text-to-image models, LeX-FLUX\nand LeX-Lumina, achieving state-of-the-art text rendering performance. To\nsystematically evaluate visual text generation, we introduce LeX-Bench, a\nbenchmark that assesses fidelity, aesthetics, and alignment, complemented by\nPairwise Normalized Edit Distance (PNED), a novel metric for robust text\naccuracy evaluation. Experiments demonstrate significant improvements, with\nLeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX\noutperforming baselines in color (+3.18%), positional (+4.45%), and font\naccuracy (+3.81%). Our codes, models, datasets, and demo are publicly\navailable.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21749.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21460",
      "authors": [
        {
          "_id": "67e609ee389245233f0d316f",
          "user": {
            "_id": "642da1cd99f3110ac27caca5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
            "isPro": false,
            "fullname": "junyu",
            "user": "luojunyu",
            "type": "user"
          },
          "name": "Junyu Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:10.158Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3170",
          "name": "Weizhi Zhang",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3171",
          "name": "Ye Yuan",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3172",
          "user": {
            "_id": "668388cb549c1b932c9fe699",
            "avatarUrl": "/avatars/aa7523fbde4c2a8508cff13c74291e6a.svg",
            "isPro": false,
            "fullname": "Yusheng Zhao",
            "user": "yszhao",
            "type": "user"
          },
          "name": "Yusheng Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:54:38.409Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3173",
          "name": "Junwei Yang",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3174",
          "user": {
            "_id": "6329a8ff688ad82b783b0e54",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1663674611122-noauth.png",
            "isPro": false,
            "fullname": "Yiyang Gu",
            "user": "evan-gyy",
            "type": "user"
          },
          "name": "Yiyang Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:55:04.907Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3175",
          "name": "Bohan Wu",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3176",
          "name": "Binqi Chen",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3177",
          "user": {
            "_id": "67d3e9f53c8b9f6c843aacaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/LTPxPALNDWWTGnP_K30hH.png",
            "isPro": false,
            "fullname": "Ziyue Qiao",
            "user": "joeyleo",
            "type": "user"
          },
          "name": "Ziyue Qiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:55:52.770Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3178",
          "user": {
            "_id": "648c0620c2e1388f44e2eddc",
            "avatarUrl": "/avatars/ab093add13d5eb6032e47aea356ca9f2.svg",
            "isPro": false,
            "fullname": "Qingqing Long",
            "user": "qqlong",
            "type": "user"
          },
          "name": "Qingqing Long",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:56:00.392Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3179",
          "name": "Rongcheng Tu",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317a",
          "name": "Xiao Luo",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317b",
          "name": "Wei Ju",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317c",
          "user": {
            "_id": "66ab566e30c55e83b02aa050",
            "avatarUrl": "/avatars/62692be88b9ad34ad3f474fb0359ae20.svg",
            "isPro": false,
            "fullname": "Zhiping Xiao",
            "user": "Shockzipper",
            "type": "user"
          },
          "name": "Zhiping Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:56:14.710Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317d",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317e",
          "name": "Meng Xiao",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d317f",
          "name": "Chenwu Liu",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3180",
          "name": "Jingyang Yuan",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3181",
          "user": {
            "_id": "64b6d98861dc301f1326341a",
            "avatarUrl": "/avatars/14df7497a1a982894f5889903793773f.svg",
            "isPro": false,
            "fullname": "Shichang Zhang",
            "user": "shichangzh",
            "type": "user"
          },
          "name": "Shichang Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:56:59.067Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3182",
          "user": {
            "_id": "60d596784cf0297c143fcd33",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d596784cf0297c143fcd33/phknQ4Z2VuUj3akhcoxLC.png",
            "isPro": false,
            "fullname": "Yiqiao Jin",
            "user": "Ahren09",
            "type": "user"
          },
          "name": "Yiqiao Jin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:57:05.559Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3183",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3184",
          "name": "Xian Wu",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3185",
          "name": "Hanqing Zhao",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3186",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3187",
          "user": {
            "_id": "67a088c531bab0a2a39665d4",
            "avatarUrl": "/avatars/7188815ff8b5e4a475e7ebc09687e10d.svg",
            "isPro": false,
            "fullname": "Philip Yu",
            "user": "philipyu",
            "type": "user"
          },
          "name": "Philip S. Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:57:54.103Z",
          "hidden": false
        },
        {
          "_id": "67e609ee389245233f0d3188",
          "name": "Ming Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/hXVaNIVxOX8326Ri2Px11.png",
        "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/UuH6cO5owx0fzF8WFnGq5.png"
      ],
      "publishedAt": "2025-03-27T12:50:17.000Z",
      "submittedOnDailyAt": "2025-03-28T01:08:58.527Z",
      "title": "大語言モデルアガント：方法、アプリケーションと課題の調査",
      "submittedOnDailyBy": {
        "_id": "642da1cd99f3110ac27caca5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
        "isPro": false,
        "fullname": "junyu",
        "user": "luojunyu",
        "type": "user"
      },
      "summary": "智能アガントの時代は、大語言モデルの革命的な進歩によって到来しています。大語言モデル（LLM）アガントは、目標駆動的な行動と動的な適応能力を持っていることで、人工知能の一般的な知能に向けた重要なパスウェイとして潜在的に重要です。この調査は、アガントシステムを方法学エンサインのタクソロジーで系統的に解体し、アーキテクチャの基盤、コラボレーション機構、エピジェントルパタワーを結びつけています。我々は、アガント設計原理と複雑な環境でのエピジェントル行動の基本的な関係を明らかにし、研究の細かな線を一つにしています。我々の仕事は、アガントがどのように構築され、どのようにコラボレート、どのように時間を経て進化するかを見積もる一連のアーキテクチャ的な視点を提供し、評価の方法学、ツールの適用、実用的な課題、多様な応用領域を取り扱います。この急速に進化する分野の最新の進歩を調査し、研究者にLLMアガントを理解するための構造化されたタクソロジーを提供し、将来の研究の有望な方向を特定します。このコレクションは、https://github.com/luo-junyu/Awesome-Agent-Papers から利用できます。",
      "upvotes": 12,
      "discussionId": "67e609ef389245233f0d31c0",
      "projectPage": "https://huggingface.co/spaces/luojunyu/Agent-Papers",
      "githubRepo": "https://github.com/luo-junyu/Awesome-Agent-Papers",
      "ai_keywords": [
        "Large Language Model (LLM) agents",
        "goal-driven behaviors",
        "dynamic adaptation capabilities",
        "artificial general intelligence",
        "methodology-centered taxonomy",
        "architectural foundations",
        "collaboration mechanisms",
        "evolutionary pathways",
        "agent design principles",
        "emergent behaviors",
        "complex environments",
        "unified architectural perspective",
        "evaluation methodologies",
        "practical challenges",
        "diverse application domains"
      ]
    },
    "publishedAt": "2025-03-27T08:50:17.000Z",
    "title": "Large Language Model Agent: A Survey on Methodology, Applications and\n  Challenges",
    "summary": "The era of intelligent agents is upon us, driven by revolutionary\nadvancements in large language models. Large Language Model (LLM) agents, with\ngoal-driven behaviors and dynamic adaptation capabilities, potentially\nrepresent a critical pathway toward artificial general intelligence. This\nsurvey systematically deconstructs LLM agent systems through a\nmethodology-centered taxonomy, linking architectural foundations, collaboration\nmechanisms, and evolutionary pathways. We unify fragmented research threads by\nrevealing fundamental connections between agent design principles and their\nemergent behaviors in complex environments. Our work provides a unified\narchitectural perspective, examining how agents are constructed, how they\ncollaborate, and how they evolve over time, while also addressing evaluation\nmethodologies, tool applications, practical challenges, and diverse application\ndomains. By surveying the latest developments in this rapidly evolving field,\nwe offer researchers a structured taxonomy for understanding LLM agents and\nidentify promising directions for future research. The collection is available\nat https://github.com/luo-junyu/Awesome-Agent-Papers.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/hXVaNIVxOX8326Ri2Px11.png",
      "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/UuH6cO5owx0fzF8WFnGq5.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21460.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642da1cd99f3110ac27caca5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
      "fullname": "junyu",
      "name": "luojunyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21758",
      "authors": [
        {
          "_id": "67e6092a40fb111ac9342c39",
          "user": {
            "_id": "66bb136002fd8eb58bc84ffb",
            "avatarUrl": "/avatars/122cb8f59c502392768099b3c2afe043.svg",
            "isPro": false,
            "fullname": "qinqi",
            "user": "Dakerqi",
            "type": "user"
          },
          "name": "Qi Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:18.440Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3a",
          "user": {
            "_id": "6358a167f56b03ec9147074d",
            "avatarUrl": "/avatars/e54ea7bf0c240cf76d538296efb3976c.svg",
            "isPro": false,
            "fullname": "Le Zhuo",
            "user": "JackyZhuo",
            "type": "user"
          },
          "name": "Le Zhuo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:50:03.385Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3b",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3c",
          "user": {
            "_id": "64a54586c0f13de8e7093314",
            "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg",
            "isPro": false,
            "fullname": "Ruoyi Du",
            "user": "RuoyiDu",
            "type": "user"
          },
          "name": "Ruoyi Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:50:09.220Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3d",
          "user": {
            "_id": "6285a9133ab6642179158944",
            "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
            "isPro": false,
            "fullname": "Zhen Li",
            "user": "Paper99",
            "type": "user"
          },
          "name": "Zhen Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:58:25.783Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3e",
          "name": "Bin Fu",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c3f",
          "user": {
            "_id": "6467ec2f374fe5728d4216e0",
            "avatarUrl": "/avatars/4d14f64572c9c0707edb54993e331a49.svg",
            "isPro": false,
            "fullname": "Yiting Lu",
            "user": "luyiting",
            "type": "user"
          },
          "name": "Yiting Lu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:50:17.491Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c40",
          "user": {
            "_id": "64a3d1ddb3239f3e3892b24b",
            "avatarUrl": "/avatars/7ce585f5fc1d077fb1d70cc18c4da2c1.svg",
            "isPro": false,
            "fullname": "Jiakang Yuan",
            "user": "JiakangYuan",
            "type": "user"
          },
          "name": "Jiakang Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:50:23.000Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c41",
          "user": {
            "_id": "66aba287b0f0b7411f511a47",
            "avatarUrl": "/avatars/1450f182c38e80066ae5ea5df4fa218f.svg",
            "isPro": false,
            "fullname": "Xinyue Li",
            "user": "Xxxy13",
            "type": "user"
          },
          "name": "Xinyue Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:20.809Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c42",
          "user": {
            "_id": "646f1bef075e11ca78da3bb7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f1bef075e11ca78da3bb7/gNS-ikyZXYeMrf4a7HTQE.jpeg",
            "isPro": false,
            "fullname": "Dongyang Liu (Chris Liu)",
            "user": "Cxxs",
            "type": "user"
          },
          "name": "Dongyang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:50:37.419Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c43",
          "name": "Xiangyang Zhu",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c44",
          "name": "Manyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c45",
          "user": {
            "_id": "629aed605ab4232a3fe266f7",
            "avatarUrl": "/avatars/53e1f4a9fc2bad17b05a80b14118442e.svg",
            "isPro": false,
            "fullname": "Will Beddow",
            "user": "willbeddow",
            "type": "user"
          },
          "name": "Will Beddow",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:51:18.703Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c46",
          "user": {
            "_id": "62a2712903bf94c3ac3ae004",
            "avatarUrl": "/avatars/2f11b73ecd7a4cb561b42c676b70b7f8.svg",
            "isPro": false,
            "fullname": "Erwann Millon",
            "user": "erwann",
            "type": "user"
          },
          "name": "Erwann Millon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:51:24.290Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c47",
          "name": "Victor Perez",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c48",
          "user": {
            "_id": "64d1c560c0c627dfa71bdbe0",
            "avatarUrl": "/avatars/f42794fe25bffcd870a1bcee69b95298.svg",
            "isPro": false,
            "fullname": "wenhai.wang",
            "user": "wangwhcore",
            "type": "user"
          },
          "name": "Wenhai Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:51:42.390Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c49",
          "user": {
            "_id": "63f9fca8d4349b157a109eec",
            "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
            "isPro": false,
            "fullname": "Conghui He",
            "user": "conghui",
            "type": "user"
          },
          "name": "Conghui He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:51:48.715Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4a",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4b",
          "name": "Xiaohong Liu",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4c",
          "user": {
            "_id": "65c04e9c27a5fdca81abcbd9",
            "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
            "isPro": false,
            "fullname": "Hongsheng LI",
            "user": "hsli-cuhk",
            "type": "user"
          },
          "name": "Hongsheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:52:25.823Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4d",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4e",
          "user": {
            "_id": "6391fa34e110d51320389b06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670510944260-noauth.jpeg",
            "isPro": false,
            "fullname": "chang xu",
            "user": "changxu-2022",
            "type": "user"
          },
          "name": "Chang Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:52:36.782Z",
          "hidden": false
        },
        {
          "_id": "67e6092a40fb111ac9342c4f",
          "name": "Peng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:57:07.000Z",
      "submittedOnDailyAt": "2025-03-28T00:59:52.114Z",
      "title": "Lumina-Image 2.0: 統一的および効率的な画像生成フレームワーク",
      "submittedOnDailyBy": {
        "_id": "6285a9133ab6642179158944",
        "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
        "isPro": false,
        "fullname": "Zhen Li",
        "user": "Paper99",
        "type": "user"
      },
      "summary": "Lumina-Image 2.0を紹介します。これは先週のLumina-Nextと比較して显著な進歩を収めた先進的な文字から画像生成フレームワークです。Lumina-Image 2.0は2つのキーの原則に基づいて構築されています。\n\n1. 統一化 - これは統一的なアーキテクチャ（Unified Next-DiT）を採用し、文字と画像トークンを共通のシーケンスとして扱い、自然なクロスモードの相互作用を可能にし、無難なタスク拡張を可能にします。また、高品質のカプチャナーソースが意味的にマッチした文字-画像の訓練ペアを提供できるため、T2I生成タスクに特化された統一的なカプチャシステム、Unified Captioner（UniCap）を導入します。UniCapは詳細かつ正確なカプチャを生成し、収束を加速し、プロンプトの従順性を向上させます。\n\n2. 効率性 - モデルの効率を向上させるために、多段階的な進歩的な訓練戦略を開発し、画像の品質を損なさない限り推論加速技術を導入します。学術的なベンチマークと公開された文字から画像のフォーラムでの検証は、Lumina-Image 2.0は26億パラメータでも強力な性能を示し、スケーラビリティと設計の効率性を特徴として示しています。訓練の詳細、コード、モデルは以下のURLで公開しています。\n\nhttps://github.com/Alpha-VLLM/Lumina-Image-2.0",
      "upvotes": 11,
      "discussionId": "67e6092e40fb111ac9342d4e",
      "projectPage": "https://github.com/Alpha-VLLM/Lumina-Image-2.0",
      "githubRepo": "https://github.com/Alpha-VLLM/Lumina-Image-2.0",
      "ai_keywords": [
        "Unified Next-DiT",
        "text and image tokens",
        "cross-modal interactions",
        "Unified Captioner (UniCap)",
        "text-to-image (T2I)",
        "multi-stage progressive training",
        "inference acceleration techniques",
        "academic benchmarks",
        "public text-to-image arenas"
      ]
    },
    "publishedAt": "2025-03-27T13:57:07.000Z",
    "title": "Lumina-Image 2.0: A Unified and Efficient Image Generative Framework",
    "summary": "We introduce Lumina-Image 2.0, an advanced text-to-image generation framework\nthat achieves significant progress compared to previous work, Lumina-Next.\nLumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts\na unified architecture (Unified Next-DiT) that treats text and image tokens as\na joint sequence, enabling natural cross-modal interactions and allowing\nseamless task expansion. Besides, since high-quality captioners can provide\nsemantically well-aligned text-image training pairs, we introduce a unified\ncaptioning system, Unified Captioner (UniCap), specifically designed for T2I\ngeneration tasks. UniCap excels at generating comprehensive and accurate\ncaptions, accelerating convergence and enhancing prompt adherence. (2)\nEfficiency - to improve the efficiency of our proposed model, we develop\nmulti-stage progressive training strategies and introduce inference\nacceleration techniques without compromising image quality. Extensive\nevaluations on academic benchmarks and public text-to-image arenas show that\nLumina-Image 2.0 delivers strong performances even with only 2.6B parameters,\nhighlighting its scalability and design efficiency. We have released our\ntraining details, code, and models at\nhttps://github.com/Alpha-VLLM/Lumina-Image-2.0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21758.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6285a9133ab6642179158944",
      "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg",
      "fullname": "Zhen Li",
      "name": "Paper99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21729",
      "authors": [
        {
          "_id": "67e610ee6e73232cf0903b73",
          "user": {
            "_id": "652542861e9db26e407aa1fc",
            "avatarUrl": "/avatars/4c47ef4564f498a7f34b4a17a1e209a8.svg",
            "isPro": false,
            "fullname": "Lee Zhicheng",
            "user": "ZhiCheng0326",
            "type": "user"
          },
          "name": "Zhicheng Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:06.279Z",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b74",
          "user": {
            "_id": "62c924a6334a6ee11c2e8dfa",
            "avatarUrl": "/avatars/3dbc37af162b94d68cb83665ac4528c3.svg",
            "isPro": false,
            "fullname": "ShulinCao",
            "user": "caoshulin",
            "type": "user"
          },
          "name": "Shulin Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:52:53.008Z",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b75",
          "name": "Jinxin Liu",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b76",
          "user": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "isPro": false,
            "fullname": "Jiajie Zhang",
            "user": "NeoZ123",
            "type": "user"
          },
          "name": "Jiajie Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:53:38.290Z",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b77",
          "user": {
            "_id": "61132a7ab75d3040e6e88a3a",
            "avatarUrl": "/avatars/faf9d96770251f31e7e4edbf1fee9798.svg",
            "isPro": false,
            "fullname": "liuweichuan",
            "user": "liuweichuan",
            "type": "user"
          },
          "name": "Weichuan Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:53:49.672Z",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b78",
          "user": {
            "_id": "65d66cb2b06abf924b07ff76",
            "avatarUrl": "/avatars/de94e2fe07040b7dc3053bcaafa64ffe.svg",
            "isPro": false,
            "fullname": "Xiaoyin Chen",
            "user": "chenyn66",
            "type": "user"
          },
          "name": "Xiaoyin Che",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:53:55.317Z",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b79",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "67e610ee6e73232cf0903b7a",
          "user": {
            "_id": "65df8cbc2705d9672f55d1aa",
            "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg",
            "isPro": false,
            "fullname": "Juanzi Li",
            "user": "juanli",
            "type": "user"
          },
          "name": "Juanzi Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:54:01.074Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/652542861e9db26e407aa1fc/OFvhHK9WLS_twsPoDxlpU.png"
      ],
      "publishedAt": "2025-03-27T17:44:18.000Z",
      "submittedOnDailyAt": "2025-03-28T01:51:24.016Z",
      "title": "ReaRAG: 知識ガイドドルモデルの理由論を強化した事実性向上のイテレーション的リビュージュアルジェネレーション",
      "submittedOnDailyBy": {
        "_id": "652542861e9db26e407aa1fc",
        "avatarUrl": "/avatars/4c47ef4564f498a7f34b4a17a1e209a8.svg",
        "isPro": false,
        "fullname": "Lee Zhicheng",
        "user": "ZhiCheng0326",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) は、驚異的な推理能力を示しますが、主にパラメーター知識を依存し、事実的な正確性を制限しています。最近の研究は、強化学習（RL）に基づく LRMs に検索機能を追加していますが、それらは過度な考え方と推理の顽健性の欠陥により、問題回答（QA）タスクの効果性を低下させています。これに対して、私たちは、事実性向上の推理モデルとして、多様なクエリを探索するための過度なイテレーションを避けるReaRAGを提案しています。私たちの解決策には、理由連鎖の長さに上限を設定した新しいデータ構築フレームワークを含みます。具体的には、最初に LRM を利用して深い考え方を生成し、事前定義された行動スペースから行動を選択します（検索と完了）。検索行動の場合、RAGエンジンに対してクエリが実行され、結果は後続の推理ステップをガイドする観察として返却されます。このプロセスは、完了行動が選択されるまで繰り返されます。ReaRAGの強力な推理能力を活用して、私たちのアプローチは既存のベースラインよりも多段階の QA に対して優れています。進一づの分析は、誤りの認識と推理プロセスの精緻化の強力な反省能力を明らかにしています。私たちの研究は、LRMs の事実性を向上させ、強力な推理をもつ検索拡張生成（RAG）に効果的に統合しています。",
      "upvotes": 11,
      "discussionId": "67e610ee6e73232cf0903ba6",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "parametric knowledge",
        "retrieval capabilities",
        "overthinking",
        "robustness",
        "question answering (QA)",
        "ReaRAG",
        "factuality-enhanced reasoning model",
        "reasoning chain length",
        "LRM",
        "deliberate thinking",
        "predefined action space",
        "Search",
        "Finish",
        "RAG engine",
        "observation",
        "multi-hop QA",
        "reflective ability",
        "reasoning trajectory",
        "Retrieval-Augmented Generation (RAG)"
      ]
    },
    "publishedAt": "2025-03-27T13:44:18.000Z",
    "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large\n  Reasoning Models with Iterative Retrieval Augmented Generation",
    "summary": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/652542861e9db26e407aa1fc/OFvhHK9WLS_twsPoDxlpU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21729.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652542861e9db26e407aa1fc",
      "avatarUrl": "/avatars/4c47ef4564f498a7f34b4a17a1e209a8.svg",
      "fullname": "Lee Zhicheng",
      "name": "ZhiCheng0326",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21248",
      "authors": [
        {
          "_id": "67e63581e8a16f38741f4baa",
          "user": {
            "_id": "661980b46a43b2760d8d551f",
            "avatarUrl": "/avatars/e4743d55303fe3a88688c29dd4c67a69.svg",
            "isPro": false,
            "fullname": "Yujie Liu",
            "user": "yujieliu",
            "type": "user"
          },
          "name": "Yujie Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:58:09.374Z",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bab",
          "user": {
            "_id": "646a11791556443f24b582e9",
            "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
            "isPro": false,
            "fullname": "Zonglin Yang",
            "user": "ZonglinY",
            "type": "user"
          },
          "name": "Zonglin Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:58:40.651Z",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bac",
          "name": "Tong Xie",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bad",
          "user": {
            "_id": "62a7362fd1e7a011fd4e31a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a7362fd1e7a011fd4e31a7/ZY_mwH-sI0o05SHpLqwc7.png",
            "isPro": false,
            "fullname": "Jinjie Ni",
            "user": "jinjieni",
            "type": "user"
          },
          "name": "Jinjie Ni",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:58:47.888Z",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bae",
          "user": {
            "_id": "64fe3c6b4c8924c4fed6d97b",
            "avatarUrl": "/avatars/5e0a49372af19aae9ec5ee84b299d111.svg",
            "isPro": false,
            "fullname": "Ben Gao",
            "user": "bgao22182",
            "type": "user"
          },
          "name": "Ben Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:58:54.480Z",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4baf",
          "user": {
            "_id": "65cb61ba0390fce629de99d1",
            "avatarUrl": "/avatars/9b4b4556ffd8de215dc37b02366d781b.svg",
            "isPro": false,
            "fullname": "Yuqiang Li",
            "user": "yuqiangli",
            "type": "user"
          },
          "name": "Yuqiang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:59:07.268Z",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bb0",
          "user": {
            "_id": "6436403bf3b08e267d9f0329",
            "avatarUrl": "/avatars/a5d9c3d47073e71e4cea124d9c17356d.svg",
            "isPro": false,
            "fullname": "SHIXIANG TANG",
            "user": "tangshixiang",
            "type": "user"
          },
          "name": "Shixiang Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:59:12.700Z",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bb1",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bb2",
          "name": "Erik Cambria",
          "hidden": false
        },
        {
          "_id": "67e63581e8a16f38741f4bb3",
          "user": {
            "_id": "6538b861613fe158bd581e35",
            "avatarUrl": "/avatars/6817dbfe903675721fd227058b0a91ac.svg",
            "isPro": false,
            "fullname": "Dongzhan Zhou",
            "user": "schrodingers-tiger",
            "type": "user"
          },
          "name": "Dongzhan Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T08:59:22.218Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T08:09:15.000Z",
      "submittedOnDailyAt": "2025-03-28T04:07:23.222Z",
      "title": "研究ベンチマーク：科学発見におけるLLMのベンチマーク化をインスピレーションベースのタスク分解によって",
      "submittedOnDailyBy": {
        "_id": "646a11791556443f24b582e9",
        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
        "isPro": false,
        "fullname": "Zonglin Yang",
        "user": "ZonglinY",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、科学研究に役立つ可能性を示していますが、高品質の研究仮説を発見する能力は、専門的なベンチマークのないため評価されていません。この欠点を解決するために、私たちは、科学発見の近満足なサブタスクの評価用の最初の大規模なベンチマークを紹介します：インスピレーション検索、仮説作成、仮説ランキング。私たちは、12分野の科学論文から研究問題、背景調査、インスピレーション、仮説を構成する重要な要素を自動化されたフレームワークで抽出し、専門家の評価でその精度を確認しました。データの汚染を防ぐために、2024年に出版された論文を専門的に焦点を当て、LLMの事前学習データとの重複を最小限に抑えました。評価結果によると、LLMsはインスピレーションの検索でよく働き、分布外のタスクでも良好な性能を示し、新しい知識関連を表面化する能力を示していることがわかります。これにより、LLMsは「研究仮説のミネラ」として位置付けられ、人間の手助けが少なくても、イノベーション的な仮説を大量に生成し、自動化された科学発見を促進することができるようになります。",
      "upvotes": 9,
      "discussionId": "67e63582e8a16f38741f4c16",
      "ai_keywords": [
        "large language models (LLMs)",
        "scientific discovery",
        "inspiration retrieval",
        "hypothesis composition",
        "hypothesis ranking",
        "automated framework",
        "research questions",
        "background surveys",
        "inspirations",
        "hypotheses",
        "out-of-distribution task",
        "automated scientific discovery",
        "innovative hypotheses"
      ]
    },
    "publishedAt": "2025-03-27T04:09:15.000Z",
    "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition",
    "summary": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21248.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646a11791556443f24b582e9",
      "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
      "fullname": "Zonglin Yang",
      "name": "ZonglinY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21696",
      "authors": [
        {
          "_id": "67e64ff17fe72aad5c26ab2f",
          "user": {
            "_id": "6485bd278d14bcd5cdbb7c8d",
            "avatarUrl": "/avatars/1427cf1a72b5db0cb263ad45885cf925.svg",
            "isPro": false,
            "fullname": "Wenqi Zhang",
            "user": "zwq2018",
            "type": "user"
          },
          "name": "Wenqi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:10:00.794Z",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab30",
          "name": "Mengna Wang",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab31",
          "user": {
            "_id": "64bb986ac733e8552fa4c5d1",
            "avatarUrl": "/avatars/78dc5f802def0bdebd890438fcb1f966.svg",
            "isPro": false,
            "fullname": "Gangao Liu",
            "user": "Gangao",
            "type": "user"
          },
          "name": "Gangao Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:09:24.111Z",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab32",
          "name": "Xu Huixin",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab33",
          "user": {
            "_id": "60abbe1fe3de7c7440abb84d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1621868056572-noauth.jpeg",
            "isPro": false,
            "fullname": "Yiwei Jiang",
            "user": "yijiang",
            "type": "user"
          },
          "name": "Yiwei Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:09:05.352Z",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab34",
          "user": {
            "_id": "5e1058e9fcf41d740b69966d",
            "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
            "isPro": false,
            "fullname": "Yongliang Shen",
            "user": "tricktreat",
            "type": "user"
          },
          "name": "Yongliang Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:08:58.629Z",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab35",
          "user": {
            "_id": "67c03110e8c7d56a8e135ac8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
            "isPro": false,
            "fullname": "Hou",
            "user": "Guiyang1001",
            "type": "user"
          },
          "name": "Guiyang Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:08:43.400Z",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab36",
          "name": "Zhe Zheng",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab37",
          "name": "Hang Zhang",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab38",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab39",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab3a",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "67e64ff17fe72aad5c26ab3b",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6485bd278d14bcd5cdbb7c8d/eTJU1eLh0tSYvjN5T6Tj4.mp4"
      ],
      "publishedAt": "2025-03-27T17:00:51.000Z",
      "submittedOnDailyAt": "2025-03-28T06:01:25.299Z",
      "title": "Embodied-Reasoner: 視覚検索、理由論と行動の協調をもって体験的な相互作用タスクを実現する",
      "submittedOnDailyBy": {
        "_id": "6485bd278d14bcd5cdbb7c8d",
        "avatarUrl": "/avatars/1427cf1a72b5db0cb263ad45885cf925.svg",
        "isPro": false,
        "fullname": "Wenqi Zhang",
        "user": "zwq2018",
        "type": "user"
      },
      "summary": "最近の深層モデルの進展は、数学およびコーディングタスクでの驚人的な推理能力を示しました。しかし、それらが環境との継続的なインタラクションを通じて行う体像的な領域での効果性は、大きく探索されていません。我々は、Interactive Embodied Searchタスクにおいてo1スタイルの推理を拡張したEmbodied Reasonerモデルを紹介します。数学的な推理は主にロジック的な推論に依存しますが、体像的なスケーナは空間的理解、時間的推理、およびインタラクションの履歴に基づく継続的な自己反省を求めています。これらの課題を解決するために、9.3kのコラーラルなObservation-Thought-Actionツレークを含む64kのインタラクティブな画像と90kの多様な思考プロセス（分析、空間的推理、反省、計画、バリデーション）を合成しました。我々は、摹様学習、拒否サンプリングによる自己探索、自己補正を通じた反省チューニングを用いた3ステップの訓練パイプラインを開発しました。評価によると、我々のモデルは、例えばOpenAI o1、o3-mini、Claude-3.7を+9%、24%、+13%より顕著に上回りました。分析によると、我々のモデルは、重複した探索とロジック的な不適切さを少なくし、複雑な長期ホーリングタスクに特に優れています。実世界的な環境でも、重複した探索とロジック的な不適切さの少ないことで我々の優れた性能を示しています。",
      "upvotes": 8,
      "discussionId": "67e64ff37fe72aad5c26ac06",
      "projectPage": "https://embodied-reasoner.github.io/",
      "githubRepo": "https://github.com/zwq2018/embodied_reasoner",
      "ai_keywords": [
        "Embodied Reasoner",
        "Observation-Thought-Action trajectories",
        "imitation learning",
        "rejection sampling",
        "reflection tuning",
        "visual reasoning models",
        "long-horizon tasks"
      ]
    },
    "publishedAt": "2025-03-27T13:00:51.000Z",
    "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks",
    "summary": "Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6485bd278d14bcd5cdbb7c8d/eTJU1eLh0tSYvjN5T6Tj4.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21696.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6485bd278d14bcd5cdbb7c8d",
      "avatarUrl": "/avatars/1427cf1a72b5db0cb263ad45885cf925.svg",
      "fullname": "Wenqi Zhang",
      "name": "zwq2018",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21774",
      "authors": [
        {
          "_id": "67e6094c48742d6df7586714",
          "name": "Jianning Pei",
          "hidden": false
        },
        {
          "_id": "67e6094c48742d6df7586715",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "67e6094c48742d6df7586716",
          "user": {
            "_id": "64c38fcf573c5a427e12cd37",
            "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
            "isPro": false,
            "fullname": "cientgu",
            "user": "cientgu",
            "type": "user"
          },
          "name": "Shuyang Gu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:01:17.103Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:59:46.000Z",
      "submittedOnDailyAt": "2025-03-28T00:58:59.244Z",
      "title": "最適ステップサイズのディフュージョンサンプリング",
      "submittedOnDailyBy": {
        "_id": "64c38fcf573c5a427e12cd37",
        "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
        "isPro": false,
        "fullname": "cientgu",
        "user": "cientgu",
        "type": "user"
      },
      "summary": "Diffusionモデルは、高い生成質量を達成しますが、不適切なステップの離散化により計算量の大きなサンプリングが課題になります。既存の研究は、ノイズの除去方向の最適化に焦点を当てていますが、我々はステップサイズスケジュールの原則的な設計に取り組んでいます。本論文では、Referenceタラクトローイズムからの知識を汲み取った理論的に最適なスケジュールを抽出するための動的計画法フレームワーク「Optimal Stepsize Distillation」を提案します。ステップサイズ最適化を再構成し、関数を最小化するようにして、我々の方法は最適部分構造を利用してグローバル的な離散化の制限を保証します。重要なことに、汲み取ったスケジュールは構造、ODEソルバー、ノイズスケジュールに対して強い軽率性を示します。実験では、GenEvalの性能を99.4%保つながら10倍速くの文脈から画像の生成を加速しました。コードは、https://github.com/bebebe666/OptimalSteps に公開されています。",
      "upvotes": 7,
      "discussionId": "67e6095248742d6df75868db",
      "ai_keywords": [
        "diffusion models",
        "generation quality",
        "computational intensive sampling",
        "step discretization",
        "denoising directions",
        "Optimal Stepsize Distillation",
        "dynamic programming framework",
        "theoretical optimal schedules",
        "knowledge distillation",
        "reference trajectories",
        "stepsize optimization",
        "recursive error minimization",
        "global discretization bounds",
        "optimal substructure exploitation",
        "robustness",
        "ODE solvers",
        "noise schedules",
        "text-to-image generation",
        "GenEval"
      ]
    },
    "publishedAt": "2025-03-27T13:59:46.000Z",
    "title": "Optimal Stepsize for Diffusion Sampling",
    "summary": "Diffusion models achieve remarkable generation quality but suffer from\ncomputational intensive sampling due to suboptimal step discretization. While\nexisting works focus on optimizing denoising directions, we address the\nprincipled design of stepsize schedules. This paper proposes Optimal Stepsize\nDistillation, a dynamic programming framework that extracts theoretically\noptimal schedules by distilling knowledge from reference trajectories. By\nreformulating stepsize optimization as recursive error minimization, our method\nguarantees global discretization bounds through optimal substructure\nexploitation. Crucially, the distilled schedules demonstrate strong robustness\nacross architectures, ODE solvers, and noise schedules. Experiments show 10x\naccelerated text-to-image generation while preserving 99.4% performance on\nGenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21774.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c38fcf573c5a427e12cd37",
      "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
      "fullname": "cientgu",
      "name": "cientgu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21765",
      "authors": [
        {
          "_id": "67e60da88decdc7da4bf69a9",
          "user": {
            "_id": "67e617ebd0fd66b1f393eedc",
            "avatarUrl": "/avatars/97a5ba0d0422f04e396399da1b74e8d4.svg",
            "isPro": false,
            "fullname": "Minghui Lin",
            "user": "minnielin",
            "type": "user"
          },
          "name": "Minghui Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:01:37.704Z",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69aa",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69ab",
          "user": {
            "_id": "65dcaf16287a93e081d9c2f0",
            "avatarUrl": "/avatars/2db4e25c6924461abb5634f8ffd1ee87.svg",
            "isPro": false,
            "fullname": "Yishanwang",
            "user": "yishanwang",
            "type": "user"
          },
          "name": "Yishan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:01:51.882Z",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69ac",
          "user": {
            "_id": "65fd82762bf2cd20ddaa193f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
            "isPro": false,
            "fullname": "Siteng Huang",
            "user": "huangsiteng",
            "type": "user"
          },
          "name": "Shu Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-28T02:47:05.703Z",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69ad",
          "name": "Fengqi Dai",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69ae",
          "name": "Pengxiang Ding",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69af",
          "user": {
            "_id": "65eaf755ab0a6a90da55ab58",
            "avatarUrl": "/avatars/a46890a9d067a913513edf3759f12c85.svg",
            "isPro": false,
            "fullname": "Cunxiang Wang",
            "user": "wangcunxiang",
            "type": "user"
          },
          "name": "Cunxiang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:02:15.357Z",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69b0",
          "name": "Zhengrong Zuo",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69b1",
          "name": "Nong Sang",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69b2",
          "user": {
            "_id": "65fd82762bf2cd20ddaa193f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
            "isPro": false,
            "fullname": "Siteng Huang",
            "user": "huangsiteng",
            "type": "user"
          },
          "name": "Siteng Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:02:43.147Z",
          "hidden": false
        },
        {
          "_id": "67e60da88decdc7da4bf69b3",
          "user": {
            "_id": "67597be2f3cd6492d4162ef8",
            "avatarUrl": "/avatars/ba580c04b7057927d4a22dcb44c52400.svg",
            "isPro": false,
            "fullname": "DONGLIN",
            "user": "wangdonglin130",
            "type": "user"
          },
          "name": "Donglin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:02:37.839Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:58:33.000Z",
      "submittedOnDailyAt": "2025-03-28T01:19:58.442Z",
      "title": "物理学知識の進化を映像生成における調査：一覧",
      "submittedOnDailyBy": {
        "_id": "65fd82762bf2cd20ddaa193f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
        "isPro": false,
        "fullname": "Siteng Huang",
        "user": "huangsiteng",
        "type": "user"
      },
      "summary": "最近の映像生成の進展において、特に拡散モデルの急速な進歩が見られています。しかし、これらの物理的な認知の欠点は、生成内容が物理学的基本法則を違反し、「視覚的写実性と物理的な荒語」の二難境に落ち込むことが多くなりました。研究者たちは、映像生成における物理的な忠実性の重要性を日々高め、動作表現と物理的知識などのヒューリスティックな物理的な認知を生成システムに統合し、実世界的な動的なスケーナrioをシミュレートすることを試みました。この分野において体系的な概要がないことを考慮し、この調査は、この欠点を埋めるために、構造設計の詳細な要約とその応用を提供することを目的としています。特に、我々は、認知科学の観点から物理的な認知の進化プロセスを議論し、基本的なスケーマ認識、物理的知識の被動的な認知、世界のシミュレーションの主動的な認知を含む三段階タクノロジーを提案します。そして、この分野における固有の鍵の課題を強調し、将来の研究の可能性のパターンを示し、学術界と業界での議論の前線を進めることを目的としています。構造的なレビューと異学連携的分析を通じて、この調査は、解釈可能、制御可能で物理的に一致した映像生成パラダイムの開発に方向性を与え、生成モデルを「視覚的模倣」のステージから「人間的物理的認識」の新しいステージへと進めることを目指しています。",
      "upvotes": 7,
      "discussionId": "67e60da98decdc7da4bf6a28",
      "githubRepo": "https://github.com/minnie-lin/Awesome-Physics-Cognition-based-Video-Generation",
      "ai_keywords": [
        "diffusion models",
        "physical cognition",
        "motion representations",
        "generative systems",
        "real-world dynamic scenarios",
        "cognitive science",
        "schema perception",
        "passive cognition",
        "active cognition",
        "state-of-the-art methods",
        "classical paradigms",
        "benchmarks",
        "key challenges",
        "interpretable",
        "controllable",
        "physically consistent",
        "human-like physical comprehension"
      ]
    },
    "publishedAt": "2025-03-27T13:58:33.000Z",
    "title": "Exploring the Evolution of Physics Cognition in Video Generation: A\n  Survey",
    "summary": "Recent advancements in video generation have witnessed significant progress,\nespecially with the rapid advancement of diffusion models. Despite this, their\ndeficiencies in physical cognition have gradually received widespread attention\n- generated content often violates the fundamental laws of physics, falling\ninto the dilemma of ''visual realism but physical absurdity\". Researchers began\nto increasingly recognize the importance of physical fidelity in video\ngeneration and attempted to integrate heuristic physical cognition such as\nmotion representations and physical knowledge into generative systems to\nsimulate real-world dynamic scenarios. Considering the lack of a systematic\noverview in this field, this survey aims to provide a comprehensive summary of\narchitecture designs and their applications to fill this gap. Specifically, we\ndiscuss and organize the evolutionary process of physical cognition in video\ngeneration from a cognitive science perspective, while proposing a three-tier\ntaxonomy: 1) basic schema perception for generation, 2) passive cognition of\nphysical knowledge for generation, and 3) active cognition for world\nsimulation, encompassing state-of-the-art methods, classical paradigms, and\nbenchmarks. Subsequently, we emphasize the inherent key challenges in this\ndomain and delineate potential pathways for future research, contributing to\nadvancing the frontiers of discussion in both academia and industry. Through\nstructured review and interdisciplinary analysis, this survey aims to provide\ndirectional guidance for developing interpretable, controllable, and physically\nconsistent video generation paradigms, thereby propelling generative models\nfrom the stage of ''visual mimicry'' towards a new phase of ''human-like\nphysical comprehension''.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21765.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd82762bf2cd20ddaa193f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
      "fullname": "Siteng Huang",
      "name": "huangsiteng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21144",
      "authors": [
        {
          "_id": "67e60e315f20b94fcd0d1f9b",
          "name": "Jinwei Qi",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1f9c",
          "name": "Chaonan Ji",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1f9d",
          "user": {
            "_id": "672d72751474234855223935",
            "avatarUrl": "/avatars/dc8a79b7d5e1175725334b337702e1fd.svg",
            "isPro": false,
            "fullname": "Sheng Xu",
            "user": "shengxu97",
            "type": "user"
          },
          "name": "Sheng Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:10:43.747Z",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1f9e",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1f9f",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "67e60e315f20b94fcd0d1fa0",
          "user": {
            "_id": "63d0cc736b985b0f25d0412c",
            "avatarUrl": "/avatars/3eb8c79f9a7c4c819038ea7b04e323dd.svg",
            "isPro": false,
            "fullname": "Bo",
            "user": "Liefeng",
            "type": "user"
          },
          "name": "Liefeng Bo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:11:02.629Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T04:18:53.000Z",
      "submittedOnDailyAt": "2025-03-28T01:20:25.562Z",
      "title": "チャットアニービー：スタイリズド・ライテコールポートレットビデオ生成における階層化モーメントディフュージョンモデル",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "実時間の相互作用ビデオチャットポートレットは、特にテキストと声のチャットテクノロジーの驚異的な進歩により、未来の趨勢としてさらに認識されてきました。しかし、現在の方法は主に頭の動きの実時間生成を焦点に置いており、その頭の動きに合わせた同期化された体の動きを生成することが難しいという問題があります。また、語り気と顔の表現の細かい制御を実現することは難しいという課題も残されています。これらの制限を解決するために、私たちは、語り気と顔の表現のスタイリズマと体の動きの同期化を可能にした実時間のポートレットビデオ生成の新しいフレームワークを紹介します。このフレームワークは、語り気と顔の表現の表現力と柔軟性を持つビデオチャットを実現し、ターニングヘッドから上半身のインタラクションまで拡張します。私たちのアプローチは以下の2つのステージから構成されています。最初のステージは、音声入力に基づいた明示的なおよび隠れた動き表現を考慮した効率的な階層的な動きディフフェレンシャルモデルによるもので、スタイリズマと頭と体の動きの同期化を可能にした多様な顔の表現を生成することができます。2つ目のステージは、上半身の動きを含むポートレットビデオの生成を目的としています。私たちは、ドライバーに明示的な手の制御シグナルを注入し、より詳細な手の動きを生成し、さらに顔のリファインメントを行い、ポートレットビデオ全体の写実性と表現力を向上させます。また、私たちのアプローチは、4090 GPUで最大512 * 768解像度で効率的な連続的な上半身ポートレットビデオの生成を可能にし、実時間の相互作用ビデオチャットを支援します。実験結果は、我々のアプローチが豊富な表現力と自然な上半身の動きを持つポートレットビデオを生成する能力を示しています。",
      "upvotes": 5,
      "discussionId": "67e60e325f20b94fcd0d1fff",
      "ai_keywords": [
        "hierarchical motion diffusion models",
        "explicit and implicit motion representations",
        "stylistic control",
        "synchronization between head and body movements",
        "face refinement",
        "continous generation",
        "upper-body portrait video",
        "interactive video-chat"
      ]
    },
    "publishedAt": "2025-03-27T00:18:53.000Z",
    "title": "ChatAnyone: Stylized Real-time Portrait Video Generation with\n  Hierarchical Motion Diffusion Model",
    "summary": "Real-time interactive video-chat portraits have been increasingly recognized\nas the future trend, particularly due to the remarkable progress made in text\nand voice chat technologies. However, existing methods primarily focus on\nreal-time generation of head movements, but struggle to produce synchronized\nbody motions that match these head actions. Additionally, achieving\nfine-grained control over the speaking style and nuances of facial expressions\nremains a challenge. To address these limitations, we introduce a novel\nframework for stylized real-time portrait video generation, enabling expressive\nand flexible video chat that extends from talking head to upper-body\ninteraction. Our approach consists of the following two stages. The first stage\ninvolves efficient hierarchical motion diffusion models, that take both\nexplicit and implicit motion representations into account based on audio\ninputs, which can generate a diverse range of facial expressions with stylistic\ncontrol and synchronization between head and body movements. The second stage\naims to generate portrait video featuring upper-body movements, including hand\ngestures. We inject explicit hand control signals into the generator to produce\nmore detailed hand movements, and further perform face refinement to enhance\nthe overall realism and expressiveness of the portrait video. Additionally, our\napproach supports efficient and continuous generation of upper-body portrait\nvideo in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supporting\ninteractive video-chat in real-time. Experimental results demonstrate the\ncapability of our approach to produce portrait videos with rich expressiveness\nand natural upper-body movements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21144.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21088",
      "authors": [
        {
          "_id": "67e602b6dabfd9d4bbf10849",
          "user": {
            "_id": "66f4bdbdc51768d9d4498d16",
            "avatarUrl": "/avatars/0f6ded5fd9cf4e6f0b180b5aa329ea33.svg",
            "isPro": false,
            "fullname": "Haoming Xu",
            "user": "HaomingXu",
            "type": "user"
          },
          "name": "Haoming Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-28T02:00:25.958Z",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084a",
          "user": {
            "_id": "66d270dc5ae47374c27c9e9a",
            "avatarUrl": "/avatars/556094d864e8e779b15bfc4360e91a44.svg",
            "isPro": false,
            "fullname": "Shuxun Wang",
            "user": "Saberlve",
            "type": "user"
          },
          "name": "Shuxun Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:57.087Z",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084b",
          "name": "Yanqiu Zhao",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084c",
          "name": "Yi Zhong",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084d",
          "name": "Ziyan Jiang",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084e",
          "name": "Ningyuan Zhao",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf1084f",
          "name": "Shumin Deng",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf10850",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67e602b6dabfd9d4bbf10851",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T02:03:25.000Z",
      "submittedOnDailyAt": "2025-03-28T00:32:00.822Z",
      "title": "ZJUKLAB at SemEval-2025 Task 4: モデル統合による忘却",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "この論文は、ZJUKLABチームが参加した2025年のSemEvalタスク4「大規模言語モデルからの敏感内容の忘却」の提出を報告しています。このタスクは、大規模言語モデルから選択的に敏感なコンテンツを削除することを目的とし、過度な忘却や欠落の問題を避けることを目指しています。私たちは、Model Merging（特にTIES-Merging）を活用した忘却システムを提案し、2つの特化されたモデルを統合してよりバランスのある忘却モデルを作成しました。私たちのシステムは、26チームの中で2位に立ち、タスクアグレゲートのオンラインスコアは0.944、全体アグレゲートのオンラインスコアは0.487として、競争的な結果を収めました。この論文では、局所的な実験を行い、忘却プロセスの詳細な分析を行い、性能のトラジェクト、損失の動態、重みの視点などを見積もり、さらに補助的な実験を行い、私たちの方法の効果性を理解することを目的としています。また、私たちの方法の欠点と評価指標について分析し、MIAスコアとROUGEに基づく指標のみで完全に忘却の成功を評価することができないことを強調しています。最後に、将来の研究で忘却の目標の再考と更なる評価手法の必要性を強調しています。コードは、https://github.com/zjunlp/unlearn/tree/main/semeval25 から利用可能です。",
      "upvotes": 4,
      "discussionId": "67e602badabfd9d4bbf10973",
      "githubRepo": "https://github.com/zjunlp/unlearn",
      "ai_keywords": [
        "Model Merging",
        "TIES-Merging",
        "large language models",
        "unlearning",
        "over-forgetting",
        "under-forgetting",
        "unlearned model",
        "performance trajectories",
        "loss dynamics",
        "weight perspectives",
        "MIA scores",
        "ROUGE-based metrics"
      ]
    },
    "publishedAt": "2025-03-26T22:03:25.000Z",
    "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
    "summary": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20990",
      "authors": [
        {
          "_id": "67e626a9cb305c5a3ee11fac",
          "user": {
            "_id": "62dd8f328456396d4f8aa894",
            "avatarUrl": "/avatars/af8f5dc7ff937e3e849ecdfd9ca4750b.svg",
            "isPro": false,
            "fullname": "Yupeng Cao",
            "user": "YupengCao",
            "type": "user"
          },
          "name": "Yupeng Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:11:18.739Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fad",
          "user": {
            "_id": "634cabd104491d9f7111eea3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665969099097-noauth.jpeg",
            "isPro": false,
            "fullname": "Haohang Li",
            "user": "Acatsama",
            "type": "user"
          },
          "name": "Haohang Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:11:24.664Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fae",
          "user": {
            "_id": "64f757c6016d60f3199ef5e6",
            "avatarUrl": "/avatars/2659ba698081265d0480b08161718013.svg",
            "isPro": false,
            "fullname": "Yangyang Yu",
            "user": "ShirleyY",
            "type": "user"
          },
          "name": "Yangyang Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:11:30.611Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11faf",
          "user": {
            "_id": "6265db3f637f6ec042b2c4d7",
            "avatarUrl": "/avatars/7bb20dce7c96059d2c4f06890344af86.svg",
            "isPro": false,
            "fullname": "Shashidhar Reddy Javaji",
            "user": "Shashidhar",
            "type": "user"
          },
          "name": "Shashidhar Reddy Javaji",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:11:36.871Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb0",
          "user": {
            "_id": "65bd14e8ce846f8aa94db1d1",
            "avatarUrl": "/avatars/76eaad15bf32eba75271f3dc315527c2.svg",
            "isPro": false,
            "fullname": "Yueru He",
            "user": "Yueru1",
            "type": "user"
          },
          "name": "Yueru He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:12:00.574Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb1",
          "user": {
            "_id": "63b58ed5889aa6707f0bb0f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/znl74_aMswlV8VtHrfj3G.jpeg",
            "isPro": true,
            "fullname": "Jimin Huang",
            "user": "jiminHuang",
            "type": "user"
          },
          "name": "Jimin Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:12:09.888Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb2",
          "user": {
            "_id": "62d63a9dc5ada8ef841b4787",
            "avatarUrl": "/avatars/a79a4ac07984d9a8623c99bdce9add54.svg",
            "isPro": false,
            "fullname": "Zining Zhu",
            "user": "ZiningZhu",
            "type": "user"
          },
          "name": "Zining Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:12:18.401Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb3",
          "user": {
            "_id": "6479f4317c18dca75e9a9324",
            "avatarUrl": "/avatars/9aa709230b057f57ee4415c04a622c63.svg",
            "isPro": false,
            "fullname": "Xie",
            "user": "QianqianXie1994",
            "type": "user"
          },
          "name": "Qianqian Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:12:33.144Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb4",
          "user": {
            "_id": "642de494b42737f9e1e0046b",
            "avatarUrl": "/avatars/01ebcb41b89b1da557abdb9cf867331f.svg",
            "isPro": false,
            "fullname": "Xiao-Yang Liu Yanglet",
            "user": "yanglet",
            "type": "user"
          },
          "name": "Xiao-yang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:12:40.590Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb5",
          "name": "Koduvayur Subbalakshmi",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb6",
          "name": "Meikang Qiu",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb7",
          "user": {
            "_id": "66f6cb352c5d4ef3578a9c3f",
            "avatarUrl": "/avatars/0a70c94072bc5e1d018cf12da0904ff0.svg",
            "isPro": false,
            "fullname": "Sophia Ananiadou",
            "user": "Effoula",
            "type": "user"
          },
          "name": "Sophia Ananiadou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:12:56.712Z",
          "hidden": false
        },
        {
          "_id": "67e626a9cb305c5a3ee11fb8",
          "name": "Jian-Yun Nie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T21:07:51.000Z",
      "submittedOnDailyAt": "2025-03-28T03:07:27.038Z",
      "title": "FinAudio: ファイナンスアプリケーションでの音声大語言モデルのベンチマーク",
      "submittedOnDailyBy": {
        "_id": "63a0c0803c8841cfe2cd1f15",
        "avatarUrl": "/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg",
        "isPro": false,
        "fullname": "Xueqing Peng",
        "user": "Xueqing",
        "type": "user"
      },
      "summary": "Audio Large Language Models (AudioLLMs) は広く注目され、会話、音声理解、自動的語音認識（ASR）などの音声タスクの性能に大幅に向上しました。これらの進歩の際に、金融シナリオでの AudioLLMs の評価に関するベンチマークは存在しません。金融分析と投資決定の重要なリソースとしての収益コンファーエンドラインや CEO の演説などの音声データを含む。本論文では、金融領域での AudioLLMs の評価能力を評価するための最初のベンチマークとして FinAudio を紹介します。金融領域の独自な特徴に基づいて 3 つのタスクを定義します：1) 短い金融音声の ASR、2) 長い金融音声の ASR、3) 長い金融音声の要約。そして、2 つの短いおよび 2 つの長い音声データセットをカレードし、金融音声要約の新しいデータセットを開発し、FinAudio ベンチマークとして構成します。そして、FinAudio 上で 7 つの主流の AudioLLMs を評価します。評価により、金融領域での既存の AudioLLMs の制限を明らかにし、AudioLLMs の改善のためのアイデアを提供します。すべてのデータセットとコードは公開します。",
      "upvotes": 4,
      "discussionId": "67e626aacb305c5a3ee12027",
      "ai_keywords": [
        "Audio Large Language Models (AudioLLMs)",
        "automatic speech recognition (ASR)",
        "financial scenarios",
        "benchmark",
        "financial analysis",
        "investment decisions",
        "summarization",
        "ASR for short financial audio",
        "ASR for long financial audio",
        "summarization of long financial audio",
        "\\textsc{FinAudio}"
      ]
    },
    "publishedAt": "2025-03-26T17:07:51.000Z",
    "title": "FinAudio: A Benchmark for Audio Large Language Models in Financial\n  Applications",
    "summary": "Audio Large Language Models (AudioLLMs) have received widespread attention\nand have significantly improved performance on audio tasks such as\nconversation, audio understanding, and automatic speech recognition (ASR).\nDespite these advancements, there is an absence of a benchmark for assessing\nAudioLLMs in financial scenarios, where audio data, such as earnings conference\ncalls and CEO speeches, are crucial resources for financial analysis and\ninvestment decisions. In this paper, we introduce FinAudio, the first\nbenchmark designed to evaluate the capacity of AudioLLMs in the financial\ndomain. We first define three tasks based on the unique characteristics of the\nfinancial domain: 1) ASR for short financial audio, 2) ASR for long financial\naudio, and 3) summarization of long financial audio. Then, we curate two short\nand two long audio datasets, respectively, and develop a novel dataset for\nfinancial audio summarization, comprising the FinAudio benchmark.\nThen, we evaluate seven prevalent AudioLLMs on FinAudio. Our\nevaluation reveals the limitations of existing AudioLLMs in the financial\ndomain and offers insights for improving AudioLLMs. All datasets and codes will\nbe released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a0c0803c8841cfe2cd1f15",
      "avatarUrl": "/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg",
      "fullname": "Xueqing Peng",
      "name": "Xueqing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20776",
      "authors": [
        {
          "_id": "67e6019af54a34f8a989d8eb",
          "user": {
            "_id": "642a276516d4d8293c9a47e8",
            "avatarUrl": "/avatars/80e6db8bc2544f3486b11b57858a8692.svg",
            "isPro": false,
            "fullname": "Shijie Zhou",
            "user": "shijiezhou",
            "type": "user"
          },
          "name": "Shijie Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-28T08:37:59.022Z",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8ec",
          "name": "Hui Ren",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8ed",
          "name": "Yijia Weng",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8ee",
          "user": {
            "_id": "67b3c26f3d0f54ab3805954d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/beyNv649ZZ5A29CH3m8mj.png",
            "isPro": false,
            "fullname": "Shuwang Zhang",
            "user": "ShuwangZhang00",
            "type": "user"
          },
          "name": "Shuwang Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:13:30.134Z",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8ef",
          "name": "Zhen Wang",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f0",
          "user": {
            "_id": "62f687a0c58915315c4ff75d",
            "avatarUrl": "/avatars/b657180c7666735062782edd4f6a69c9.svg",
            "isPro": false,
            "fullname": "Dejia Xu",
            "user": "ir1d",
            "type": "user"
          },
          "name": "Dejia Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:13:40.451Z",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f1",
          "user": {
            "_id": "6526386e1c6a09292d8d0a22",
            "avatarUrl": "/avatars/471de830de2d775d35368678c1579f87.svg",
            "isPro": false,
            "fullname": "fan",
            "user": "Fanzhiwen",
            "type": "user"
          },
          "name": "Zhiwen Fan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:16:41.739Z",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f2",
          "name": "Suya You",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f3",
          "name": "Zhangyang Wang",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f4",
          "name": "Leonidas Guibas",
          "hidden": false
        },
        {
          "_id": "67e6019af54a34f8a989d8f5",
          "name": "Achuta Kadambi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:56:16.000Z",
      "submittedOnDailyAt": "2025-03-28T00:26:28.356Z",
      "title": "Feature4X: モノカメラビデオを4DアウトプットAIにつなぐ多様なガウシアン特徴量フィールド",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "最近の2次元および多モーダルモデルの進展は、大規模な訓練を通じて極めて驚異的な成功を達成しました。しかし、これらの達成を、複雑な3次元/4次元スケーンとの自由形のインタラクションと高レベルの語意的操作に拡張することは難しいです。この難しさは、大規模な、標準化された3次元/4次元または多角度データセットの有限な可利用性に基づいています。これらのデータセットは、開放ボックスのセグメンテーション、プロンプトベースのセグメンテーション、言語ガイドされた編集、および視覚的質問回答（VQA）などの一般化可能な視覚と言語タスクに重要です。本論文では、ユーザー生成内容から広く利用できるモノクロビデオ入力を用いて、2次元視覚ファンダメンタルモデルの機能を4次元の世界に拡張するための、普遍的なフレームワークFeature4Xを紹介します。Feature4Xの「X」は、その多様性を表し、可変性のある、モデル条件付きの4次元特徴フィールドの煉成を通じて、どのようなタスクを可能にします。本フレームワークの核心は、複数のモデル能力を一つの表現に統合する動的な最適化戦略です。また、知られている限り、Feature4Xは、ガウススプレッティングを用いて、ビデオファンダメンタルモデル（例：SAM2, InternVideo2）の特徴を明示的な4次元特徴フィールドに煉成し、それを初めて実現しています。本論文では、LLMによるフュージョンループを通じて、時間ステップ全体での新視点セグメントその他、構造と外観のスケーン編集、自由形のVQAなどの新しいタスクを実現することを示します。これらの進歩は、インタラクティブな動的な4次元スケーンインタラクションを可能にし、スケーラブルで、時間空間によって認識できるシステムを提供し、アウトプットのAIアプリケーションの範囲を広げます。",
      "upvotes": 4,
      "discussionId": "67e6019ff54a34f8a989d9d6",
      "ai_keywords": [
        "Feature4X",
        "4D feature field distillation",
        "Gaussian Splatting",
        "SAM2",
        "InternVideo2",
        "novel view segment anything",
        "geometric and appearance scene editing",
        "free-form VQA",
        "LLMs",
        "agentic AI applications",
        "scalable systems",
        "contextually aware",
        "spatiotemporally aware",
        "immersive dynamic 4D scene interaction"
      ]
    },
    "publishedAt": "2025-03-26T13:56:16.000Z",
    "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields",
    "summary": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20822",
      "authors": [
        {
          "_id": "67e61017b116b3c559188a0f",
          "name": "Qi Zhao",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a10",
          "user": {
            "_id": "67b576e39b7058fa21ab72a3",
            "avatarUrl": "/avatars/5393231dc585950d9579323521f41ff4.svg",
            "isPro": false,
            "fullname": "Xingyu Ni",
            "user": "Univstar",
            "type": "user"
          },
          "name": "Xingyu Ni",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:17:44.258Z",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a11",
          "name": "Ziyu Wang",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a12",
          "user": {
            "_id": "65c3dfb180497543ca257ffd",
            "avatarUrl": "/avatars/1eb9e114e2c4dc2dc2c3b2e3f387d214.svg",
            "isPro": false,
            "fullname": "Feng Cheng",
            "user": "fengcheng1",
            "type": "user"
          },
          "name": "Feng Cheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:18:07.030Z",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a13",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a14",
          "name": "Lu Jiang",
          "hidden": false
        },
        {
          "_id": "67e61017b116b3c559188a15",
          "name": "Bohan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T00:45:07.000Z",
      "submittedOnDailyAt": "2025-03-28T01:28:16.803Z",
      "title": "合成ビデオがビデオ合成で物理的な精度を向上させる",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "私たちは、コンピューターグラフィックスパイプラインからの合成ビデオを利用して、ビデオ生成モデルの物理的な精度を向上させる方法について検討しています。これらの渲染ビデオは、3次元一致性などの実世界の物理法則を尊重し、ビデオ生成モデルの改善に役立つ有價なリソースとして役立ちます。この可能性を活用するために、私たちは、合成データを選択・統合する方法を提案し、その物理的な写真をモデルに伝える方法を導入し、不適切なアーティファクトを大幅に減少することを目指しています。物理的な一致性を重視する3つの代表的なタスクにおいて実験を行い、その効果を示しました。しかし、私たちのモデルは深い物理的理解を持っていませんが、私たちの研究は、合成ビデオがビデオ合成の物理的な精度を向上させることを初めて実験的に示したことを示しています。ウェブサイト：https://kevinz8866.github.io/simulation/",
      "upvotes": 4,
      "discussionId": "67e6101bb116b3c559188b67",
      "ai_keywords": [
        "video generation models",
        "synthetic videos",
        "computer graphics pipelines",
        "3D consistency",
        "transfering physical realism",
        "unwanted artifacts",
        "physical consistency",
        "physical fidelity",
        "video synthesis"
      ]
    },
    "publishedAt": "2025-03-25T20:45:07.000Z",
    "title": "Synthetic Video Enhances Physical Fidelity in Video Synthesis",
    "summary": "We investigate how to enhance the physical fidelity of video generation\nmodels by leveraging synthetic videos derived from computer graphics pipelines.\nThese rendered videos respect real-world physics, such as maintaining 3D\nconsistency, and serve as a valuable resource that can potentially improve\nvideo generation models. To harness this potential, we propose a solution that\ncurates and integrates synthetic data while introducing a method to transfer\nits physical realism to the model, significantly reducing unwanted artifacts.\nThrough experiments on three representative tasks emphasizing physical\nconsistency, we demonstrate its efficacy in enhancing physical fidelity. While\nour model still lacks a deep understanding of physics, our work offers one of\nthe first empirical demonstrations that synthetic video enhances physical\nfidelity in video synthesis. Website: https://kevinz8866.github.io/simulation/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20822.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6494
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20853",
      "authors": [
        {
          "_id": "67e5ff24cce3913200f29387",
          "user": {
            "_id": "62f6bd1dd278a8f3e7867392",
            "avatarUrl": "/avatars/7e5b6014d99909958eb0f95c486b2226.svg",
            "isPro": false,
            "fullname": "Alexander Swerdlow",
            "user": "aswerdlow",
            "type": "user"
          },
          "name": "Alexander Swerdlow",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:18:36.245Z",
          "hidden": false
        },
        {
          "_id": "67e5ff24cce3913200f29388",
          "user": {
            "_id": "6310ff7dd43c55e811f8772f",
            "avatarUrl": "/avatars/0dcab81771329d13cc62b08e55a71c9a.svg",
            "isPro": false,
            "fullname": "Mihir Prabhudesai",
            "user": "mihirpd",
            "type": "user"
          },
          "name": "Mihir Prabhudesai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:18:30.166Z",
          "hidden": false
        },
        {
          "_id": "67e5ff24cce3913200f29389",
          "user": {
            "_id": "67ad6a0924ad6fd76672ff2f",
            "avatarUrl": "/avatars/141280d9d1b97740f0e8acf9b681411d.svg",
            "isPro": false,
            "fullname": "Siddharth Gandhi",
            "user": "Sid1275",
            "type": "user"
          },
          "name": "Siddharth Gandhi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:18:42.006Z",
          "hidden": false
        },
        {
          "_id": "67e5ff24cce3913200f2938a",
          "user": {
            "_id": "653692e8ac570e90963cf2c5",
            "avatarUrl": "/avatars/e391ca21c2292e916ab0ab00f8ee2ba6.svg",
            "isPro": false,
            "fullname": "Deepak pathak",
            "user": "Deepak765",
            "type": "user"
          },
          "name": "Deepak Pathak",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:18:48.839Z",
          "hidden": false
        },
        {
          "_id": "67e5ff24cce3913200f2938b",
          "name": "Katerina Fragkiadaki",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:59:51.000Z",
      "submittedOnDailyAt": "2025-03-28T00:17:26.527Z",
      "title": "統合多モデル離散拡散",
      "submittedOnDailyBy": {
        "_id": "6310ff7dd43c55e811f8772f",
        "avatarUrl": "/avatars/0dcab81771329d13cc62b08e55a71c9a.svg",
        "isPro": false,
        "fullname": "Mihir Prabhudesai",
        "user": "mihirpd",
        "type": "user"
      },
      "summary": "多タイプ生成モデルは、複数のモディュールで理解し生成することができるものが、自動順次進行（AR）アプローチによって主導されています。これらのモデルは左から右または上から下までの順番でタグンを順次処理します。これらのモデルは画像キャプチング、質問回答、画像生成などの多様なタスクを共に処理します。本研究では、最近のテキスト生成の成功を基に、統一的な生成設定での文と画像の共に処理する離散ディフュージョンモデルを検討します。離散ディフュージョンモデルは、ARモデルよりも複数の優れた点があります。その中には、生成サンプルの質と多様性の制御、文と画像の両方の領域での共に補間、生成の過程での制御の幅が広いことが含まれます。これらの利点を活用し、最初の統一的な多タイプ離散ディフュージョン（UniDisc）モデルを提案します。このモデルは、多様な下流タスクにおいて文と画像を共に理解し生成することができます。UniDiscと多タイプARモデルを比較し、スケーリング分析を行い、性能と推論時の計算量、制御性、編集性、補間、推論時間と生成質の柔軟なトレードオフについて、UniDiscがより優れていることを示します。コードと追加の可視化は、https://unidisc.github.io にアクセス可能です。",
      "upvotes": 2,
      "discussionId": "67e5ff28cce3913200f2951e",
      "projectPage": "https://unidisc.github.io/",
      "githubRepo": "https://github.com/alexanderswerdlow/unidisc",
      "ai_keywords": [
        "autoregressive (AR) approaches",
        "discrete diffusion models",
        "multimodal inpainting",
        "control over quality versus diversity",
        "Unity Multimodal Discrete Diffusion (UniDisc)",
        "scaling analysis",
        "generation quality",
        "inference-time compute",
        "controllability",
        "editability"
      ]
    },
    "publishedAt": "2025-03-26T13:59:51.000Z",
    "title": "Unified Multimodal Discrete Diffusion",
    "summary": "Multimodal generative models that can understand and generate across multiple\nmodalities are dominated by autoregressive (AR) approaches, which process\ntokens sequentially from left to right, or top to bottom. These models jointly\nhandle images, text, video, and audio for various tasks such as image\ncaptioning, question answering, and image generation. In this work, we explore\ndiscrete diffusion models as a unified generative formulation in the joint text\nand image domain, building upon their recent success in text generation.\nDiscrete diffusion models offer several advantages over AR models, including\nimproved control over quality versus diversity of generated samples, the\nability to perform joint multimodal inpainting (across both text and image\ndomains), and greater controllability in generation through guidance.\nLeveraging these benefits, we present the first Unified Multimodal Discrete\nDiffusion (UniDisc) model which is capable of jointly understanding and\ngenerating text and images for a variety of downstream tasks. We compare\nUniDisc to multimodal AR models, performing a scaling analysis and\ndemonstrating that UniDisc outperforms them in terms of both performance and\ninference-time compute, enhanced controllability, editability, inpainting, and\nflexible trade-off between inference time and generation quality. Code and\nadditional visualizations are available at https://unidisc.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310ff7dd43c55e811f8772f",
      "avatarUrl": "/avatars/0dcab81771329d13cc62b08e55a71c9a.svg",
      "fullname": "Mihir Prabhudesai",
      "name": "mihirpd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20578",
      "authors": [
        {
          "_id": "67e5fd806e73232cf08af3bb",
          "user": {
            "_id": "659430138fec845e50a27558",
            "avatarUrl": "/avatars/f5de806f55c90ae303cd94af7c15005c.svg",
            "isPro": false,
            "fullname": "Alif Al Hasan",
            "user": "alifalhasan",
            "type": "user"
          },
          "name": "Alif Al Hasan",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-28T04:39:45.135Z",
          "hidden": false
        },
        {
          "_id": "67e5fd806e73232cf08af3bc",
          "user": {
            "_id": "6664836f3038d313d1ac7867",
            "avatarUrl": "/avatars/30de3b29cf06b5481f100ace55a85f29.svg",
            "isPro": false,
            "fullname": "Subarna Saha",
            "user": "Subarna10",
            "type": "user"
          },
          "name": "Subarna Saha",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-28T09:19:03.409Z",
          "hidden": false
        },
        {
          "_id": "67e5fd806e73232cf08af3bd",
          "user": {
            "_id": "6331c3f618711776b468e9ec",
            "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
            "isPro": false,
            "fullname": "Mia Mohammad Imran",
            "user": "imranraad",
            "type": "user"
          },
          "name": "Mia Mohammad Imran",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-28T01:38:09.589Z",
          "hidden": false
        },
        {
          "_id": "67e5fd806e73232cf08af3be",
          "name": "Tarannum Shaila Zaman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T14:25:01.000Z",
      "submittedOnDailyAt": "2025-03-28T00:08:31.084Z",
      "title": "LLPut: バグ報告ベースの入力生成における大規模言語モデルの検証",
      "submittedOnDailyBy": {
        "_id": "6331c3f618711776b468e9ec",
        "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
        "isPro": false,
        "fullname": "Mia Mohammad Imran",
        "user": "imranraad",
        "type": "user"
      },
      "summary": "ファイルドラインデータがソフトウェアバグの診断と分析に重要な役割を果たしています。バグ報告書は通常、これらのデータを含み、開発者はこれらを抽出してデバッグを促進します。バグ報告書は自然言語で書かれているため、先行研究は、ネイティブランゲージ処理（NLP）技術を利用して自動的な入力抽出を実現しました。大規模言語モデル（LLMs）の到来に伴い、重要な研究課題が発生しました：生成的なLLMsはバグ報告書からファイルドラインデータをどのように効果的に抽出できるか？この論文では、LLPutという技術を提案し、LLaMA、Qwen、Qwen-Coderの3つの開源ソース生成的なLLMsの性能を実験的に評価することを目的とします。206件のバグ報告書のデータセットに対して実験的評価を行い、これらのモデルの精度と効果性を評価します。我々の発見は、生成的なLLMsの自動的なバグ診断の能力と限界についてのエイリアスを提供します。",
      "upvotes": 1,
      "discussionId": "67e5fd816e73232cf08af3e2",
      "projectPage": "https://zenodo.org/records/15092886",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "generative LLMs",
        "LLPut",
        "LLaMA",
        "Qwen",
        "Qwen-Coder",
        "natural language",
        "empirical evaluation",
        "bug diagnosis"
      ]
    },
    "publishedAt": "2025-03-26T10:25:01.000Z",
    "title": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation",
    "summary": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20578.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6331c3f618711776b468e9ec",
      "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
      "fullname": "Mia Mohammad Imran",
      "name": "imranraad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]