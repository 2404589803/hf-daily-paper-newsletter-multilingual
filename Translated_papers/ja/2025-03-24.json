[
  {
    "paper": {
      "id": "2503.16905",
      "authors": [
        {
          "_id": "67e0c13fe5fa0da84e134581",
          "user": {
            "_id": "658be7fe135580745c510323",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
            "isPro": false,
            "fullname": "Jian Zhang",
            "user": "VentureZJ",
            "type": "user"
          },
          "name": "Jian Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:07:08.476Z",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134582",
          "name": "Zhiyuan Wang",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134583",
          "name": "Zhangqi Wang",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134584",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134585",
          "name": "Fangzhi Xu",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134586",
          "user": {
            "_id": "66ac77011cfb12c087605acb",
            "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg",
            "isPro": false,
            "fullname": "Lin",
            "user": "Qika",
            "type": "user"
          },
          "name": "Qika Lin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-24T02:19:51.913Z",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134587",
          "name": "Rui Mao",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134588",
          "name": "Erik Cambria",
          "hidden": false
        },
        {
          "_id": "67e0c13fe5fa0da84e134589",
          "name": "Jun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T07:13:45.000Z",
      "submittedOnDailyAt": "2025-03-24T00:51:41.644Z",
      "title": "MAPS: ビッグセブンページャーオファーシャルとソクラトリックガイドニングに基づく多モデル科学問題解決のためのマルチアガントフレームワーク",
      "submittedOnDailyBy": {
        "_id": "658be7fe135580745c510323",
        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
        "isPro": false,
        "fullname": "Jian Zhang",
        "user": "VentureZJ",
        "type": "user"
      },
      "summary": "多タイプ科学問題（MSPs）は、テキストや図式などの複数のモデルの統合が必要となり、人工知能にとっては大きな課題です。傳統的科学問題に対して進歩があったのに対して、MSPsは主に2つの問題を見る必要があります：科学問題解決における多タイプの整備的な推理の課題と、反省と再考の能力の欠如です。これらの問題を解決するために、私たちは、Big Seven PersonalityとSocraticガイダンスに基づくMulti-Agentフレームワーク（MAPS）を介して提案します。このフレームワークは、7つの異なるアガントを使用し、フィードバック機構とSocratic方法を活用してMSPsの解決をガイドするものです。1つ目の問題に対して、私たちは進歩的な4アガントの解決策戦略を提案し、各アガントは問題解決プロセスの特定のステージを焦点にします。2つ目の問題に対して、Socraticの質問にモデル化されたCriticアガントを導入し、批判的な思考を促し、自動学的学習を刺激します。EMMA、オリンピア、およびMathVistaデータセットに対して拡大的な実験を行い、すべてのタスクで現在のSOTAモデルを15.84%超える熱望の結果を収めました。同時に、追加的分析的な実験もモデルの進歩および一般化能力を証明しました。",
      "upvotes": 33,
      "discussionId": "67e0c147e5fa0da84e1347f5",
      "githubRepo": "https://github.com/exoskeletonzj/MAPS"
    },
    "publishedAt": "2025-03-21T03:13:45.000Z",
    "title": "MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving",
    "summary": "Multimodal scientific problems (MSPs) involve complex issues that require the\nintegration of multiple modalities, such as text and diagrams, presenting a\nsignificant challenge in artificial intelligence. While progress has been made\nin addressing traditional scientific problems, MSPs still face two primary\nissues: the challenge of multi-modal comprehensive reasoning in scientific\nproblem-solving and the lack of reflective and rethinking capabilities. To\naddress these issues, we introduce a Multi-Agent framework based on the Big\nSeven Personality and Socratic guidance (MAPS). This framework employs seven\ndistinct agents that leverage feedback mechanisms and the Socratic method to\nguide the resolution of MSPs. To tackle the first issue, we propose a\nprogressive four-agent solving strategy, where each agent focuses on a specific\nstage of the problem-solving process. For the second issue, we introduce a\nCritic agent, inspired by Socratic questioning, which prompts critical thinking\nand stimulates autonomous learning. We conduct extensive experiments on the\nEMMA, Olympiad, and MathVista datasets, achieving promising results that\noutperform the current SOTA model by 15.84% across all tasks. Meanwhile, the\nadditional analytical experiments also verify the model's progress as well as\ngeneralization ability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658be7fe135580745c510323",
      "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
      "fullname": "Jian Zhang",
      "name": "VentureZJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16874",
      "authors": [
        {
          "_id": "67e0c1ce151ca9ed9284dc52",
          "user": {
            "_id": "658be7fe135580745c510323",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
            "isPro": false,
            "fullname": "Jian Zhang",
            "user": "VentureZJ",
            "type": "user"
          },
          "name": "Jian Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:07:05.271Z",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc53",
          "name": "Zhangqi Wang",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc54",
          "name": "Haiping Zhu",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc55",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc56",
          "user": {
            "_id": "66ac77011cfb12c087605acb",
            "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg",
            "isPro": false,
            "fullname": "Lin",
            "user": "Qika",
            "type": "user"
          },
          "name": "Qika Lin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-24T02:22:15.812Z",
          "hidden": false
        },
        {
          "_id": "67e0c1ce151ca9ed9284dc57",
          "name": "Erik Cambria",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T06:19:55.000Z",
      "submittedOnDailyAt": "2025-03-24T00:56:26.218Z",
      "title": "MARS: ソクラティックガイドニングを採用した多効果的なプロンプト最適化フレームワーク",
      "submittedOnDailyBy": {
        "_id": "658be7fe135580745c510323",
        "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
        "isPro": false,
        "fullname": "Jian Zhang",
        "user": "VentureZJ",
        "type": "user"
      },
      "summary": "大語言モデルの基本的な質問回答形式は、プロンプトの入力と回答の受信を含むもので、プロンプトの質量は回答の有効性に直接影響します。自動化プロンプト最適化（APO）は、手動設計されたプロンプトの認知バイアスから自由に離れ、プロンプトの設計空間を広げることを目的としています。しかし、現在のAPO手法は固定テンプレートの柔軟性の限界とプロンプト空間の効率の悪い検索を主な問題としています。これに対して、私たちは、多エージェント融合テクノロジーを用いた自動計画を行うMulti-Agentフレームワーク Incorporating Socratic guidance（MARS）を提案します。特に、MARSは7つのエージェントを構成し、それぞれの機能が異なるもので、自動的にPlannerを使用して柔軟性を確保する最適化パスを設計することを主な機能としています。また、Teacher-Critic-Studentのソクラティックダイアロジーパターンを使用して、効果的な検索を行う間にプロンプトを連続的に最適化します。私たちは、様々なデータセットに対して拡大的な実験を行い、手法の有効性を証明し、追加的分析的な実験を行い、モデルの進歩と解釈性を評価します。",
      "upvotes": 31,
      "discussionId": "67e0c1d7151ca9ed9284ded7",
      "githubRepo": "https://github.com/exoskeletonzj/MARS",
      "ai_keywords": [
        "Multi-Agent framework",
        "Socratic guidance",
        "multi-agent fusion technology",
        "Planner",
        "Teacher-Critic-Student Socratic dialogue pattern"
      ]
    },
    "publishedAt": "2025-03-21T02:19:55.000Z",
    "title": "MARS: A Multi-Agent Framework Incorporating Socratic Guidance for\n  Automated Prompt Optimization",
    "summary": "The basic question-answering format of large language models involves\ninputting a prompt and receiving a response, and the quality of the prompt\ndirectly impacts the effectiveness of the response. Automated Prompt\nOptimization (APO) aims to break free from the cognitive biases of manually\ndesigned prompts and explores a broader design space for prompts. However,\nexisting APO methods suffer from limited flexibility of fixed templates and\ninefficient search in prompt spaces as key issues. To this end, we propose a\nMulti-Agent framework Incorporating Socratic guidance (MARS), which utilizes\nmulti-agent fusion technology for automatic planning, with gradual continuous\noptimization and evaluation. Specifically, MARS comprises seven agents, each\nwith distinct functionalities, which autonomously use the Planner to devise an\noptimization path that ensures flexibility. Additionally, it employs a\nTeacher-Critic-Student Socratic dialogue pattern to iteratively optimize the\nprompts while conducting effective search. We conduct extensive experiments on\nvarious datasets to validate the effectiveness of our method, and perform\nadditional analytical experiments to assess the model's advancement as well as\nthe interpretability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16874.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658be7fe135580745c510323",
      "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg",
      "fullname": "Jian Zhang",
      "name": "VentureZJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16408",
      "authors": [
        {
          "_id": "67dcdedbeff29d0d52c739e4",
          "user": {
            "_id": "658a6c1399ed106ac8c822b1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a6c1399ed106ac8c822b1/Wk2KXCcK39rUvXx6mpmGD.jpeg",
            "isPro": false,
            "fullname": "yiranqin",
            "user": "IranQin",
            "type": "user"
          },
          "name": "Yiran Qin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-21T11:40:36.686Z",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e5",
          "user": {
            "_id": "64eadcb03d76028d805a7818",
            "avatarUrl": "/avatars/528e4fded4419caf08589b2ed40437bc.svg",
            "isPro": false,
            "fullname": "Li Kang",
            "user": "FACEONG",
            "type": "user"
          },
          "name": "Li Kang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-21T11:40:38.834Z",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e6",
          "name": "Xiufeng Song",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e7",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e8",
          "name": "Xiaohong Liu",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739e9",
          "name": "Xihui Liu",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739ea",
          "name": "Ruimao Zhang",
          "hidden": false
        },
        {
          "_id": "67dcdedbeff29d0d52c739eb",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:58:38.000Z",
      "submittedOnDailyAt": "2025-03-24T01:35:09.139Z",
      "title": "ロボファクトリー：構成的制約を持つ具象化アグエントの協調を探求する",
      "submittedOnDailyBy": {
        "_id": "658a6c1399ed106ac8c822b1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a6c1399ed106ac8c822b1/Wk2KXCcK39rUvXx6mpmGD.jpeg",
        "isPro": false,
        "fullname": "yiranqin",
        "user": "IranQin",
        "type": "user"
      },
      "summary": "実体化マルチアグエントシステムの有効設計は、複雑な実世界のタスクを解決するために重要です。マルチアグエントシステムの複雑性により、現在の方法はこれらのシステムに対して安全で効率的な訓練データを自動的に生成できません。この点に対して、我々は実体化マルチアグエントシステムの構成制約の概念を提案し、アグエント間の協力による課題を解決しようとします。様々な種類の制約に合わせたフォーマットのフィールドを設計し、物理的な世界との無難な相互作用を実現します。構成制約と特別に設計されたフィールドを活用し、我々は実体化マルチアグエントシステムの自動的なデータ収集フレームワークを開発し、初めての実体化マルチアグエント操作のベンチマーク「RoboFactory」を介しています。「RoboFactory」ベンチマークに基づいて、我々は学習を模倣する方法を適用し、その性能を異なる難易度のアグエントタスクで分析します。また、多アグエントの学習を模倣するためのアーキテクチャと訓練戦略を検討し、安全で効率的な実体化マルチアグエントシステムを構築することを目指します。",
      "upvotes": 27,
      "discussionId": "67dcdedeeff29d0d52c73abc",
      "projectPage": "https://iranqin.github.io/robofactory/",
      "ai_keywords": [
        "compositional constraints",
        "embodied multi-agent systems",
        "data collection framework",
        "benchmark",
        "RoboFactory",
        "imitation learning",
        "multi-agent imitation learning",
        "training strategies"
      ]
    },
    "publishedAt": "2025-03-20T13:58:38.000Z",
    "title": "RoboFactory: Exploring Embodied Agent Collaboration with Compositional\n  Constraints",
    "summary": "Designing effective embodied multi-agent systems is critical for solving\ncomplex real-world tasks across domains. Due to the complexity of multi-agent\nembodied systems, existing methods fail to automatically generate safe and\nefficient training data for such systems. To this end, we propose the concept\nof compositional constraints for embodied multi-agent systems, addressing the\nchallenges arising from collaboration among embodied agents. We design various\ninterfaces tailored to different types of constraints, enabling seamless\ninteraction with the physical world. Leveraging compositional constraints and\nspecifically designed interfaces, we develop an automated data collection\nframework for embodied multi-agent systems and introduce the first benchmark\nfor embodied multi-agent manipulation, RoboFactory. Based on RoboFactory\nbenchmark, we adapt and evaluate the method of imitation learning and analyzed\nits performance in different difficulty agent tasks. Furthermore, we explore\nthe architectures and training strategies for multi-agent imitation learning,\naiming to build safe and efficient embodied multi-agent systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16408.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658a6c1399ed106ac8c822b1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a6c1399ed106ac8c822b1/Wk2KXCcK39rUvXx6mpmGD.jpeg",
      "fullname": "yiranqin",
      "name": "IranQin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16430",
      "authors": [
        {
          "_id": "67e0bd81b04d9e836829c468",
          "user": {
            "_id": "63ea23b9dedfeebe54d02bdf",
            "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
            "isPro": false,
            "fullname": "Yuqing Wang",
            "user": "Epiphqny",
            "type": "user"
          },
          "name": "Yuqing Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:10:42.267Z",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c469",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46a",
          "name": "Yao Teng",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46b",
          "name": "Yuanzhi Zhu",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46c",
          "user": {
            "_id": "60d2e681b8448e1785bbda06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
            "isPro": false,
            "fullname": "Shuhuai Ren",
            "user": "ShuhuaiRen",
            "type": "user"
          },
          "name": "Shuhuai Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:10:44.045Z",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46d",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67e0bd81b04d9e836829c46e",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:59.000Z",
      "submittedOnDailyAt": "2025-03-24T00:50:05.627Z",
      "title": "連続と離散トークンのバリューを統合した自動順次的視覚生成",
      "submittedOnDailyBy": {
        "_id": "63ea23b9dedfeebe54d02bdf",
        "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
        "isPro": false,
        "fullname": "Yuqing Wang",
        "user": "Epiphqny",
        "type": "user"
      },
      "summary": "自動回帰的画像生成モデルは通常トークナイザーを使用して画像をトークンに圧縮し、順番に予測できるようにする。トークン表現には基本的な論理的問題があります：離散トークンは標準的な交差エントロピー損失を用いて簡単にモデリングできますが、情報の損失とトークナイザーの訓練不穩定に苦労します；連続トークンは視覚詳細をより良く保存できますが、複雑な分布モデリングが必要で、生成パイプラインを複雑化します。本論文では、連続トークンの強力な表現能力を維持しながら離散トークンのモデリングの簡単性を維持することでこの間違いを経由するようにTokenBridgeを提案します。これを実現するために、トークナイザーの訓練プロセスから離散化を分離し、連続的表現から直接離散トークンを得るための後学習減量化を使用します。特に、次元ごとの減量化戦略を導入し、独立に各特徴次元を離散化し、その結果の大きなトークンスペースを効率的にモデル化する軽量自動回帰的予測機構を組み合わせます。厳密な実験は、標準的な分類予測を使用して連続的な方法と同等の再構築と生成質量を達成することを示します。この研究は、離散と連続のパラダイムを経由することで両方のアプローチの強さを効果的に活用できることを示し、簡単な自動回帰的モデリングを用いた高品質画像生成の可能性を提供します。プロジェクトページ：https://yuqingwang1029.github.io/TokenBridge.",
      "upvotes": 19,
      "discussionId": "67e0bd85b04d9e836829c55f",
      "projectPage": "https://yuqingwang1029.github.io/TokenBridge/",
      "githubRepo": "https://github.com/YuqingWang1029/TokenBridge",
      "ai_keywords": [
        "autoregressive visual generation models",
        "tokenizers",
        "tokens",
        "discrete tokens",
        "continuous tokens",
        "cross-entropy loss",
        "tokenizer training",
        "TokenBridge",
        "post-training quantization",
        "dimension-wise quantization",
        "lightweight autoregressive prediction mechanism",
        "reconstruction quality",
        "generation quality"
      ]
    },
    "publishedAt": "2025-03-20T13:59:59.000Z",
    "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual\n  Generation",
    "summary": "Autoregressive visual generation models typically rely on tokenizers to\ncompress images into tokens that can be predicted sequentially. A fundamental\ndilemma exists in token representation: discrete tokens enable straightforward\nmodeling with standard cross-entropy loss, but suffer from information loss and\ntokenizer training instability; continuous tokens better preserve visual\ndetails, but require complex distribution modeling, complicating the generation\npipeline. In this paper, we propose TokenBridge, which bridges this gap by\nmaintaining the strong representation capacity of continuous tokens while\npreserving the modeling simplicity of discrete tokens. To achieve this, we\ndecouple discretization from the tokenizer training process through\npost-training quantization that directly obtains discrete tokens from\ncontinuous representations. Specifically, we introduce a dimension-wise\nquantization strategy that independently discretizes each feature dimension,\npaired with a lightweight autoregressive prediction mechanism that efficiently\nmodel the resulting large token space. Extensive experiments show that our\napproach achieves reconstruction and generation quality on par with continuous\nmethods while using standard categorical prediction. This work demonstrates\nthat bridging discrete and continuous paradigms can effectively harness the\nstrengths of both approaches, providing a promising direction for high-quality\nvisual generation with simple autoregressive modeling. Project page:\nhttps://yuqingwang1029.github.io/TokenBridge.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16430.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63ea23b9dedfeebe54d02bdf",
      "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
      "fullname": "Yuqing Wang",
      "name": "Epiphqny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16660",
      "authors": [
        {
          "_id": "67e0ffb029682c8065e1c223",
          "name": "Eduard Allakhverdov",
          "hidden": false
        },
        {
          "_id": "67e0ffb029682c8065e1c224",
          "name": "Elizaveta Goncharova",
          "hidden": false
        },
        {
          "_id": "67e0ffb029682c8065e1c225",
          "name": "Andrey Kuznetsov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T19:17:08.000Z",
      "submittedOnDailyAt": "2025-03-24T05:20:19.676Z",
      "title": "それほど記述がありません。",
      "submittedOnDailyBy": {
        "_id": "6310ff34bc152fa3e810c186",
        "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
        "isPro": false,
        "fullname": "Elizaveta Goncharova",
        "user": "Elizaveta",
        "type": "user"
      },
      "summary": "ビジョンエンコーダーは通常大量のビジョントークンを生成し、情報豊富な表現を提供しますが、計算負担を大幅に増加させます。これにより、全ての生成されたトークンが等価であるか、その一部を捨てることで計算コストを減らせばよいかどうかの問題が浮かびます。本論文では、価値の低い特徴量は価値の高い特徴量から再構築できることを基準に特徴量の有用性を決定する新しい方法を介します。この概念を実現するために、ガンブル・ソフトマックス選択機能を持つ自動エンコーダーと統合し、最も情報豊富なビジョントークンのみを特定して残すことができます。この方法による特徴量を選択した場合とランダムに選択した場合のLLaVA-NeXTモデルの性能を比較し、OCRベースのタスクでは50%以上のビジョンコンテキストを削除できることを確認しましたが、同じ割合の特徴量をランダムに捨てることでモデルの機能が大幅に影響されます。また、一般領域のタスクでは、ランダムに30%のトークンのみを残した場合でも、全ビジョントークンを使用した場合と同等の性能を達成します。これらの結果は、性能を損なわずにスケーラブルで負荷が低い推論を実現するための適応的で効率的な多タイププリングの有望な方向を示しています。",
      "upvotes": 17,
      "discussionId": "67e0ffb229682c8065e1c2c6",
      "ai_keywords": [
        "autoencoder",
        "Gumbel-Softmax selection mechanism",
        "feature utility",
        "LLaVA-NeXT model",
        "OCR-based tasks",
        "visual context",
        "performance loss",
        "general-domain tasks",
        "multimodal pruning"
      ]
    },
    "publishedAt": "2025-03-20T15:17:08.000Z",
    "title": "When Less is Enough: Adaptive Token Reduction for Efficient Image\n  Representation",
    "summary": "Vision encoders typically generate a large number of visual tokens, providing\ninformation-rich representations but significantly increasing computational\ndemands. This raises the question of whether all generated tokens are equally\nvaluable or if some of them can be discarded to reduce computational costs\nwithout compromising quality. In this paper, we introduce a new method for\ndetermining feature utility based on the idea that less valuable features can\nbe reconstructed from more valuable ones. We implement this concept by\nintegrating an autoencoder with a Gumbel-Softmax selection mechanism, that\nallows identifying and retaining only the most informative visual tokens. To\nvalidate our approach, we compared the performance of the LLaVA-NeXT model,\nusing features selected by our method with randomly selected features. We found\nthat on OCR-based tasks, more than 50% of the visual context can be removed\nwith minimal performance loss, whereas randomly discarding the same proportion\nof features significantly affects the model capabilities. Furthermore, in\ngeneral-domain tasks, even randomly retaining only 30% of tokens achieves\nperformance comparable to using the full set of visual tokens. Our results\nhighlight a promising direction towards adaptive and efficient multimodal\npruning that facilitates scalable and low-overhead inference without\ncompromising performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16660.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310ff34bc152fa3e810c186",
      "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
      "fullname": "Elizaveta Goncharova",
      "name": "Elizaveta",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17352",
      "authors": [
        {
          "_id": "67e0bcc9e5fa0da84e121032",
          "name": "Yihe Deng",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121033",
          "name": "Hritik Bansal",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121034",
          "name": "Fan Yin",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121035",
          "name": "Nanyun Peng",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121036",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67e0bcc9e5fa0da84e121037",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T17:52:43.000Z",
      "submittedOnDailyAt": "2025-03-24T00:31:06.884Z",
      "title": "OpenVLThinker: 複雑な視覚言語理由論の初期探索\nそして、反復的な自己改良を通じて",
      "submittedOnDailyBy": {
        "_id": "642f4c789b2484d7d8551a93",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
        "isPro": true,
        "fullname": "Yihe Deng",
        "user": "ydeng9",
        "type": "user"
      },
      "summary": "最近のDeepSeek-R1の進展は、大規模言語モデル（LLMs）における複雑な理由論能力、特に自覚証明と自覚補正のような複雑な行動を、可証明な報酬を持つRLにより実現できることを示し、AIMEのような艱難なタスクでのモデルの性能を大幅に向上させることができることを示しています。これらの発見に励まし、本研究は、類似の理由論能力が大規模ビジョン言語モデル（LVLMs）に成功により統合できるかどうかを調査し、その影響を評価しています。これらの調査は、軽量データに対する監督的微調節（SFT）とReinforcement Learning（RL）の繰り返し的利用を考慮しています。最初に、理由論能力は、多様なビジョンデータセットからの高品質の写真のキャプションを用いて生成された理由論ステップから、pure-text R1モデルからドキュメントされます。次に、繰り返しのRL訓練は、各繰り返しのRL向上モデルが次のラウンドのSFTデータセットを生成することで理由論スキルを進化させます。この繰り返し的プロセスは、MathVista、MathVerse、MathVisionなどの艱難なベンチマークで理由論性能を一貫して向上させるLVLMであるOpenVLThinkerを収穫し、我々の戦略の強固なビジョン言語理由論の可能性を示しています。コード、モデルとデータは、https://github.com/yihedeng9/OpenVLThinkerにアクセスできます。",
      "upvotes": 11,
      "discussionId": "67e0bccae5fa0da84e121079",
      "projectPage": "https://yihe-deng.notion.site/openvlthinker",
      "githubRepo": "https://github.com/yihedeng9/OpenVLThinker",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "verifiable rewards",
        "large language models (LLMs)",
        "self-verification",
        "self-correction",
        "large vision-language models (LVLMs)",
        "multimodal reasoning tasks",
        "supervised fine-tuning (SFT)",
        "lightweight training data",
        "reasoning steps",
        "high-quality captions",
        "diversity",
        "visual datasets",
        "iterative process",
        "OpenVLThinker",
        "reasoning performance",
        "challenging benchmarks",
        "MathVista",
        "MathVerse",
        "MathVision",
        "robust vision-language reasoning"
      ]
    },
    "publishedAt": "2025-03-21T13:52:43.000Z",
    "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement",
    "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17352.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642f4c789b2484d7d8551a93",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642f4c789b2484d7d8551a93/0lH4YXcbZa-Xlzj6ESo7F.jpeg",
      "fullname": "Yihe Deng",
      "name": "ydeng9",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17126",
      "authors": [
        {
          "_id": "67e0beb474fc794321fb4ad7",
          "name": "John Joon Young Chung",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4ad8",
          "name": "Vishakh Padmakumar",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4ad9",
          "name": "Melissa Roemmele",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4ada",
          "name": "Yuqian Sun",
          "hidden": false
        },
        {
          "_id": "67e0beb474fc794321fb4adb",
          "name": "Max Kreminski",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T13:21:45.000Z",
      "submittedOnDailyAt": "2025-03-24T00:39:24.717Z",
      "title": "大規模言語モデルの訓練後における多様な創造的な書き込みのための調整",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "シンガラスワープタスクは単一の正しい答えがないため、これらのタスクに向けて訓練された大規模な言語モデル（LLMs）は、多様な有効な出力を生成できるべきである。しかし、LLMの訓練後の処理は、生成質量の向上を優先し、出力の多様性を促進していないことが多い。そこで、シンガラスワープタの生成において、出力の多様性と質の両方を促進する訓練後のアプローチについて検討した。私たちの核心のアイデアは、訓練サンプルと同じプロンプトを持つすべてのサンプルとの違いの度合い（deviation）を訓練目標に含め、稀しい高品質のインスタンスから学習を促進することである。私たちのアプローチを直接好み最適化（DPO）と確率比好み最適化（ORPO）に採用し、訓練されたモデルの出力の多様性を促進しながら質を最小限に減少させることができることを示した。私たちの最良モデル（8Bパラメータ）は、人間が作ったデータセットと同等の多様性を達成でき、調査した最良の指示されたモデル（GPT-4oとDeepSeek-R1）と同じ程度の出力質量を維持できることを示した。また、私たちのアプローチを人間評価、消去試験、既存の多様化アプローチ（DivPO）との比較で進一に検証した。",
      "upvotes": 9,
      "discussionId": "67e0beb574fc794321fb4b04",
      "ai_keywords": [
        "direct preference optimization (DPO)",
        "odds ratio preference optimization (ORPO)",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-03-21T09:21:45.000Z",
    "title": "Modifying Large Language Model Post-Training for Diverse Creative\n  Writing",
    "summary": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6443
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16867",
      "authors": [
        {
          "_id": "67e0d31f151ca9ed92898fff",
          "user": {
            "_id": "63bbf071d8d676a2299c7d0b",
            "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
            "isPro": false,
            "fullname": "Guan",
            "user": "Guan123",
            "type": "user"
          },
          "name": "Kaisi Guan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:06:57.786Z",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899000",
          "name": "Zhengfeng Lai",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899001",
          "name": "Yuchong Sun",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899002",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899003",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899004",
          "name": "Kieran Liu",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899005",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "67e0d31f151ca9ed92899006",
          "name": "Ruihua Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T05:52:50.000Z",
      "submittedOnDailyAt": "2025-03-24T07:22:50.935Z",
      "title": "ETVA: 文字から動画のアラインメントの評価による細かい問題生成と答える",
      "submittedOnDailyBy": {
        "_id": "63bbf071d8d676a2299c7d0b",
        "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
        "isPro": false,
        "fullname": "Guan",
        "user": "Guan123",
        "type": "user"
      },
      "summary": "テキストプロンプトと生成されたビデオの意味的な対応関係の精密な評価は、テキストからビデオ（T2V）生成では難題である。既存のテキストからビデオの対応関係評価指標であるCLIPScoreは、細かい対応詳細を持たず、粗略なスコアを生成し、人間の好みに合わずない。この制限を解決するために、私たちはETVA（テキストからビデオの対応関係の新しい評価方法）を提案します。ETVAは、細かい質問生成と回答を通じた対応関係評価の新しい方法です。まず、多変数システムがプロンプトを意味的なスケーングラフに解析し、原子的な質問を生成します。次に、知識を拡張した多段階推理フレームワークを設計し、協力LLMは物理の法則などの関係知識を検索し、ビデオLLMは生成された質問を多段階推理機制を通じて答えます。拡張された実験により、ETVAはシューマーンの相関係数58.47を達成し、人間の判断に対してより高い相関を示し、既存の指標と比べて31.0のみの相関を示しているものに比べて高い。また、ETVAは、15つの既存のテキストからビデオモデルの主要な能力と制限を特定し、次世代のT2V生成のための道を開けます。",
      "upvotes": 5,
      "discussionId": "67e0d322151ca9ed928990c0",
      "projectPage": "https://eftv-eval.github.io/etva-eval/",
      "ai_keywords": [
        "semantic alignment",
        "Text-to-Video (T2V) Generation",
        "CLIPScore",
        "fine-grained alignment details",
        "multi-agent system",
        "semantic scene graphs",
        "atomic questions",
        "knowledge-augmented",
        "multi-stage reasoning framework",
        "auxiliary LLM",
        "common-sense knowledge",
        "video LLM",
        "multi-stage reasoning mechanism",
        "Spearman's correlation coefficient",
        "benchmark",
        "text-to-video models"
      ]
    },
    "publishedAt": "2025-03-21T01:52:50.000Z",
    "title": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question\n  Generation and Answering",
    "summary": "Precisely evaluating semantic alignment between text prompts and generated\nvideos remains a challenge in Text-to-Video (T2V) Generation. Existing\ntext-to-video alignment metrics like CLIPScore only generate coarse-grained\nscores without fine-grained alignment details, failing to align with human\npreference. To address this limitation, we propose ETVA, a novel Evaluation\nmethod of Text-to-Video Alignment via fine-grained question generation and\nanswering. First, a multi-agent system parses prompts into semantic scene\ngraphs to generate atomic questions. Then we design a knowledge-augmented\nmulti-stage reasoning framework for question answering, where an auxiliary LLM\nfirst retrieves relevant common-sense knowledge (e.g., physical laws), and then\nvideo LLM answers the generated questions through a multi-stage reasoning\nmechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's\ncorrelation coefficient of 58.47, showing a much higher correlation with human\njudgment than existing metrics which attain only 31.0. We also construct a\ncomprehensive benchmark specifically designed for text-to-video alignment\nevaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10\ncategories. Through a systematic evaluation of 15 existing text-to-video\nmodels, we identify their key capabilities and limitations, paving the way for\nnext-generation T2V generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16867.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbf071d8d676a2299c7d0b",
      "avatarUrl": "/avatars/4bb1c86ef8651c75b9761afee2865267.svg",
      "fullname": "Guan",
      "name": "Guan123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16549",
      "authors": [
        {
          "_id": "67e0d11eb04d9e83682f222a",
          "name": "Felix Chen",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222b",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:06:59.943Z",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222c",
          "name": "Yunqiu Xu",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222d",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222e",
          "name": "Jun Cen",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f222f",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f2230",
          "name": "Zeying Huang",
          "hidden": false
        },
        {
          "_id": "67e0d11eb04d9e83682f2231",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T11:46:19.000Z",
      "submittedOnDailyAt": "2025-03-24T01:59:23.638Z",
      "title": "MathFlow: ビジュアル数理学問題のMLLMの認識フローを高める",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "多モダル大語言モデル（MLLMs）は、多様なタスクでの驚異な実績を示しているにも関わらず、視覚的数理問題解決においてその潜在的な可能性を完全に示すことはできていません、特に図の正確な認識と解釈について。人間の通常のプロセスにもっとも類似したものを参考に、図から意味のある情報を抽出する視覚的認識能力が重要であることを仮定しています、これは後続する推論プロセスに直接影響を及ぼします。この仮説を検証するために、FlowVerseという詳細なベンチマークを開発しました。これは、問題解決の際に使用されるすべての情報を4つのコンポーネントに分類し、それらを6つの問題バージョンに組み合わせて評価するものです。FlowVerseにおける初步的な結果から、現在のMLLMは図から重要な情報と理由的な性質を抽出し、それに基づく複雑な推理を行うことについては、極めて限られた制限を示しています。これに対応して、MathFlowというモジュール化された問題解決パイプラインを導入しました。これは視覚的認識と推論を違うステップに分け、それぞれ独立に最適化することです。現在のMLLMにおける視覚的認識の限界を踏まえ、MathFlow-P-7Bという専門的な認識モデルを訓練しました。実験結果から、MathFlow-P-7Bと閉源や開源の推論モデルを組み合わせた場合には、大幅な性能向上を示しています。これはMathFlowパイプラインの効果と、多様な推論フレームワークとの相容性を示しています。FlowVerseベンチマークとコードは、https://github.com/MathFlow-zju/MathFlowに公開されています。",
      "upvotes": 4,
      "discussionId": "67e0d11fb04d9e83682f2267",
      "githubRepo": "https://github.com/MathFlow-zju/MathFlow",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "visual mathematical problem-solving",
        "diagrams",
        "perception capabilities",
        "inference processes",
        "FlowVerse",
        "problem-solving",
        "essential information",
        "reasoned property",
        "MathFlow",
        "problem-solving pipeline",
        "perception model",
        "MathFlow-P-7B",
        "closed-source inference models",
        "open-source inference models"
      ]
    },
    "publishedAt": "2025-03-19T07:46:19.000Z",
    "title": "MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical\n  Problems",
    "summary": "Despite impressive performance across diverse tasks, Multimodal Large\nLanguage Models (MLLMs) have yet to fully demonstrate their potential in visual\nmathematical problem-solving, particularly in accurately perceiving and\ninterpreting diagrams. Inspired by typical processes of humans, we hypothesize\nthat the perception capabilities to extract meaningful information from\ndiagrams is crucial, as it directly impacts subsequent inference processes. To\nvalidate this hypothesis, we developed FlowVerse, a comprehensive benchmark\nthat categorizes all information used during problem-solving into four\ncomponents, which are then combined into six problem versions for evaluation.\nOur preliminary results on FlowVerse reveal that existing MLLMs exhibit\nsubstantial limitations when extracting essential information and reasoned\nproperty from diagrams and performing complex reasoning based on these visual\ninputs. In response, we introduce MathFlow, a modular problem-solving pipeline\nthat decouples perception and inference into distinct stages, thereby\noptimizing each independently. Given the perceptual limitations observed in\ncurrent MLLMs, we trained MathFlow-P-7B as a dedicated perception model.\nExperimental results indicate that MathFlow-P-7B yields substantial performance\ngains when integrated with various closed-source and open-source inference\nmodels. This demonstrates the effectiveness of the MathFlow pipeline and its\ncompatibility to diverse inference frameworks. The FlowVerse benchmark and code\nare available at https://github.com/MathFlow-zju/MathFlow.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16549.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16983",
      "authors": [
        {
          "_id": "67e0c303ff27a08e3896134a",
          "name": "Xu Zhang",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134b",
          "name": "Hao Zhou",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134c",
          "name": "Haoming Qin",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134d",
          "name": "Xiaobin Lu",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134e",
          "name": "Jiaxing Yan",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e3896134f",
          "name": "Guanzhong Wang",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e38961350",
          "name": "Zeyu Chen",
          "hidden": false
        },
        {
          "_id": "67e0c303ff27a08e38961351",
          "name": "Yi Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T09:48:00.000Z",
      "submittedOnDailyAt": "2025-03-24T00:58:01.102Z",
      "title": "「ビデオディフュージョンモデルの機能を広範囲に制御するための機能」",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "テキスト・タグ生成の進展により、ビデオ生成研究の中で、精細な空間時間属性に対する精度と柔軟性の高い制御を実現するのは難しい課題である。これらの制限を解決するために、VCtrl（もしくはPP-VCtrlと呼ばれる）という新しいフレームワークを紹介します。このフレームワークは、一貫した手がかりで、事前学習されたビデオディフューションモデルに対して精細な制御を可能にします。VCtrlは、Cannyエッジ、セグメンテーションマスク、ヒマンキーポイントなどの多様なユーザー指定の制御シグナルを、変更しないようにして、一般化可能な条件モジュールによって事前学習されたビデオディフューションモデルに統一的に組み込みます。また、一貫した制御シグナルのエンコーディングパイプラインと、スパースな残差接続機構を設計し、制御表現を効率的に組み込みます。詳細な実験と人間評価により、VCtrlは制御可能性と生成質の向上を実現します。ソースコードと事前学習モデルは、PaddlePaddleフレームワークを用いて公開的に提供されており、以下のURLで利用できます：http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl。",
      "upvotes": 3,
      "discussionId": "67e0c306ff27a08e38961428",
      "ai_keywords": [
        "VCtrl",
        "PP-VCtrl",
        "fine-grained control",
        "pre-trained video diffusion models",
        "conditional module",
        "Canny edges",
        "segmentation masks",
        "human keypoints",
        "unified control signal encoding pipeline",
        "sparse residual connection mechanism",
        "controllability",
        "PaddlePaddle",
        "PaddleMIX",
        "ppdiffusers"
      ]
    },
    "publishedAt": "2025-03-21T05:48:00.000Z",
    "title": "Enabling Versatile Controls for Video Diffusion Models",
    "summary": "Despite substantial progress in text-to-video generation, achieving precise\nand flexible control over fine-grained spatiotemporal attributes remains a\nsignificant unresolved challenge in video generation research. To address these\nlimitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework\ndesigned to enable fine-grained control over pre-trained video diffusion models\nin a unified manner. VCtrl integrates diverse user-specified control\nsignals-such as Canny edges, segmentation masks, and human keypoints-into\npretrained video diffusion models via a generalizable conditional module\ncapable of uniformly encoding multiple types of auxiliary signals without\nmodifying the underlying generator. Additionally, we design a unified control\nsignal encoding pipeline and a sparse residual connection mechanism to\nefficiently incorporate control representations. Comprehensive experiments and\nhuman evaluations demonstrate that VCtrl effectively enhances controllability\nand generation quality. The source code and pre-trained models are publicly\navailable and implemented using the PaddlePaddle framework at\nhttp://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16983.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6443
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16921",
      "authors": [
        {
          "_id": "67e0bb1665e294ad989334ea",
          "name": "Lingfan Zhang",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334eb",
          "name": "Chen Liu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ec",
          "name": "Chengming Xu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ed",
          "name": "Kai Hu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ee",
          "name": "Donghao Luo",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334ef",
          "name": "Chengjie Wang",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334f0",
          "name": "Yanwei Fu",
          "hidden": false
        },
        {
          "_id": "67e0bb1665e294ad989334f1",
          "name": "Yuan Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T07:33:44.000Z",
      "submittedOnDailyAt": "2025-03-24T00:24:41.729Z",
      "title": "ポリタージャーが離れる時：少数者に対して認識するアダプティブなDPOでの拡散モデルの調整",
      "submittedOnDailyBy": {
        "_id": "652fab9d04a34a9282bf29d6",
        "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
        "isPro": false,
        "fullname": "Chengming Xu",
        "user": "ChengmingX",
        "type": "user"
      },
      "summary": "近年、画像生成の分野では、特にモデルを普遍的な人間の好みに合わせる微調節手法において顕著な進展が見られています。本論文では、画像生成の過程で好みデータの重要な役割を調査し、特にDiffusion-DPOとその後続的な変更の背景での重要性を説明します。私たちは、画像生成における普遍的な人間の好みに関する複雑性を調査し、これらの好みの主観的な性質と好みデータセットにおける少数派サンプルによる課題を明らかにします。プイルト実験を通じて、少数派サンプルの存在とそのモデルの性能に及ぼす有害な影響を示します。私たちは、少数派サンプルに関心を持つメトリックをDPOの目標関数に組み込む新しいアプローチ、Adaptive-DPOを提案します。このメトリックは、内間者の信頼度と間間者の安定性を含むことで多数派と少数派サンプルを区別します。私たちは、Adaptive-DPO損失関数を紹介し、これがDPO損失を2つの方法で改善します：多数派ラベルの学習を強化しながら少数派サンプルの負面影響を軽減します。私たちの実験は、合成少数派データと実世界的な好みデータを効果的に扱うことを示し、画像生成タスクでより効果的な訓練メソッドの開発に道を開けます。",
      "upvotes": 3,
      "discussionId": "67e0bb1a65e294ad9893361c",
      "ai_keywords": [
        "diffusion models",
        "Diffusion-DPO",
        "adaptive-DPO",
        "intra-annotator confidence",
        "inter-annotator stability",
        "DPO objective",
        "Adaptive-DPO loss function"
      ]
    },
    "publishedAt": "2025-03-21T03:33:44.000Z",
    "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware\n  Adaptive DPO",
    "summary": "In recent years, the field of image generation has witnessed significant\nadvancements, particularly in fine-tuning methods that align models with\nuniversal human preferences. This paper explores the critical role of\npreference data in the training process of diffusion models, particularly in\nthe context of Diffusion-DPO and its subsequent adaptations. We investigate the\ncomplexities surrounding universal human preferences in image generation,\nhighlighting the subjective nature of these preferences and the challenges\nposed by minority samples in preference datasets. Through pilot experiments, we\ndemonstrate the existence of minority samples and their detrimental effects on\nmodel performance. We propose Adaptive-DPO -- a novel approach that\nincorporates a minority-instance-aware metric into the DPO objective. This\nmetric, which includes intra-annotator confidence and inter-annotator\nstability, distinguishes between majority and minority samples. We introduce an\nAdaptive-DPO loss function which improves the DPO loss in two ways: enhancing\nthe model's learning of majority labels while mitigating the negative impact of\nminority samples. Our experiments demonstrate that this method effectively\nhandles both synthetic minority data and real-world preference data, paving the\nway for more effective training methodologies in image generation tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652fab9d04a34a9282bf29d6",
      "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
      "fullname": "Chengming Xu",
      "name": "ChengmingX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16025",
      "authors": [
        {
          "_id": "67dd02594aa37abf77af416b",
          "user": {
            "_id": "63eb8b1113a3eb9b0dc89d8c",
            "avatarUrl": "/avatars/d9cb7bdf4f3d2218f7d84120a00054bb.svg",
            "isPro": false,
            "fullname": "Yair Shpitzer",
            "user": "yairshp",
            "type": "user"
          },
          "name": "Yair Shpitzer",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-21T11:40:09.783Z",
          "hidden": false
        },
        {
          "_id": "67dd02594aa37abf77af416c",
          "name": "Gal Chechik",
          "hidden": false
        },
        {
          "_id": "67dd02594aa37abf77af416d",
          "name": "Idan Schwartz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T10:45:04.000Z",
      "submittedOnDailyAt": "2025-03-24T08:16:33.033Z",
      "title": "シングルイメージイテレーショナル主題駆動生成と編集",
      "submittedOnDailyBy": {
        "_id": "63eb8b1113a3eb9b0dc89d8c",
        "avatarUrl": "/avatars/d9cb7bdf4f3d2218f7d84120a00054bb.svg",
        "isPro": false,
        "fullname": "Yair Shpitzer",
        "user": "yairshp",
        "type": "user"
      },
      "summary": "個人化画像生成と編集は、主題の画像が少ない場合、または1枚の画像しかない場合に特に難しい。個人化の一般的なアプローチとしては、概念学習があり、主題を既存モデルに納めることができるが、主題画像の数が少ないと画像の質が速やかに悪化する。質を向上させるためには、エンコーダーの事前学習を行うことができるが、これは生成を学習分布に限定し、時間がかかる。単一画像から画像生成と編集を個人化するのは難しい課題であり、まだ解決していない。ここでは、入力された主題画像との類似度スコアを最適化するための新しい、学習不要のアプローチを紹介します。具体的には、SISOは、類似度の損失に基づいて画像を連続的に生成し、モデルを最適化し、満足したレベルの類似度を達成するまで繰り返し行い、どの画像ジェネレーターにもプラッグとパートンの最適化を可能にします。SISOは、個人の多様なデータセットを使用して画像編集と画像生成の2つのタスクで評価され、画像質、主題の忠実性、背景の保存において既存の方法より显著な向上を示しました。",
      "upvotes": 3,
      "discussionId": "67dd025f4aa37abf77af42db",
      "projectPage": "https://siso-paper.github.io/",
      "githubRepo": "https://github.com/yairshp/SISO",
      "ai_keywords": [
        "concept learning",
        "encoder",
        "pre-training",
        "similarity score",
        "iterative generation",
        "model optimization",
        "plug-and-play optimization"
      ]
    },
    "publishedAt": "2025-03-20T06:45:04.000Z",
    "title": "Single Image Iterative Subject-driven Generation and Editing",
    "summary": "Personalizing image generation and editing is particularly challenging when\nwe only have a few images of the subject, or even a single image. A common\napproach to personalization is concept learning, which can integrate the\nsubject into existing models relatively quickly, but produces images whose\nquality tends to deteriorate quickly when the number of subject images is\nsmall. Quality can be improved by pre-training an encoder, but training\nrestricts generation to the training distribution, and is time consuming. It is\nstill an open hard challenge to personalize image generation and editing from a\nsingle image without training. Here, we present SISO, a novel, training-free\napproach based on optimizing a similarity score with an input subject image.\nMore specifically, SISO iteratively generates images and optimizes the model\nbased on loss of similarity with the given subject image until a satisfactory\nlevel of similarity is achieved, allowing plug-and-play optimization to any\nimage generator. We evaluated SISO in two tasks, image editing and image\ngeneration, using a diverse data set of personal subjects, and demonstrate\nsignificant improvements over existing methods in image quality, subject\nfidelity, and background preservation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16025.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63eb8b1113a3eb9b0dc89d8c",
      "avatarUrl": "/avatars/d9cb7bdf4f3d2218f7d84120a00054bb.svg",
      "fullname": "Yair Shpitzer",
      "name": "yairshp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12821",
      "authors": [
        {
          "_id": "67e0cc432bbf376bdb18623b",
          "user": {
            "_id": "66aca01e33f6b27979856f6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
            "isPro": false,
            "fullname": "Mingyang Song",
            "user": "hitsmy",
            "type": "user"
          },
          "name": "Mingyang Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:07:01.991Z",
          "hidden": false
        },
        {
          "_id": "67e0cc432bbf376bdb18623c",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67e0cc432bbf376bdb18623d",
          "name": "Jiawei Zhou",
          "hidden": false
        },
        {
          "_id": "67e0cc432bbf376bdb18623e",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T05:01:09.000Z",
      "submittedOnDailyAt": "2025-03-24T02:58:13.280Z",
      "title": "頭から尾まで：大規模な視覚言語モデルでのバランスのある表現を目指して、適応的なデータ調整を通じて",
      "submittedOnDailyBy": {
        "_id": "66aca01e33f6b27979856f6f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
        "isPro": false,
        "fullname": "Mingyang Song",
        "user": "hitsmy",
        "type": "user"
      },
      "summary": "大型ビジュアル言語モデル（LVLMs）は、視覚理解と言語生成の統合に関して顕著な進展を達成しました。この成功の上にも、LVLMsの訓練データは、データ分布が高度に不平衡しているような長尾（LT）問題に直面しています。先行研究は主に、テロピー（CLIP）やViTのような伝統的なVLMアーキテクチャや特定のタスク（例えば識別と分類）に焦点を当てていました。しかし、LVLM（例えばLLaVA）やより一般的なタスク（例えば視覚問題解答と視覚推論）に関する探索はまだ課題です。本論文では、LVLMのLT問題について深い分析を行い、頭概念の過剰表現と尾概念の欠乏表現が2つの核心的な原因であることを識別します。この見地に基づいて、我々は、データ再平衡（DR）とデータ合成（DS）の2段階構造を構成する適応的データ精細化フレームワーク（ADR）を提案します。DR段階では、エンティティ分布に基づいて冗長なデータを適応的に再平衡し、DS段階では、デノイズディフフィジェンシャルプロビブリスティックモデル（DDPMs）と稀少な画像を利用し、欠代表された部分を補完します。11テストベンチマークの幅広い評価を通じて、我々の提案されたADRは、訓練データの長尾問題を効果的に解決し、LLaVA 1.5の平均性能を約4.36%上げることができました。",
      "upvotes": 3,
      "discussionId": "67e0cc442bbf376bdb186293",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "Long-Tail (LT) problems",
        "CLIP",
        "ViT",
        "LLaVA",
        "Visual Question Answering",
        "Visual Reasoning",
        "Adaptive Data Refinement Framework (ADR)",
        "Data Rebalancing (DR)",
        "Data Synthesis (DS)",
        "Denoising Diffusion Probabilistic Models (DDPMs)"
      ]
    },
    "publishedAt": "2025-03-17T01:01:09.000Z",
    "title": "From Head to Tail: Towards Balanced Representation in Large\n  Vision-Language Models through Adaptive Data Calibration",
    "summary": "Large Vision-Language Models (LVLMs) have achieved significant progress in\ncombining visual comprehension with language generation. Despite this success,\nthe training data of LVLMs still suffers from Long-Tail (LT) problems, where\nthe data distribution is highly imbalanced. Previous works have mainly focused\non traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as\nrecognition and classification. Nevertheless, the exploration of LVLM (e.g.\nLLaVA) and more general tasks (e.g. Visual Question Answering and Visual\nReasoning) remains under-explored. In this paper, we first conduct an in-depth\nanalysis of the LT issues in LVLMs and identify two core causes: the\noverrepresentation of head concepts and the underrepresentation of tail\nconcepts. Based on the above observation, we propose an Adaptive\nData Refinement Framework (ADR), which\nconsists of two stages: Data Rebalancing (DR)\nand Data Synthesis (DS). In the DR stage, we\nadaptively rebalance the redundant data based on entity distributions, while in\nthe DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and\nscarce images to supplement underrepresented portions. Through comprehensive\nevaluations across eleven benchmarks, our proposed ADR effectively mitigates\nthe long-tail problem in the training data, improving the average performance\nof LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66aca01e33f6b27979856f6f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg",
      "fullname": "Mingyang Song",
      "name": "hitsmy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.17287",
      "authors": [
        {
          "_id": "67e0bfcc8fb92b0edaa78dc0",
          "name": "Mingyang Song",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc1",
          "name": "Mao Zheng",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc2",
          "name": "Zheng Li",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc3",
          "name": "Wenjie Yang",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc4",
          "name": "Xuan Luo",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc5",
          "name": "Yue Pan",
          "hidden": false
        },
        {
          "_id": "67e0bfcc8fb92b0edaa78dc6",
          "name": "Feng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T16:35:31.000Z",
      "submittedOnDailyAt": "2025-03-24T00:43:41.116Z",
      "title": "FastCuRL: 進歩的コンテキストを用いたカリキュラム強化学習\n  R1より適応性の高い理由構造モデルの効率的な訓練用拡張",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "この論文では、\\textsc{FastCuRL}という簡単で効率的なコースチューリング強化学習アプローチを提案します。これは、長文脈拡大戦略を用いて、R1ような理由論モデルの強化学習トレーニング効率を加速し、長文脈の理由論を含む複雑な理由論タスクの解決策においてモデルの性能を向上させるために設計されています。特に、1.5Bパラメータの言語モデルを使用しています。\n\n\\textsc{FastCuRL}は主に2つの手順から成る：入力プロンプトの長さに基づいてトレーニングデータを3つのレベルに分割し、長文脈拡大トレーニングを行う。具体的には、前の手順は、入力プロンプトの長さに基づいて元のトレーニングデータを3つのレベルに分割し、後の手順は、長文脈拡大コンテキストウィンドウを使用して分割されたトレーニングデータを用いて理由論モデルをトレーニングします。\n\n実験結果は、\\textsc{FastCuRL}-1.5B-Previewは、5つのデータセット（MATH 500、AIME 2024、AMC 2023、Minerva Math、OlympiadBench）のすべてに対して、DeepScaleR-1.5B-Previewを超えることを示し、トレーニングステップの50％を利用しています。また、\\textsc{FastCuRL}-1.5B-Previewのすべてのトレーニングステップは、8ガボンプのノードで実行されました。",
      "upvotes": 2,
      "discussionId": "67e0bfcd8fb92b0edaa78e17",
      "githubRepo": "https://github.com/nick7nlp/FastCuRL",
      "ai_keywords": [
        "Curriculum Reinforcement Learning",
        "context window",
        "reinforcement learning",
        "training efficiency",
        "R1-like reasoning models",
        "long chain-of-thought",
        "rationales",
        "length-aware training",
        "training data segmentation",
        "context window extension",
        "progressively increasing context window length",
        "DeepScaleR-1.5B-Preview",
        "MATH 500",
        "AIME 2024",
        "AMC 2023",
        "Minerva Math",
        "OlympiadBench",
        "training steps"
      ]
    },
    "publishedAt": "2025-03-21T12:35:31.000Z",
    "title": "FastCuRL: Curriculum Reinforcement Learning with Progressive Context\n  Extension for Efficient Training R1-like Reasoning Models",
    "summary": "In this paper, we propose \\textsc{FastCuRL}, a simple yet efficient\nCurriculum Reinforcement Learning approach with\ncontext window extending strategy to accelerate the reinforcement learning\ntraining efficiency for R1-like reasoning models while enhancing their\nperformance in tackling complex reasoning tasks with long chain-of-thought\nrationales, particularly with a 1.5B parameter language model.\n\\textsc{FastCuRL} consists of two main procedures: length-aware\ntraining data segmentation and context window extension training. Specifically,\nthe former first splits the original training data into three different levels\nby the input prompt length, and then the latter leverages segmented training\ndatasets with a progressively increasing context window length to train the\nreasoning model. Experimental results demonstrate that\n\\textsc{FastCuRL}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview\nacross all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva\nMath, and OlympiadBench) while only utilizing 50\\% of training steps.\nFurthermore, all training stages for FastCuRL-1.5B-Preview are completed using\njust a single node with 8 GPUs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17287.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6443
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17069",
      "authors": [
        {
          "_id": "67e0ba47753cfd5e438d3814",
          "user": {
            "_id": "63be636387619d1458c2e8e0",
            "avatarUrl": "/avatars/83e14735760c5cadd5341ebcb4cf9556.svg",
            "isPro": false,
            "fullname": "SHI YUFEI",
            "user": "Master-Shi",
            "type": "user"
          },
          "name": "Yufei Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-24T08:10:50.772Z",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3815",
          "name": "Weilong Yan",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3816",
          "name": "Gang Xu",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3817",
          "name": "Yumeng Li",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3818",
          "name": "Yuchen Li",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d3819",
          "name": "Zhenxi Li",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d381a",
          "name": "Fei Richard Yu",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d381b",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67e0ba47753cfd5e438d381c",
          "name": "Si Yong Yeo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T11:50:06.000Z",
      "submittedOnDailyAt": "2025-03-24T07:28:42.136Z",
      "title": "PVChat: 個人化ビデオチャット（単次学習）",
      "submittedOnDailyBy": {
        "_id": "63be636387619d1458c2e8e0",
        "avatarUrl": "/avatars/83e14735760c5cadd5341ebcb4cf9556.svg",
        "isPro": false,
        "fullname": "SHI YUFEI",
        "user": "Master-Shi",
        "type": "user"
      },
      "summary": "Video large language models (ViLLMs) は一般的な映画理解に特化しています、例えば話しから食事をするようなアクティビティの認識に成功していますが、「Wilson は化学療法を受けています」や「Tom はSarahと議論しています」のような身分情報に関する理解に難しい点があり、スマートハウスやスマートハウス環境での応用が制限されています。この制限を解決するために、私たちは、個人化されたViLLMを設計した1ステップ学習フレームワークPVChatを提案します。これは、各個体に対して1枚の映画から主題に関する問い合わせの答えを提供するための最初の個人化ViLLMです。私たちのアプローチは、合成的に増強された映画QAデータセットを用いて、Mixture-of-Heads (MoH) を強化したViLLMを最適化し、進歩的な画像から映画の学習戦略を活用しています。特に、私たちは、自動化された増強プイプラインを導入し、既存の映画コーパスから身分情報を保持した正例と難しい負例を検索し、存在、外見、行動、位置の4つのQAタイプを持つ多様な学習データセットを生成します。主題特化の学習を強化するために、私たちはReLU Routing MoH注意機構を提案し、2つの新しい目標を提出します：1) スムーズな近接正規化を実現するための指数関数的な距離スケーリングと、2) 平衡な注意路由を実現するためのHead Activation Enhancement。最後に、私たちは2ステップの学習戦略を採用し、画像の事前学習から映画の微調節に移行し、静的な属性から動的な表現による進歩的な学習を可能にします。PVChatは、医学スケーナー、テレビシリーズ、アニメ、実世界的なフードアップを含む多様なデータセットで評価され、個人化の特徴理解の上位レベルを示し、1枚の映画から学習した後に比較的ViLLMsの上位レベルであることを示します。",
      "upvotes": 2,
      "discussionId": "67e0ba4b753cfd5e438d391e",
      "ai_keywords": [
        "ViLLMs",
        "one-shot learning",
        "PVChat",
        "Mixture-of-Heads (MoH)",
        "image-to-video learning",
        "automated augmentation pipeline",
        "identity-preserving positive samples",
        "hard negatives",
        "QA types",
        "ReLU Routing MoH attention mechanism",
        "Smooth Proximity Regularization",
        "Head Activation Enhancement",
        "two-stage training strategy",
        "image pre-training",
        "video fine-tuning",
        "personalized feature understanding",
        "state-of-the-art ViLLMs"
      ]
    },
    "publishedAt": "2025-03-21T07:50:06.000Z",
    "title": "PVChat: Personalized Video Chat with One-Shot Learning",
    "summary": "Video large language models (ViLLMs) excel in general video understanding,\ne.g., recognizing activities like talking and eating, but struggle with\nidentity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or\n\"Tom is discussing with Sarah\", limiting their applicability in smart\nhealthcare and smart home environments. To address this limitation, we propose\na one-shot learning framework PVChat, the first personalized ViLLM that enables\nsubject-aware question answering (QA) from a single video for each subject. Our\napproach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically\naugmented video-QA dataset, leveraging a progressive image-to-video learning\nstrategy. Specifically, we introduce an automated augmentation pipeline that\nsynthesizes identity-preserving positive samples and retrieves hard negatives\nfrom existing video corpora, generating a diverse training dataset with four QA\ntypes: existence, appearance, action, and location inquiries. To enhance\nsubject-specific learning, we propose a ReLU Routing MoH attention mechanism,\nalongside two novel objectives: (1) Smooth Proximity Regularization for\nprogressive learning through exponential distance scaling and (2) Head\nActivation Enhancement for balanced attention routing. Finally, we adopt a\ntwo-stage training strategy, transitioning from image pre-training to video\nfine-tuning, enabling a gradual learning process from static attributes to\ndynamic representations. We evaluate PVChat on diverse datasets covering\nmedical scenarios, TV series, anime, and real-world footage, demonstrating its\nsuperiority in personalized feature understanding after learning from a single\nvideo, compared to state-of-the-art ViLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17069.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63be636387619d1458c2e8e0",
      "avatarUrl": "/avatars/83e14735760c5cadd5341ebcb4cf9556.svg",
      "fullname": "SHI YUFEI",
      "name": "Master-Shi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.11572",
      "authors": [
        {
          "_id": "67e0530f151ca9ed9265e949",
          "user": {
            "_id": "64c5d832d68946edad7d5536",
            "avatarUrl": "/avatars/5d38217d4ab99cdcf53c52661b8baa0d.svg",
            "isPro": false,
            "fullname": "Messi Lee",
            "user": "l048596",
            "type": "user"
          },
          "name": "Messi H. J. Lee",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-23T18:30:22.473Z",
          "hidden": false
        },
        {
          "_id": "67e0530f151ca9ed9265e94a",
          "name": "Calvin K. Lai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-14T16:40:02.000Z",
      "submittedOnDailyAt": "2025-03-24T02:15:38.327Z",
      "title": "理由モデルにおけるインプリットバイアスのようなパターン",
      "submittedOnDailyBy": {
        "_id": "64c5d832d68946edad7d5536",
        "avatarUrl": "/avatars/5d38217d4ab99cdcf53c52661b8baa0d.svg",
        "isPro": false,
        "fullname": "Messi Lee",
        "user": "l048596",
        "type": "user"
      },
      "summary": "インプリットバイアスは、観察、判断、行動を形成する自動的または自発的な心鎖的プロセスを指します。先行研究では、大規模言語モデル（LLMs）におけるインプリットバイアスの研究は、人間での研究方法と異なり、主にモデルの出力を焦点にしていました。モデルのプロセスを調べるためには、理由モデルインプリットアソシエーションテスト（RM-IAT）という方法を提案します。この方法を使用して、理由モデルは、関連性のある情報と比較して、関連性のない情報を処理する際には、モジュールド数が多くなることを見出しました。これらの発見は、AIシステムが情報の処理において人間のインプリットバイアスに似たパターンを持っていることを示します。これらのインプリットバイアスのようなパターンが実世界的なアプリケーションでの機能にどのような影響を及ぼすかについて議論します。",
      "upvotes": 2,
      "discussionId": "67e05311151ca9ed9265e9c1",
      "githubRepo": "https://github.com/lee-messi/RM-IAT",
      "ai_keywords": [
        "Reasoning Model Implicit Association Test (RM-IAT)",
        "reasoning models",
        "LLMs (Large Language Models)",
        "tokens",
        "association-incompatible information",
        "association-compatible information",
        "implicit bias-like patterns"
      ]
    },
    "publishedAt": "2025-03-14T12:40:02.000Z",
    "title": "Implicit Bias-Like Patterns in Reasoning Models",
    "summary": "Implicit bias refers to automatic or spontaneous mental processes that shape\nperceptions, judgments, and behaviors. Previous research examining `implicit\nbias' in large language models (LLMs) has often approached the phenomenon\ndifferently than how it is studied in humans by focusing primarily on model\noutputs rather than on model processing. To examine model processing, we\npresent a method called the Reasoning Model Implicit Association Test (RM-IAT)\nfor studying implicit bias-like patterns in reasoning models: LLMs that employ\nstep-by-step reasoning to solve complex tasks. Using this method, we find that\nreasoning models require more tokens when processing association-incompatible\ninformation compared to association-compatible information. These findings\nsuggest AI systems harbor patterns in processing information that are analogous\nto human implicit bias. We consider the implications of these implicit\nbias-like patterns for their deployment in real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.11572.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c5d832d68946edad7d5536",
      "avatarUrl": "/avatars/5d38217d4ab99cdcf53c52661b8baa0d.svg",
      "fullname": "Messi Lee",
      "name": "l048596",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]