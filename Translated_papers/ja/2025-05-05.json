[
  {
    "paper": {
      "id": "2504.20438",
      "authors": [
        {
          "_id": "6814e35a19162d7749852c4b",
          "user": {
            "_id": "633d4630e1aec4b8b33ad5b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633d4630e1aec4b8b33ad5b8/owFqnyCMW8yuo9VcjA1hx.jpeg",
            "isPro": false,
            "fullname": "Ziyang Xu",
            "user": "Uyoung",
            "type": "user"
          },
          "name": "Ziyang Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-04T10:04:40.638Z",
          "hidden": false
        },
        {
          "_id": "6814e35a19162d7749852c4c",
          "name": "Kangsheng Duan",
          "hidden": false
        },
        {
          "_id": "6814e35a19162d7749852c4d",
          "user": {
            "_id": "627a34dac488a8ce15a2dc4a",
            "avatarUrl": "/avatars/61aecef507dea6620fe5574493f83595.svg",
            "isPro": false,
            "fullname": "ShenXiaolei",
            "user": "SmileTAT",
            "type": "user"
          },
          "name": "Xiaolei Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:32:16.185Z",
          "hidden": false
        },
        {
          "_id": "6814e35a19162d7749852c4e",
          "name": "Zhifeng Ding",
          "hidden": false
        },
        {
          "_id": "6814e35a19162d7749852c4f",
          "user": {
            "_id": "66c2e7fc934e2f07753542ac",
            "avatarUrl": "/avatars/f6fa3f94435cf1c1d06daa6c925d07d0.svg",
            "isPro": false,
            "fullname": "LWY",
            "user": "wenyuliu",
            "type": "user"
          },
          "name": "Wenyu Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:32:31.210Z",
          "hidden": false
        },
        {
          "_id": "6814e35a19162d7749852c50",
          "name": "Xiaohu Ruan",
          "hidden": false
        },
        {
          "_id": "6814e35a19162d7749852c51",
          "user": {
            "_id": "65389a669c474315d7425f96",
            "avatarUrl": "/avatars/2fa3828ca489cfe1948129a0eccf264f.svg",
            "isPro": false,
            "fullname": "chenxiaoxin",
            "user": "steelozazala",
            "type": "user"
          },
          "name": "Xiaoxin Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:32:55.012Z",
          "hidden": false
        },
        {
          "_id": "6814e35a19162d7749852c52",
          "user": {
            "_id": "62600de6d47e3dbae32ce1ce",
            "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg",
            "isPro": false,
            "fullname": "Xinggang Wang",
            "user": "xinggangw",
            "type": "user"
          },
          "name": "Xinggang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:33:01.396Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T05:28:36.000Z",
      "submittedOnDailyAt": "2025-05-05T05:42:06.841Z",
      "title": "PixelHacker: 構造的と語義的な一致性を持つ画像インパインティング",
      "submittedOnDailyBy": {
        "_id": "633d4630e1aec4b8b33ad5b8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633d4630e1aec4b8b33ad5b8/owFqnyCMW8yuo9VcjA1hx.jpeg",
        "isPro": false,
        "fullname": "Ziyang Xu",
        "user": "Uyoung",
        "type": "user"
      },
      "summary": "画像のインパインティングは、画像編集と画像生成の間にある基本的研究領域です。最近の最先端（SOTA）の方法は、新しいアテンション機構、軽量構造、コンテキストに関するモデリングを試み、驚異的な性能を示しています。しかし、それらは複雑な構造（例：テクスチャ、形状、空間関係）と意味（例：色の一貫性、物体の復元、ロジック的正確性）に対して困難をもっています、これによりアーティファクトと不適切な生成が起きます。この挑戦に対処するために、私たちは簡単で効果的なインパインティングパラダイム「潜在カテゴリーガイド」を設計し、またディフュージョンベースのモデル「PixelHacker」を提案しました。特に、私たちは最初に前景と背景（潜在的に116と21カテゴリ）を記述した1400万枚の画像-マスクペアを含む大きなデータセットを構築しました。次に、2つの固定サイズの埋め込みを使って潜在的な前景と背景の表現を別々に変換し、線形アテンションを通じてこれらの特徴を周期的にデノイズプロセスに注入しました。最後に、私たちのデータセットで事前学習し、開放ソースベンチマークで微調節して、PixelHackerを得ました。拡大的な実験は、PixelHackerが様々なデータセット（Places2、CelebA-HQ、FFHQ）でSOTAを全面的に上回り、構造と意味の両方で驚異的な一貫性を示していることを示しました。プロジェクトページは、https://hustvl.github.io/PixelHacker。",
      "upvotes": 17,
      "discussionId": "6814e35c19162d7749852caa",
      "projectPage": "https://hustvl.github.io/PixelHacker",
      "githubRepo": "https://github.com/hustvl/PixelHacker",
      "ai_keywords": [
        "latent categories guidance",
        "diffusion-based model",
        "PixelHacker",
        "image-mask pairs",
        "fixed-size embeddings",
        "linear attention",
        "pre-training",
        "fine-tuning",
        "Places2",
        "CelebA-HQ",
        "FFHQ"
      ]
    },
    "publishedAt": "2025-04-29T01:28:36.000Z",
    "title": "PixelHacker: Image Inpainting with Structural and Semantic Consistency",
    "summary": "Image inpainting is a fundamental research area between image editing and\nimage generation. Recent state-of-the-art (SOTA) methods have explored novel\nattention mechanisms, lightweight architectures, and context-aware modeling,\ndemonstrating impressive performance. However, they often struggle with complex\nstructure (e.g., texture, shape, spatial relations) and semantics (e.g., color\nconsistency, object restoration, and logical correctness), leading to artifacts\nand inappropriate generation. To address this challenge, we design a simple yet\neffective inpainting paradigm called latent categories guidance, and further\npropose a diffusion-based model named PixelHacker. Specifically, we first\nconstruct a large dataset containing 14 million image-mask pairs by annotating\nforeground and background (potential 116 and 21 categories, respectively).\nThen, we encode potential foreground and background representations separately\nthrough two fixed-size embeddings, and intermittently inject these features\ninto the denoising process via linear attention. Finally, by pre-training on\nour dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.\nExtensive experiments show that PixelHacker comprehensively outperforms the\nSOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits\nremarkable consistency in both structure and semantics. Project page at\nhttps://hustvl.github.io/PixelHacker.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20438.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "633d4630e1aec4b8b33ad5b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633d4630e1aec4b8b33ad5b8/owFqnyCMW8yuo9VcjA1hx.jpeg",
      "fullname": "Ziyang Xu",
      "name": "Uyoung",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01079",
      "authors": [
        {
          "_id": "68183d9ae65ec5d5716c6d94",
          "user": {
            "_id": "636b20591340f879a2eb98d0",
            "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
            "isPro": false,
            "fullname": "Daneul Kim",
            "user": "carpedkm",
            "type": "user"
          },
          "name": "Daneul Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-05T07:30:39.678Z",
          "hidden": false
        },
        {
          "_id": "68183d9ae65ec5d5716c6d95",
          "name": "Jaeah Lee",
          "hidden": false
        },
        {
          "_id": "68183d9ae65ec5d5716c6d96",
          "name": "Jaesik Park",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-02T07:36:49.000Z",
      "submittedOnDailyAt": "2025-05-05T02:56:01.277Z",
      "title": "層ごとのメモリを用いた画像生成の編集可能性向上",
      "submittedOnDailyBy": {
        "_id": "636b20591340f879a2eb98d0",
        "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
        "isPro": false,
        "fullname": "Daneul Kim",
        "user": "carpedkm",
        "type": "user"
      },
      "summary": "ほとんどの実世界の画像編集タスクは、望む結果を達成するために、複数の連続的な編集が必要です。現在の編集アプローチは、主に単一オブジェクトの記述に設計されているため、連続的な編集には難しく、特に、前回の編集を維持しながら新しいオブジェクトを自然に既存のコンテンツに統合することに難しくなります。これらの制限は、複数のオブジェクトを修正しながらそのコンテキスト関係を保持する複雑な編集シナリオに大きな影響を与えます。私たちは、この基本的な課題を解決するために、2つの主な提案を提出しています：現在のコンテンツを維持するために粗略なマスクを使用し、新しい要素を自然に統合することと、複数の修正を行う際にも一貫性のある編集をサポートすること。我々のフレームワークは、階層的なメモリーを使用して、前回の編集中の潜在的表現とプロンプト埋め込みを保存してそのようなものを実現しています。私たちは、メモリされた潜在的表現を使用してスペクトル一致性ガイドニングを提案し、既存のコンテンツに自然な対応を確保するためにクロスアテンションの多クエリディセンタンジュを提案しています。私たちの方法を評価するために、セマンティックなアライメントメトリックと相互作用編集シナリオを含む新しいベンチマークデータセットを提出しています。詳細な実験を通じて、私たちは、最小限のユーザーの努力を必要とするために、粗略なマスクを使用しながら複数の編集ステップを通じて高品質な結果を維持するために迭り返し画像編集タスクで上位の性能を示しています。",
      "upvotes": 12,
      "discussionId": "68183d9de65ec5d5716c6e78",
      "projectPage": "https://carpedkm.github.io/projects/improving_edit/index.html",
      "githubRepo": "https://github.com/carpedkm/improving-editability",
      "ai_keywords": [
        "layer-wise memory",
        "latent representations",
        "prompt embeddings",
        "Background Consistency Guidance",
        "Multi-Query Disentanglement",
        "cross-attention",
        "semantic alignment metrics",
        "interactive editing scenarios",
        "iterative image editing"
      ]
    },
    "publishedAt": "2025-05-02T03:36:49.000Z",
    "title": "Improving Editability in Image Generation with Layer-wise Memory",
    "summary": "Most real-world image editing tasks require multiple sequential edits to\nachieve desired results. Current editing approaches, primarily designed for\nsingle-object modifications, struggle with sequential editing: especially with\nmaintaining previous edits along with adapting new objects naturally into the\nexisting content. These limitations significantly hinder complex editing\nscenarios where multiple objects need to be modified while preserving their\ncontextual relationships. We address this fundamental challenge through two key\nproposals: enabling rough mask inputs that preserve existing content while\nnaturally integrating new elements and supporting consistent editing across\nmultiple modifications. Our framework achieves this through layer-wise memory,\nwhich stores latent representations and prompt embeddings from previous edits.\nWe propose Background Consistency Guidance that leverages memorized latents to\nmaintain scene coherence and Multi-Query Disentanglement in cross-attention\nthat ensures natural adaptation to existing content. To evaluate our method, we\npresent a new benchmark dataset incorporating semantic alignment metrics and\ninteractive editing scenarios. Through comprehensive experiments, we\ndemonstrate superior performance in iterative image editing tasks with minimal\nuser effort, requiring only rough masks while maintaining high-quality results\nthroughout multiple editing steps.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636b20591340f879a2eb98d0",
      "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
      "fullname": "Daneul Kim",
      "name": "carpedkm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.21117",
      "authors": [
        {
          "_id": "681889891bcdcf80a49d9ef8",
          "name": "Hanhua Hong",
          "hidden": false
        },
        {
          "_id": "681889891bcdcf80a49d9ef9",
          "name": "Chenghao Xiao",
          "hidden": false
        },
        {
          "_id": "681889891bcdcf80a49d9efa",
          "user": {
            "_id": "60f313f4adf471cbdf8bb66a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f313f4adf471cbdf8bb66a/5NJFqnldE_0fdE_mEvz9V.jpeg",
            "isPro": false,
            "fullname": "Yang Wang",
            "user": "yangwang825",
            "type": "user"
          },
          "name": "Yang Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-05T09:48:58.639Z",
          "hidden": false
        },
        {
          "_id": "681889891bcdcf80a49d9efb",
          "name": "Yiqi Liu",
          "hidden": false
        },
        {
          "_id": "681889891bcdcf80a49d9efc",
          "name": "Wenge Rong",
          "hidden": false
        },
        {
          "_id": "681889891bcdcf80a49d9efd",
          "name": "Chenghua Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63108cc834c7d77420b0fd68/6pjQShgH8JjsuyXvDdwv1.png"
      ],
      "publishedAt": "2025-04-29T18:56:12.000Z",
      "submittedOnDailyAt": "2025-05-05T08:29:07.779Z",
      "title": "1サイズだけではない：逆強化学習による高い効果性のNLGの評価プロンプト",
      "submittedOnDailyBy": {
        "_id": "63108cc834c7d77420b0fd68",
        "avatarUrl": "/avatars/2721e573a417a8ec0b81ee048c4b42ba.svg",
        "isPro": false,
        "fullname": "chenghao xiao",
        "user": "gowitheflow",
        "type": "user"
      },
      "summary": "自然言語生成（NLG）システムの評価は、有効な出力の多様性により難しい。人間評価はゴールドスタンダードとしてあるが、不均一性、標準化の欠如、人口学的バイアスにより再現性が限られている。LLMベースの評価はスケーラブルな代替があるが、プロンプトの設計に非常に敏感で、小さな変化が大きな差異を引き起こす。本稿では、モデルの出力からモデルの入力指示に逆向きに効果的な逆変換を学習するイベンション学習法を提案し、高度に効果的なモデル特有の評価プロンプトの自動生成を可能にします。本方法は、一つの評価サンプルだけを必要とし、時間の計測の試作プロンプトエンジニアリングの必要性を排除し、そのほかも効率と強固性を向上させます。本稿は、LLMベースの評価の新しい方向への貢献を提供します。",
      "upvotes": 5,
      "discussionId": "6818898a1bcdcf80a49d9f28",
      "ai_keywords": [
        "natural language generation (NLG)",
        "human evaluation",
        "LLM-based evaluation",
        "prompt design",
        "inversion learning",
        "reverse mappings",
        "model outputs",
        "input instructions",
        "automatic generation",
        "model-specific evaluation prompts",
        "evaluation sample",
        "manual prompt engineering"
      ]
    },
    "publishedAt": "2025-04-29T14:56:12.000Z",
    "title": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\n  Evaluation Prompts",
    "summary": "Evaluating natural language generation (NLG) systems is challenging due to\nthe diversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluation offers a scalable alternative\nbut is highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63108cc834c7d77420b0fd68/6pjQShgH8JjsuyXvDdwv1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21117.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63108cc834c7d77420b0fd68",
      "avatarUrl": "/avatars/2721e573a417a8ec0b81ee048c4b42ba.svg",
      "fullname": "chenghao xiao",
      "name": "gowitheflow",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.00023",
      "authors": [
        {
          "_id": "6818413000ee590453feaf66",
          "user": {
            "_id": "65169ea3fbfe82f36fc6655c",
            "avatarUrl": "/avatars/01714ad316a2e06488246e4fe7dcdb52.svg",
            "isPro": false,
            "fullname": "Hyun Ji Lee",
            "user": "hyunjilee",
            "type": "user"
          },
          "name": "Hyunji Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:34:41.006Z",
          "hidden": false
        },
        {
          "_id": "6818413000ee590453feaf67",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-05T07:30:36.382Z",
          "hidden": false
        },
        {
          "_id": "6818413000ee590453feaf68",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "6818413000ee590453feaf69",
          "user": {
            "_id": "6690ef3db70d356ed3e05cb0",
            "avatarUrl": "/avatars/530f3a0bd7b93e1e7f385c2708335728.svg",
            "isPro": false,
            "fullname": "yoon seung-hyun",
            "user": "aifactoryysh",
            "type": "user"
          },
          "name": "Seunghyun Yoon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:35:01.378Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-25T02:40:48.000Z",
      "submittedOnDailyAt": "2025-05-05T03:10:26.249Z",
      "title": "CORG: 複雑な、相互関係のあるコンテキストからの答えの生成",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "実世界のコーパスでは、知識が文書間で頻繁に再現しますが、様々な理由により不適切な情報が含まれ、それにより文脈間の複雑な相互関係が生じます。先行研究は、これらの複雑性に対して言語モデルが苦戦していることを示しています。これらの関係を4つの種類に分類します：デトラクター、不明確、反対事実、ダブリック。分析により、一つのアプローチですべての関係を同時に対処することはできませんでした。そこで、Context Organizer (CORG) を提案します。CORG は、独立に処理されるグループに文脈を組み立てるフレームワークです。このデザインにより、モデルは適切な答えを効率的に見つけられ、不明確性を排除できます。CORG は、グラフ構築者、リランキング、アグラグレーターの3つのキーコンポーネントから構成されています。結果として、CORG は性能と効率のバランスをよく調整し、現在のグループ化方法を超え、計算量の大きな単一文脈アプローチと比較的結果を実現しました。",
      "upvotes": 4,
      "discussionId": "6818413100ee590453feaf97",
      "ai_keywords": [
        "graph constructor",
        "reranker",
        "aggregator",
        "Context Organizer (CORG)"
      ]
    },
    "publishedAt": "2025-04-24T22:40:48.000Z",
    "title": "CORG: Generating Answers from Complex, Interrelated Contexts",
    "summary": "In a real-world corpus, knowledge frequently recurs across documents but\noften contains inconsistencies due to ambiguous naming, outdated information,\nor errors, leading to complex interrelationships between contexts. Previous\nresearch has shown that language models struggle with these complexities,\ntypically focusing on single factors in isolation. We classify these\nrelationships into four types: distracting, ambiguous, counterfactual, and\nduplicated. Our analysis reveals that no single approach effectively addresses\nall these interrelationships simultaneously. Therefore, we introduce Context\nOrganizer (CORG), a framework that organizes multiple contexts into\nindependently processed groups. This design allows the model to efficiently\nfind all relevant answers while ensuring disambiguation. CORG consists of three\nkey components: a graph constructor, a reranker, and an aggregator. Our results\ndemonstrate that CORG balances performance and efficiency effectively,\noutperforming existing grouping methods and achieving comparable results to\nmore computationally intensive, single-context approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00023.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.00174",
      "authors": [
        {
          "_id": "681594746a80babbe28775ba",
          "user": {
            "_id": "66225f7100352aeea584d02a",
            "avatarUrl": "/avatars/ca13f59bebf73d03a63a935f628aea5c.svg",
            "isPro": false,
            "fullname": "Ilan Strauss",
            "user": "strauss-NYC",
            "type": "user"
          },
          "name": "Ilan Strauss",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-04T10:04:25.496Z",
          "hidden": false
        },
        {
          "_id": "681594746a80babbe28775bb",
          "user": {
            "_id": "67535114d2a628475a0e7a6e",
            "avatarUrl": "/avatars/d7eb574c026817bbc204843c96f1caa6.svg",
            "isPro": false,
            "fullname": "Isobel Moure",
            "user": "isobelmoure",
            "type": "user"
          },
          "name": "Isobel Moure",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:35:12.655Z",
          "hidden": false
        },
        {
          "_id": "681594746a80babbe28775bc",
          "name": "Tim O'Reilly",
          "hidden": false
        },
        {
          "_id": "681594746a80babbe28775bd",
          "user": {
            "_id": "64582e94f8bdc3512d1ee940",
            "avatarUrl": "/avatars/e0eb72f06c7da58cb569198540484ab1.svg",
            "isPro": false,
            "fullname": "Sruly Rosenblat",
            "user": "sruly",
            "type": "user"
          },
          "name": "Sruly Rosenblat",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:35:24.305Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T20:44:42.000Z",
      "submittedOnDailyAt": "2025-05-05T02:50:33.572Z",
      "title": "AIガバナンス研究のリアルワールド隙間",
      "submittedOnDailyBy": {
        "_id": "66225f7100352aeea584d02a",
        "avatarUrl": "/avatars/ca13f59bebf73d03a63a935f628aea5c.svg",
        "isPro": false,
        "fullname": "Ilan Strauss",
        "user": "strauss-NYC",
        "type": "user"
      },
      "summary": "ジョーカービットマイクロファイルバージョンの翻訳結果を返します。",
      "upvotes": 3,
      "discussionId": "681594746a80babbe28775e5"
    },
    "publishedAt": "2025-04-30T16:44:42.000Z",
    "title": "Real-World Gaps in AI Governance Research",
    "summary": "Drawing on 1,178 safety and reliability papers from 9,439 generative AI\npapers (January 2020 - March 2025), we compare research outputs of leading AI\ncompanies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI\nuniversities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of\nWashington). We find that corporate AI research increasingly concentrates on\npre-deployment areas -- model alignment and testing & evaluation -- while\nattention to deployment-stage issues such as model bias has waned. Significant\nresearch gaps exist in high-risk deployment domains, including healthcare,\nfinance, misinformation, persuasive and addictive features, hallucinations, and\ncopyright. Without improved observability into deployed AI, growing corporate\nconcentration could deepen knowledge deficits. We recommend expanding external\nresearcher access to deployment data and systematic observability of in-market\nAI behaviors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00174.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66225f7100352aeea584d02a",
      "avatarUrl": "/avatars/ca13f59bebf73d03a63a935f628aea5c.svg",
      "fullname": "Ilan Strauss",
      "name": "strauss-NYC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.00949",
      "authors": [
        {
          "_id": "681885e585df02e13b44d3f1",
          "name": "Akhiad Bercovich",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d3f2",
          "name": "Itay Levy",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d3f3",
          "name": "Izik Golan",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d3f4",
          "name": "Mohammad Dabbah",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d3f5",
          "name": "Ran El-Yaniv",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d3f6",
          "name": "Omri Puny",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d3f7",
          "name": "Ido Galil",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d3f8",
          "name": "Zach Moshe",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d3f9",
          "name": "Tomer Ronen",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d3fa",
          "name": "Najeeb Nabwani",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d3fb",
          "name": "Ido Shahaf",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d3fc",
          "name": "Oren Tropp",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d3fd",
          "name": "Ehud Karpas",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d3fe",
          "name": "Ran Zilberstein",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d3ff",
          "name": "Jiaqi Zeng",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d400",
          "name": "Soumye Singhal",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d401",
          "name": "Alexander Bukharin",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d402",
          "name": "Yian Zhang",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d403",
          "name": "Tugrul Konuk",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d404",
          "name": "Gerald Shen",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d405",
          "name": "Ameya Sunil Mahabaleshwarkar",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d406",
          "name": "Bilal Kartal",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d407",
          "name": "Yoshi Suhara",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d408",
          "name": "Olivier Delalleau",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d409",
          "name": "Zijia Chen",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d40a",
          "name": "Zhilin Wang",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d40b",
          "name": "David Mosallanezhad",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d40c",
          "name": "Adi Renduchintala",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d40d",
          "name": "Haifeng Qian",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d40e",
          "name": "Dima Rekesh",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d40f",
          "name": "Fei Jia",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d410",
          "name": "Somshubra Majumdar",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d411",
          "name": "Vahid Noroozi",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d412",
          "name": "Wasi Uddin Ahmad",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d413",
          "name": "Sean Narenthiran",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d414",
          "name": "Aleksander Ficek",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d415",
          "name": "Mehrzad Samadi",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d416",
          "name": "Jocelyn Huang",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d417",
          "name": "Siddhartha Jain",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d418",
          "name": "Igor Gitman",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d419",
          "name": "Ivan Moshkov",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d41a",
          "name": "Wei Du",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d41b",
          "name": "Shubham Toshniwal",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d41c",
          "name": "George Armstrong",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d41d",
          "name": "Branislav Kisacanin",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d41e",
          "name": "Matvei Novikov",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d41f",
          "name": "Daria Gitman",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d420",
          "name": "Evelina Bakhturina",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d421",
          "name": "Jane Polak Scowcroft",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d422",
          "name": "John Kamalu",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d423",
          "name": "Dan Su",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d424",
          "name": "Kezhi Kong",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d425",
          "name": "Markus Kliegl",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d426",
          "name": "Rabeeh Karimi",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d427",
          "name": "Ying Lin",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d428",
          "name": "Sanjeev Satheesh",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d429",
          "name": "Jupinder Parmar",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d42a",
          "name": "Pritam Gundecha",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d42b",
          "name": "Brandon Norick",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d42c",
          "name": "Joseph Jennings",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d42d",
          "name": "Shrimai Prabhumoye",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d42e",
          "name": "Syeda Nahida Akter",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d42f",
          "name": "Mostofa Patwary",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d430",
          "name": "Abhinav Khattar",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d431",
          "name": "Deepak Narayanan",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d432",
          "name": "Roger Waleffe",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d433",
          "name": "Jimmy Zhang",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d434",
          "name": "Bor-Yiing Su",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d435",
          "name": "Guyue Huang",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d436",
          "name": "Terry Kong",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d437",
          "name": "Parth Chadha",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d438",
          "name": "Sahil Jain",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d439",
          "name": "Christine Harvey",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d43a",
          "name": "Elad Segal",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d43b",
          "name": "Jining Huang",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d43c",
          "name": "Sergey Kashirsky",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d43d",
          "name": "Robert McQueen",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d43e",
          "name": "Izzy Putterman",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d43f",
          "name": "George Lam",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d440",
          "name": "Arun Venkatesan",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d441",
          "name": "Sherry Wu",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d442",
          "name": "Vinh Nguyen",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d443",
          "name": "Manoj Kilaru",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d444",
          "name": "Andrew Wang",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d445",
          "name": "Anna Warno",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d446",
          "name": "Abhilash Somasamudramath",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d447",
          "name": "Sandip Bhaskar",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d448",
          "name": "Maka Dong",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d449",
          "name": "Nave Assaf",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d44a",
          "name": "Shahar Mor",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d44b",
          "name": "Omer Ullman Argov",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d44c",
          "name": "Scot Junkin",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d44d",
          "name": "Oleksandr Romanenko",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d44e",
          "name": "Pedro Larroy",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d44f",
          "name": "Monika Katariya",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d450",
          "name": "Marco Rovinelli",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d451",
          "name": "Viji Balas",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d452",
          "name": "Nicholas Edelman",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d453",
          "name": "Anahita Bhiwandiwalla",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d454",
          "name": "Muthu Subramaniam",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d455",
          "name": "Smita Ithape",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d456",
          "name": "Karthik Ramamoorthy",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d457",
          "name": "Yuting Wu",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d458",
          "name": "Suguna Varshini Velury",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d459",
          "name": "Omri Almog",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d45a",
          "name": "Joyjit Daw",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d45b",
          "name": "Denys Fridman",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d45c",
          "name": "Erick Galinkin",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d45d",
          "name": "Michael Evans",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d45e",
          "name": "Katherine Luna",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d45f",
          "name": "Leon Derczynski",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d460",
          "name": "Nikki Pope",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d461",
          "name": "Eileen Long",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d462",
          "name": "Seth Schneider",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d463",
          "name": "Guillermo Siman",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d464",
          "name": "Tomasz Grzegorzek",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d465",
          "name": "Pablo Ribalta",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d466",
          "name": "Monika Katariya",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d467",
          "name": "Joey Conway",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d468",
          "name": "Trisha Saar",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d469",
          "name": "Ann Guan",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d46a",
          "name": "Krzysztof Pawelec",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d46b",
          "name": "Shyamala Prayaga",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d46c",
          "name": "Oleksii Kuchaiev",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d46d",
          "name": "Boris Ginsburg",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d46e",
          "name": "Oluwatobi Olabiyi",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d46f",
          "name": "Kari Briski",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d470",
          "name": "Jonathan Cohen",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d471",
          "name": "Bryan Catanzaro",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d472",
          "name": "Jonah Alben",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d473",
          "name": "Yonatan Geifman",
          "hidden": false
        },
        {
          "_id": "681885e585df02e13b44d474",
          "name": "Eric Chung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-02T01:35:35.000Z",
      "submittedOnDailyAt": "2025-05-05T08:04:15.299Z",
      "title": "Llama-Nemotron: 効率的な論理モデル",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Llama-Nemotronモデルシリーズを紹介します。これは開放ファミリーであり、異なる理由論モデルを持ち、超卓な理由論能力、推論効率と企業用の開放許可を提供します。このファミリーは3つのサイズであり、Nano（8B）、Super（49B）、Ultra（253B）です。DeepSeek-R1と同じレベルの最先端の理由論モデルと競争的に働き、優れた推論トランスポートとメモリエフフィシーンを提供します。このレポートでは、これらのモデルの訓練手順を議論します。これは、Llama 3モデルからのニューラルアーキテクチャ検索を使用した加速推論、知識転写、継続予習、理由論フォーカスの後訓練ステージ（規範化微調と大規模再励誘学習の2つの主な部分）を含みます。Llama-Nemotronモデルは、標準チャットと理由論モードを推論中に切り替える可能な動的な理由論モードをサポートする最初の開放ソースモデルです。また、開放研究のさらなる支援とモデル開発の促進を目的に、以下のリソースを提供します：1. 商用に許可されたNVIDIA Open Model License Agreementの下で、LN-Nano、LN-Super、LN-UltraのLlama-Nemotron理由論モデルをリリースします。2. 完全な後訓練データセット：Llama-Nemotron-Post-Training-Datasetをリリースします。3. さらに、訓練コードベース：NeMo、NeMo-Aligner、Megatron-LMをリリースします。",
      "upvotes": 2,
      "discussionId": "681885e685df02e13b44d4b1",
      "ai_keywords": [
        "neural architecture search",
        "knowledge distillation",
        "supervised fine-tuning",
        "reinforcement learning",
        "dynamic reasoning toggle",
        "NVIDIA Open Model License Agreement"
      ]
    },
    "publishedAt": "2025-05-01T21:35:35.000Z",
    "title": "Llama-Nemotron: Efficient Reasoning Models",
    "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6779
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.00562",
      "authors": [
        {
          "_id": "68184c3fb727bc3cb7301e15",
          "user": {
            "_id": "668e100b97171f3399e07f5d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg",
            "isPro": false,
            "fullname": "Yue Meng",
            "user": "yuemithucsd",
            "type": "user"
          },
          "name": "Yue Meng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:36:14.168Z",
          "hidden": false
        },
        {
          "_id": "68184c3fb727bc3cb7301e16",
          "name": "Chuchu Fan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T14:40:07.000Z",
      "submittedOnDailyAt": "2025-05-05T03:58:02.420Z",
      "title": "テログラフ: 時系列ロジック計画によるグラフエンコードフローマッチング",
      "submittedOnDailyBy": {
        "_id": "668e100b97171f3399e07f5d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg",
        "isPro": false,
        "fullname": "Yue Meng",
        "user": "yuemithucsd",
        "type": "user"
      },
      "summary": "信号時間論理（STL）の規範で複雑なタスクを解決するテクニックは、複数の実世界のアプリケーションにとって重要です。しかし、多くの先行研究は、多様性のないSTLデータセットとエンコーダーの欠如により、固定的かパラメタ化されたSTL規範を考慮しています。本論文では、時間論理グラフエンコーディングフロー（Temporal Logic Graph-encoded Flow）という手法を提案します。これは、グラフニューラルネットワーク（GNN）エンコーダーとフローマッチングを利用して、一般的なSTL規範の解決策を学習することを目的としています。4つの通常的なSTLテンプレートを特定し、200Kの規範を集め、それぞれの規範に対応する示唆をペアで取得しました。5つのシミュレーション環境で検証し、2D空間の簡単な力学モデルから7自由度のフランカパンダロボットアームまで、アントヘッドペタルモーダーナビゲーションまで幅広く実験を実施しました。結果として、我々の方法はSTL満足率において他の基準に対して優れています。古典的なSTL計画アルゴリズムと比較して、推論時間で10-100倍高速であり、どのシステム動力学にも適用可能です。また、我々のグラフエンコーディング手法の複雑なSTLを解決する能力と、外分布のSTL規範に対する強固性を示しました。コードは、https://github.com/mengyuest/TeLoGraF から利用できます。",
      "upvotes": 1,
      "discussionId": "68184c41b727bc3cb7301e6c",
      "ai_keywords": [
        "TeLoGraF",
        "Temporal Logic Graph-encoded Flow",
        "Graph Neural Networks (GNN)",
        "flow-matching",
        "STL specifications",
        "STL templates",
        "STL satisfaction rate",
        "dynamical models",
        "Franka Panda robot arm",
        "Ant quadruped navigation",
        "classical STL planning algorithms"
      ]
    },
    "publishedAt": "2025-05-01T10:40:07.000Z",
    "title": "TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching",
    "summary": "Learning to solve complex tasks with signal temporal logic (STL)\nspecifications is crucial to many real-world applications. However, most\nprevious works only consider fixed or parametrized STL specifications due to\nthe lack of a diverse STL dataset and encoders to effectively extract temporal\nlogic information for downstream tasks. In this paper, we propose TeLoGraF,\nTemporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN)\nencoder and flow-matching to learn solutions for general STL specifications. We\nidentify four commonly used STL templates and collect a total of 200K\nspecifications with paired demonstrations. We conduct extensive experiments in\nfive simulation environments ranging from simple dynamical models in the 2D\nspace to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped\nnavigation. Results show that our method outperforms other baselines in the STL\nsatisfaction rate. Compared to classical STL planning algorithms, our approach\nis 10-100X faster in inference and can work on any system dynamics. Besides, we\nshow our graph-encoding method's capability to solve complex STLs and\nrobustness to out-distribution STL specifications. Code is available at\nhttps://github.com/mengyuest/TeLoGraF",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.00562.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668e100b97171f3399e07f5d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pgzatAZut-uoXOBOwgiFB.jpeg",
      "fullname": "Yue Meng",
      "name": "yuemithucsd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.20859",
      "authors": [
        {
          "_id": "681713aec075e49c1b22500e",
          "user": {
            "_id": "630d180f3dc31beba6f061c3",
            "avatarUrl": "/avatars/7cf70bff453b6e64fcac52f45c6b3730.svg",
            "isPro": false,
            "fullname": "guy hadad",
            "user": "guyhadad01",
            "type": "user"
          },
          "name": "Guy Hadad",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-04T07:14:56.074Z",
          "hidden": false
        },
        {
          "_id": "681713aec075e49c1b22500f",
          "name": "Haggai Roitman",
          "hidden": false
        },
        {
          "_id": "681713aec075e49c1b225010",
          "user": {
            "_id": "638f42e4c4444c6ca8715a06",
            "avatarUrl": "/avatars/aae741d00ed1f5ead516c07543e59f3e.svg",
            "isPro": false,
            "fullname": "yotam eshel",
            "user": "yeshel",
            "type": "user"
          },
          "name": "Yotam Eshel",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-04T07:13:50.951Z",
          "hidden": false
        },
        {
          "_id": "681713aec075e49c1b225011",
          "user": {
            "_id": "63aace84785b8279fe30b5f9",
            "avatarUrl": "/avatars/7b4793f6f0a0a8b608d0395c0e92a7eb.svg",
            "isPro": false,
            "fullname": "Bracha Shapira",
            "user": "Bshapira",
            "type": "user"
          },
          "name": "Bracha Shapira",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:35:41.141Z",
          "hidden": false
        },
        {
          "_id": "681713aec075e49c1b225012",
          "user": {
            "_id": "64141f0365f4b23aa99507a4",
            "avatarUrl": "/avatars/46d599acaa0f492139949dba0f00e030.svg",
            "isPro": false,
            "fullname": "Lior Rokach",
            "user": "liorrokach",
            "type": "user"
          },
          "name": "Lior Rokach",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-05T07:35:47.274Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T15:33:20.000Z",
      "submittedOnDailyAt": "2025-05-05T03:47:16.624Z",
      "title": "X-Cross: クロスドメインの順序リカレンシングに向けた言語モデルの動的な統合",
      "submittedOnDailyBy": {
        "_id": "630d180f3dc31beba6f061c3",
        "avatarUrl": "/avatars/7cf70bff453b6e64fcac52f45c6b3730.svg",
        "isPro": false,
        "fullname": "guy hadad",
        "user": "guyhadad01",
        "type": "user"
      },
      "summary": "新産品が毎日発表される中、推薦システムは、様々な新しいドメインに対して迅速に適応するために、拡大な再学習を必要とさせないように必要とされています。本稿では、「X-Cross」という新しいクロスドメインの順序推薦モデルを紹介します。このモデルは、数々のドメイン専用の言語モデルを統合して新しいドメインでの産品を推薦することができます。各モデルは、低レンジアダプター（LoRA）で微調節されています。推薦プロンプトを与えた場合、X-Crossは層ごとに運用され、各ソース言語モデルの表現をすべてのモデルからの知識を統合して動的に補完します。これらの補完された表現は、各ドメインアダプターからの活性化を活用して、ドメイン特有のニュアンスを保存しながら、クロスドメインの適応性を確保します。Sequential RecommendationのAmazonデータセットを使用して、X-CrossはLoRAで微調節されたモデルと同等の性能を達成し、ただし追加パラメーターの25%だけを使用しています。クロスドメインタスクの例にToysドメインからTool、ElectronicsまたはSportsへの適応で、X-Crossは穩健な性能を示し、LoRAでの微調節に必要なデータ量を約50%-75%減らしながら、微調節が効果的に行えることを示します。また、X-Crossは代替のクロスドメインベースラインと比較して精度の大幅な向上を収めます。全体として、X-Crossは可換性と適応性のあるクロスドメイン推薦を可能にし、計算オーバーヘッドを減らし、データ制限された環境での効率的な解決策を提供します。",
      "upvotes": 1,
      "discussionId": "681713aec075e49c1b22503e",
      "ai_keywords": [
        "cross-domain sequential-recommendation",
        "domain-specific language models",
        "low-rank adapters (LoRA)",
        "recommendation prompt",
        "activations",
        "domain-specific nuances",
        "cross-domain tasks",
        "computational overhead",
        "data-constrained environments"
      ]
    },
    "publishedAt": "2025-04-29T11:33:20.000Z",
    "title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain\n  Sequential Recommendation",
    "summary": "As new products are emerging daily, recommendation systems are required to\nquickly adapt to possible new domains without needing extensive retraining.\nThis work presents ``X-Cross'' -- a novel cross-domain\nsequential-recommendation model that recommends products in new domains by\nintegrating several domain-specific language models; each model is fine-tuned\nwith low-rank adapters (LoRA). Given a recommendation prompt, operating layer\nby layer, X-Cross dynamically refines the representation of each source\nlanguage model by integrating knowledge from all other models. These refined\nrepresentations are propagated from one layer to the next, leveraging the\nactivations from each domain adapter to ensure domain-specific nuances are\npreserved while enabling adaptability across domains. Using Amazon datasets for\nsequential recommendation, X-Cross achieves performance comparable to a model\nthat is fine-tuned with LoRA, while using only 25% of the additional\nparameters. In cross-domain tasks, such as adapting from Toys domain to Tools,\nElectronics or Sports, X-Cross demonstrates robust performance, while requiring\nabout 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.\nFurthermore, X-Cross achieves significant improvement in accuracy over\nalternative cross-domain baselines. Overall, X-Cross enables scalable and\nadaptive cross-domain recommendations, reducing computational overhead and\nproviding an efficient solution for data-constrained environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20859.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630d180f3dc31beba6f061c3",
      "avatarUrl": "/avatars/7cf70bff453b6e64fcac52f45c6b3730.svg",
      "fullname": "guy hadad",
      "name": "guyhadad01",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]