[
  {
    "paper": {
      "id": "2505.18125",
      "authors": [
        {
          "_id": "6833f8b419852283c4b3bbd6",
          "name": "Alan Arazi",
          "hidden": false
        },
        {
          "_id": "6833f8b419852283c4b3bbd7",
          "user": {
            "_id": "64802fb6c57f629056c59966",
            "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
            "isPro": false,
            "fullname": "Eilam Shapira",
            "user": "EilamSha",
            "type": "user"
          },
          "name": "Eilam Shapira",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:08.206Z",
          "hidden": false
        },
        {
          "_id": "6833f8b419852283c4b3bbd8",
          "name": "Roi Reichart",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:34:28.000Z",
      "submittedOnDailyAt": "2025-05-26T03:50:53.260Z",
      "title": "TabSTAR: セマンティックにターゲットに関心を持つ基盤的なテーブルモデル",
      "submittedOnDailyBy": {
        "_id": "64802fb6c57f629056c59966",
        "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
        "isPro": false,
        "fullname": "Eilam Shapira",
        "user": "EilamSha",
        "type": "user"
      },
      "summary": "ディープラーニングは多くの領域で驚異的な成功を収めていますが、テーブル学習タスクでは歴史的にグラデイションブーストリングディーシェント木（GBDTs）が主導しています。しかし、最近の進歩は、テーブルベースファイドモデルの準備をしています。これらは実世界的知識を活用し、多様なデータセットで一般化することができます、特にテキストが含まれる場合は。しかし、現在の方法では、言語モデルの能力をテーブルタスクに組み込むことは試みられており、ほとんどの方法は静的で目標無関係なテキスト表現を利用しています、その効果を制限しています。TabSTARを紹介します：セマンティック的に目標に関係した表現を持つベーステーブルモデルです。TabSTARは、テキスト特徴量を含むテーブルデータ上でのタンスファー学習を可能にします、データセットに特に依存しないアーキテクチャで設計されています。このモデルは、事前学習されたテキストエンコーダーを解放し、目標トークンを入力として受け取ります、これはモデルにタスク特有の埋め込みを学習するための必要なコンテキストを提供します。TabSTARは、既知の分類タスクのベンチマークで、中間サイズと大きなデータセットでの状態の最先端の性能を達成し、事前学習されたデータセットの数によるスケーリングラーズを示し、ファイナルの性能向上のパスわけを提供します。",
      "upvotes": 66,
      "discussionId": "6833f8b419852283c4b3bc02",
      "ai_summary": "TabSTAR, a tabular foundation model with semantically target-aware representations, achieves state-of-the-art performance in classification tasks with text features through transfer learning without dataset-specific parameters.",
      "ai_keywords": [
        "TabSTAR",
        "foundation tabular model",
        "semantically target-aware representations",
        "transfer learning",
        "pretrained text encoder",
        "target tokens",
        "task-specific embeddings",
        "scaling laws"
      ]
    },
    "publishedAt": "2025-05-23T13:34:28.000Z",
    "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware\n  Representations",
    "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18125.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64802fb6c57f629056c59966",
      "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg",
      "fullname": "Eilam Shapira",
      "name": "EilamSha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17667",
      "authors": [
        {
          "_id": "6833d7c5a3262d6b1e4d358e",
          "user": {
            "_id": "62ecbffd99112e99c5f7fded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
            "isPro": false,
            "fullname": "Fanqi Wan",
            "user": "Wanfq",
            "type": "user"
          },
          "name": "Fanqi Wan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:44.880Z",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d358f",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3590",
          "name": "Shengyi Liao",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3591",
          "name": "Yingcheng Shi",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3592",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3593",
          "name": "Ziyi Yang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3594",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3595",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3596",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "6833d7c5a3262d6b1e4d3597",
          "name": "Ming Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T09:31:55.000Z",
      "submittedOnDailyAt": "2025-05-26T03:36:36.885Z",
      "title": "QwenLong-L1: 長文脈大論理モデルに向けた強化学習の向こう",
      "submittedOnDailyBy": {
        "_id": "62ecbffd99112e99c5f7fded",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
        "isPro": false,
        "fullname": "Fanqi Wan",
        "user": "Wanfq",
        "type": "user"
      },
      "summary": "最近の大規模な理由モデル（LRMs）は、強力な理由論能力を示し、強化学習（RL）を通じて証明されました。これらの改善は、主に短いコンテキストの理由論タスクで見落とされています。対照的に、LRMsを長いコンテキストの入力によって効果的に処理し、理由論するためのRLを実現することは、重要な解決されていない挑戦です。この隙を橋渡すために、まず長コンテキストの理由論RLのパラダイムを正式化し、最適なトレーニング効率と不穩定的な最適化プロセスにおける鍵の挑戦を特定しました。これらの問題に対処するために、QwenLong-L1フレームワークを提案しました。このフレームワークは、進歩的なコンテキストスケーリングをもとに短コンテキストのLRMsを長コンテキストのシナリオに適用します。特に、強固な初期ポリシーを構築するためにワームアップ制御された規範化微調（SFT）ステージを利用し、その後、カレキュリウムガイドされた段階的RL技術を用いてポリシーの進化を安定化し、難易度に関するレビューを増強してポリシーの検索を促進します。7つの長コンテキストドキュメントクエストアンサーテストベンチマークでの実験は、QwenLong-L1-32BがOpenAI-o3-miniやQwen3-235B-A22BというフラグシップLRMsを上回り、Claude-3.7-Sonnet-Thinkingと同等の性能を達成し、最先端のLRMsの中でリードプロフェッショナルな性能を示しました。この研究は、情報密集な環境で強力な理由論を可能にする実用的な長コンテキストLRMsの開発に転換します。",
      "upvotes": 42,
      "discussionId": "6833d7c6a3262d6b1e4d35c5",
      "githubRepo": "https://github.com/Tongyi-Zhiwen/QwenLong-L1",
      "ai_summary": "A framework called QwenLong-L1 enhances large reasoning models for long-context reasoning through reinforcement learning, achieving leading performance on document question-answering benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "long-context reasoning",
        "short-context reasoning",
        "training efficiency",
        "optimization process",
        "QwenLong-L1",
        "progressive context scaling",
        "supervised fine-tuning",
        "curriculum-guided phased RL",
        "difficulty-aware retrospective sampling",
        "document question-answering benchmarks",
        "OpenAI-o3-mini",
        "Qwen3-235B-A22B",
        "Claude-3.7-Sonnet-Thinking"
      ]
    },
    "publishedAt": "2025-05-23T05:31:55.000Z",
    "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning",
    "summary": "Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17667.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ecbffd99112e99c5f7fded",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
      "fullname": "Fanqi Wan",
      "name": "Wanfq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17612",
      "authors": [
        {
          "_id": "6833c9fd298a7bec9c3da3b0",
          "name": "Minki Kang",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b1",
          "name": "Jongwon Jeong",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b2",
          "name": "Seanie Lee",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b3",
          "name": "Jaewoong Cho",
          "hidden": false
        },
        {
          "_id": "6833c9fd298a7bec9c3da3b4",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T08:20:15.000Z",
      "submittedOnDailyAt": "2025-05-26T00:25:44.604Z",
      "title": "LLMアウトプットをリテライションとコードツールを用いて小さなモデルに収納する方法",
      "submittedOnDailyBy": {
        "_id": "64b74920fe6a108d03fed767",
        "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
        "isPro": false,
        "fullname": "Minki Kang",
        "user": "Nardien",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は複雑な理由論タスクで優れていますが、計算費用が高いことで実用的な扱いに適していません。この問題に対して、最近の研究は、教師LLMsからのChain-of-Thought（CoT）トレースを用いて小さな言語モデル（sLMs）に理由論能力をディスティルする方法を焦点にしています。しかし、このアプローチは、特別な事実的知識や正確な計算が必要な場合には、sLMsが能力の限りでハロウィングする場合が多いため、このアプローチは困難になっています。本研究では、Agent Distillationというフレームワークを提案し、理由論能力だけでなく、LLMベースのアガントからの全タスク解決バランスをsLMsに伝えることを目的としています。Agent Distillationは2つの補間的な軸に沿って改善されています：（1）教師生成されたタライトの品質を向上させるためのプロンプティング方法を提案しています；（2）小さなアガントのテスト時のロバスト性を向上させるための自発的な行動生成を提案しています。8つの理由論タスクを評価し、事実的および数学的領域を被り、領域内と領域外の一般化を扱います。結果として、0.5B、1.5B、3Bパラメータの小さなsLMsが、CoTディスティルを用いて最も大きな1.5B、3B、7Bモデルと比較しても競争的な性能を示し、Agent Distillationの実用的な、ツール使用可能な小さなアガントの構築の可能性を示しています。コードはhttps://github.com/Nardien/agent-distillationに公開されています。",
      "upvotes": 34,
      "discussionId": "6833ca00298a7bec9c3da444",
      "githubRepo": "https://github.com/Nardien/agent-distillation",
      "ai_summary": "Agent Distillation transfers reasoning and task-solving capabilities from large language models to smaller models using enhanced prompts and self-consistent actions, matching performance of larger models on various reasoning tasks.",
      "ai_keywords": [
        "Large language models",
        "small language models",
        "chain-of-thought",
        "agent distillation",
        "prompting method",
        "first-thought prefix",
        "self-consistent action generation",
        "task-solving behavior",
        "retrieval tools",
        "code tools",
        "in-domain generalization",
        "out-of-domain generalization"
      ]
    },
    "publishedAt": "2025-05-23T04:20:15.000Z",
    "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
    "summary": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17612.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64b74920fe6a108d03fed767",
      "avatarUrl": "/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg",
      "fullname": "Minki Kang",
      "name": "Nardien",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15929",
      "authors": [
        {
          "_id": "6830404effb59afb6569273a",
          "name": "Hui Shen",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273b",
          "user": {
            "_id": "6621cea88850e38ffbb1854f",
            "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
            "isPro": false,
            "fullname": "Taki WU",
            "user": "taki555",
            "type": "user"
          },
          "name": "Taiqiang Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:14:30.851Z",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273c",
          "name": "Qi Han",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273d",
          "name": "Yunta Hsieh",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273e",
          "user": {
            "_id": "67fe265f9698ae4f5f4db718",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0PLJEbUyJbM9BacFxcScP.png",
            "isPro": false,
            "fullname": "Jizhou Wang",
            "user": "John-ai-bee",
            "type": "user"
          },
          "name": "Jizhou Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:14:33.171Z",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569273f",
          "name": "Yuyue Zhang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692740",
          "name": "Yuxin Cheng",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692741",
          "name": "Zijian Hao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692742",
          "name": "Yuansheng Ni",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692743",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692744",
          "name": "Zhongwei Wan",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692745",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692746",
          "name": "Wendong Xu",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692747",
          "name": "Jing Xiong",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692748",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb65692749",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274a",
          "name": "Chaofan Tao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274b",
          "name": "Zhuoqing Mao",
          "hidden": false
        },
        {
          "_id": "6830404effb59afb6569274c",
          "name": "Ngai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T18:33:50.000Z",
      "submittedOnDailyAt": "2025-05-26T05:21:02.238Z",
      "title": "PhyX: 物理論理の「知恵」を持っているのか？",
      "submittedOnDailyBy": {
        "_id": "6621cea88850e38ffbb1854f",
        "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
        "isPro": false,
        "fullname": "Taki WU",
        "user": "taki555",
        "type": "user"
      },
      "summary": "現在のベンチマークは、脳の重要な面を捉えていません：物理的な理由論、ドメインの知識と符号的な理由論、そして実世界的な制約の理解を統合する能力。この欠点を解決するために、我々はPhyXを紹介します：最初の大規模なベンチマークで、モデルがビジュアルシナリオで物理的な理由論の能力を評価するものです。PhyXは、細かくカレントされた多タイプの質問を含み、6種類の理由論型を組み合わせて、25サブドメインと6コアの物理ドメイン（熱力学、電磁気学、力学、現代物理学、光学、波と音響学）にわたります。我々の詳細な評価により、最先端のモデルも物理的な理由論について大きな難関をさせています。GPT-4o、Claude3.7-Sonnet、GPT-o4-miniは、それぞれ32.5%、42.2%、45.8%の精度を達成しましたが、人間の専門家との性能間隔は29%を超えました。我々の分析は、現在のモデルにおける重要な制限を明らかにします：学問的な知識のメモリー依存性、数学的式の過度依存性、そして真の物理的な理解よりも表面的な視覚パターンマッチングに焦点を当てています。我々は、詳細なステータス、詳細なケーススタディ、複数の評価パラダイムを通じて、物理的な理由論能力を極めて詳細に評価します。再現性を確保するために、VLMEvalKitなどの広く使用されているツールキットに基づいた相容性のある評価プロトコルを実装しました。",
      "upvotes": 34,
      "discussionId": "68304052ffb59afb6569282f",
      "projectPage": "https://phyx-bench.github.io/",
      "githubRepo": "https://github.com/NastyMarcus/PhyX",
      "ai_summary": "A new benchmark, PhyX, evaluates models' physics-grounded reasoning in visual scenarios, revealing significant limitations in current models' physical understanding compared to human experts.",
      "ai_keywords": [
        "multimodal questions",
        "reasoning types",
        "sub-domains",
        "core physics domains",
        "thermodynamics",
        "electromagnetism",
        "mechanics",
        "modern physics",
        "optics",
        "wave\\&acoustics",
        "fine-grained statistics",
        "case studies",
        "evaluation paradigms",
        "VLMEvalKit"
      ]
    },
    "publishedAt": "2025-05-21T14:33:50.000Z",
    "title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?",
    "summary": "Existing benchmarks fail to capture a crucial aspect of intelligence:\nphysical reasoning, the integrated ability to combine domain knowledge,\nsymbolic reasoning, and understanding of real-world constraints. To address\nthis gap, we introduce PhyX: the first large-scale benchmark designed to assess\nmodels capacity for physics-grounded reasoning in visual scenarios. PhyX\nincludes 3K meticulously curated multimodal questions spanning 6 reasoning\ntypes across 25 sub-domains and 6 core physics domains: thermodynamics,\nelectromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In\nour comprehensive evaluation, even state-of-the-art models struggle\nsignificantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and\nGPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy\nrespectively-performance gaps exceeding 29\\% compared to human experts. Our\nanalysis exposes critical limitations in current models: over-reliance on\nmemorized disciplinary knowledge, excessive dependence on mathematical\nformulations, and surface-level visual pattern matching rather than genuine\nphysical understanding. We provide in-depth analysis through fine-grained\nstatistics, detailed case studies, and multiple evaluation paradigms to\nthoroughly examine physical reasoning capabilities. To ensure reproducibility,\nwe implement a compatible evaluation protocol based on widely-used toolkits\nsuch as VLMEvalKit, enabling one-click evaluation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15929.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6621cea88850e38ffbb1854f",
      "avatarUrl": "/avatars/6d73d947046faa32260ee325069976d9.svg",
      "fullname": "Taki WU",
      "name": "taki555",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18129",
      "authors": [
        {
          "_id": "6833cf89df7cbb5c087a4caa",
          "name": "Yan Ma",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cab",
          "name": "Linge Du",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cac",
          "user": {
            "_id": "642e4d4d6748dd4f8eeb7732",
            "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
            "isPro": false,
            "fullname": "Xuyang Shen",
            "user": "Ryan1122",
            "type": "user"
          },
          "name": "Xuyang Shen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:03.920Z",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cad",
          "name": "Shaoxiang Chen",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cae",
          "name": "Pengfei Li",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4caf",
          "name": "Qibing Ren",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb0",
          "name": "Lizhuang Ma",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb1",
          "name": "Yuchao Dai",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb2",
          "name": "Pengfei Liu",
          "hidden": false
        },
        {
          "_id": "6833cf89df7cbb5c087a4cb3",
          "name": "Junjie Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:41:14.000Z",
      "submittedOnDailyAt": "2025-05-26T00:54:44.420Z",
      "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning\n\nひとりのRLですべてを見る：可視化三重統合強化学習",
      "submittedOnDailyBy": {
        "_id": "642e4d4d6748dd4f8eeb7732",
        "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
        "isPro": false,
        "fullname": "Xuyang Shen",
        "user": "Ryan1122",
        "type": "user"
      },
      "summary": "強化學習（RL）は、視覚言語モデル（VLMs）の論理能力を大幅に向上させました。しかし、RLが論理タスクよりも遠く及ぶ場合の使用は、特に物体検出や基底化のような観察力集中タスクにおいては、主に探索されていません。私たちは、V-TriuneというVisual Triple Unified Reinforcement Learningシステムを提案します。これは、VLMsが視覚論理や観察力タスクを一つの訓練パイプライン内で共に学習できるようにします。V-Triuneは、サンプルレベルデータフォーマッティング（多様なタスクの入力を統一する）、バリデーターレベル報酬計算（専門的なバリデーターをよってカスタム報酬を提供する）、ソースレベルメトリクスモニタリング（データソースレベルでの問題を診断する）の3つの補完的なコンポーネントからなります。また、V-Triuneが扱う観察力タスクに対して、適応的、進歩的、確定的なフィードバックを提供する新しいDynamic IoU報酬を紹介します。我々のアプローチは、オープンソースの7Bと32Bバックボーンモデルを使用したオフシャールのRL訓練フレームワーク内で実現されます。結果として得られたモデルは、Orsta（一つのRLですべてを見る）という名前で、論理や観察力タスクの両方で一致した向上を示します。この幅広い能力は、4つの代表的な視覚論理タスク（数学、パズル、チャート、科学）と4つの視覚観察力タスク（基底化、検出、カウント、OCR）を中心に構築された多様なデータセットによって形成されました。その後、OrstaはMEGA-Bench Coreで大きな効果を示し、7Bと32Bの各モデルバージョンでの改善は+2.1から+14.1まで範囲を広げ、下流タスクへの性能向上も広く及ぶことがわかります。これらの結果は、VLMsのユニット化RLアプローチの効果性とスケーラビリティを明らかにします。V-TriuneシステムとOrstaモデルは、https://github.com/MiniMax-AIで公開しています。",
      "upvotes": 33,
      "discussionId": "6833cf8adf7cbb5c087a4d0c",
      "githubRepo": "https://github.com/MiniMax-AI/One-RL-to-See-Them-All",
      "ai_summary": "A unified reinforcement learning system, V-Triune, combines visual reasoning and perception tasks in vision-language models through a single training pipeline, achieving significant improvements across various tasks.",
      "ai_keywords": [
        "visual triple unified reinforcement learning",
        "sample-level data formatting",
        "verifier-level reward computation",
        "source-level metric monitoring",
        "dynamic IoU reward",
        "reinforcement learning",
        "vision-language models",
        "object detection",
        "grounding",
        "Orsta",
        "MEGA-Bench Core"
      ]
    },
    "publishedAt": "2025-05-23T13:41:14.000Z",
    "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
    "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e4d4d6748dd4f8eeb7732",
      "avatarUrl": "/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg",
      "fullname": "Xuyang Shen",
      "name": "Ryan1122",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18092",
      "authors": [
        {
          "_id": "6833ea049f968fc5c6b64486",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64487",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64488",
          "user": {
            "_id": "62ecbffd99112e99c5f7fded",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png",
            "isPro": false,
            "fullname": "Fanqi Wan",
            "user": "Wanfq",
            "type": "user"
          },
          "name": "Fanqi Wan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:25.991Z",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64489",
          "name": "Shengyi Liao",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448a",
          "name": "Shaopeng Lai",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448b",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448c",
          "name": "Yingcheng Shi",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448d",
          "name": "Yuning Wu",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448e",
          "name": "Gang Fu",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b6448f",
          "name": "Zhansheng Li",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64490",
          "name": "Bin Yang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64491",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64492",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64493",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "6833ea049f968fc5c6b64494",
          "name": "Ming Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T16:47:00.000Z",
      "submittedOnDailyAt": "2025-05-26T04:43:04.143Z",
      "title": "QwenLong-CPRS: 無限LLMsへの向けて - 動的コンテキスト最適化に向けて",
      "submittedOnDailyBy": {
        "_id": "64777a346e6c7ac608c1e9bf",
        "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
        "isPro": false,
        "fullname": "Weizhou Shen",
        "user": "shenwzh3",
        "type": "user"
      },
      "summary": "この技術報告では、長文脈の明確な最適化を目的としたQwenLong-CPRSのコンテキスト圧縮フレームワークを介して、予準埋めステージの計算オーバーヘッドと長文脈処理時の大語言モデル（LLMs）の「中間迷失」の性能低下を解決する。新しい動的なコンテキスト最適化機構を通じて実装され、自然言語指示による多粒度のコンテキスト圧縮を可能にし、その両方の効率と性能向上を実現します。\n\nQwenアーキテクチャシリーズから発展したQwenLong-CPRSは、4つのキーのイノベーションを導入します：自然言語による動的な最適化、極性認識の向上を図るバイデリクトレジョン層、言語モデリングヘッドを搭載したトーククリティック機構、ウィンドウ並列推論。\n\n5ベンチマーク（4K-2M単語コンテキスト）を横断的に評価した結果、QwenLong-CPRSの3つの効果を示します：他のコンテキスト管理方法（RAG、スパースアテンション）と比較して精度と効率の両方で絶えず優位を果たす。アーキテクチャ無依存性で、GPT-4o、Gemini2.0-pro、Claude3.7-sonnet、DeepSeek-v3、Qwen2.5-maxなどのすべてのフラグシップLLMsと統合し、21.59倍のコンテキスト圧縮と19.15点の平均性能向上を実現します。Qwen2.5-32B-Instructとの採用により、Ruler-128KとInfiniteBenchではそれぞれ4.85点と10.88点の性能を超え、新たなSOTA性能を確立します。",
      "upvotes": 31,
      "discussionId": "6833ea059f968fc5c6b644c1",
      "ai_summary": "QwenLong-CPRS enhances large language models with multi-granularity context compression, dynamic optimization guided by natural language, and efficient bidirectional reasoning and parallel inference, achieving superior performance and context management.",
      "ai_keywords": [
        "context compression",
        "dynamic context optimization",
        "bidirectional reasoning layers",
        "token critic mechanisms",
        "window-parallel inference",
        "Qwen",
        "RAG",
        "sparse attention",
        "large language models",
        "SOTA performance"
      ]
    },
    "publishedAt": "2025-05-23T12:47:00.000Z",
    "title": "QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization",
    "summary": "This technical report presents QwenLong-CPRS, a context compression framework\ndesigned for explicit long-context optimization, addressing prohibitive\ncomputation overhead during the prefill stage and the \"lost in the middle\"\nperformance degradation of large language models (LLMs) during long sequence\nprocessing. Implemented through a novel dynamic context optimization mechanism,\nQwenLong-CPRS enables multi-granularity context compression guided by natural\nlanguage instructions, achieving both efficiency gains and improved\nperformance.\n  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key\ninnovations: (1) Natural language-guided dynamic optimization, (2)\nBidirectional reasoning layers for enhanced boundary awareness, (3) Token\ncritic mechanisms with language modeling heads, and (4) Window-parallel\ninference.\n  Comprehensive evaluations across five benchmarks (4K-2M word contexts)\ndemonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority\nover other context management methods like RAG and sparse attention in both\naccuracy and efficiency. (2) Architecture-agnostic integration with all\nflagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,\nand Qwen2.5-max, achieves 21.59times context compression alongside\n19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,\nQwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on\nRuler-128K and InfiniteBench, establishing new SOTA performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18092.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64777a346e6c7ac608c1e9bf",
      "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg",
      "fullname": "Weizhou Shen",
      "name": "shenwzh3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17225",
      "authors": [
        {
          "_id": "6833c65d49b9e903d3ddbd11",
          "user": {
            "_id": "62845957b410bd779033759c",
            "avatarUrl": "/avatars/4feef73c06f2f7de6abf7a4789ac13f9.svg",
            "isPro": false,
            "fullname": "Doohyuk Jang",
            "user": "jadohu",
            "type": "user"
          },
          "name": "Doohyuk Jang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:25.026Z",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd12",
          "user": {
            "_id": "61b15ce1a5dd7dc7024406dc",
            "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
            "isPro": false,
            "fullname": "Yoonjeon Kim",
            "user": "yjyjyj98",
            "type": "user"
          },
          "name": "Yoonjeon Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:27.112Z",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd13",
          "name": "Chanjae Park",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd14",
          "name": "Hyun Ryu",
          "hidden": false
        },
        {
          "_id": "6833c65d49b9e903d3ddbd15",
          "name": "Eunho Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T19:00:01.000Z",
      "submittedOnDailyAt": "2025-05-26T00:11:09.797Z",
      "title": "Reasoningモデルは固執である：Reasoningモデルでのインストラクションオーバーライドの診断",
      "submittedOnDailyBy": {
        "_id": "61b15ce1a5dd7dc7024406dc",
        "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
        "isPro": false,
        "fullname": "Yoonjeon Kim",
        "user": "yjyjyj98",
        "type": "user"
      },
      "summary": "大語言モデルは長いおよび複雑な理由論の仕事において驚異的な優秀性を示しています。しかし、これらのモデルは問題的な理由論の剛性を見出し、この現象を理由論の剛性と呼びます。ユーザーからの明確な指示にもかかわらず、これらのモデルは習慣的な理由論のタライトを優先し、間違った結論を得ることが多いです。この行為は、特に数学とロジックパズルのような領域で、特定の制約に精密に従うことが重要な場合に特に大きな課題を呈しています。理由論の剛性をシステマティックに調査するために、先行の研究では大きく調査されていない行為を調査するために、エクスパートがカレードした診断セットを紹介します。このデータセットにより、モデルが習慣的な理由論に従う際に繰り返される再現的なサイバリゼーションパターンを識別します。特に、このサイバリゼーションは以下の3つのモードに分類されます： (i) 解釈オーバーロード、(ii) 入力不信、(iii) 部分プロジェクトアテンション、これらはモデルが提供された指示を無視または歪めることを促成します。このデータセットを公開し、将来の研究で理由論の剛性を軽減するための研究を促進することを目的としています。",
      "upvotes": 30,
      "discussionId": "6833c65e49b9e903d3ddbd6a",
      "projectPage": "https://reasoningtrap.github.io/",
      "githubRepo": "https://github.com/ReasoningTrap/ReasoningTrap",
      "ai_summary": "A diagnostic set examines and categorizes reasoning rigidity in large language models, identifying patterns where models ignore instructions and default to familiar reasoning.",
      "ai_keywords": [
        "reasoning rigidity",
        "large language models",
        "long and complex reasoning tasks",
        "reasoning trajectories",
        "diagnostic set",
        "AIME",
        "MATH500",
        "Interpretation Overload",
        "Input Distrust",
        "Partial Instruction Attention"
      ]
    },
    "publishedAt": "2025-05-22T15:00:01.000Z",
    "title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in\n  Reasoning Models",
    "summary": "Large language models have demonstrated remarkable proficiency in long and\ncomplex reasoning tasks. However, they frequently exhibit a problematic\nreliance on familiar reasoning patterns, a phenomenon we term reasoning\nrigidity. Despite explicit instructions from users, these models often\noverride clearly stated conditions and default to habitual reasoning\ntrajectories, leading to incorrect conclusions. This behavior presents\nsignificant challenges, particularly in domains such as mathematics and logic\npuzzle, where precise adherence to specified constraints is critical. To\nsystematically investigate reasoning rigidity, a behavior largely unexplored in\nprior work, we introduce a expert-curated diagnostic set, . Our\ndataset includes specially modified variants of existing mathematical\nbenchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately\nredesigned to require deviation from familiar reasoning strategies. Using this\ndataset, we identify recurring contamination patterns that occur when models\ndefault to ingrained reasoning. Specifically, we categorize this contamination\ninto three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,\nand (iii) Partial Instruction Attention, each causing models to ignore or\ndistort provided instructions. We publicly release our diagnostic set to\nfacilitate future research on mitigating reasoning rigidity in language models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b15ce1a5dd7dc7024406dc",
      "avatarUrl": "/avatars/682ce5ee7d2fec7180dc8e1144cd12ab.svg",
      "fullname": "Yoonjeon Kim",
      "name": "yjyjyj98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17941",
      "authors": [
        {
          "_id": "6833cc35015eb19058ed83d9",
          "user": {
            "_id": "65811eeaa2284a018e51f1ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
            "isPro": true,
            "fullname": "Zigeng Chen",
            "user": "Zigeng",
            "type": "user"
          },
          "name": "Zigeng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:12.285Z",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83da",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83db",
          "name": "Gongfan Fang",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83dc",
          "name": "Ruonan Yu",
          "hidden": false
        },
        {
          "_id": "6833cc35015eb19058ed83dd",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T14:17:56.000Z",
      "submittedOnDailyAt": "2025-05-26T00:36:09.618Z",
      "title": "VeriThinker: バリデーション学習による推論モデルの効率化",
      "submittedOnDailyBy": {
        "_id": "65811eeaa2284a018e51f1ba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
        "isPro": true,
        "fullname": "Zigeng Chen",
        "user": "Zigeng",
        "type": "user"
      },
      "summary": "大論理モデル（LRMs）は、Chain-of-Thought（CoT）論理を使用して複雑なタスクを優れています。しかし、その過度な考え方により、不必要に長い論理連鎖が生じ、推論コストを大幅に上げます。この問題を軽減するために、私たちはVeriThinkerという新しいアプローチを紹介します。これは、合成的な簡潔なCoTデータを用いて直接LRMsを調整する傳統的な方法と異なり、アシスティント的な証明タスクのみでモデルを調整します。LRMsをCoT解決策の正確性を正確に証明することにより、LRMsは後続の自覚反省ステップの必要性についてより判別力を持つことになり、過度な考え方を抑制します。拡張された実験は、VeriThinkerが理由連鎖の長さを大幅に減少し、精度を維持しても少しだけ向上させることを証明しました。DeepSeek-R1-Distill-Qwen-7Bに適用された場合、MATH500では理由トークンが3790から2125に減少し、精度が0.8%上昇（94.0%→94.8%）し、AIME25ではトークンが14321から10287に減少し、精度が2.1%上昇（38.7%→40.8%）しました。また、実験はVeriThinkerが推測論理にもゼロショット一般化可能であることを示しました。コードは、https://github.com/czg1225/VeriThinkerから利用できます。",
      "upvotes": 20,
      "discussionId": "6833cc36015eb19058ed8419",
      "githubRepo": "https://github.com/czg1225/VeriThinker",
      "ai_summary": "VeriThinker reduces the length of complex reasoning chains in Large Reasoning Models (LRMs) by fine-tuning them on a verification task, thereby decreasing inference costs without significantly sacrificing accuracy.",
      "ai_keywords": [
        "Large Reasoning Models (LRMs)",
        "Chain-of-Thought (CoT) reasoning",
        "CoT compression",
        "verification task",
        "reasoning chain lengths",
        "reasoning tokens",
        "accuracy",
        "DeepSeek-R1-Distill-Qwen-7B",
        "MATH500",
        "AIME25",
        "speculative reasoning"
      ]
    },
    "publishedAt": "2025-05-23T10:17:56.000Z",
    "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
    "summary": "Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought\n(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily\nlengthy reasoning chains, dramatically increasing inference costs. To mitigate\nthis issue, we introduce VeriThinker, a novel approach for CoT compression.\nUnlike conventional methods that fine-tune LRMs directly on the original\nreasoning task using synthetic concise CoT data, we innovatively fine-tune the\nmodel solely through an auxiliary verification task. By training LRMs to\naccurately verify the correctness of CoT solutions, the LRMs inherently become\nmore discerning about the necessity of subsequent self-reflection steps,\nthereby effectively suppressing overthinking. Extensive experiments validate\nthat VeriThinker substantially reduces reasoning chain lengths while\nmaintaining or even slightly improving accuracy. When applied to\nDeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500\nfrom 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on\nAIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to\n40.8%). Additionally, our experiments demonstrate that VeriThinker can also be\nzero-shot generalized to speculative reasoning. Code is available at\nhttps://github.com/czg1225/VeriThinker",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65811eeaa2284a018e51f1ba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg",
      "fullname": "Zigeng Chen",
      "name": "Zigeng",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17561",
      "authors": [
        {
          "_id": "6833cb9030cd9df52a117557",
          "name": "Kwanyoung Kim",
          "hidden": false
        },
        {
          "_id": "6833cb9030cd9df52a117558",
          "name": "Sanghyun Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T07:09:10.000Z",
      "submittedOnDailyAt": "2025-05-26T00:33:24.403Z",
      "title": "モデルは既に最適なノイズを知っている：ビデオディフュージョンモデルでのアテンションを通じたベイズアクティブノイズ選択",
      "submittedOnDailyBy": {
        "_id": "63973ee44e7b4959dc98028f",
        "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
        "isPro": false,
        "fullname": "Kwanyoung",
        "user": "kwanyoung",
        "type": "user"
      },
      "summary": "初期ノイズの選択は、ビデオディフュージョンモデルの質とプロンプトのアラインメントに重大な影響を及ぼします。同じプロンプトに対して異なるノイズシードが使用されると、生成されるビデオが大幅に異なることがあります。最近の方法は、周辺設計された先驅（例：周波数フィルター、隣れのフレームの平滑化）を依存していますが、モデル内部にある信号を見漏らしています。これを解決するために、ANSE（Active Noise Selection for Generation）を提案します。ANSEは、アテンションベースの不確実性を定量化して高品質のノイズシードを選択するモデルによるフレームワークです。その核心はBANSA（Bayesian Active Noise Selection via Attention）です。BANSAは、複数のストロースアテンションサンプルのエントロピー不協和を測定してモデルの信頼性と一致性を推定します。推論時の効率的な扱いに向けて、BANSAのベルノルムスケープマスク付近似を導入し、1ステップのディフュージョンと一部のアテンション層を使用してスコア推定を行うことができます。CogVideoX-2Bと5Bの実験では、ANSEは推論時間が8%と13%の増加でビデオの質と時系列的な一致性を向上させ、ビデオディフュージョンでのノイズ選択において原理的で一般化可能なアプローチを提供します。プロジェクトページを参照：https://anse-project.github.io/anse-project/",
      "upvotes": 18,
      "discussionId": "6833cb9430cd9df52a11765d",
      "projectPage": "https://anse-project.github.io/anse-project/",
      "ai_summary": "ANSE enhances video diffusion models by selecting noise seeds based on model confidence, improving video quality and temporal coherence with minimal increase in inference time.",
      "ai_keywords": [
        "video diffusion models",
        "noise seeds",
        "prompt alignment",
        "external priors",
        "frequency filters",
        "inter-frame smoothing",
        "ANSE",
        "Active Noise Selection for Generation",
        "BANSA",
        "Bayesian Active Noise Selection via Attention",
        "acquisition function",
        "entropy disagreement",
        "stochastic attention samples",
        "score estimation",
        "diffusion step",
        "temporal coherence"
      ]
    },
    "publishedAt": "2025-05-23T03:09:10.000Z",
    "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via\n  Attention in Video Diffusion Model",
    "summary": "The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17561.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63973ee44e7b4959dc98028f",
      "avatarUrl": "/avatars/2e166fee60844729479bfa4291796c8a.svg",
      "fullname": "Kwanyoung",
      "name": "kwanyoung",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17873",
      "authors": [
        {
          "_id": "68341f661d53989a8ecb685d",
          "user": {
            "_id": "6684b284dc7b0ae2cc67660c",
            "avatarUrl": "/avatars/54b3c0c4d808f293d78085d4d504570a.svg",
            "isPro": false,
            "fullname": "liuwanhao",
            "user": "wanhaoliu",
            "type": "user"
          },
          "name": "Wanhao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:54.880Z",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb685e",
          "user": {
            "_id": "646a11791556443f24b582e9",
            "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
            "isPro": false,
            "fullname": "Zonglin Yang",
            "user": "ZonglinY",
            "type": "user"
          },
          "name": "Zonglin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:08:35.046Z",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb685f",
          "name": "Jue Wang",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6860",
          "name": "Lidong Bing",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6861",
          "user": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:40:05.397Z",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6862",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6863",
          "name": "Yuqiang Li",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6864",
          "name": "Houqiang Li",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6865",
          "name": "Erik Cambria",
          "hidden": false
        },
        {
          "_id": "68341f661d53989a8ecb6866",
          "name": "Wanli Ouyang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T13:24:50.000Z",
      "submittedOnDailyAt": "2025-05-26T06:33:21.775Z",
      "title": "MOOSE-Chem3: 実験へのフィードバックを通じた仮説の順位付けへの向け",
      "submittedOnDailyBy": {
        "_id": "646a11791556443f24b582e9",
        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
        "isPro": false,
        "fullname": "Zonglin Yang",
        "user": "ZonglinY",
        "type": "user"
      },
      "summary": "仮説順位付けは、自動化科学発見の重要な構成要素であり、特に湿式実験が費用高騰し、プロットフォーマットに制限された自然科学において特に重要です。現在のアプローチは、実験前の順位付けを焦点に、大語言モデルの内部的な理由にだけ依存し、実験の実際の結果を含めていません。私たちは、実験ガイドされた順位付けの任務を紹介し、先に測定されたものの結果に基づいて候補の仮説を優先順位付けすることを目指しています。しかし、このような戦略の開発は、自然科学領域で実際の実験を再繰り返し行うことの不実用性により難しいです。これに対して、私たちは、3つの領域情報を基にした仮想器を提案し、仮説の性能を既知の事実仮説との類似性によって決定し、ノイズによってピークされるものとしてモデル化します。124つの化学仮説の実験報告の結果をもとにデータセットを整頓し、この仮想器により実験ガイドされた順位付けの仮的な方法を開発します。機能的な特徴を共有するような仮説をクラスタリングし、シミュレーションされた実験のフィードバックから得られる洞察に基づいて候補を優先順位付けます。実験は、私たちの方法が実験前の基準と強力なデリミテーションに優れていることを示します。",
      "upvotes": 14,
      "discussionId": "68341f671d53989a8ecb68b8",
      "ai_summary": "A novel simulator and experiment-guided ranking method improve hypothesis prioritization in scientific discovery by incorporating simulated experimental outcomes.",
      "ai_keywords": [
        "hypothesis ranking",
        "automated scientific discovery",
        "natural sciences",
        "wet-lab experiments",
        "large language model",
        "pre-experiment ranking",
        "experiment-guided ranking",
        "hypothesis performance",
        "similarity",
        "noise",
        "dataset",
        "pseudo experiment-guided ranking",
        "clustering",
        "functional characteristics",
        "simulated experimental feedback"
      ]
    },
    "publishedAt": "2025-05-23T09:24:50.000Z",
    "title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated\n  Experimental Feedback",
    "summary": "Hypothesis ranking is a crucial component of automated scientific discovery,\nparticularly in natural sciences where wet-lab experiments are costly and\nthroughput-limited. Existing approaches focus on pre-experiment ranking,\nrelying solely on large language model's internal reasoning without\nincorporating empirical outcomes from experiments. We introduce the task of\nexperiment-guided ranking, which aims to prioritize candidate hypotheses based\non the results of previously tested ones. However, developing such strategies\nis challenging due to the impracticality of repeatedly conducting real\nexperiments in natural science domains. To address this, we propose a simulator\ngrounded in three domain-informed assumptions, modeling hypothesis performance\nas a function of similarity to a known ground truth hypothesis, perturbed by\nnoise. We curate a dataset of 124 chemistry hypotheses with experimentally\nreported outcomes to validate the simulator. Building on this simulator, we\ndevelop a pseudo experiment-guided ranking method that clusters hypotheses by\nshared functional characteristics and prioritizes candidates based on insights\nderived from simulated experimental feedback. Experiments show that our method\noutperforms pre-experiment baselines and strong ablations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17873.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "646a11791556443f24b582e9",
      "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
      "fullname": "Zonglin Yang",
      "name": "ZonglinY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16211",
      "authors": [
        {
          "_id": "6833d9cfdf7cbb5c087cb9cd",
          "name": "Kai Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ce",
          "name": "Can Shen",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9cf",
          "name": "Yile Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d0",
          "name": "Jirui Han",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d1",
          "name": "Kelong Zheng",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d2",
          "name": "Xuechao Zou",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d3",
          "name": "Zhe Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d4",
          "name": "Xingjian Du",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d5",
          "name": "Shun Zhang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d6",
          "name": "Hanjun Luo",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d7",
          "name": "Yingbin Jin",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d8",
          "name": "Xinxin Xing",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9d9",
          "name": "Ziyang Ma",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9da",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9db",
          "user": {
            "_id": "64c6627d5671d42e0adfad56",
            "avatarUrl": "/avatars/8b98054b2911b86dcc4856a15306e60f.svg",
            "isPro": false,
            "fullname": "jiaxiaojunQAQ",
            "user": "jiaxiaojunQAQ",
            "type": "user"
          },
          "name": "Xiaojun Jia",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:37.847Z",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9dc",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9dd",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9de",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9df",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e0",
          "name": "Haoyang Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e1",
          "name": "Yiming Li",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e2",
          "name": "Xiaobin Zhuang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e3",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e4",
          "name": "Haibo Hu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e5",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e6",
          "name": "Zhizheng Wu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e7",
          "name": "Xiaolin Hu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e8",
          "name": "Eng-Siong Chng",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9e9",
          "name": "XiaoFeng Wang",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ea",
          "name": "Wenyuan Xu",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9eb",
          "name": "Wei Dong",
          "hidden": false
        },
        {
          "_id": "6833d9cfdf7cbb5c087cb9ec",
          "name": "Xinfeng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T04:27:46.000Z",
      "submittedOnDailyAt": "2025-05-26T01:33:43.107Z",
      "title": "AudioTrust: 音声大語言モデルの多面性の信頼性のベンチマーク",
      "submittedOnDailyBy": {
        "_id": "6387676c23da90491eb9fb16",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
        "isPro": false,
        "fullname": "Kai Li",
        "user": "JusperLee",
        "type": "user"
      },
      "summary": "オーディオラージング大語言モデル（ALLMs）の急速な進歩と拡大するアプリケーションに伴い、その信頼性の理解が厳しく求められています。しかし、これらのモデルの評価に関する体系的な研究、特にオーディオモデールに特有のリスクに関しては、主に探索されていません。現在の評価フレームワークは主にテキストモデールに焦点を当てているか、または限定的な安全性次元を扱い、オーディオモデールに固有の特徴とアプリケーションシナリオに適切に対応していません。ここで、オーディオトラスト（AudioTrust）を紹介します。これは、最初の多面性のある信頼性評価フレームワークとベンチマークです。これは、ALLMsに特に設計されています。オーディオトラストは、公平性、ハウシャリング、安全性、プライバシー、ロバスト性、アウテンティフィケーションの6つのキー次元の評価を促進します。これらの次元を詳細に評価するために、オーディオトラストは18種類の異なる実験設定をもって構成されています。その核心は、4,420サンプル以上のオーディオ/テキストサンプルから構築された細心謹銓なデータセットです。このデータセットは、日常の会話、緊急コール、ボイスアシスタントの相互作用などの実世界的なシナリオから抽出されており、ALLMsの多面性の信頼性を調査するために特に設計されています。評価のために、ベンチマークは9つのオーディオ特有の評価指標を謹めて設計し、大規模な自動プロセスを使用してモデルの出力を客観的でスケーラブルなスコアを与えます。実験結果から、現在の最先端の開放ソースおよびクローズドソースのALLMsが、多様な高リスクのオーディオシナリオに直面した際の信頼性の境界と制限を明らかにし、将来のオーディオモデルの安全な信頼性の採用において有價値なエンドツーエンドのプライバシーを提供します。プラットフォームとベンチマークは、https://github.com/JusperLee/AudioTrust から利用可能です。",
      "upvotes": 14,
      "discussionId": "6833d9d1df7cbb5c087cba85",
      "githubRepo": "https://github.com/JusperLee/AudioTrust",
      "ai_summary": "AudioTrust evaluates the trustworthiness of Audio Large Language Models across multifaceted dimensions, using a comprehensive dataset and specific metrics to assess their performance in real-world audio scenarios.",
      "ai_keywords": [
        "Audio Large Language Models",
        "ALLMs",
        "trustworthiness",
        "fairness",
        "hallucination",
        "safety",
        "privacy",
        "robustness",
        "authentication",
        "AudioTrust",
        "experimental setups",
        "audio-specific evaluation metrics",
        "automated pipeline"
      ]
    },
    "publishedAt": "2025-05-22T00:27:46.000Z",
    "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models",
    "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16211.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6387676c23da90491eb9fb16",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
      "fullname": "Kai Li",
      "name": "JusperLee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17618",
      "authors": [
        {
          "_id": "6833eeaf98515618764fc204",
          "user": {
            "_id": "6672937ceac0fb1b9e516595",
            "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg",
            "isPro": false,
            "fullname": "haoran he",
            "user": "haoranhe",
            "type": "user"
          },
          "name": "Haoran He",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:19.593Z",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc205",
          "name": "Jiajun Liang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc206",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc207",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc208",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc209",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "6833eeaf98515618764fc20a",
          "name": "Ling Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T08:25:46.000Z",
      "submittedOnDailyAt": "2025-05-26T04:32:07.257Z",
      "title": "テストタイムでの進化計算を用いた画像とビデオの生成のスケーリング",
      "submittedOnDailyBy": {
        "_id": "667187ba9ab144eb3ac43a1b",
        "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
        "isPro": false,
        "fullname": "Runze Liu",
        "user": "RyanLiu112",
        "type": "user"
      },
      "summary": "モデル予約訓練時に拡張計算量（データとパラメータ）の費用が大幅に上昇する中、推論時の拡張計算（TTS）は、推論時に追加計算量を割り当てて生成モデルの性能を向上させる有望な方向として登場しました。TTSは複数の言語タスクで显著な成功を示していますが、画像や映像生成モデル（拡散ベースまたはフローベースモデル）の検証時の拡張挙動について理解が遅れています。最近の研究は視覚タスクの推論時の戦略について試みていますが、これらのアプローチは重要な制限を持っており、タスク特有の領域に限定され、拡張性が悪く、または報酬の過度最適化によりサンプルの多様性を失います。本論文では、新しい、一般的な、効率的なTTS手法「進化探索（EvoSearch）」を提案します。この手法は、拡散モデルやフローモデルの画像や映像生成の拡張性を効果的に向上させ、追加の訓練やモデル拡張を必要としません。EvoSearchは、生物的進化の原則を活用して、拡散モデルやフローモデルの検証時の拡張を進化探索問題として再設計し、汎用性の高い計算軌跡を効率的に探索し、改良します。適切に設計された選択と突然変異機構を拡散方程式のディズションプロセスに合わせたものを組み込み、EvoSearchは、ポピュラシーの多様性を保っながら高品質の子孫を連続的に生成します。画像や映像生成タスクの拡散モデルやフローモデルの幅広い評価で、我々の方法は既存のアプローチを経験的に上回り、高い多様性を収め、新しい評価基準に強い一般化性能を示します。本プロジェクトは、https://tinnerhrhe.github.io/evosearch にアクセスできます。",
      "upvotes": 11,
      "discussionId": "6833eeb198515618764fc277",
      "projectPage": "https://tinnerhrhe.github.io/evosearch/",
      "githubRepo": "https://github.com/tinnerhrhe/EvoSearch-codes",
      "ai_summary": "EvoSearch, an evolutionary search method, enhances test-time scaling for diffusion and flow-based generative models, improving image and video generation quality, diversity, and generalizability.",
      "ai_keywords": [
        "test-time scaling",
        "TTS",
        "image generation",
        "video generation",
        "diffusion models",
        "flow-based models",
        "denoising trajectory",
        "stochastic differential equation",
        "selection",
        "mutation",
        "EvoSearch"
      ]
    },
    "publishedAt": "2025-05-23T04:25:46.000Z",
    "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
    "summary": "As the marginal cost of scaling computation (data and parameters) during\nmodel pre-training continues to increase substantially, test-time scaling (TTS)\nhas emerged as a promising direction for improving generative model performance\nby allocating additional computation at inference time. While TTS has\ndemonstrated significant success across multiple language tasks, there remains\na notable gap in understanding the test-time scaling behaviors of image and\nvideo generative models (diffusion-based or flow-based models). Although recent\nworks have initiated exploration into inference-time strategies for vision\ntasks, these approaches face critical limitations: being constrained to\ntask-specific domains, exhibiting poor scalability, or falling into reward\nover-optimization that sacrifices sample diversity. In this paper, we propose\nEvolutionary Search (EvoSearch), a novel, generalist, and\nefficient TTS method that effectively enhances the scalability of both image\nand video generation across diffusion and flow models, without requiring\nadditional training or model expansion. EvoSearch reformulates test-time\nscaling for diffusion and flow models as an evolutionary search problem,\nleveraging principles from biological evolution to efficiently explore and\nrefine the denoising trajectory. By incorporating carefully designed selection\nand mutation mechanisms tailored to the stochastic differential equation\ndenoising process, EvoSearch iteratively generates higher-quality offspring\nwhile preserving population diversity. Through extensive evaluation across both\ndiffusion and flow architectures for image and video generation tasks, we\ndemonstrate that our method consistently outperforms existing approaches,\nachieves higher diversity, and shows strong generalizability to unseen\nevaluation metrics. Our project is available at the website\nhttps://tinnerhrhe.github.io/evosearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667187ba9ab144eb3ac43a1b",
      "avatarUrl": "/avatars/db5558aa1c5160b9aee8b58573271959.svg",
      "fullname": "Runze Liu",
      "name": "RyanLiu112",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15692",
      "authors": [
        {
          "_id": "68306ffdff038ca6400a153a",
          "user": {
            "_id": "6747de57f8cab58c22ec94a2",
            "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
            "isPro": false,
            "fullname": "Jinyang Wu",
            "user": "Jinyang23",
            "type": "user"
          },
          "name": "Jinyang Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:13:57.397Z",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153b",
          "user": {
            "_id": "667fdaee20ee9ac417c7708c",
            "avatarUrl": "/avatars/69dfba6ff392643af1dcfe8af0a42ae9.svg",
            "isPro": false,
            "fullname": "Chonghua Liao",
            "user": "ChonghuaLiao",
            "type": "user"
          },
          "name": "Chonghua Liao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:16:00.056Z",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153c",
          "name": "Mingkuan Feng",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153d",
          "name": "Shuai Zhang",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153e",
          "name": "Zhengqi Wen",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a153f",
          "name": "Pengpeng Shao",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a1540",
          "name": "Huazhe Xu",
          "hidden": false
        },
        {
          "_id": "68306ffdff038ca6400a1541",
          "name": "Jianhua Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T16:06:10.000Z",
      "submittedOnDailyAt": "2025-05-26T01:19:22.736Z",
      "title": "シンデレラポリシー最適化：外部ガイドと内部能力のバリューフル接続",
      "submittedOnDailyBy": {
        "_id": "6747de57f8cab58c22ec94a2",
        "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
        "isPro": false,
        "fullname": "Jinyang Wu",
        "user": "Jinyang23",
        "type": "user"
      },
      "summary": "強化学習（RL）は、理由論モデルの訓練に有效な方法として現れたが、現在のRLアプローチは通常、モデルの出力分布を報酬最大化のパスにバイアスさせ、外部知識を引き込まない。これは、探索能力を制限し、基礎モデルに比べて理由論能力の境界が狭まることを実現している。この制限を解決するために、我々はTAPO（テンプレート追加ポリシー最適化）という新しいフレームワークを提案しています。TAPOは、外部の高レベルのガイド（「考えのパターン」）を統合してRLを増強しています。トレーニング中に構造化された考えを適応的に統合することで、TAPOはモデル内部の探索と外部ガイドの利用をよりよくバランスしています。拡張された実験により、我々のアプローチはAIMEで99%、AMCで41%、Minerva Mathで17%以上の性能を向上させ、これらの高レベルの考えのパターンは、先に500件のサンプルから抽象されたが、多様なタスクとモデルに広範囲的に一般化しています。これは、TAPOが多様なタスクとドメインで広範囲的に応用可能であることを示しています。我々の進ける分析により、外部ガイドの引き込みは、推論行為の説明性の向上と出力の読み込み性の向上を伴う強力な理由論モデルを作成することができることが明らかになっています。",
      "upvotes": 11,
      "discussionId": "68306ffeff038ca6400a1569",
      "ai_summary": "A novel RL framework, TAPO, integrates external guidance to enhance model performance and exploration compared to existing methods.",
      "ai_keywords": [
        "reinforcement learning",
        "TAPO",
        "Thought-Augmented Policy Optimization",
        "high-level guidance",
        "thought patterns",
        "model exploration",
        "AIME",
        "AMC",
        "Minerva Math",
        "reasoning models",
        "explainability",
        "output readability"
      ]
    },
    "publishedAt": "2025-05-21T12:06:10.000Z",
    "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities",
    "summary": "Reinforcement learning (RL) has emerged as an effective method for training\nreasoning models. However, existing RL approaches typically bias the model's\noutput distribution toward reward-maximizing paths without introducing external\nknowledge. This limits their exploration capacity and results in a narrower\nreasoning capability boundary compared to base models. To address this\nlimitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel\nframework that augments RL by incorporating external high-level guidance\n(\"thought patterns\"). By adaptively integrating structured thoughts during\ntraining, TAPO effectively balances model-internal exploration and external\nguidance exploitation. Extensive experiments show that our approach\nsignificantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva\nMath. Notably, these high-level thought patterns, abstracted from only 500\nprior samples, generalize effectively across various tasks and models. This\nhighlights TAPO's potential for broader applications across multiple tasks and\ndomains. Our further analysis reveals that introducing external guidance\nproduces powerful reasoning models with superior explainability of inference\nbehavior and enhanced output readability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15692.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6747de57f8cab58c22ec94a2",
      "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg",
      "fullname": "Jinyang Wu",
      "name": "Jinyang23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17399",
      "authors": [
        {
          "_id": "6833fd69fe87d9433d098068",
          "name": "Haoyu Sun",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d098069",
          "name": "Huichen Will Wang",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d09806a",
          "user": {
            "_id": "645b4819f9d4ec91fdd54852",
            "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
            "isPro": false,
            "fullname": "Kuvvi Gu",
            "user": "Kuvvi",
            "type": "user"
          },
          "name": "Jiawei Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:01.542Z",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d09806b",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "6833fd69fe87d9433d09806c",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T02:16:11.000Z",
      "submittedOnDailyAt": "2025-05-26T08:05:22.618Z",
      "title": "FullFront: フルフォーナント: 全前端工学ワークフローのMLLMベンチマーク\n\n（注：「FullFront」はそのまま英語にしておき、その他の部分は日本語に翻訳しました。）",
      "submittedOnDailyBy": {
        "_id": "645b4819f9d4ec91fdd54852",
        "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
        "isPro": false,
        "fullname": "Kuvvi Gu",
        "user": "Kuvvi",
        "type": "user"
      },
      "summary": "前端工程は、エンジニアがデザインを概念化し、コードに翻訳し、実装を反復的に改善する複雑なワークフローを含む。最近のベンチマークは主に可視的なデザインをコードに変換することを焦点にしているが、私たちは、全前端開発パイプラインを評価するためのベンチマーク「FullFront」を紹介します。FullFrontは、Webページデザイン（概念化フェーズ）、Webページ認識QA（可視的な組織と要素の理解）、Webページコード生成（実装フェーズ）の3つの基本的なタスクを評価します。現在のベンチマークは、コーディングが膨らんでいるスクレイプされたウェブサイトまたは簡略化されたLLM生成ハチマキHTMLを使用しているが、FullFrontは、実世界的なウェブページをクリーンな、標準化されたHTMLに変換するための新しい2段階プロセスを使用し、多様な可視的なデザインを維持し、コpyright問題を避けるものとして開発されました。最先端のMLLMの拡張試験により、ページ認識、コード生成（特に画像処理とレイアウト）、相互作用実装において显著な制限が見られました。私たちの結果は、モデルやタスクの挙動における性能の差異を定量的に示し、現在のMLLMの能力と前端工程の人間の専門家の性能の間の大きな間違いを明らかにします。FullFrontベンチマークとコードは、https://github.com/Mikivishy/FullFrontにアクセスできます。",
      "upvotes": 10,
      "discussionId": "6833fd6bfe87d9433d0980c2",
      "githubRepo": "https://github.com/Mikivishy/FullFront",
      "ai_summary": "FullFront is a benchmark evaluating Multimodal Large Language Models across conceptualization, comprehension, and implementation phases in front-end engineering.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "Webpage Design",
        "Webpage Perception QA",
        "Webpage Code Generation",
        "front-end engineering"
      ]
    },
    "publishedAt": "2025-05-22T22:16:11.000Z",
    "title": "FullFront: Benchmarking MLLMs Across the Full Front-End Engineering\n  Workflow",
    "summary": "Front-end engineering involves a complex workflow where engineers\nconceptualize designs, translate them into code, and iteratively refine the\nimplementation. While recent benchmarks primarily focus on converting visual\ndesigns to code, we present FullFront, a benchmark designed to evaluate\nMultimodal Large Language Models (MLLMs) across the full front-end\ndevelopment pipeline. FullFront assesses three fundamental tasks that map\ndirectly to the front-end engineering pipeline: Webpage Design\n(conceptualization phase), Webpage Perception QA (comprehension of visual\norganization and elements), and Webpage Code Generation (implementation phase).\nUnlike existing benchmarks that use either scraped websites with bloated code\nor oversimplified LLM-generated HTML, FullFront employs a novel, two-stage\nprocess to transform real-world webpages into clean, standardized HTML while\nmaintaining diverse visual designs and avoiding copyright issues. Extensive\ntesting of state-of-the-art MLLMs reveals significant limitations in page\nperception, code generation (particularly for image handling and layout), and\ninteraction implementation. Our results quantitatively demonstrate performance\ndisparities across models and tasks, and highlight a substantial gap between\ncurrent MLLM capabilities and human expert performance in front-end\nengineering. The FullFront benchmark and code are available in\nhttps://github.com/Mikivishy/FullFront.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17399.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b4819f9d4ec91fdd54852",
      "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg",
      "fullname": "Kuvvi Gu",
      "name": "Kuvvi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14669",
      "authors": [
        {
          "_id": "682da9d3781210358218a950",
          "name": "Roberto L. Castro",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a951",
          "name": "Andrei Panferov",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a952",
          "name": "Soroush Tabesh",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a953",
          "name": "Oliver Sieberling",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a954",
          "name": "Jiale Chen",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a955",
          "name": "Mahdi Nikdan",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a956",
          "name": "Saleh Ashkboos",
          "hidden": false
        },
        {
          "_id": "682da9d3781210358218a957",
          "name": "Dan Alistarh",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/N9QxR8-CzRd2UcOj9uwRR.png"
      ],
      "publishedAt": "2025-05-20T17:55:50.000Z",
      "submittedOnDailyAt": "2025-05-26T08:34:57.302Z",
      "title": "キャプテッド：ネイティブ FP4 トレーニングは、大規模言語モデルにとって最適である",
      "submittedOnDailyBy": {
        "_id": "623753b5eddd7763adc9346a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
        "isPro": false,
        "fullname": "Andrei Panferov",
        "user": "BlackSamorez",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）の急速な進歩は、計算要求の前所未有の増加と並行しています。最先端のモデルの訓練費用は、数ヶ月で倍になります。低精度計算を直接用いてモデルを訓練することは、計算スローショットとエネルギー効率の両方を向上させる解決策となります。特に、NVIDIAの最近のBlackwellアーキテクチャは、特にFP4バージョンの非常に低精度の操作を促進し、大幅な効率向上を目指しています。しかし、現在のLLMsのFP4精度での訓練に使用されるアルゴリズムは、精度の低下として大きな問題を見せ、通常は混合精度のフォーバックを依存しています。この論文では、硬件サポートされたFP4訓練を系統的に調査し、Quartetという新しいアプローチを紹介します。このアプローチは、全ての主要な計算（例えば、線形層）が低精度で行われるような、正確な端末からのFP4訓練を可能にします。Llamaタイプのモデルに対して拡張的な評価を通じて、ビット幅の変化に伴う性能のトレードオフを定量化し、精度と計算の関係における「近似最適」の低精度訓練手法を特定する新しいスケーリングラーを明らかにします。これをQuartetと呼び、NVIDIA Blackwell GPUにタイラーマッチされた最適化されたCUDAキャンバーを用いて実装し、FP4精度で最先端の精度を達成し、ビリオンスケールのモデルを成功に訓練できることを示します。我々の方法は、標準精度とFP8訓練と比較して競争的な選択肢として、完全なFP4ベースの訓練を示しています。我々のコードは、https://github.com/IST-DASLab/Quartet に公開されています。",
      "upvotes": 10,
      "discussionId": "682da9d4781210358218a982",
      "ai_summary": "Quartet, a hardware-supported FP4 training approach for large language models, demonstrates state-of-the-art accuracy while significantly reducing computational costs compared to standard or FP8 precision.",
      "ai_keywords": [
        "large language models",
        "low-precision arithmetic",
        "Blackwell architecture",
        "FP4",
        "mixed-precision",
        "linear layers",
        "low-precision scaling law",
        "CUDA kernels"
      ]
    },
    "publishedAt": "2025-05-20T13:55:50.000Z",
    "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
    "summary": "The rapid advancement of large language models (LLMs) has been paralleled by\nunprecedented increases in computational demands, with training costs for\nstate-of-the-art models doubling every few months. Training models directly in\nlow-precision arithmetic offers a solution, by improving both computational\nthroughput and energy efficiency. Specifically, NVIDIA's recent Blackwell\narchitecture facilitates extremely low-precision operations, specifically FP4\nvariants, promising substantial efficiency gains. Yet, current algorithms for\ntraining LLMs in FP4 precision face significant accuracy degradation and often\nrely on mixed-precision fallbacks. In this paper, we systematically investigate\nhardware-supported FP4 training and introduce Quartet, a new approach enabling\naccurate, end-to-end FP4 training with all the major computations (in e.g.\nlinear layers) being performed in low precision. Through extensive evaluations\non Llama-type models, we reveal a new low-precision scaling law that quantifies\nperformance trade-offs across varying bit-widths and allows us to identify a\n\"near-optimal\" low-precision training technique in terms of\naccuracy-vs-computation, called Quartet. We implement Quartet using optimized\nCUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve\nstate-of-the-art accuracy for FP4 precision, successfully training\nbillion-scale models. Our method demonstrates that fully FP4-based training is\na competitive alternative to standard-precision and FP8 training. Our code is\navailable at https://github.com/IST-DASLab/Quartet.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/N9QxR8-CzRd2UcOj9uwRR.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623753b5eddd7763adc9346a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg",
      "fullname": "Andrei Panferov",
      "name": "BlackSamorez",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17558",
      "authors": [
        {
          "_id": "6833c8af029c4a53a60a5dfa",
          "user": {
            "_id": "648749094dea003c6dae810f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
            "isPro": false,
            "fullname": "Shrey Pandit",
            "user": "SP2001",
            "type": "user"
          },
          "name": "Shrey Pandit",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-26T01:49:36.568Z",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfb",
          "user": {
            "_id": "62fa7294363251ee40a41dba",
            "avatarUrl": "/avatars/869c6de9a1cb2ded690ae56559916cae.svg",
            "isPro": false,
            "fullname": "Ashwin V",
            "user": "ashwinnv",
            "type": "user"
          },
          "name": "Ashwin Vinod",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:22.347Z",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfc",
          "name": "Liu Leqi",
          "hidden": false
        },
        {
          "_id": "6833c8af029c4a53a60a5dfd",
          "name": "Ying Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T07:05:09.000Z",
      "submittedOnDailyAt": "2025-05-26T00:20:35.511Z",
      "title": "ライブラリーを使用して教える：合成的な負のデータに基づくカリキュラムDPOでの幻想検出",
      "submittedOnDailyBy": {
        "_id": "648749094dea003c6dae810f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
        "isPro": false,
        "fullname": "Shrey Pandit",
        "user": "SP2001",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）のハウシュニング検出に対しての対応は、ハウシュニングテキストの複雑な性質により重大な課題として残されています。ハウシュニングサンプルが通常、傳統的な負例サンプルよりも高い偽装質量を示すことを認識し、これらのよく工夫されたハウシュニングをDPO対応手順の負例として使用しています。我々の方法は、カレクルミュード学習ステージングを採用し、独立した事実検証モデルからの確率スコアの最大限の減少に基づいて見出されたようなより簡単なサンプルから、進段的に難しいサンプルへと適応されています。この構造化された難易度スケーリングは、安定したおよび進段的な学習を確保します。実験的な評価により、我々のHaluCheckモデルは、カレクルDPOアプローチと高品質の負例サンプルを用いて訓練されていることを証明し、様々なメトリックでのモデル性能を大幅に向上させ、例えばMedHalluとHaluEvalの難しいベンチマークでは24%程度の向上を収めました。また、HaluCheckモデルはゼロショット設定での強固性を示し、様々なベンチマークでは大きな状態の最先端モデルを大幅に超えました。",
      "upvotes": 9,
      "discussionId": "6833c8b0029c4a53a60a5e3a",
      "ai_summary": "The use of carefully crafted hallucinations in a curriculum learning approach within the DPO alignment procedure significantly enhances LLMs' hallucination detection abilities.",
      "ai_keywords": [
        "LLMs",
        "hallucinations",
        "DPO alignment procedure",
        "curriculum learning",
        "probability scores",
        "fact checking models",
        "HaluCheck models",
        "MedHallu",
        "HaluEval",
        "zero-shot settings"
      ]
    },
    "publishedAt": "2025-05-23T03:05:09.000Z",
    "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for\n  Hallucination Detection",
    "summary": "Aligning large language models (LLMs) to accurately detect hallucinations\nremains a significant challenge due to the sophisticated nature of hallucinated\ntext. Recognizing that hallucinated samples typically exhibit higher deceptive\nquality than traditional negative samples, we use these carefully engineered\nhallucinations as negative examples in the DPO alignment procedure. Our method\nincorporates a curriculum learning strategy, gradually transitioning the\ntraining from easier samples, identified based on the greatest reduction in\nprobability scores from independent fact checking models, to progressively\nharder ones. This structured difficulty scaling ensures stable and incremental\nlearning. Experimental evaluation demonstrates that our HaluCheck models,\ntrained with curriculum DPO approach and high quality negative samples,\nsignificantly improves model performance across various metrics, achieving\nimprovements of upto 24% on difficult benchmarks like MedHallu and HaluEval.\nAdditionally, HaluCheck models demonstrate robustness in zero-shot settings,\nsignificantly outperforming larger state-of-the-art models across various\nbenchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17558.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648749094dea003c6dae810f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648749094dea003c6dae810f/gHUHSBt1zrt8wjO1YwTNu.jpeg",
      "fullname": "Shrey Pandit",
      "name": "SP2001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16479",
      "authors": [
        {
          "_id": "682fdc63bf762029ddcad451",
          "user": {
            "_id": "6640c647acae6bb179eedff5",
            "avatarUrl": "/avatars/bcaafaaa1d4b4c241d72a886401772e3.svg",
            "isPro": false,
            "fullname": "Yuetong Liu",
            "user": "YuetongLiu",
            "type": "user"
          },
          "name": "Yuetong Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:16:39.979Z",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad452",
          "user": {
            "_id": "646c77911ee398a4e9404b8b",
            "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
            "isPro": false,
            "fullname": "Yunqiu Xu",
            "user": "Yunqiu",
            "type": "user"
          },
          "name": "Yunqiu Xu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T04:01:10.107Z",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad453",
          "name": "Yang Wei",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad454",
          "name": "Xiuli Bi",
          "hidden": false
        },
        {
          "_id": "682fdc63bf762029ddcad455",
          "name": "Bin Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T10:06:35.000Z",
      "submittedOnDailyAt": "2025-05-26T05:13:10.671Z",
      "title": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration",
      "submittedOnDailyBy": {
        "_id": "646c77911ee398a4e9404b8b",
        "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
        "isPro": false,
        "fullname": "Yunqiu Xu",
        "user": "Yunqiu",
        "type": "user"
      },
      "summary": "夜景画像の悪天候効果による損傷を修復する研究問題は、実用的であるが、調査が不足しています。悪天候の状態は夜の時には多くの照明効果と共に存在します。本研究では、悪天候の影響とフレア効果が混在した夜景画像の修復作業を課題として検討します。また、これを研究にサポートするために、オールウェザーニャートのデータセットを提供します。このデータセットは、夜景画像の高品質のデータを含み、多様な構成的な損傷を含むものです。これらの画像は、我々が提案した照明に関する損傷生成によって合成されています。また、我々は、ClearNightという統一的な夜景画像の修復フレームワークを紹介します。このフレームワークは、一気に複雑な損傷を除去することができます。特に、ClearNightは、Retinexベースの双重先驅を抽出し、不均等な照明領域および固有のテクスチャ内容に焦点を当て、夜景の場合に修復の効果を高めます。また、多様な悪天候の共通点と異なりをより良く表現するために、気候に関する動的な特定-共通性の協調方法を提案します。これは、悪天候の損傷を識別し、特定の気候に関連付けられた最適なユニットを選択することを可能にします。我々のClearNightは、合成画像と実写画像で最先端の性能を収めます。詳細な消去実験は、オールウェザーニャートデータセットの必要性とClearNightの効果性を証明します。プロジェクトページは、https://henlyta.github.io/ClearNight/mainpage.html です。",
      "upvotes": 9,
      "discussionId": "682fdc67bf762029ddcad58c",
      "projectPage": "https://henlyta.github.io/ClearNight/mainpage.html",
      "githubRepo": "https://github.com/henlyta/ClearNight",
      "ai_summary": "A unified framework for restoring nighttime images under diverse weather conditions using dual priors and adaptive collaboration.",
      "ai_keywords": [
        "Retinex-based dual priors",
        "illumination-aware degradation generation",
        "weather-aware dynamic specific-commonality collaboration",
        "nighttime image restoration"
      ]
    },
    "publishedAt": "2025-05-22T06:06:35.000Z",
    "title": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration",
    "summary": "Restoring nighttime images affected by multiple adverse weather conditions is\na practical yet under-explored research problem, as multiple weather conditions\noften coexist in the real world alongside various lighting effects at night.\nThis paper first explores the challenging multi-weather nighttime image\nrestoration task, where various types of weather degradations are intertwined\nwith flare effects. To support the research, we contribute the AllWeatherNight\ndataset, featuring large-scale high-quality nighttime images with diverse\ncompositional degradations, synthesized using our introduced illumination-aware\ndegradation generation. Moreover, we present ClearNight, a unified nighttime\nimage restoration framework, which effectively removes complex degradations in\none go. Specifically, ClearNight extracts Retinex-based dual priors and\nexplicitly guides the network to focus on uneven illumination regions and\nintrinsic texture contents respectively, thereby enhancing restoration\neffectiveness in nighttime scenarios. In order to better represent the common\nand unique characters of multiple weather degradations, we introduce a\nweather-aware dynamic specific-commonality collaboration method, which\nidentifies weather degradations and adaptively selects optimal candidate units\nassociated with specific weather types. Our ClearNight achieves\nstate-of-the-art performance on both synthetic and real-world images.\nComprehensive ablation experiments validate the necessity of AllWeatherNight\ndataset as well as the effectiveness of ClearNight. Project page:\nhttps://henlyta.github.io/ClearNight/mainpage.html",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16479.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646c77911ee398a4e9404b8b",
      "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
      "fullname": "Yunqiu Xu",
      "name": "Yunqiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16483",
      "authors": [
        {
          "_id": "6833cb27e10e89e250a6d9ae",
          "name": "Shuzheng Si",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9af",
          "user": {
            "_id": "63ff09a02098b9ad105a09f6",
            "avatarUrl": "/avatars/4409ca5d320050cf4c3df05962c7ff58.svg",
            "isPro": false,
            "fullname": "Hans Zhao",
            "user": "BleachNick",
            "type": "user"
          },
          "name": "Haozhe Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:14.872Z",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b0",
          "name": "Cheng Gao",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b1",
          "name": "Yuzhuo Bai",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b2",
          "name": "Zhitong Wang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b3",
          "name": "Bofei Gao",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b4",
          "name": "Kangyang Luo",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b5",
          "name": "Wenhao Li",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b6",
          "name": "Yufei Huang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b7",
          "name": "Gang Chen",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b8",
          "name": "Fanchao Qi",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9b9",
          "name": "Minjia Zhang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9ba",
          "name": "Baobao Chang",
          "hidden": false
        },
        {
          "_id": "6833cb27e10e89e250a6d9bb",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T10:10:07.000Z",
      "submittedOnDailyAt": "2025-05-26T00:36:30.930Z",
      "title": "合成タスクと強化学習を用いて、大規模言語モデルのコンテキスト忠実性を維持する方法",
      "submittedOnDailyBy": {
        "_id": "637c99bbfe115289cfedfb44",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
        "isPro": false,
        "fullname": "ssz",
        "user": "ssz1111",
        "type": "user"
      },
      "summary": "テコードーラージャングルモデル（LLMs）を提供されたコンテキストで忠実にすることは、信頼性の高い情報探査システムの構築に重要です。そこで、私たちは、人間のアノテーションを除くことで、短形と長形の生成タスクでの忠実性を向上させるためのシステマティックなフレームワーク、CANOEを提案します。具体的には、私たちは、四つの異なるタスクを含む短形の質問回答データを合成し、人間のアノテーションを除いた高品質で簡単に検証できる訓練データを構築します。また、私たちは、合成された短形の質問回答データから得られた三つのティアリゼットルールベースの報酬を含むルールベースの強化学習方法、Dual-GRPOを提案します。この方法では、短形と長形の回答生成を同時に最適化します。特に、Dual-GRPOは、報酬モデルの学習にマニュアルラベルされた好みデータの手動ラベリングを必要としないように、合成された短形の質問回答データのみを依存した場合に短形生成を過度に最適化しないようにします。実験結果は、CANOEが11種類の異なる次世代タスクでのLLMsの忠実性を大幅に向上させ、例えばGPT-4oとOpenAI o1の最先端のLLMsを超えることを示します。",
      "upvotes": 8,
      "discussionId": "6833cb28e10e89e250a6da0a",
      "projectPage": "https://github.com/S1s-Z/CANOE",
      "githubRepo": "https://github.com/S1s-Z/CANOE",
      "ai_summary": "CANOE improves LLM faithfulness in generation tasks using synthetic QA data and Dual-GRPO reinforcement learning without human annotations.",
      "ai_keywords": [
        "teaching large language models",
        "faithfulness",
        "context",
        "CANOE",
        "short-form generation",
        "long-form generation",
        "question-answering",
        "Dual-GRPO",
        "rule-based reinforcement learning",
        "preference data",
        "reward models"
      ]
    },
    "publishedAt": "2025-05-22T06:10:07.000Z",
    "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via\n  Synthetic Tasks and Reinforcement Learning",
    "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16483.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "637c99bbfe115289cfedfb44",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png",
      "fullname": "ssz",
      "name": "ssz1111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.13508",
      "authors": [
        {
          "_id": "683148c0018bba5b656c94e3",
          "user": {
            "_id": "65d188a4aa309d842e438ef1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
            "isPro": false,
            "fullname": "Zijia Liu",
            "user": "m-serious",
            "type": "user"
          },
          "name": "Zijia Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:18.318Z",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e4",
          "name": "Peixuan Han",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e5",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e6",
          "name": "Haoru Li",
          "hidden": false
        },
        {
          "_id": "683148c0018bba5b656c94e7",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T13:46:28.000Z",
      "submittedOnDailyAt": "2025-05-26T06:47:47.579Z",
      "title": "時間R1: 時系列論理の全面的な解決法に向けて",
      "submittedOnDailyBy": {
        "_id": "65d188a4aa309d842e438ef1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
        "isPro": false,
        "fullname": "Zijia Liu",
        "user": "m-serious",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は驚異的な能力を示しますが、強力的な時間知識が欠けており、過去の理由を考えることと未来の予測や適当な生成を統合することが難しい。同時に、現在の方法は通常孤立した時間スキルを目指し、過去の事実についての質問回答や基本的な予測を含むことが多いですが、特に知識の切り替わりより遠い事件や創造的な先見力が必要な場合には一般化能力が悪いです。これらの制限を克服するために、私たちはTime-R1を紹介します。Time-R1は最初のフレームワークであり、中間サイズ（3Bパラメータ）のLLMに理解、予測、創造的な生成を含む時間的な能力をもたらします。私たちのアプローチは新しい三段階開発パスを特徴的に持ち、最初の2段階は細かく設計された動的なルールベースの報酬システムによる強化学習（RL）カレキュリウムで構成されています。このフレームワークは、(1)歴史データからの基盤的な時間的理解と事象の時間の関係マッピングを、(2)知識の切り替わりより遠い事象の予測スキルを、最終的に(3)創造的な未来のスキネーション生成の優れた一般化を可能にします。特に、実験はTime-R1が200倍以上のモデルを上回り、特に最先端の671B DeepSeek-R1を含む、非常に難しい未来の事象予測と創造的なスキネーション生成ベンチマークで上位を取ります。この研究は、周密なエンジニアリングと進歩的なRL微調練習が小さい、効率的なモデルが優れた時間的性能を達成することが可能であることを強い証拠とします。また、これらの研究を進めるために、私たちは10年のニュースデータからの大規模な多タスク時間的推理データセットであるTime-Benchを公開し、Time-R1のチェックポイントシリーズを提供します。",
      "upvotes": 8,
      "discussionId": "683148c1018bba5b656c9511",
      "githubRepo": "https://github.com/ulab-uiuc/Time-R1",
      "ai_summary": "A novel framework, Time-R1, enhances moderate-sized LLMs with comprehensive temporal abilities through a reinforcement learning curriculum, outperforming larger models on future event prediction and creative scenario generation benchmarks.",
      "ai_keywords": [
        "Large Language Models",
        "reinforcement learning",
        "RL curriculum",
        "rule-based reward system",
        "temporal understanding",
        "event-time mappings",
        "future event prediction",
        "creative scenario generation",
        "Time-Bench",
        "Time-R1 checkpoints"
      ]
    },
    "publishedAt": "2025-05-16T09:46:28.000Z",
    "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs",
    "summary": "Large Language Models (LLMs) demonstrate impressive capabilities but lack\nrobust temporal intelligence, struggling to integrate reasoning about the past\nwith predictions and plausible generations of the future. Meanwhile, existing\nmethods typically target isolated temporal skills, such as question answering\nabout past events or basic forecasting, and exhibit poor generalization,\nparticularly when dealing with events beyond their knowledge cutoff or\nrequiring creative foresight. To address these limitations, we introduce\nTime-R1, the first framework to endow a moderate-sized (3B-parameter)\nLLM with comprehensive temporal abilities: understanding, prediction, and\ncreative generation. Our approach features a novel three-stage development\npath; the first two constitute a reinforcement learning (RL)\ncurriculum driven by a meticulously designed dynamic rule-based reward system.\nThis framework progressively builds (1) foundational temporal understanding and\nlogical event-time mappings from historical data, (2) future event prediction\nskills for events beyond its knowledge cutoff, and finally (3) enables\nremarkable generalization to creative future scenario generation without any\nfine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms\nmodels over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,\non highly challenging future event prediction and creative scenario generation\nbenchmarks. This work provides strong evidence that thoughtfully engineered,\nprogressive RL fine-tuning allows smaller, efficient models to achieve superior\ntemporal performance, offering a practical and scalable path towards truly\ntime-aware AI. To foster further research, we also release Time-Bench,\na large-scale multi-task temporal reasoning dataset derived from 10 years of\nnews data, and our series of Time-R1 checkpoints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13508.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65d188a4aa309d842e438ef1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
      "fullname": "Zijia Liu",
      "name": "m-serious",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17417",
      "authors": [
        {
          "_id": "6833d2df73bebebe5cd6604e",
          "user": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "isPro": false,
            "fullname": "Alan Dao",
            "user": "alandao",
            "type": "user"
          },
          "name": "Alan Dao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-26T02:38:31.379Z",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd6604f",
          "name": "Dinh Bach Vu",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66050",
          "name": "Huy Hoang Ha",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66051",
          "name": "Tuan Le Duc Anh",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66052",
          "name": "Shreyas Gopal",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66053",
          "name": "Yue Heng Yeo",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66054",
          "name": "Warren Keng Hoong Low",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66055",
          "name": "Eng Siong Chng",
          "hidden": false
        },
        {
          "_id": "6833d2df73bebebe5cd66056",
          "name": "Jia Qi Yip",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T03:05:47.000Z",
      "submittedOnDailyAt": "2025-05-26T01:03:18.385Z",
      "title": "無声の言語教師訓練：低資源言語の言語教師訓練での言語のない言語指示トレーニング",
      "submittedOnDailyBy": {
        "_id": "62d7b2339b629105a5d6888a",
        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
        "isPro": false,
        "fullname": "Alan Dao",
        "user": "alandao",
        "type": "user"
      },
      "summary": "大語言モデル（LLM）をもった声助手の急速な成長は、これらのシステムの訓練に必要な声の指示データの必要性を明らかにしました。語識別データの豊富さに反して、語の指示データの不足が特に見られ、これはモデルの調整において言語を理解して実行することができるようにするために重要です。高品質の合成語を生成するには、よい文字から声への変換（TTS）モデルが必要ですが、資源の少ない言語にはそのモデルが見られません。私たちの新しいアプローチは、この課題を解決するために、合成を意味的表現レベルで停止し、TTSの必要性を回避します。これを実現するために、合成的意味的表現を事前学習されたWhisperエンコーダーとアラインし、LLMは文の指示による訓練を行うことができながら、推論時に語の指示を理解する能力を維持します。この簡略化された訓練プロセスは、資源の少ない言語の声助手を構築するための有望なアプローチです。",
      "upvotes": 6,
      "discussionId": "6833d2df73bebebe5cd66074",
      "githubRepo": "https://github.com/menloresearch/ichigo",
      "ai_summary": "A method bypasses the need for TTS models by aligning semantic representations with a Whisper encoder, enabling LLMs to understand both text and spoken instructions for low-resource languages.",
      "ai_keywords": [
        "large language models",
        "LLM",
        "speech instruction data",
        "TTS",
        "semantic representation",
        "Whisper encoder",
        "fine-tuning",
        "low-resource languages"
      ]
    },
    "publishedAt": "2025-05-22T23:05:47.000Z",
    "title": "Speechless: Speech Instruction Training Without Speech for Low Resource\n  Languages",
    "summary": "The rapid growth of voice assistants powered by large language models (LLM)\nhas highlighted a need for speech instruction data to train these systems.\nDespite the abundance of speech recognition data, there is a notable scarcity\nof speech instruction data, which is essential for fine-tuning models to\nunderstand and execute spoken commands. Generating high-quality synthetic\nspeech requires a good text-to-speech (TTS) model, which may not be available\nto low resource languages. Our novel approach addresses this challenge by\nhalting synthesis at the semantic representation level, bypassing the need for\nTTS. We achieve this by aligning synthetic semantic representations with the\npre-trained Whisper encoder, enabling an LLM to be fine-tuned on text\ninstructions while maintaining the ability to understand spoken instructions\nduring inference. This simplified training process is a promising approach to\nbuilding voice assistant for low-resource languages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d7b2339b629105a5d6888a",
      "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
      "fullname": "Alan Dao",
      "name": "alandao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16770",
      "authors": [
        {
          "_id": "68308e2697d9a81c8521bc6a",
          "user": {
            "_id": "62145614b670cb63a38075ba",
            "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
            "isPro": false,
            "fullname": "MenghaoGuo",
            "user": "MenghaoGuo",
            "type": "user"
          },
          "name": "Meng-Hao Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:57.462Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6b",
          "user": {
            "_id": "66b711f9512dac2ac08bc5e5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b711f9512dac2ac08bc5e5/n2kSqNhg-TE56iN_V0xHm.png",
            "isPro": false,
            "fullname": "Xuanyu Chu",
            "user": "CXY07",
            "type": "user"
          },
          "name": "Xuanyu Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:48.661Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6c",
          "name": "Qianrui Yang",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6d",
          "user": {
            "_id": "6816bd8e0499f6c7c7b89601",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6816bd8e0499f6c7c7b89601/-dkIPxjOGbdwDZFxhkBMC.jpeg",
            "isPro": false,
            "fullname": "Zhe-Han Mo",
            "user": "Mo-ZheHan",
            "type": "user"
          },
          "name": "Zhe-Han Mo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:52.048Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6e",
          "name": "Yiqing Shen",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc6f",
          "name": "Pei-lin Li",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc70",
          "name": "Xinjie Lin",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc71",
          "name": "Jinnian Zhang",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc72",
          "name": "Xin-Sheng Chen",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc73",
          "user": {
            "_id": "63b2efb5922f26a27e76381c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b2efb5922f26a27e76381c/zOQAt_xywiY8eTvvQOrmQ.png",
            "isPro": false,
            "fullname": "Yi Zhang",
            "user": "uyzhang",
            "type": "user"
          },
          "name": "Yi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:54.379Z",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc74",
          "name": "Kiyohiro Nakayama",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc75",
          "name": "Zhengyang Geng",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc76",
          "name": "Houwen Peng",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc77",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "68308e2697d9a81c8521bc78",
          "name": "Shi-Nin Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T15:11:57.000Z",
      "submittedOnDailyAt": "2025-05-26T06:55:20.321Z",
      "title": "RBench-V: ビジュアル理由モデルの基礎的な評価における多モーダル出力",
      "submittedOnDailyBy": {
        "_id": "62145614b670cb63a38075ba",
        "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
        "isPro": false,
        "fullname": "MenghaoGuo",
        "user": "MenghaoGuo",
        "type": "user"
      },
      "summary": "ノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデックス\n\nノートインデック",
      "upvotes": 6,
      "discussionId": "68308e2797d9a81c8521bca5",
      "ai_summary": "A benchmark called RBench-V evaluates multi-modal models' vision-indispensable reasoning through image manipulation and auxiliary line construction, demonstrating that current models struggle with multi-modal outputs.",
      "ai_keywords": [
        "multi-modal models",
        "omni-models",
        "GPT-4o",
        "Gemini",
        "o3",
        "multi-modal chain of thought",
        "M-CoT",
        "RBench-V",
        "image manipulation",
        "auxiliary lines"
      ]
    },
    "publishedAt": "2025-05-22T11:11:57.000Z",
    "title": "RBench-V: A Primary Assessment for Visual Reasoning Models with\n  Multi-modal Outputs",
    "summary": "The rapid advancement of native multi-modal models and omni-models,\nexemplified by GPT-4o, Gemini, and o3, with their capability to process and\ngenerate content across modalities such as text and images, marks a significant\nmilestone in the evolution of intelligence. Systematic evaluation of their\nmulti-modal output capabilities in visual thinking processes (also known as\nmulti-modal chain of thought, M-CoT) becomes critically important. However,\nexisting benchmarks for evaluating multi-modal models primarily focus on\nassessing multi-modal inputs and text-only reasoning while neglecting the\nimportance of reasoning through multi-modal outputs. In this paper, we present\na benchmark, dubbed RBench-V, designed to assess models' vision-indispensable\nreasoning abilities. To construct RBench-V, we carefully hand-pick 803\nquestions covering math, physics, counting, and games. Unlike previous\nbenchmarks that typically specify certain input modalities, RBench-V presents\nproblems centered on multi-modal outputs, which require image manipulation such\nas generating novel images and constructing auxiliary lines to support the\nreasoning process. We evaluate numerous open- and closed-source models on\nRBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the\nbest-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below\nthe human score of 82.3%, highlighting that current models struggle to leverage\nmulti-modal reasoning. Data and code are available at\nhttps://evalmodels.github.io/rbenchv",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16770.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62145614b670cb63a38075ba",
      "avatarUrl": "/avatars/5e33debde75ae6c87640f63c48c560c6.svg",
      "fullname": "MenghaoGuo",
      "name": "MenghaoGuo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15389",
      "authors": [
        {
          "_id": "682f518184a99219c4b3090c",
          "user": {
            "_id": "6540fbf9cb7fffd683942b43",
            "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
            "isPro": false,
            "fullname": "DongGeon Lee",
            "user": "oneonlee",
            "type": "user"
          },
          "name": "DongGeon Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:17:11.519Z",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090d",
          "name": "Joonwon Jang",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090e",
          "name": "Jihae Jeong",
          "hidden": false
        },
        {
          "_id": "682f518184a99219c4b3090f",
          "name": "Hwanjo Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T11:26:40.000Z",
      "submittedOnDailyAt": "2025-05-26T00:35:59.051Z",
      "title": "ビジョン-ラングラウジモデルは野生で安全か？メムベースベンチマークスチュディー",
      "submittedOnDailyBy": {
        "_id": "6540fbf9cb7fffd683942b43",
        "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
        "isPro": false,
        "fullname": "DongGeon Lee",
        "user": "oneonlee",
        "type": "user"
      },
      "summary": "迅速部署的视觉语言模型（VLMs）放大了安全风险，然而大多数评估依赖于人工图像。本研究探讨了当前VLMs在面对普通用户共享的meme图像时的安全性。为了调查这一问题，我们引入了MemeSafetyBench，这是一个包含50,430个实例的基准测试，将真实的meme图像与有害和无害的指令配对。通过使用全面的安全分类法和基于LLM的指令生成，我们对多个VLMs进行了单轮和多轮交互的评估。我们研究了真实世界中的meme如何影响有害输出，对话语境的缓解效果，以及模型规模与安全指标之间的关系。我们的研究结果表明，VLMs对基于meme的有害提示比对合成或印刷图像更易受攻击。与仅文本输入相比，meme显著增加了有害响应并减少了拒绝。尽管多轮交互提供了部分缓解，但高风险仍然存在。这些结果强调了进行生态学上有效的评估和加强安全机制的必要性。",
      "upvotes": 6,
      "discussionId": "682f518184a99219c4b30956",
      "ai_summary": "VLMs are more vulnerable to harmful meme-based prompts than to synthetic images, and while multi-turn interactions offer some protection, significant vulnerabilities remain.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "MemeSafetyBench",
        "safety taxonomy",
        "LLM-based instruction generation",
        "single and multi-turn interactions"
      ]
    },
    "publishedAt": "2025-05-21T07:26:40.000Z",
    "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
    "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs show greater vulnerability to\nmeme-based harmful prompts than to synthetic or typographic images. Memes\nsignificantly increase harmful responses and decrease refusals compared to\ntext-only inputs. Though multi-turn interactions provide partial mitigation,\nelevated vulnerability persists. These results highlight the need for\necologically valid evaluations and stronger safety mechanisms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15389.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6540fbf9cb7fffd683942b43",
      "avatarUrl": "/avatars/d4a64fbde511d0949e1c339179586850.svg",
      "fullname": "DongGeon Lee",
      "name": "oneonlee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17826",
      "authors": [
        {
          "_id": "6833ce1bd5c438959f750d57",
          "name": "Xuchen Pan",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d58",
          "name": "Yanxi Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d59",
          "name": "Yushuo Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5a",
          "name": "Yuchang Sun",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5b",
          "name": "Daoyuan Chen",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5c",
          "name": "Wenhao Zhang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5d",
          "name": "Yuexiang Xie",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5e",
          "name": "Yilun Huang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d5f",
          "name": "Yilei Zhang",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d60",
          "name": "Dawei Gao",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d61",
          "name": "Yaliang Li",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d62",
          "name": "Bolin Ding",
          "hidden": false
        },
        {
          "_id": "6833ce1bd5c438959f750d63",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6576f9f4654561a1b345610b/kDRshLvxO0EynfeH0BOZK.png"
      ],
      "publishedAt": "2025-05-23T12:41:09.000Z",
      "submittedOnDailyAt": "2025-05-26T00:50:50.296Z",
      "title": "トリニティー-RFT: 強化学習の一般的なフレームワークとしての大規模言語モデルの微調節の統一フレームワーク",
      "submittedOnDailyBy": {
        "_id": "6576f9f4654561a1b345610b",
        "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
        "isPro": false,
        "fullname": "Yanxi Chen",
        "user": "yanxi-chen",
        "type": "user"
      },
      "summary": "トリニティー-RFTは、強化学習の微調節（RFT）に適した一般的的、柔軟的でスケーラブルなフレームワークです。このフレームワークは、解結設計を採用しています。その構成要素としては、(1) RFT-coreで、同期/非同期、オンプライス/オフプライス、オンライン/オフラインのRFTの標準化と一般化を実現します、(2) 効率と強固性を重視したアガント-環境相互作用の無間違ない統合、(3) RFTに最適化されたシステマティックなデータパイプラインを構成しています。トリニティー-RFTは、多様なアプリケーションシーンに適用でき、先進的な強化学習パラダイムの探索のための統一的なプラットフォームとして役立ちます。この技術報告書では、トリニティー-RFTの観点、機能、設計と実装を詳細に説明し、様々な例を組み込んで、提案されたフレームワークの有用性とユーザーフレンドリーさを示します。",
      "upvotes": 5,
      "discussionId": "6833ce1cd5c438959f750dab",
      "projectPage": "https://github.com/modelscope/Trinity-RFT",
      "githubRepo": "https://github.com/modelscope/Trinity-RFT",
      "ai_summary": "Trinity-RFT is a flexible and scalable framework for reinforcement fine-tuning of large language models, supporting various interaction modes and data pipelines.",
      "ai_keywords": [
        "reinforcement fine-tuning",
        "RFT-core",
        "synchronous/asynchronous",
        "on-policy/off-policy",
        "online/offline",
        "agent-environment interaction",
        "data pipelines",
        "reinforcement learning paradigms"
      ]
    },
    "publishedAt": "2025-05-23T08:41:09.000Z",
    "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models",
    "summary": "Trinity-RFT is a general-purpose, flexible and scalable framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na decoupled design, consisting of (1) an RFT-core that unifies and generalizes\nsynchronous/asynchronous, on-policy/off-policy, and online/offline modes of\nRFT, (2) seamless integration for agent-environment interaction with high\nefficiency and robustness, and (3) systematic data pipelines optimized for RFT.\nTrinity-RFT can be easily adapted for diverse application scenarios, and serves\nas a unified platform for exploring advanced reinforcement learning paradigms.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples demonstrating the utility and\nuser-friendliness of the proposed framework.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6576f9f4654561a1b345610b/kDRshLvxO0EynfeH0BOZK.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17826.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576f9f4654561a1b345610b",
      "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg",
      "fullname": "Yanxi Chen",
      "name": "yanxi-chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17508",
      "authors": [
        {
          "_id": "6833cf5a2d728e2330d572e3",
          "user": {
            "_id": "647bf082aba7062fe5c51ca9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
            "isPro": false,
            "fullname": "Yifan Zhang",
            "user": "yifAI",
            "type": "user"
          },
          "name": "Yifan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:09.930Z",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e4",
          "user": {
            "_id": "653d276681f52ceb4d12bd85",
            "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg",
            "isPro": false,
            "fullname": "Yifeng Liu",
            "user": "Lewis-Lau",
            "type": "user"
          },
          "name": "Yifeng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:10:06.913Z",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e5",
          "name": "Huizhuo Yuan",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e6",
          "name": "Yang Yuan",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e7",
          "name": "Quanquan Gu",
          "hidden": false
        },
        {
          "_id": "6833cf5a2d728e2330d572e8",
          "name": "Andrew C Yao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/mGUuNpUhjfafWqcJJZ1V1.png"
      ],
      "publishedAt": "2025-05-23T06:01:21.000Z",
      "submittedOnDailyAt": "2025-05-26T00:50:14.655Z",
      "title": "KL-正則化ポリシージェント勾配アルゴリズムの設計に関する理由",
      "submittedOnDailyBy": {
        "_id": "647bf082aba7062fe5c51ca9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
        "isPro": false,
        "fullname": "Yifan Zhang",
        "user": "yifAI",
        "type": "user"
      },
      "summary": "ポリシーグラディエントアルゴリズムは、大規模言語モデル（LLMs）の論理能力を向上させるために成功して適用されています。しかし、カンブラー-レイヤール（KL）正則化がポリシーグラディエントアルゴリズムに広く使用されており、トレーニングを安定化するためにありますが、異なるKLデータ行列の構成を評価し、線上強化学習（RL）のために代理損失関数に統合することができることをシステマ的に探索することは、複雑でシステマ的に探索可能なデザイン空間になります。本論文では、線上RL設定でKL正則化されたポリシーグラディエントメソッドの汎用と分析のためのシステマ的なフレームワークである正則化ポリシーグラディエント（RPG）を提案します。両方の正規化されたと非正規化されたポリシー分布を検討し、両方の正向きと逆向きのKLデータ行列で正則化された目標に対応するポリシーグラディエントと相対的な代理損失関数を取り出します。また、全て微分可能な損失関数とREINFORCEスタイルの勾配推定器の計算を行い、多様なアルゴリズムの需要を満たすことを調整します。LLMの論理におけるRLにおいてこれらの方法を用いて拡大的な実験を実施し、トレーニングの安定性と性能において優れたまたは競争的な結果を示します。コードは、https://github.com/complex-reasoning/RPG にアクセスできます。",
      "upvotes": 4,
      "discussionId": "6833cf5b2d728e2330d57313",
      "projectPage": "https://complex-reasoning.github.io/RPG",
      "githubRepo": "https://github.com/complex-reasoning/RPG",
      "ai_summary": "A regularized policy gradient framework is introduced to explore KL divergence formulations for enhancing the reasoning capabilities of LLMs in online reinforcement learning, demonstrating improved training stability and performance.",
      "ai_keywords": [
        "policy gradient algorithms",
        "KL regularization",
        "KL divergence",
        "surrogate loss functions",
        "online reinforcement learning",
        "full differentiable loss functions",
        "REINFORCE-style gradient estimators",
        "GRPO",
        "REINFORCE++",
        "DAPO",
        "regularized policy gradient (RPG)",
        "forward KL divergence",
        "reverse KL divergence",
        "normalized policy distributions",
        "unnormalized policy distributions"
      ]
    },
    "publishedAt": "2025-05-23T02:01:21.000Z",
    "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM\n  Reasoning",
    "summary": "Policy gradient algorithms have been successfully applied to enhance the\nreasoning capabilities of large language models (LLMs). Despite the widespread\nuse of Kullback-Leibler (KL) regularization in policy gradient algorithms to\nstabilize training, the systematic exploration of how different KL divergence\nformulations can be estimated and integrated into surrogate loss functions for\nonline reinforcement learning (RL) presents a nuanced and systematically\nexplorable design space. In this paper, we propose regularized policy gradient\n(RPG), a systematic framework for deriving and analyzing KL-regularized policy\ngradient methods in the online RL setting. We derive policy gradients and\ncorresponding surrogate loss functions for objectives regularized by both\nforward and reverse KL divergences, considering both normalized and\nunnormalized policy distributions. Furthermore, we present derivations for\nfully differentiable loss functions as well as REINFORCE-style gradient\nestimators, accommodating diverse algorithmic needs. We conduct extensive\nexperiments on RL for LLM reasoning using these methods, showing improved or\ncompetitive results in terms of training stability and performance compared to\nstrong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at\nhttps://github.com/complex-reasoning/RPG.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/mGUuNpUhjfafWqcJJZ1V1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647bf082aba7062fe5c51ca9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
      "fullname": "Yifan Zhang",
      "name": "yifAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17412",
      "authors": [
        {
          "_id": "6833e93697966d18e7c1e4d7",
          "name": "Shuang Wu",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4d8",
          "name": "Youtian Lin",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4d9",
          "name": "Feihu Zhang",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4da",
          "name": "Yifei Zeng",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4db",
          "name": "Yikang Yang",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4dc",
          "name": "Yajie Bao",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4dd",
          "name": "Jiachen Qian",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4de",
          "name": "Siyu Zhu",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4df",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4e0",
          "name": "Xun Cao",
          "hidden": false
        },
        {
          "_id": "6833e93697966d18e7c1e4e1",
          "name": "Yao Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T02:58:01.000Z",
      "submittedOnDailyAt": "2025-05-26T03:12:06.518Z",
      "title": "Direct3D-S2: ギガスケール 3D ジェネレーションをスペクトラルスパースアテンションで簡単にする",
      "submittedOnDailyBy": {
        "_id": "645a24779f06c5897254d14b",
        "avatarUrl": "/avatars/dd0a635674025dcc9a94ee0f4c952083.svg",
        "isPro": false,
        "fullname": "Youtian Lin",
        "user": "LoYoT",
        "type": "user"
      },
      "summary": "ディレクト3D S2は、スパースなボリュームを基にした可換性のある3D生成フレームワークで、Signed Distance Functionsなどの体積的表現を使用して高解像度の3D形状を生成するには、計算量とメモリの大きな課題がある。我々は、スパースなボリューム内での大きなトークンセットを効果的に処理することを可能にし、計算オーバーヘッドを大幅に減少し、順伝播で3.9倍、逆伝播で9.6倍のスピードアップを実現するために、空間的なスパースアテンション機構を導入しました。また、フレームワークは、入力、潜在、出力ステージで一貫したスパースなボリューム形式を維持する変分エンコーダーを含み、3D VAEでの異なる表現を使用した先行手法に比べ、学習の効率と安定性を大幅に向上させます。我々のモデルは公開で利用可能なデータセットで訓練されており、実験は、Direct3D S2は生成品質と効率について最先端の手法を超え、1024解像度の3D生成を8カーネルで実現できることを示し、256解像度の体積的表現では通常32カーネル以上必要なものとしていたものを実現できることを示し、ギガスケールの3D生成は実用的でアクセス可能になります。プロジェクトページ：https://nju3dv.github.io/projects/Direct3D-S2/。",
      "upvotes": 4,
      "discussionId": "6833e93b97966d18e7c1e676",
      "projectPage": "https://nju-3dv.github.io/projects/Direct3D-S2/",
      "githubRepo": "https://github.com/DreamTechAI/Direct3D-S2",
      "ai_summary": "A scalable 3D shape generation framework using sparse volumes and spatial sparse attention, enabling high-resolution generation with reduced computational requirements.",
      "ai_keywords": [
        "Signed Distance Functions",
        "sparse volumes",
        "Spatial Sparse Attention",
        "Diffusion Transformer",
        "variational autoencoder",
        "gigascale 3D generation"
      ]
    },
    "publishedAt": "2025-05-22T22:58:01.000Z",
    "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse\n  Attention",
    "summary": "Generating high resolution 3D shapes using volumetric representations such as\nSigned Distance Functions presents substantial computational and memory\nchallenges. We introduce Direct3D S2, a scalable 3D generation framework based\non sparse volumes that achieves superior output quality with dramatically\nreduced training costs. Our key innovation is the Spatial Sparse Attention\nmechanism, which greatly enhances the efficiency of Diffusion Transformer\ncomputations on sparse volumetric data. SSA allows the model to effectively\nprocess large token sets within sparse volumes, significantly reducing\ncomputational overhead and achieving a 3.9x speedup in the forward pass and a\n9.6x speedup in the backward pass. Our framework also includes a variational\nautoencoder that maintains a consistent sparse volumetric format across input,\nlatent, and output stages. Compared to previous methods with heterogeneous\nrepresentations in 3D VAE, this unified design significantly improves training\nefficiency and stability. Our model is trained on public available datasets,\nand experiments demonstrate that Direct3D S2 not only surpasses\nstate-of-the-art methods in generation quality and efficiency, but also enables\ntraining at 1024 resolution using only 8 GPUs, a task typically requiring at\nleast 32 GPUs for volumetric representations at 256 resolution, thus making\ngigascale 3D generation both practical and accessible. Project page:\nhttps://nju3dv.github.io/projects/Direct3D-S2/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17412.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645a24779f06c5897254d14b",
      "avatarUrl": "/avatars/dd0a635674025dcc9a94ee0f4c952083.svg",
      "fullname": "Youtian Lin",
      "name": "LoYoT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17091",
      "authors": [
        {
          "_id": "6833cb25fe87d9433dfd2b1c",
          "name": "Prateek Verma",
          "hidden": false
        },
        {
          "_id": "6833cb25fe87d9433dfd2b1d",
          "name": "Mert Pilanci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T22:20:16.000Z",
      "submittedOnDailyAt": "2025-05-26T00:30:41.320Z",
      "title": "大語言モデルは、読み込みによって自動的に視聴能力を学習している",
      "submittedOnDailyBy": {
        "_id": "62d7f1119b629105a5d84aad",
        "avatarUrl": "/avatars/c74045063e7c06cb7be0fa41ebb1d824.svg",
        "isPro": false,
        "fullname": "Prateek Verma",
        "user": "prateekv",
        "type": "user"
      },
      "summary": "この論文では、興味深い発見を報告します：文トークンに対して自動復元型LLMモデルを訓練することで、文モデルは内在的に画像と音声を理解する能力を発展させ、読み込みでも視聴能力を獲得することができます。ホテックな音声と視覚LLMモデルは、画像と音声の埋め込みを基準にした文出力を与えることで文LLMモデルを微調節しています。一方、我々のアーキテクチャは画像パッチ、音声ワーブフォームまたはトークンを入力として受け取ります。分類プイルプリンのような典型的な埋め込みまたはカテゴリーラベルを与えます。我々は、文重みがFSD-50KとGTZANデータセットの音声分類に役立つ一般性を示します。さらに、CIFAR-10とFashion-MNISTの画像分類や画像パッチについても同様の効果を示します。これは、文LLMが学ぶ強力な内部回路を、必要な連結を活性化したり、それほどそれぞれのアプリケーションに対して新しいモデルを作成することを避けることによって利用できることを強調します。",
      "upvotes": 4,
      "discussionId": "6833cb26fe87d9433dfd2b64",
      "ai_summary": "Auto-regressive text LLMs trained on text can develop internal capabilities for understanding images and audio, enabling them to perform classification tasks across different modalities without fine-tuning.",
      "ai_keywords": [
        "auto-regressive",
        "LLM",
        "text tokens",
        "audio",
        "visual",
        "embeddings",
        "category labels",
        "classification",
        "FSD-50K",
        "GTZAN",
        "CIFAR-10",
        "Fashion-MNIST",
        "image patches"
      ]
    },
    "publishedAt": "2025-05-20T18:20:16.000Z",
    "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading",
    "summary": "This paper presents a fascinating find: By training an auto-regressive LLM\nmodel on text tokens, the text model inherently develops internally an ability\nto understand images and audio, thereby developing the ability to see and hear\njust by reading. Popular audio and visual LLM models fine-tune text LLM models\nto give text output conditioned on images and audio embeddings. On the other\nhand, our architecture takes in patches of images, audio waveforms or tokens as\ninput. It gives us the embeddings or category labels typical of a\nclassification pipeline. We show the generality of text weights in aiding audio\nclassification for datasets FSD-50K and GTZAN. Further, we show this working\nfor image classification on CIFAR-10 and Fashion-MNIST, as well on image\npatches. This pushes the notion of text-LLMs learning powerful internal\ncircuits that can be utilized by activating necessary connections for various\napplications rather than training models from scratch every single time.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17091.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62d7f1119b629105a5d84aad",
      "avatarUrl": "/avatars/c74045063e7c06cb7be0fa41ebb1d824.svg",
      "fullname": "Prateek Verma",
      "name": "prateekv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16270",
      "authors": [
        {
          "_id": "6833d08edf7cbb5c087a8bf1",
          "user": {
            "_id": "65c288280aa2d53135734a42",
            "avatarUrl": "/avatars/960422a1482ac8b4a52dd08c02d901f6.svg",
            "isPro": false,
            "fullname": "Jiaru Zou",
            "user": "jiaruz2",
            "type": "user"
          },
          "name": "Jiaru Zou",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-26T02:41:41.249Z",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf2",
          "name": "Yikun Ban",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf3",
          "name": "Zihao Li",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf4",
          "name": "Yunzhe Qi",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf5",
          "name": "Ruizhong Qiu",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf6",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "6833d08edf7cbb5c087a8bf7",
          "name": "Jingrui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T06:00:45.000Z",
      "submittedOnDailyAt": "2025-05-26T00:53:39.701Z",
      "title": "Transformer Copilot: 学習ファインチューニングのミスログから",
      "submittedOnDailyBy": {
        "_id": "64fde4e252e82dd432b74ce9",
        "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
        "isPro": false,
        "fullname": "Ling Yang",
        "user": "Lingaaaaaaa",
        "type": "user"
      },
      "summary": "大語言モデルは、ディレインタスクに適用されるために、ディレインデータによる規範的な微調節を通じて通常に導入されます。標準的な微調節は、生成損失を最小化してモデルパラメータを最適化することを焦点にしていますが、私たちはモデルの学習シグナルを残し、その利用を深めることに取り組んでいます。これは、人間の学習者が過去の誤りを反省して将来の実績を改善するようなものとして見なします。まず、モデルの学習挙動と再現的誤りをシステマティックに記録するための「ミスティーログ」の概念を紹介します。元のtransformerベースモデルを「プリオ」として、それに対応して「コピオ」モデルを設計し、プリオの推論性能をロジット補正を通じて改善します。「Transformer コピオ」という名前をプリオとコピオのフレームワーク全体につけ、(i) 新しいコピオモデルの設計、(ii) コピオがプリオとともに進化するミスティーログから継続的に学習するジョイントトレーニングパラダイム、(iii) コピオがプリオのロジットを補正して生成を向上させるフュージョン推論パラダイムを導入します。新しい学習フレームワークについて理論的および実験的な分析を提供します。12つのベンチマーク（通常知識、算術、推薦タスク）に対する実験は、Transformer コピオがプリオモデルに比べて計算量の追加が微調節であり、強いスケーラビリティとトランスフェージビリティを示し、性能を平均で34.5%程度向上させることを示します。",
      "upvotes": 3,
      "discussionId": "6833d08fdf7cbb5c087a8c29",
      "ai_summary": "The Transformer Copilot framework enhances large language model performance through a Copilot model that refines the Pilot's logits based on a Mistake Log, leading to consistent performance improvements across various benchmarks.",
      "ai_keywords": [
        "large language models",
        "supervised fine-tuning",
        "domain-specific data",
        "generation loss",
        "model parameters",
        "learning signals",
        "Mistake Log",
        "transformer-based model",
        "Copilot model",
        "logits rectification",
        "joint training paradigm",
        "fused inference paradigm",
        "performance improvements",
        "computational overhead",
        "scalability",
        "transferability"
      ]
    },
    "publishedAt": "2025-05-22T02:00:45.000Z",
    "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
    "summary": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fde4e252e82dd432b74ce9",
      "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
      "fullname": "Ling Yang",
      "name": "Lingaaaaaaa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17063",
      "authors": [
        {
          "_id": "6833e65bf9ae3819ea4c568e",
          "name": "Yiduo Guo",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c568f",
          "user": {
            "_id": "638e4e66629b4d0a62ce1bf3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
            "isPro": false,
            "fullname": "Zhen Guo",
            "user": "zguo0525",
            "type": "user"
          },
          "name": "Zhen Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:28.399Z",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5690",
          "name": "Chuanwei Huang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5691",
          "name": "Zi-Ang Wang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5692",
          "name": "Zekai Zhang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5693",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5694",
          "name": "Huishuai Zhang",
          "hidden": false
        },
        {
          "_id": "6833e65bf9ae3819ea4c5695",
          "name": "Yikang Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-18T05:35:13.000Z",
      "submittedOnDailyAt": "2025-05-26T02:26:50.238Z",
      "title": "Synthetic Data RL: Task Definition Is All You Need",
      "submittedOnDailyBy": {
        "_id": "638e4e66629b4d0a62ce1bf3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
        "isPro": false,
        "fullname": "Zhen Guo",
        "user": "zguo0525",
        "type": "user"
      },
      "summary": "強化学習（RL）は、基盤モデルを専門的なタスクに適用する有力な方法ですが、大規模な人間ラベルデータの依存関係が広く採用されることに限ります。私たちは、タスク定義から生成される合成データをだけ用いて強化学習でモデルを調整する簡単で一般的なフレームワーク「Synthetic Data RL」を紹介します。私たちの方法は、タスク定義と検索された記事からの質問と回答ペアを生成し、モデルの解題能力に基づいて質問の難易度を調整し、モデルの平均合格率を用いてRLトレーニング用の質問を選択します。Qwen-2.5-7Bでは、基盤モデルに対してGSM8Kでは絶対的に29.2%の向上（実訓練モデルに対しては+2.9pp、Self-Instructに対しては+6.6pp）、MATHでは8.7%、GPQAでは+7.0pp（SynthLLMに対して）、MedQAでは8.9%、CQA（法律）では17.7%、CFA（財務）では13.7%の向上を収めました。同じデータバッジでは、教師あり学習よりも優れて、全ての人間データを用いたRLに近い性能を達成しました（GSM8Kでは+17.2pp）。100件の人間デモンストレーションを追加してもGSM8Kの性能には0.4ppの向上しか見られませんでした。人間データの記録を減らし、Synthetic Data RLはスケーラブルで効率的なRLベースのモデル適用を可能にします。コードとデモは、https://github.com/gydpku/Data_Synthesis_RL/ にアクセスできます。",
      "upvotes": 3,
      "discussionId": "6833e65cf9ae3819ea4c56c9",
      "projectPage": "https://github.com/gydpku/Data_Synthesis_RL",
      "githubRepo": "https://github.com/gydpku/Data_Synthesis_RL",
      "ai_summary": "Synthetic Data RL enhances foundation models through reinforcement learning using only synthetic data, achieving performance comparable to models trained with full human-labeled data.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "synthetic data",
        "reinforcement fine-tuning",
        "question and answer pairs",
        "model solvability",
        "average pass rate",
        "data budget",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-05-18T01:35:13.000Z",
    "title": "Synthetic Data RL: Task Definition Is All You Need",
    "summary": "Reinforcement learning (RL) is a powerful way to adapt foundation models to\nspecialized tasks, but its reliance on large-scale human-labeled data limits\nbroad adoption. We introduce Synthetic Data RL, a simple and general framework\nthat reinforcement fine-tunes models using only synthetic data generated from a\ntask definition. Our method first generates question and answer pairs from the\ntask definition and retrieved documents, then adapts the difficulty of the\nquestion based on model solvability, and selects questions using the average\npass rate of the model across samples for RL training. On Qwen-2.5-7B, our\nmethod achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9\npp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on\nGPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA\n(finance). It surpasses supervised fine-tuning under the same data budget and\nnearly matches RL with full human data across datasets (e.g., +17.2 pp on\nGSM8K). Adding 100 human demonstrations improves the performance of GSM8K only\nby 0.4 pp, showing a limited added value. By reducing human data annotation,\nSynthetic Data RL enables scalable and efficient RL-based model adaptation.\nCode and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17063.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638e4e66629b4d0a62ce1bf3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e4e66629b4d0a62ce1bf3/s7uQ2qmee2CaXfjZDOovZ.jpeg",
      "fullname": "Zhen Guo",
      "name": "zguo0525",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17540",
      "authors": [
        {
          "_id": "683409de1869c47bd0c423a4",
          "name": "Mingrui Wu",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a5",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a6",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a7",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a8",
          "name": "Jianjin Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423a9",
          "name": "Jianfeng Liu",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423aa",
          "name": "Yuefeng Zhan",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ab",
          "name": "Weihao Han",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ac",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ad",
          "name": "Jiayi Ji",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423ae",
          "name": "Xiaoshuai Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423af",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b0",
          "name": "Weiwei Deng",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b1",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b2",
          "name": "Feng Sun",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b3",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "683409de1869c47bd0c423b4",
          "name": "Rongrong Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T06:44:26.000Z",
      "submittedOnDailyAt": "2025-05-26T04:59:40.648Z",
      "title": "RePrompt: 理由付き再調整プロンプト実行による画像生成\n  強化学習による",
      "submittedOnDailyBy": {
        "_id": "6416d0b2058f65de43191027",
        "avatarUrl": "/avatars/2d99114e5cff39dccc385adfad7032c5.svg",
        "isPro": false,
        "fullname": "Mingrui Wu",
        "user": "mrwu",
        "type": "user"
      },
      "summary": "最近のテキストから画像生成（T2I）の進歩にもとづいても、現在のモデルは短いおみやげや欠陥のあるプロンプトからユーザーの意図を忠実に捉えることが難しい。先行研究は、大規模な言語モデル（LLMs）を使用してプロンプトの機能を向上させようと試みましたが、これらの方法は、画像的語意と実世界的な組み合わせにおいて足りない基礎を持たず、スタイリッシュや不実験的な内容を生成することも多い。言語モデルの推理についての最近の進歩をヒントに、私たちはRePromptという新しいプロンプトの拡張フレームワークを提案します。これは、強化学習を通じて、プロンプトの拡張プロセスに明記的な推理を導入します。手作りのルールやスタイリッシュな改変に依存しない代わりに、私たちの方法は、画像レベルの結果を最適化することで、構造的で自覚的なプロンプトを生成するラベルモデルを訓練します。製品化された報酬モデルは、人間の好み、語意的な一致、および画像的な組み合わせにおいて生成された画像を評価し、プロンプトの生成を補助的に改善します。私たちのアプローチは、人間のデータを使用しないように、端末からの訓練を可能にします。GenEvalとT2I-Compbenchでの実験は、RePromptは多様なT2Iバックボーンでの空間的な配置の忠実性と組み合わせの一般化を大幅に向上させ、新しい最先端の結果を奪いました。",
      "upvotes": 2,
      "discussionId": "683409e21869c47bd0c4248e",
      "ai_summary": "RePrompt, a reprompting framework using reinforcement learning, enhances text-to-image generation by optimizing for image-level outcomes, significantly improving spatial layout and compositional generalization.",
      "ai_keywords": [
        "text-to-image",
        "T2I",
        "large language models",
        "LLMs",
        "reinforcement learning",
        "structured prompts",
        "self-reflective prompts",
        "reward models",
        "human preference",
        "semantic alignment",
        "visual composition",
        "GenEval",
        "T2I-Compbench",
        "spatial layout fidelity",
        "compositional generalization"
      ]
    },
    "publishedAt": "2025-05-23T02:44:26.000Z",
    "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation\n  via Reinforcement Learning",
    "summary": "Despite recent progress in text-to-image (T2I) generation, existing models\noften struggle to faithfully capture user intentions from short and\nunder-specified prompts. While prior work has attempted to enhance prompts\nusing large language models (LLMs), these methods frequently generate stylistic\nor unrealistic content due to insufficient grounding in visual semantics and\nreal-world composition. Inspired by recent advances in reasoning for language\nmodel, we propose RePrompt, a novel reprompting framework that introduces\nexplicit reasoning into the prompt enhancement process via reinforcement\nlearning. Instead of relying on handcrafted rules or stylistic rewrites, our\nmethod trains a language model to generate structured, self-reflective prompts\nby optimizing for image-level outcomes. The tailored reward models assesse the\ngenerated images in terms of human preference, semantic alignment, and visual\ncomposition, providing indirect supervision to refine prompt generation. Our\napproach enables end-to-end training without human-annotated data. Experiments\non GenEval and T2I-Compbench show that RePrompt significantly boosts spatial\nlayout fidelity and compositional generalization across diverse T2I backbones,\nestablishing new state-of-the-art results.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17540.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6416d0b2058f65de43191027",
      "avatarUrl": "/avatars/2d99114e5cff39dccc385adfad7032c5.svg",
      "fullname": "Mingrui Wu",
      "name": "mrwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17016",
      "authors": [
        {
          "_id": "6833f7847e0c637c71de0ec6",
          "user": {
            "_id": "64b64debeb9a833e08d079fd",
            "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
            "isPro": false,
            "fullname": "Shuhan Tan",
            "user": "tanshh97",
            "type": "user"
          },
          "name": "Shuhan Tan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:09:10.881Z",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec7",
          "name": "Kairan Dou",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec8",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "6833f7847e0c637c71de0ec9",
          "name": "Philipp Krähenbühl",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:45.000Z",
      "submittedOnDailyAt": "2025-05-26T03:40:36.660Z",
      "title": "インタラクティブな視覚言語行動モデルの後学習",
      "submittedOnDailyBy": {
        "_id": "64b64debeb9a833e08d079fd",
        "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
        "isPro": false,
        "fullname": "Shuhan Tan",
        "user": "tanshh97",
        "type": "user"
      },
      "summary": "RIPT-VLAは、簡単でスケーラブルな強化学習に基づく相互作用的な後学習パラダイムです。これは、稀疏な二値成功報酬をだけによって学習されたプレトレーニングビジョンラングアウェイ（Vision-Language-Action）モデルを微調適します。現在のVLAトレーニングパイプラインは、オフラインエクスプERTデモンストレーションデータとチェーンニング学習により、新しいタスクと環境に対応できることが限られています。RIPT-VLAは、動的なロールアウトサンプリングとleave-one-out優位評価に基づく安定したポリシー最適化アルゴリズムを使用して、この問題を解決しています。\n\nRIPT-VLAは以下の特徴を持ちます。最初に、多様なVLAモデルに適用可能で、軽量モデルQueSTの性能を21.2%向上させ、7B OpenVLA-OFTモデルの成功率を前所未有の97.5%に達します。第二に、計算的にエフカシャンでデータエフカシャンです：一つのデモンストレーションだけで、RIPT-VLAは無効なSFTモデル（4%）を15イテレーションで97%の成功率で動かします。さらに、RIPT-VLAが学習したポリシーは、異なるタスクとシナリオに広範囲的に一般化し、初期状態コンテキストに対して強健です。これらの結果は、RIPT-VLAが最小限のサブプロバイションで後学習ビジョンラングアウェイモデルを実用的かつ効果的に学習するパラダイムとしての重要性を明らかにしています。",
      "upvotes": 1,
      "discussionId": "6833f7857e0c637c71de0f07",
      "projectPage": "https://ariostgx.github.io/ript_vla/",
      "githubRepo": "https://github.com/Ariostgx/ript-vla",
      "ai_summary": "RIPT-VLA is a reinforcement learning-based interactive post-training paradigm that enhances pretrained Vision-Language-Action models using sparse binary success rewards, improving adaptability and generalization.",
      "ai_keywords": [
        "reinforcement-learning-based",
        "interactive post-training",
        "Vision-Language-Action (VLA) models",
        "sparse binary success rewards",
        "offline expert demonstration",
        "supervised imitation",
        "dynamic rollout sampling",
        "leave-one-out advantage estimation",
        "policy optimization",
        "lightweight QueST model",
        "OpenVLA-OFT model",
        "success rate",
        "computational efficiency",
        "data-efficient",
        "policy learned",
        "generalization",
        "initial state context"
      ]
    },
    "publishedAt": "2025-05-22T13:59:45.000Z",
    "title": "Interactive Post-Training for Vision-Language-Action Models",
    "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17016.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b64debeb9a833e08d079fd",
      "avatarUrl": "/avatars/62ad6f5a8c1b69252e855ef26cc4e7c2.svg",
      "fullname": "Shuhan Tan",
      "name": "tanshh97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16293",
      "authors": [
        {
          "_id": "683400b5231225ee202c20b7",
          "user": {
            "_id": "645c26d423ed9b7788d5e24b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
            "isPro": false,
            "fullname": "Rishabh Maheshwary",
            "user": "rmahesh",
            "type": "user"
          },
          "name": "Rishabh Maheshwary",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:08:47.558Z",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20b8",
          "name": "Masoud Hashemi",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20b9",
          "name": "Khyati Mahajan",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20ba",
          "name": "Shiva Krishna Reddy Malay",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bb",
          "name": "Sai Rajeswar",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bc",
          "name": "Sathwik Tejaswi Madhusudhan",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20bd",
          "name": "Spandana Gella",
          "hidden": false
        },
        {
          "_id": "683400b5231225ee202c20be",
          "name": "Vikas Yadav",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T06:45:05.000Z",
      "submittedOnDailyAt": "2025-05-26T04:22:12.337Z",
      "title": "LLMの理由論理を動的なノート書きによって強化した複雑なQAの解決策",
      "submittedOnDailyBy": {
        "_id": "645c26d423ed9b7788d5e24b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
        "isPro": false,
        "fullname": "Rishabh Maheshwary",
        "user": "rmahesh",
        "type": "user"
      },
      "summary": "Iterative RAG for multi-hop question answering faces challenges with lengthy contexts and the buildup of irrelevant information. This hinders a model's capacity to process and reason over retrieved content and limits performance. While recent methods focus on compressing retrieved information, they are either restricted to single-round RAG, require finetuning or lack scalability in iterative RAG. To address these challenges, we propose Notes Writing, a method that generates concise and relevant notes from retrieved documents at each step, thereby reducing noise and retaining only essential information. This indirectly increases the effective context length of Large Language Models (LLMs), enabling them to reason and plan more effectively while processing larger volumes of input text. Notes Writing is framework agnostic and can be integrated with different iterative RAG methods. We demonstrate its effectiveness with three iterative RAG methods, across two models and four evaluation datasets. Notes writing yields an average improvement of 15.6 percentage points overall, with minimal increase in output tokens.",
      "upvotes": 1,
      "discussionId": "683400b6231225ee202c20e3",
      "ai_summary": "Notes Writing enhances iterative RAG by generating concise notes at each step, improving reasoning and performance while minimizing output increase.",
      "ai_keywords": [
        "Iterative RAG",
        "multi-hop question answering",
        "context length",
        "irrelevant information",
        "dimensionality reduction",
        "Notes Writing",
        "Large Language Models",
        "LLMs",
        "framework agnostic",
        "evaluation datasets"
      ]
    },
    "publishedAt": "2025-05-22T02:45:05.000Z",
    "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA",
    "summary": "Iterative RAG for multi-hop question answering faces challenges with lengthy\ncontexts and the buildup of irrelevant information. This hinders a model's\ncapacity to process and reason over retrieved content and limits performance.\nWhile recent methods focus on compressing retrieved information, they are\neither restricted to single-round RAG, require finetuning or lack scalability\nin iterative RAG. To address these challenges, we propose Notes Writing, a\nmethod that generates concise and relevant notes from retrieved documents at\neach step, thereby reducing noise and retaining only essential information.\nThis indirectly increases the effective context length of Large Language Models\n(LLMs), enabling them to reason and plan more effectively while processing\nlarger volumes of input text. Notes Writing is framework agnostic and can be\nintegrated with different iterative RAG methods. We demonstrate its\neffectiveness with three iterative RAG methods, across two models and four\nevaluation datasets. Notes writing yields an average improvement of 15.6\npercentage points overall, with minimal increase in output tokens.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16293.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645c26d423ed9b7788d5e24b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/cZMUluWpYUlSLcn6yoC7c.jpeg",
      "fullname": "Rishabh Maheshwary",
      "name": "rmahesh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16022",
      "authors": [
        {
          "_id": "68342cb2924393051af84722",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84723",
          "name": "Siya Qi",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84724",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84725",
          "name": "Chen Qian",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84726",
          "name": "Yali Du",
          "hidden": false
        },
        {
          "_id": "68342cb2924393051af84727",
          "name": "Yulan He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T21:12:35.000Z",
      "submittedOnDailyAt": "2025-05-26T07:38:18.450Z",
      "title": "NOVER: 無評価者付きの強化学習による言語モデルのインシチュアティングトレーニング",
      "submittedOnDailyBy": {
        "_id": "66e2932e5c100c12aa2def39",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FiQ5Fap-qVqnXeULGPYs6.png",
        "isPro": false,
        "fullname": "weiliu",
        "user": "thinkwee",
        "type": "user"
      },
      "summary": "最近の進展で、DeepSeek R1-Zeroなどの機能は、報酬の計算が言語モデルの出力の最終回答部分に基づく報酬学習パラダイムの効果性を強調しています。これらの方法は、外部のバリデーターを基に報酬を計算するため、数学やコーディングなどのバリデーターが容易に得られる領域に限定されています。報酬モデルは、高品質のデータの必要性と訓練費用の高さにより、これらの問題を解決することが難しいです。本論文では、外部のバリデーターを必要としない一般的な報酬学習フレームワーク、NOVER（NO-VERifier Reinforcement Learning）を提案します。NOVERは、標準的な超フィニングデータを必要とし、報酬学習を行うための外部のバリデーターが必要なくなります。NOVERは、広範囲の文から文への問題解決タスクでの報酬学習を可能にし、DeepSeek R1 671Bのような大規模な理由モデルからの同サイズのモデルを7.7%以上の改善率で上回ります。また、NOVERの柔軟性は、大規模な言語モデルの最適化の新しい可能性を提供し、逆報酬学習などの新しい可能性を開拓します。",
      "upvotes": 1,
      "discussionId": "68342cb3924393051af8476b",
      "githubRepo": "https://github.com/thinkwee/NOVER",
      "ai_summary": "NOVER, a reinforcement learning framework that eliminates the need for external verifiers, enhances language model performance across text-to-text tasks.",
      "ai_keywords": [
        "incentive training",
        "reinforcement learning",
        "NOVER",
        "NO-VERifier Reinforcement Learning",
        "DeepSeek R1-Zero",
        "DeepSeek R1 671B",
        "inverse incentive training"
      ]
    },
    "publishedAt": "2025-05-21T17:12:35.000Z",
    "title": "NOVER: Incentive Training for Language Models via Verifier-Free\n  Reinforcement Learning",
    "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e2932e5c100c12aa2def39",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FiQ5Fap-qVqnXeULGPYs6.png",
      "fullname": "weiliu",
      "name": "thinkwee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15805",
      "authors": [
        {
          "_id": "682eeb06720821973d643576",
          "user": {
            "_id": "647c4a2692182942d7c2e698",
            "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
            "isPro": false,
            "fullname": "HWANCHANG",
            "user": "HwanChang0106",
            "type": "user"
          },
          "name": "Hwan Chang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:18:42.752Z",
          "hidden": false
        },
        {
          "_id": "682eeb06720821973d643577",
          "name": "Yumin Kim",
          "hidden": false
        },
        {
          "_id": "682eeb06720821973d643578",
          "name": "Yonghyun Jun",
          "hidden": false
        },
        {
          "_id": "682eeb06720821973d643579",
          "name": "Hwanhee Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:58:11.000Z",
      "submittedOnDailyAt": "2025-05-26T07:18:19.930Z",
      "title": "セキュリティを守れ！ 大規模な言語モデルコンテキストでのセキュリティポリシーの保存に対する問答タイプの間接攻撃に対するベンチマーク",
      "submittedOnDailyBy": {
        "_id": "647c4a2692182942d7c2e698",
        "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
        "isPro": false,
        "fullname": "HWANCHANG",
        "user": "HwanChang0106",
        "type": "user"
      },
      "summary": "LLMが企業や政府などの敏銳な領域で日々拡大している中、ユーザー定義されたセキュリティポリシーをコンテキスト内で遵守させることが重要であり、情報の非公開化に関しては特に重要です。先行のLLM研究は一般的な安全性と社会的に敏感なデータに焦点を当ててきましたが、攻撃に対するコンテキスト的なセキュリティの保存に関する大規模なベンチマークは欠如していました。これに対して、私たちは新しい大規模なベンチマークデータセット、CoPrivaを紹介し、問答におけるコンテキスト的な非公開ポリシーに従うLLMの従順性を評価します。実際的なコンテキストからのデータセットであり、明記されたポリシーとクエリを含むことで、直接的や間接的な攻撃に対する禁止された情報を探求するものです。私たちのベンチマークで10つのLLMを評価し、重要な脆弱性を明らかにしました：多数のモデルはユーザー定義されたポリシーを破り、敏感な情報を漏れ出しています。この失敗は特に間接的な攻撃に対して特に厳しいです、現在のLLMの安全性に関する敏感なアプリケーションにおける重要な欠点を明らかにしています。私たちの分析により、モデルはクエリの正しい答えを特定することができることを示しますが、生成の際にポリシーの制約を取り入れることが難しいことがわかります。対照的に、明示的にプロンプトされた場合には出力を修正する部分の能力を示しています。この見つかりは、コンテキスト的なセキュリティを確保するためにより強固な方法が急発的に必要とすることを強調しています。",
      "upvotes": 1,
      "discussionId": "682eeb07720821973d6435ec",
      "githubRepo": "https://github.com/hwanchang00/CoPriva",
      "ai_summary": "LLMs frequently violate contextual security policies by leaking sensitive information, particularly under indirect attacks, indicating a critical gap in current safety mechanisms.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "security policies",
        "information non-disclosure",
        "CoPriva",
        "contextual security preservation",
        "question answering",
        "explicit policies",
        "indirect attacks",
        "virus",
        "policy constraints",
        "output revision"
      ]
    },
    "publishedAt": "2025-05-21T13:58:11.000Z",
    "title": "Keep Security! Benchmarking Security Policy Preservation in Large\n  Language Model Contexts Against Indirect Attacks in Question Answering",
    "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15805.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647c4a2692182942d7c2e698",
      "avatarUrl": "/avatars/bcddf5fe49aa092a2645f70812108348.svg",
      "fullname": "HWANCHANG",
      "name": "HwanChang0106",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12891",
      "authors": [
        {
          "_id": "683059e8e2f446ed653e8512",
          "name": "Shaohang Wei",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8513",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8514",
          "user": {
            "_id": "6447ca6ca478b20f1755b294",
            "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
            "isPro": false,
            "fullname": "Feifan Song",
            "user": "songff",
            "type": "user"
          },
          "name": "Feifan Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:14:11.572Z",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8515",
          "name": "Wen Luo",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8516",
          "name": "Tianyi Zhuang",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8517",
          "name": "Haochen Tan",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8518",
          "name": "Zhijiang Guo",
          "hidden": false
        },
        {
          "_id": "683059e8e2f446ed653e8519",
          "name": "Houfeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T09:22:02.000Z",
      "submittedOnDailyAt": "2025-05-26T03:54:28.256Z",
      "title": "TIME: リアルウォールシナリオでのLLMsの時間論理の多レベルベンチマーク",
      "submittedOnDailyBy": {
        "_id": "6447ca6ca478b20f1755b294",
        "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
        "isPro": false,
        "fullname": "Feifan Song",
        "user": "songff",
        "type": "user"
      },
      "summary": "時間論理は、大規模言語モデル（LLMs）がリアルウォールを理解するために重要である。しかし、現在の研究は、時間論理におけるリアルウォールの課題を間違えている。それは、(1) 強力な時間情報、(2) 迅速に変化するイベントダイナミクス、(3) 複雑な時間関係性の社会的相互作用である。この隙を埋めるために、私たちは、リアルウォールのスケーラーベンチマークであるTIMEを提案しています。TIMEは、38,522個のQAペアを含み、3レベルに分けられ、11つの細かいサブタスクを持っています。このベンチマークは、3つのサブデータセットを含み、異なるリアルウォールの課題を反映しています：TIME-Wiki、TIME-News、TIME-Dial。私たちは、理由論モデルと非理由論モデルに対して拡張的な実験を行い、時間論理の性能を詳細に分析し、時間論理能力に対するテストタイムスケーリングの影響をまとめました。また、私たちは、未来の研究と標準化された評価を促進するために、TIME-Liteをリリースしています。コードは、https://github.com/sylvain-wei/TIME にアクセス可能です。データセットは、https://huggingface.co/datasets/SylvainWei/TIME にアクセス可能です。",
      "upvotes": 1,
      "discussionId": "683059eae2f446ed653e85d7",
      "ai_summary": "A benchmark called TIME assesses temporal reasoning in LLMs across varied real-world challenges, including intensive temporal information, fast-changing event dynamics, and complex social interactions, and evaluates the impact of test-time scaling.",
      "ai_keywords": [
        "Temporal reasoning",
        "Large Language Models (LLMs)",
        "QA pairs",
        "benchmark",
        "TIME-Wiki",
        "TIME-News",
        "TIME-Dial",
        "reasoning models",
        "non-reasoning models",
        "TIME-Lite"
      ]
    },
    "publishedAt": "2025-05-19T05:22:02.000Z",
    "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in\n  Real-World Scenarios",
    "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12891.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6447ca6ca478b20f1755b294",
      "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
      "fullname": "Feifan Song",
      "name": "songff",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16056",
      "authors": [
        {
          "_id": "6830894db51948863e05b68c",
          "user": {
            "_id": "64bfa1401d40292dd32f93d7",
            "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
            "isPro": false,
            "fullname": "Leo Liang",
            "user": "ljcleo",
            "type": "user"
          },
          "name": "Jingcong Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:13:21.361Z",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b68d",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b68e",
          "name": "Miren Tian",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b68f",
          "name": "Yitong Li",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b690",
          "name": "Duyu Tang",
          "hidden": false
        },
        {
          "_id": "6830894db51948863e05b691",
          "name": "Zhongyu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T22:13:09.000Z",
      "submittedOnDailyAt": "2025-05-26T07:53:13.385Z",
      "title": "全てのモデルは専門家のオフラッディングに適していません：Mixture-of-Expertモデルのローカルルーティングの一貫性",
      "submittedOnDailyBy": {
        "_id": "64bfa1401d40292dd32f93d7",
        "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
        "isPro": false,
        "fullname": "Leo Liang",
        "user": "ljcleo",
        "type": "user"
      },
      "summary": "Mixture-of-Experts (MoE)は、推論時にスパースに活性化されるエキスパートを用いて、大規模な言語モデル（LLMs）の効率的なスケーリングを可能にします。メモリ制限のあるデバイスでMoEモデルを有効に採用するために、複数のシステムは*エキスパートオフローディング*を導入し、高速メモリにエキスパートの一部をキャッシュし、残りはCPUで実行するか、調達されたらロードするように設計します。一方、エキスパートの活性化の局所性（連続したトークンが類似したエキスパートを活性化する）を利用する研究もありますが、この**局所路由の一致性**はモデルごとに異なり、まだ調査が不足しています。本論文では、MoEモデルの局所路由の一致性を評価するために2つのメトリックを提案します：1. **Segment Routing Best Performance (SRP)**は、固定グループのエキスパートがトークンのセグメントの需要をどれだけに応えるかを評価します。2. **Segment Cache Best Hit Rate (SCH)**は、与えられたキャッシュサイズ制限下で最適なセグメントレベルのハッチ率を評価します。20つのエフェクティブサイズと構造が異なるMoE LLMsを分析し、それらのモデルが各層でMoEを適用し、共有エキスパートを使用しないものが最高の局所路由の一致性を示していることを発見しました。また、領域専門のエキスパートが路由の一致性に比較的より多くの貢献を与え、ほとんどのモデルは活性エキスパートの2倍程度のキャッシュサイズでキャッシュの効果と効率をバランスすることができることを示しました。これらの発見は、推論速度を犠牲にしないようにメモリ効率的なMoEの設計と採用につながります。本論文では、実験の再現に使用するコードを公開します（https://github.com/ljcleo/moe-lrc）。",
      "upvotes": 0,
      "discussionId": "6830894eb51948863e05b6e8",
      "ai_summary": "MoE models achieve efficient scaling in LLMs with expert offloading, emphasizing the importance of local routing consistency and cache effectiveness.",
      "ai_keywords": [
        "Mixture-of-Experts (MoE)",
        "large language models (LLMs)",
        "expert offloading",
        "fast memory",
        "slow memory",
        "local routing consistency",
        "Segment Routing Best Performance (SRP)",
        "Segment Cache Best Hit Rate (SCH)",
        "domain-specialized experts",
        "vocabulary-specialized experts"
      ]
    },
    "publishedAt": "2025-05-21T18:13:09.000Z",
    "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
    "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16056.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bfa1401d40292dd32f93d7",
      "avatarUrl": "/avatars/39a3d1772e0bc54f9bb2db7ce7047784.svg",
      "fullname": "Leo Liang",
      "name": "ljcleo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11881",
      "authors": [
        {
          "_id": "6833ebdb142b0e50399413d3",
          "name": "Giyeong Oh",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d4",
          "name": "Woohyun Cho",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d5",
          "name": "Siyeol Kim",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d6",
          "name": "Suhwan Choi",
          "hidden": false
        },
        {
          "_id": "6833ebdb142b0e50399413d7",
          "name": "Younjae Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-17T07:16:11.000Z",
      "submittedOnDailyAt": "2025-05-26T02:52:51.028Z",
      "title": "リターンした残差コネクション：安定したおおきな深いネットワークの効率的な正規化更新",
      "submittedOnDailyBy": {
        "_id": "63d93667255ef6add20f9272",
        "avatarUrl": "/avatars/99a3aeadcc81ef85164cdfb6ab186b17.svg",
        "isPro": false,
        "fullname": "Giyeong Oh",
        "user": "BootsofLagrangian",
        "type": "user"
      },
      "summary": "残差コネクションは深いニューラルネットワークにとって重要であり、ゲインの消失を軽減して深さを増やすことができます。しかし、通常の残差更新では、モジュールの出力が直接入力ストリームに加えられます。これは、既存のストリームの方向を主に強化または調節する更新が発生し、モジュールの学習能力を完全に利用しない可能性があります。本稿では、正交残差更新を導入します：モジュールの出力を入力ストリームに対して分解し、このストリームに正交した成分だけを加えます。この設計は、モジュールが新しい表現方向を主に提供することを促し、豊富な特徴学習を促進し、より効率的な訓練を推進することを目指しています。正交更新ステラテジーが様々なアーキテクチャ（ResNetV2、Vision Transformers）とデータセット（CIFARs、TinyImageNet、ImageNet-1k）で一般化精度と訓練安定性を向上させることを示し、例えば、ViT-BのImageNet-1kでは、+4.3%pのtop-1精度の向上を実現しました。",
      "upvotes": 0,
      "discussionId": "6833ebdc142b0e5039941420",
      "ai_summary": "Orthogonal Residual Updates enhance feature learning and training stability by decomposing module outputs to contribute primarily novel features.",
      "ai_keywords": [
        "residual connections",
        "vanishing gradients",
        "orthogonal update",
        "ResNetV2",
        "Vision Transformers",
        "CIFARs",
        "TinyImageNet",
        "ImageNet-1k",
        "top-1 accuracy"
      ]
    },
    "publishedAt": "2025-05-17T03:16:11.000Z",
    "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and\n  Efficient Deep Networks",
    "summary": "Residual connections are pivotal for deep neural networks, enabling greater\ndepth by mitigating vanishing gradients. However, in standard residual updates,\nthe module's output is directly added to the input stream. This can lead to\nupdates that predominantly reinforce or modulate the existing stream direction,\npotentially underutilizing the module's capacity for learning entirely novel\nfeatures. In this work, we introduce Orthogonal Residual Update: we decompose\nthe module's output relative to the input stream and add only the component\northogonal to this stream. This design aims to guide modules to contribute\nprimarily new representational directions, fostering richer feature learning\nwhile promoting more efficient training. We demonstrate that our orthogonal\nupdate strategy improves generalization accuracy and training stability across\ndiverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,\nTinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy\ngain for ViT-B on ImageNet-1k.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11881.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d93667255ef6add20f9272",
      "avatarUrl": "/avatars/99a3aeadcc81ef85164cdfb6ab186b17.svg",
      "fullname": "Giyeong Oh",
      "name": "BootsofLagrangian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]