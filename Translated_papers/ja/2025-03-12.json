[
  {
    "paper": {
      "id": "2503.07920",
      "authors": [
        {
          "_id": "67d0f9c95f0fcc0c38902b8e",
          "name": "Samuel Cahyawijaya",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b8f",
          "name": "Holy Lovenia",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b90",
          "name": "Joel Ruben Antony Moniz",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b91",
          "name": "Tack Hwa Wong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b92",
          "name": "Mohammad Rifqi Farhansyah",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b93",
          "name": "Thant Thiri Maung",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b94",
          "name": "Frederikus Hudi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b95",
          "name": "David Anugraha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b96",
          "user": {
            "_id": "63ddfced5ea8577c8d5fb421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677144169806-63ddfced5ea8577c8d5fb421.jpeg",
            "isPro": false,
            "fullname": "Muhammad Ravi Shulthan Habibi",
            "user": "muhammadravi251001",
            "type": "user"
          },
          "name": "Muhammad Ravi Shulthan Habibi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:20.672Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b97",
          "name": "Muhammad Reza Qorib",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b98",
          "name": "Amit Agarwal",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b99",
          "name": "Joseph Marvin Imperial",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9a",
          "name": "Hitesh Laxmichand Patel",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9b",
          "user": {
            "_id": "67d1039a3e0dca11407f9460",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tN5K_Gc8oAlw0ADYuyc1s.png",
            "isPro": false,
            "fullname": "Vicky Feliren",
            "user": "feliren",
            "type": "user"
          },
          "name": "Vicky Feliren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:30.804Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9c",
          "name": "Bahrul Ilmi Nasution",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9d",
          "user": {
            "_id": "67559e52860bd4d8f4e9beeb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/I_SNcfwTifgHtL9NFLLli.jpeg",
            "isPro": false,
            "fullname": "Manuel Antonio Rufino",
            "user": "antonrufino",
            "type": "user"
          },
          "name": "Manuel Antonio Rufino",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:33.476Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9e",
          "name": "Genta Indra Winata",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9f",
          "name": "Rian Adam Rajagede",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba0",
          "name": "Carlos Rafael Catalan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba1",
          "name": "Mohamed Fazli Imam",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba2",
          "name": "Priyaranjan Pattnayak",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba3",
          "name": "Salsabila Zahirah Pranida",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba4",
          "name": "Kevin Pratama",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba5",
          "name": "Yeshil Bangera",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba6",
          "user": {
            "_id": "66d03a984505b8d635183aaa",
            "avatarUrl": "/avatars/0eab10dfad243d9dc19318b0f88de496.svg",
            "isPro": false,
            "fullname": "Adisai Na-Thalang",
            "user": "ensmart72",
            "type": "user"
          },
          "name": "Adisai Na-Thalang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:50:17.122Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba7",
          "name": "Patricia Nicole Monderin",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba8",
          "name": "Yueqi Song",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba9",
          "name": "Christian Simon",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902baa",
          "name": "Lynnette Hui Xian Ng",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bab",
          "name": "Richardy Lobo' Sapan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bac",
          "name": "Taki Hasan Rafi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bad",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bae",
          "name": "Supryadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902baf",
          "name": "Kanyakorn Veerakanjana",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb0",
          "name": "Piyalitt Ittichaiwong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb1",
          "name": "Matthew Theodore Roque",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb2",
          "name": "Karissa Vincentio",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb3",
          "name": "Takdanai Kreangphet",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb4",
          "user": {
            "_id": "631a4855300a072a8da70abd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631a4855300a072a8da70abd/jRnzdW5JBjICYKCmkUFI-.jpeg",
            "isPro": false,
            "fullname": "phakphum artkaew",
            "user": "pakphum",
            "type": "user"
          },
          "name": "Phakphum Artkaew",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:41.811Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb5",
          "name": "Kadek Hendrawan Palgunadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb6",
          "name": "Yanzhi Yu",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb7",
          "name": "Rochana Prih Hastuti",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb8",
          "name": "William Nixon",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb9",
          "name": "Mithil Bangera",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bba",
          "name": "Adrian Xuan Wei Lim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbb",
          "user": {
            "_id": "64f2e3b87244601d8f4365cf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f2e3b87244601d8f4365cf/QHKB8DOMBoKSXgMo6nY6z.jpeg",
            "isPro": false,
            "fullname": "Aye Hninn Khine",
            "user": "ayehninnkhine",
            "type": "user"
          },
          "name": "Aye Hninn Khine",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:27.900Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbc",
          "name": "Hanif Muhammad Zhafran",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbd",
          "name": "Teddy Ferdinan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbe",
          "name": "Audra Aurora Izzani",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbf",
          "name": "Ayushman Singh",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc0",
          "name": "Evan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc1",
          "name": "Jauza Akbar Krito",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc2",
          "name": "Michael Anugraha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc3",
          "name": "Fenal Ashokbhai Ilasariya",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc4",
          "name": "Haochen Li",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc5",
          "name": "John Amadeo Daniswara",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc6",
          "name": "Filbert Aurelian Tjiaranata",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc7",
          "name": "Eryawan Presma Yulianrifat",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc8",
          "name": "Can Udomcharoenchaikit",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc9",
          "name": "Fadil Risdian Ansori",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bca",
          "name": "Mahardika Krisna Ihsani",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcb",
          "name": "Giang Nguyen",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcc",
          "name": "Anab Maulana Barik",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcd",
          "name": "Dan John Velasco",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bce",
          "name": "Rifo Ahmad Genadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcf",
          "name": "Saptarshi Saha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd0",
          "user": {
            "_id": "66a31819b839c8994e5c3815",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a31819b839c8994e5c3815/ARVZtfxJYyGvZ0zHyaaBP.png",
            "isPro": false,
            "fullname": "Chengwei Wei",
            "user": "amao0o0",
            "type": "user"
          },
          "name": "Chengwei Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:39.101Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd1",
          "name": "Isaiah Flores",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd2",
          "name": "Kenneth Ko Han Chen",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd3",
          "name": "Anjela Gail Santos",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd4",
          "name": "Wan Shen Lim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd5",
          "name": "Kaung Si Phyo",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd6",
          "name": "Tim Santos",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd7",
          "name": "Meisyarah Dwiastuti",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd8",
          "name": "Jiayun Luo",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd9",
          "name": "Jan Christian Blaise Cruz",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bda",
          "name": "Ming Shan Hee",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdb",
          "name": "Ikhlasul Akmal Hanif",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdc",
          "name": "M. Alif Al Hakim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdd",
          "name": "Muhammad Rizky Sya'ban",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bde",
          "name": "Kun Kerdthaisong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdf",
          "name": "Lester James V. Miranda",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be0",
          "name": "Fajri Koto",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be1",
          "name": "Tirana Noor Fatyanosa",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be2",
          "name": "Alham Fikri Aji",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be3",
          "name": "Jostin Jerico Rosal",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be4",
          "name": "Jun Kevin",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be5",
          "user": {
            "_id": "640ead243830fd441c2e9838",
            "avatarUrl": "/avatars/4083942ce6b432a4cfb3524f72bcffb0.svg",
            "isPro": false,
            "fullname": "Robert Wijaya",
            "user": "wijayarobert",
            "type": "user"
          },
          "name": "Robert Wijaya",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:17.433Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be6",
          "name": "Onno P. Kampman",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be7",
          "name": "Ruochen Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be8",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:36.440Z",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be9",
          "name": "Peerat Limkonchotiwat",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T23:54:52.000Z",
      "title": "クラウドソース、クロール、または生成？ SEA-VL、東南アジアのマルチカルチャルなビジョン言語データセットを作成する",
      "summary": "東南アジア（SEA）は、異なる言語と文化の多様性を特徴としていますが、この地域の視覚言語（VL）研究には明らかに代表的な存在ではありません。これは、AIモデルがSEAの文化の微妙なニュアンスを捉えずに失敗することを招きます。この空間を填ぐために、SEA-VLというオープンソースイニシアティブを提案しています。これは、SEA語言に関連した高品質データの開発を目的としています。SEA国からの貢献者を含め、SEA-VLは文化の関連性と多様性を確保し、VL研究における代表的性の低い言語のより大きな広がりを促進します。クラウドソーシングよりもコストと時間のコストが低く、文化の関連性は約85%に達します。しかし、生成的視覚モデルの進歩にもかかわらず、合成画像はSEA文化を正確に反映することができないことが明らかです。生成された画像は、地域の微妙な伝統と文化のコンテキストを反映することができません。SEA-VLでは、128万枚のSEA文化に関連した画像を集め、それらの他のデータセットよりも50倍以上のサイズを持つことを目指しています。SEA-VLでは、SEAの代表性の欠如を埋め、多様な文化を真実的に表現するより広がるAIシステムの開発を促進しようとしています。",
      "upvotes": 44,
      "discussionId": "67d0f9cd5f0fcc0c38902cdf",
      "ai_keywords": [
        "vision-language (VL) research",
        "cultural relevance",
        "crowdsourcing",
        "image crawling",
        "image generation",
        "generative vision models",
        "synthesized images",
        "datasets"
      ]
    },
    "publishedAt": "2025-03-10T19:54:52.000Z",
    "title": "Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia",
    "summary": "Southeast Asia (SEA) is a region of extraordinary linguistic and cultural\ndiversity, yet it remains significantly underrepresented in vision-language\n(VL) research. This often results in artificial intelligence (AI) models that\nfail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an\nopen-source initiative dedicated to developing high-quality, culturally\nrelevant data for SEA languages. By involving contributors from SEA countries,\nSEA-VL aims to ensure better cultural relevance and diversity, fostering\ngreater inclusivity of underrepresented languages in VL research. Beyond\ncrowdsourcing, our initiative goes one step further in the exploration of the\nautomatic collection of culturally relevant images through crawling and image\ngeneration. First, we find that image crawling achieves approximately ~85%\ncultural relevance while being more cost- and time-efficient than\ncrowdsourcing. Second, despite the substantial progress in generative vision\nmodels, synthetic images remain unreliable in accurately reflecting SEA\ncultures. The generated images often fail to reflect the nuanced traditions and\ncultural contexts of the region. Collectively, we gather 1.28M SEA\nculturally-relevant images, more than 50 times larger than other existing\ndatasets. Through SEA-VL, we aim to bridge the representation gap in SEA,\nfostering the development of more inclusive AI systems that authentically\nrepresent diverse cultures across SEA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07920.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07536",
      "authors": [
        {
          "_id": "67d04f248f79213c2fc0ba04",
          "name": "Yingzhe Peng",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba05",
          "name": "Gongrui Zhang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba06",
          "name": "Miaosen Zhang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba07",
          "name": "Zhiyuan You",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba08",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba09",
          "name": "Qipeng Zhu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0a",
          "name": "Kai Yang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0b",
          "name": "Xingzhong Xu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0c",
          "name": "Xin Geng",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0d",
          "name": "Xu Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:04:14.000Z",
      "title": "LMM-R1: 2段階ルールベースのRLをもとに強力な理由論能力を持つ30億単語モデルを強化する",
      "summary": "大規模多モデル（LMMs）の理由論を強化するには、視覚的認識と理性的理由論の複雑な相互作用からの特有の課題があります。特に、パラメータ数が30億の縮小されたアーキテクチャでは、アーキテクチャの制約が理由論の能力とモディバルの対位を制限します。\n\nルールベースの強化学習（RL）は文脈だけの領域では優れていますが、その多モディアル拡張版では2つの重大なバリアがあります。1. データの制限は、不明確な回答や複雑な理由論の例が少ないことによって生じます。2. モディバルの事前学習による基盤的な理由論の低下も原因です。\n\nこれらの課題を解決するために、我々は\\methodという2段階のフレームワークを提案します。これは、ルールベースのRLを多モディアル理由論に適用するための基盤的な理由論の向上（FRE）と、多モディアルの一般化訓練（MGT）を通じて行います。FREステージは、ルールベースのRLを用いた文脈だけのデータを用いて理由論の能力を強化し、その後、MGTステージではこれらの理由論の能力を多モディアル領域に一般化します。\n\nQwen2.5-VL-Instruct-3Bにおいての実験は、\\methodが多モディアルベンチャーと文脳だけベンチャーで基準に対してそれぞれ4.83%と4.5%の平均改善率を達成し、複雑なフットボールゲームタスクでは3.63%の効果を示しました。これらの結果は、文脳ベースの理由論の向上が多モディアルの一般化に効果的であることを証明し、高品質な多モディアル訓練データを必要とする高コストのパラダイムを回避するデータ効率的なパラダイムを提供しています。",
      "upvotes": 41,
      "discussionId": "67d04f268f79213c2fc0ba8b",
      "projectPage": "https://forjadeforest.github.io/LMM-R1-ProjectPage",
      "githubRepo": "https://github.com/TideDra/lmm-r1",
      "ai_keywords": [
        "Large Multimodal Models (LMMs)",
        "visual perception",
        "logical reasoning",
        "3B-parameter architectures",
        "rule-based reinforcement learning (RL)",
        "multimodal extension",
        "ambiguous answers",
        "complex reasoning examples",
        "degraded foundational reasoning",
        "multimodal pretraining",
        "Foundational Reasoning Enhancement (FRE)",
        "Multimodal Generalization Training (MGT)",
        "Qwen2.5-VL-Instruct-3B",
        "multimodal benchmarks",
        "text-only benchmarks",
        "complex Football Game tasks",
        "text-based reasoning enhancement",
        "data-efficient paradigm"
      ]
    },
    "publishedAt": "2025-03-10T13:04:14.000Z",
    "title": "LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through\n  Two-Stage Rule-Based RL",
    "summary": "Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges\nfrom the complex interplay between visual perception and logical reasoning,\nparticularly in compact 3B-parameter architectures where architectural\nconstraints limit reasoning capacity and modality alignment.\n  While rule-based reinforcement learning (RL) excels in text-only domains, its\nmultimodal extension confronts two critical barriers: (1) data limitations due\nto ambiguous answers and scarce complex reasoning examples, and (2) degraded\nfoundational reasoning induced by multimodal pretraining.\n  To address these challenges, we propose \\method, a two-stage\nframework adapting rule-based RL for multimodal reasoning through\nFoundational Reasoning Enhancement (FRE) followed by\nMultimodal Generalization Training (MGT). The FRE stage first\nstrengthens reasoning abilities using text-only data with rule-based RL, then\nthe MGT stage generalizes these reasoning capabilities to multimodal domains.\n  Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves\n4.83\\% and 4.5\\% average improvements over baselines in multimodal and\ntext-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game\ntasks. These results validate that text-based reasoning enhancement enables\neffective multimodal generalization, offering a data-efficient paradigm that\nbypasses costly high-quality multimodal training data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07536.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08638",
      "authors": [
        {
          "_id": "67d1027435066eade61549ae",
          "user": {
            "_id": "5fd6f670053c8345eddc1b68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd6f670053c8345eddc1b68/cuTsu2krRYHC6zYGD2dpQ.jpeg",
            "isPro": false,
            "fullname": "Ruibin Yuan",
            "user": "a43992899",
            "type": "user"
          },
          "name": "Ruibin Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:33.054Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549af",
          "name": "Hanfeng Lin",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b0",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b1",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-12T06:24:13.961Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b2",
          "name": "Jiahao Pan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b3",
          "name": "Yongyi Zang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b4",
          "name": "Haohe Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b5",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b6",
          "name": "Wenye Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b7",
          "user": {
            "_id": "654907a4a1faff97850c4eff",
            "avatarUrl": "/avatars/458c90151614bc7f116943b6e67d6b8a.svg",
            "isPro": false,
            "fullname": "du",
            "user": "dododododo",
            "type": "user"
          },
          "name": "Xingjian Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:36.330Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b8",
          "name": "Xinrun Du",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b9",
          "name": "Zhen Ye",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ba",
          "name": "Tianyu Zheng",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bb",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bc",
          "user": {
            "_id": "6417d9ea8f689506e7148417",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
            "isPro": false,
            "fullname": "minghao",
            "user": "Liam-Liu",
            "type": "user"
          },
          "name": "Minghao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:39.193Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bd",
          "name": "Zeyue Tian",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549be",
          "name": "Ziya Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bf",
          "name": "Liumeng Xue",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c0",
          "user": {
            "_id": "6628adb14277eae0da5eee28",
            "avatarUrl": "/avatars/6cb41b80cc5e014e455dfc2a22682e64.svg",
            "isPro": true,
            "fullname": "HKUST Audio",
            "user": "HKUST-Audio",
            "type": "user"
          },
          "name": "Xingwei Qu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-12T03:41:43.139Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c1",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c2",
          "name": "Shangda Wu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c3",
          "name": "Tianhao Shen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c4",
          "name": "Ziyang Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c5",
          "name": "Jun Zhan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c6",
          "name": "Chunhui Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c7",
          "name": "Yatian Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c8",
          "name": "Xiaowei Chi",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c9",
          "name": "Xinyue Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ca",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cb",
          "name": "Xiangzhou Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cc",
          "name": "Shansong Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cd",
          "name": "Lingrui Mei",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ce",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cf",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d0",
          "name": "Jianwei Yu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d1",
          "name": "Guojian Pang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d2",
          "name": "Xu Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d3",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d4",
          "name": "Xiaohuan Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d5",
          "name": "Lijun Yu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d6",
          "name": "Emmanouil Benetos",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d7",
          "name": "Yong Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d8",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d9",
          "name": "Xie Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549da",
          "name": "Gus Xia",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549db",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549dc",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549dd",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549de",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549df",
          "name": "Xipeng Qiu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e0",
          "name": "Roger Dannenberg",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e1",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e2",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e3",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e4",
          "name": "Wei Xue",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e5",
          "name": "Xu Tan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e6",
          "name": "Yike Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:26:50.000Z",
      "title": "ユーエ：長音の音楽生成に向けた開放型基盤モデルのスケーリング",
      "summary": "We は、長文の音楽生成タスクを特に歌詞から歌曲への難しい問題を解決するために、LLaMA2アーキテクチャに基づく開放的な基盤モデルの家族「YuE」を紹介します。特に、YuEはトークン数トリリオンを超え、5分の音楽を生成することができ、同時に歌詞の一致性、コラージュな音楽構造、魅力的なボーカルメロディーを保ちます。これは、(1)トラック離脱の次トークン予測、(2)長文脈の歌詞一致性の進歩的な条件付け、(3)多タスク、多段階の事前学習プロキシを通じて実現されます。また、YuEは、ミュージック生成の事前学習技術を再設計し、多様なスタイルトランスフォーメーション（例：日本のシティーポップを英語のラップに変換しながら元の伴奏を保つ）と双方向的な生成を可能にします。詳細な評価を通じて、YuEは音楽性とボーカルの柔軟性において、一部のプロピエティーシステムを追い越すことができることを示します。また、YuEの微調節では、追加の制御と追加言語サポートを可能にします。また、生成のものとしても、YuEの学習された表現は音楽理解タスクでも優れた性能を示すことができ、MARBLEベンチマークでは最先端の方法を超えることができます。キーワード：歌詞2歌曲、歌曲生成、長文、基盤モデル、音楽生成",
      "upvotes": 39,
      "discussionId": "67d1027735066eade6154a7e",
      "ai_keywords": [
        "track-decoupled next-token prediction",
        "dense mixture signals",
        "structural progressive conditioning",
        "long-context lyrical alignment",
        "multitask, multiphase pre-training",
        "in-context learning",
        "versatile style transfer",
        "bidirectional generation",
        "musicality",
        "vocal agility",
        "tail languages",
        "music understanding tasks",
        "MARBLE benchmark"
      ]
    },
    "publishedAt": "2025-03-11T13:26:50.000Z",
    "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
    "summary": "We tackle the task of long-form music generation--particularly the\nchallenging lyrics-to-song problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08638.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08120",
      "authors": [
        {
          "_id": "67d0f5ace3c8042929eea946",
          "user": {
            "_id": "64c860d23a3f428da65ea499",
            "avatarUrl": "/avatars/f0bcc6ae7e558babe691b6bbf1059c9d.svg",
            "isPro": false,
            "fullname": "lijunzhe",
            "user": "tulvgengenr",
            "type": "user"
          },
          "name": "Junzhe Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:39:26.197Z",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea947",
          "name": "Xuerui Qiu",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea948",
          "name": "Linrui Xu",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea949",
          "name": "Liya Guo",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94a",
          "user": {
            "_id": "64daecec888b7e9c400f59b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png",
            "isPro": false,
            "fullname": "Delin Qu",
            "user": "delinqu",
            "type": "user"
          },
          "name": "Delin Qu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:39:29.349Z",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94b",
          "name": "Tingting Long",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94c",
          "name": "Chun Fan",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94d",
          "name": "Ming Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T07:34:59.000Z",
      "title": "UniF^2ace: 細分化の顔理解と生成を行う統一モノモダルモデル",
      "summary": "Unified multimodal models (UMMs) は、基盤的なコンピュータビジョン研究のパラダイムとして強力な役割を果たし、画像理解と生成において显著なポテンシャルを示しています。しかし、既存の顔ファイル領域の研究は、粗略な顔特徴量理解を中心に焦点を当て、微妙な顔特徴量の処理能力が限られ、生成能力を課題としています。これらの制限を克服するために、私たちは UniF^2ace を提案します。これは、まずに、微妙な顔理解と生成に特化された最初のUMMです。一般的に、私たちは、2つの相互に利益を得るディフュージョン手法と2レベルのミクスオブエクスプロターズアーキテクチャを用いて、自ら構築した特別なデータセットで UniF^2ace を訓練します。特に、私たちは、最初に、130K画像-テキストペアを含む大規模な顔データセット、UniF^2ace-130K を構築し、これには様々な顔特徴量を範囲に広げた100万の質問回答ペアを含みます。次に、離散ディフュージョンスコアマッチングとマスク付き生成モデルの理論的な連携を確立し、両方の証拠下限を同時に最適化し、これにより、モデルの顔の詳細を合成する能力を大幅に向上させます。最後に、トークンレベルとシーケンスレベルのミクスオブエクスプロターズを導入し、理解と生成タスクの両方に効率的な微妙な表現学習を可能にします。UniF^2ace-130K 上での拡張的な実験は、UniF^2ace が、既存のUMMsと生成モデルを超え、理解と生成タスクの両方で優れた性能を達成していることを示しています。",
      "upvotes": 23,
      "discussionId": "67d0f5b4e3c8042929eeab49",
      "ai_keywords": [
        "diffusion score matching",
        "masked generative models",
        "evidence lower bounds",
        "mixture-of-experts",
        "token-level",
        "sequence-level"
      ]
    },
    "publishedAt": "2025-03-11T03:34:59.000Z",
    "title": "UniF^2ace: Fine-grained Face Understanding and Generation\n  with Unified Multimodal Models",
    "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm in\nfoundational computer vision research, demonstrating significant potential in\nboth image understanding and generation. However, existing research in the face\ndomain primarily focuses on coarse facial attribute understanding,\nwith limited capacity to handle fine-grained facial attributes and\nwithout addressing generation capabilities. To overcome these limitations, we\npropose UniF^2ace, the first UMM tailored specifically for\nfine-grained face understanding and generation. In general, we train\nUniF^2ace on a self-constructed, specialized dataset utilizing two\nmutually beneficial diffusion techniques and a two-level mixture-of-experts\narchitecture. Specifically, we first build a large-scale facial dataset,\nUniF^2ace-130K, which contains 130K image-text pairs with one\nmillion question-answering pairs that span a wide range of facial attributes.\nSecond, we establish a theoretical connection between discrete diffusion score\nmatching and masked generative models, optimizing both evidence lower bounds\nsimultaneously, which significantly improves the model's ability to synthesize\nfacial details. Finally, we introduce both token-level and sequence-level\nmixture-of-experts, enabling efficient fine-grained representation learning for\nboth understanding and generation tasks. Extensive experiments on\nUniF^2ace-130K demonstrate that UniF^2ace outperforms\nexisting UMMs and generative models, achieving superior performance across both\nunderstanding and generation tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08120.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07703",
      "authors": [
        {
          "_id": "67d0f422a3158b8e55d3562f",
          "name": "Lixue Gong",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35630",
          "name": "Xiaoxia Hou",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35631",
          "name": "Fanshi Li",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35632",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35633",
          "name": "Xiaochen Lian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35634",
          "name": "Fei Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35635",
          "name": "Liyang Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35636",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35637",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35638",
          "name": "Yichun Shi",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35639",
          "name": "Shiqi Sun",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563a",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563b",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563c",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563d",
          "name": "Xun Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563e",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563f",
          "name": "Guofeng Wu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35640",
          "user": {
            "_id": "6381c5d63680a7cf34e08ca9",
            "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
            "isPro": false,
            "fullname": "wujie10558@gmail.com",
            "user": "wujie10",
            "type": "user"
          },
          "name": "Jie Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:40:44.088Z",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35641",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35642",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35643",
          "name": "Linjie Yang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35644",
          "name": "Zhonghua Zhai",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35645",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35646",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35647",
          "name": "Yuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35648",
          "name": "Shijia Zhao",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35649",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3564a",
          "name": "Weilin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:33.000Z",
      "title": "Seedream 2.0: シードリーム 2.0: 母語中国語英語の双語画像生成ファンダメンタルモデル",
      "summary": "Rapidなディフォーマンスモデルの進歩は、画像生成分野における驚異的な進歩を促進しています。しかし、Flux、SD3.5、Midjourneyなどの一般的なモデルは、モデルバイアス、限定的なテキストレンディング能力、中国文化のニュアンスの理解不足などの問題に直面しています。これらの制限を解決するために、発表します。Seedream 2.0は、多様な次元で優れている母国語と英語のバイリンガル画像生成ベースモデルです。これは、中国語と英語の両方でテキストプロンプトを手に入れ、バイリンガル画像生成とテキストレンディングをサポートすることを通じて、テキストプロンプトを管理します。私たちは、知識統合を促進する強力なデータシステムと、画像の説明の精度と豊富さをバランスにするキャプションシステムを開発しました。特に、Seedreamは、自発的なバイリンガル大規模言語モデルをテキストエンコーダーとして組み込み、マススターデータから直接の母国語知識を学習することを可能にします。これにより、中国語または英語で記述された文化のニュアンスと美術的表現を高精度で生成することができます。また、Glyph-Aligned ByT5は、柔軟な文字レベルのテキストレンディングを支援し、Scaled ROPEは未学習の解像度に対してよい拡張性を持ちます。多段階の後学習最適化、SFTとRLHFのイテレーションを含む、全体の能力を進化させます。拡張的な実験を通じて、Seedream 2.0は、プロンプト従順性、美術性、テキストレンディング、構造的な正確性など、複数の面で最先端の性能を達成していることを示します。また、Seedream 2.0は、人間の好みに対して最も近い出力を実現するために、多次のRLHFイテレーションで最適化されています。そして、ELOスコアによる出色な結果を示しています。また、SeedEditやそのようなインストラクションベースの画像編集モデルに変更可能で、インストラクション従順性と画像の一致性をバランスにする強力な編集能力を持つことができます。",
      "upvotes": 21,
      "discussionId": "67d0f42fa3158b8e55d358ea",
      "projectPage": "https://team.doubao.com/zh/tech/seedream",
      "ai_keywords": [
        "diffusion models",
        "Flux",
        "SD3.5",
        "Midjourney",
        "model bias",
        "Seedream 2.0",
        "bilingual image generation",
        "text prompt",
        "data system",
        "caption system",
        "bilingual large language model",
        "high-fidelity images",
        "cultural nuances",
        "aesthetic expressions",
        "Glyph-Aligned ByT5",
        "character-level text rendering",
        "Scaled ROPE",
        "multi-phase post-training optimizations",
        "SFT",
        "RLHF",
        "prompt-following",
        "structural correctness",
        "ELO score",
        "instruction-based image editing model",
        "SeedEdit"
      ]
    },
    "publishedAt": "2025-03-10T13:58:33.000Z",
    "title": "Seedream 2.0: A Native Chinese-English Bilingual Image Generation\n  Foundation Model",
    "summary": "Rapid advancement of diffusion models has catalyzed remarkable progress in\nthe field of image generation. However, prevalent models such as Flux, SD3.5\nand Midjourney, still grapple with issues like model bias, limited text\nrendering capabilities, and insufficient understanding of Chinese cultural\nnuances. To address these limitations, we present Seedream 2.0, a native\nChinese-English bilingual image generation foundation model that excels across\ndiverse dimensions, which adeptly manages text prompt in both Chinese and\nEnglish, supporting bilingual image generation and text rendering. We develop a\npowerful data system that facilitates knowledge integration, and a caption\nsystem that balances the accuracy and richness for image description.\nParticularly, Seedream is integrated with a self-developed bilingual large\nlanguage model as a text encoder, allowing it to learn native knowledge\ndirectly from massive data. This enable it to generate high-fidelity images\nwith accurate cultural nuances and aesthetic expressions described in either\nChinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible\ncharacter-level text rendering, while a Scaled ROPE generalizes well to\nuntrained resolutions. Multi-phase post-training optimizations, including SFT\nand RLHF iterations, further improve the overall capability. Through extensive\nexperimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art\nperformance across multiple aspects, including prompt-following, aesthetics,\ntext rendering, and structural correctness. Furthermore, Seedream 2.0 has been\noptimized through multiple RLHF iterations to closely align its output with\nhuman preferences, as revealed by its outstanding ELO score. In addition, it\ncan be readily adapted to an instruction-based image editing model, such as\nSeedEdit, with strong editing capability that balances instruction-following\nand image consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07703.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05978",
      "authors": [
        {
          "_id": "67d129d732b4bbfb938321a1",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a2",
          "user": {
            "_id": "66015e8aa4d296af07de538e",
            "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
            "isPro": false,
            "fullname": "Ye",
            "user": "Owen777",
            "type": "user"
          },
          "name": "Tian Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:50:12.319Z",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a3",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a4",
          "name": "Xuancheng Yang",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a5",
          "name": "Jiantong Zhao",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a6",
          "name": "Hanzhong Guo",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a7",
          "name": "Terrance Wang",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a8",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a9",
          "name": "Zeke Xie",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321aa",
          "name": "Lei Zhu",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ab",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ac",
          "name": "Michael Lingelbach",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ad",
          "name": "Daquan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T23:21:11.000Z",
      "title": "MagicInfinite: 言葉と声で無限の話しからめ映像を生成する",
      "summary": "マジックインフィニットは、ディフュージョン・トランスフォーマー（DiT）フレームワークで、歴代の肖像動画の制限を克服し、多様なキャラクタータイプで高品質な結果を提供します。これには、実写的な人間、全身のフィギュア、スタイリズドアニメキャラクターを含むものです。それには、変わりゆく顔の姿勢をサポートし、対面の視点も含むことができ、入力マスクを用いて多キャラクターシーンでの決約者の指定を精密に行うことができます。我々のアプローチは、3つの革新的な実装で挙動の主な課題を解決しています。1. 3D全注意機能とスライディングウィンドウデノイジングステラチャーを用いた無限長のビデオ生成を可能にし、時系列的な協調性と多様なキャラクタースタイルの視覚品質を維持します。2. 2段階のカレクルラーラーニングスキームを導入し、音声を口唇同期、文章を表現的な動作、参照画像を身份保持に用いることで、長期間の長いシーンでの多モーダル制御を可能にします。3. 領域別のマスクと適応的な損失関数を用いて、グローバル的な文章制御と局所的な音声ガイドをバランスづけ、特定の決約者のアニメーションをサポートします。効率化は、我々の革新的な統一ステップとcfgディスティルティション技術を通じて、基盤モデルより20倍の推論スピードアップを実現します。8枚のH100GPUで10秒で10秒の10秒の540x540pビデオを生成し、30秒で720x720pのビデオを生成することができます。質量損失なしです。新しいベンチマークでの評価は、音声口唇同期、身份保持、自然な動作の優れた性能を示します。公開的に利用可能です。https://www.hedra.com/、例はhttps://magicinfinite.github.io/。",
      "upvotes": 19,
      "discussionId": "67d129e332b4bbfb938324a0",
      "projectPage": "https://magicinfinite.github.io/",
      "ai_keywords": [
        "diffusion Transformer (DiT)",
        "3D full-attention mechanisms",
        "sliding window denoising strategy",
        "infinite video generation",
        "temporal coherence",
        "two-stage curriculum learning scheme",
        "audio for lip sync",
        "text for expressive dynamics",
        "reference images for identity preservation",
        "region-specific masks",
        "adaptive loss functions",
        "unified step and cfg distillation techniques",
        "inference speed",
        "audio-lip synchronization",
        "identity preservation",
        "motion naturalness"
      ]
    },
    "publishedAt": "2025-03-07T18:21:11.000Z",
    "title": "MagicInfinite: Generating Infinite Talking Videos with Your Words and\n  Voice",
    "summary": "We present MagicInfinite, a novel diffusion Transformer (DiT) framework that\novercomes traditional portrait animation limitations, delivering high-fidelity\nresults across diverse character types-realistic humans, full-body figures, and\nstylized anime characters. It supports varied facial poses, including\nback-facing views, and animates single or multiple characters with input masks\nfor precise speaker designation in multi-character scenes. Our approach tackles\nkey challenges with three innovations: (1) 3D full-attention mechanisms with a\nsliding window denoising strategy, enabling infinite video generation with\ntemporal coherence and visual quality across diverse character styles; (2) a\ntwo-stage curriculum learning scheme, integrating audio for lip sync, text for\nexpressive dynamics, and reference images for identity preservation, enabling\nflexible multi-modal control over long sequences; and (3) region-specific masks\nwith adaptive loss functions to balance global textual control and local audio\nguidance, supporting speaker-specific animations. Efficiency is enhanced via\nour innovative unified step and cfg distillation techniques, achieving a 20x\ninference speed boost over the basemodel: generating a 10 second 540x540p video\nin 10 seconds or 720x720p in 30 seconds on 8 H100 GPUs, without quality loss.\nEvaluations on our new benchmark demonstrate MagicInfinite's superiority in\naudio-lip synchronization, identity preservation, and motion naturalness across\ndiverse scenarios. It is publicly available at https://www.hedra.com/, with\nexamples at https://magicinfinite.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05978.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08625",
      "authors": [
        {
          "_id": "67d0fd74f8595b656f921a48",
          "user": {
            "_id": "632179745fc60c44fd91fc33",
            "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
            "isPro": false,
            "fullname": "zhumuzhi",
            "user": "Z-MU-Z",
            "type": "user"
          },
          "name": "Muzhi Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:42.495Z",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a49",
          "name": "Yuzhuo Tian",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4a",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4b",
          "name": "Chunluan Zhou",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4c",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4d",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4e",
          "name": "Ming Yang",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4f",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:08:54.000Z",
      "title": "SegAgent: マルチモデルラインモデルのピクセル理解能力を探索するために、人間のアノテーターの軌跡を模倣する",
      "summary": "マルチプリオンラインモデル（MLLMs）は、画像理解能力を示していますが、ピクセルレベルの理解には難しいという点で実用的な應用には限られています。現在の評価タスクのように、VQAと視覚ジョージングは、ピクセルレベルの理解を精確に評価するには過大です。ピクセルレベルの理解の基礎となる分割はありますが、現在の方法は、MLLMsが外部のピクセルデコーダーで解釈される隠れトークンを生成する必要があります。このアプローチは、MLLMのテキスト出力スペースを破壊し、言語能力を潜在的に損なって、機能性と拡張性を減少し、モデルの固有のピクセルレベルの理解を反映しないことがあります。\n\nそこで、私たちは、人間のマスクアノテーションタスク（HLMAT）を紹介します。これは、MLLMsが人間のアノテーターに似たように、インタラクティブな分割ツールを使用してアノテートする新しいパラダイムです。分割を多段階マルコフ決定過程としてモデリングし、HLMATは、構造的変更や隠れトークンを使用しないように、MLLMsが繰り返しにテキストベースのクリックポイントを生成し、高品質のマスクを実現することができます。このシステムで、SegAgentを開発しました。これは、人間のようなアノテーショントラジェクトにファイナルチューニングされたモデルです。このモデルは、最先端（SOTA）の方法と同等の性能を達成し、マスクの精練やアノテーションのフィルタリングなどの追加タスクをサポートします。\n\nHLMATは、MLLMsのピクセルレベルの理解を評価するプロトコルを提供し、視覚ベースの、多段階の決策タスクを導入し、MLLMsの視覚推理能力を探索することを促進します。我々の政策改善法StaRとPRMガイドドの木検索の改良は、複雑な分割タスクでのモデルの強固性を進め、MLLMsの精確な視覚認識と多段階の決策システムの未来の進歩に基盤を打ちます。",
      "upvotes": 18,
      "discussionId": "67d0fd76f8595b656f921ae8",
      "projectPage": "https://aim-uofa.github.io/SegAgent/",
      "githubRepo": "https://github.com/aim-uofa/SegAgent",
      "ai_keywords": [
        "Human-Like Mask Annotation Task (HLMAT)",
        "Markov Decision Process",
        "multi-step decision-making",
        "click points",
        "masks",
        "policy improvement method StaR",
        "PRM-guided tree search",
        "mask refinement",
        "annotation filtering",
        "fine-grained pixel understanding",
        "vision-centric task",
        "visual reasoning abilities"
      ]
    },
    "publishedAt": "2025-03-11T13:08:54.000Z",
    "title": "SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by\n  Imitating Human Annotator Trajectories",
    "summary": "While MLLMs have demonstrated adequate image understanding capabilities, they\nstill struggle with pixel-level comprehension, limiting their practical\napplications. Current evaluation tasks like VQA and visual grounding remain too\ncoarse to assess fine-grained pixel comprehension accurately. Though\nsegmentation is foundational for pixel-level understanding, existing methods\noften require MLLMs to generate implicit tokens, decoded through external pixel\ndecoders. This approach disrupts the MLLM's text output space, potentially\ncompromising language capabilities and reducing flexibility and extensibility,\nwhile failing to reflect the model's intrinsic pixel-level understanding.\n  Thus, we introduce the Human-Like Mask Annotation Task (HLMAT), a new\nparadigm where MLLMs mimic human annotators using interactive segmentation\ntools. Modeling segmentation as a multi-step Markov Decision Process, HLMAT\nenables MLLMs to iteratively generate text-based click points, achieving\nhigh-quality masks without architectural changes or implicit tokens. Through\nthis setup, we develop SegAgent, a model fine-tuned on human-like annotation\ntrajectories, which achieves performance comparable to state-of-the-art (SOTA)\nmethods and supports additional tasks like mask refinement and annotation\nfiltering.\n  HLMAT provides a protocol for assessing fine-grained pixel understanding in\nMLLMs and introduces a vision-centric, multi-step decision-making task that\nfacilitates exploration of MLLMs' visual reasoning abilities. Our adaptations\nof policy improvement method StaR and PRM-guided tree search further enhance\nmodel robustness in complex segmentation tasks, laying a foundation for future\nadvancements in fine-grained visual perception and multi-step decision-making\nfor MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08625.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07604",
      "authors": [
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b5",
          "name": "Tianhe Lin",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b6",
          "user": {
            "_id": "62d65139667051e0a29bffe7",
            "avatarUrl": "/avatars/0252aa2bcd4cf1c8e4b87e5f164b6da5.svg",
            "isPro": false,
            "fullname": "Jian Xie",
            "user": "hsaest",
            "type": "user"
          },
          "name": "Jian Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:42:36.765Z",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b7",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b8",
          "name": "Deqing Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:31.000Z",
      "title": "Transformerのインプリットリジニングは、スロートスによる推論です。",
      "summary": "測試時の計算は、言語モデルの複雑な多ステップ推理能力を向上させる新しいパラダイムとして現れています。OpenAIのo1、o3およびDeepSeekのR1の成功により明らかになっています。測試時の計算での明示的な推理に比べ、隠れた推理はより推論効率が高く、生成されるトークン数が少ないです。しかしながら、進歩的な推理能力が隠れた推理のスタイルではなぜ現れないのかは不明です。本稿では、GPT-2をカレーレットされた多ステップ数学推理データセットでシャットカットから訓練し、隠れた推理が多ステップタスクでどのように行われるかを調査する解析的な実験を行います。私たちの発見は以下のようになります：1）言語モデルは、固定パターンデータで訓練されると、ステップごとの推理を行い、領域内と領域外のテストで高い精度を達成できます。しかし、この能力は固定パターンデータでのみ現れます。2）一方、不固定パターンデータで訓練された隠れた推理能力は特定のパターンに過学習し、進展的な応用に失敗します。特に、この制限は最先端の大規模な言語モデルにも見られます。これらの発見は、言語モデルは短絡学習を通じて隠れた推理を得、類似なパターンのタスクに強い性能を獲得しながら、一般化能力を欠けていることを示しています。",
      "upvotes": 14,
      "discussionId": "67cfa4edd8cb8688d7d6d908",
      "githubRepo": "https://github.com/TianheL/LM-Implicit-Reasoning",
      "ai_keywords": [
        "test-time compute",
        "multi-step reasoning",
        "OpenAI's o1",
        "OpenAI's o3",
        "DeepSeek's R1",
        "implicit reasoning",
        "inference-efficient",
        "generated tokens",
        "explicit reasoning",
        "GPT-2",
        "multi-step mathematical reasoning dataset",
        "step-by-step reasoning",
        "in-domain tests",
        "out-of-domain tests",
        "fixed-pattern data",
        "unfixed-pattern data",
        "overfit",
        "generalization",
        "shortcut learning"
      ]
    },
    "publishedAt": "2025-03-10T13:58:31.000Z",
    "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
    "summary": "Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07604.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08605",
      "authors": [
        {
          "_id": "67d0ed0877b0c8ac3f304ef1",
          "name": "Subin Kim",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef2",
          "name": "Seoung Wug Oh",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef3",
          "name": "Jui-Hsien Wang",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef4",
          "name": "Joon-Young Lee",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef5",
          "name": "Jinwoo Shin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:43:45.000Z",
      "title": "フリーチューニングの多イベント長ビデオ生成による同期コピンサンプリング",
      "summary": "最近のテキストからビデオの拡散モデルの進展により、1つのプロンプトから高品質の短いビデオの生成が可能になりましたが、限られたデータ量と高い計算コストにより、1回のパスで実世界の長いビデオを生成するのは難しいです。これに対して、数多くの研究はチューニング無しアプローチを提案し、すでに存在するモデルを長いビデオ生成に拡張し、複数のプロンプトを使用して動的で制御可能な内容変化を許可します。しかし、これらの方法は主に隣接するフレームの平滑な遷移を確保することを焦点とし、長いシーケンスでは意味的な一致性の徐々な失われによる内容漂流により、この問題を主に解決しています。このような問題に対処するために、我々はSynchronized Coupled Sampling (SynCoS)を提案します。SynCoSは、全ビデオの拡散パスを同期させ、隣接したものも遠く離れたフレームも長距離的な一致性を確保する新しい推論フレームです。我々のアプローチは、逆向きサンプリングと最適化ベースサンプリングの2つの補間的なサンプリング戦略を組み合わせ、それぞれにおいて、隣接するフレームの平滑な遷移を確保し、全局的な一致性を強制します。しかし、これらのサンプリングを直接交換することは、拡散タライトを非対称にし、プロンプトガイドを破壊し、独立に動作するために非預想の内容変化を引き起こすことになります。これを解決するために、SynCoSは、基礎時間ステップと固定ベースノイズを用いてサンプリングを同期させ、完全なサンプリングを対応させ、拡散タライトを対応させます。拡張された実験は、SynCoSが多イベントの長いビデオ生成を大幅に改善し、平滑な遷移と上位の長距離的な一致性を実現し、定量的および定性的に以前のアプローチを上回ることを示しました。",
      "upvotes": 13,
      "discussionId": "67d0ed0b77b0c8ac3f304f7c",
      "projectPage": "https://syncos2025.github.io/",
      "githubRepo": "https://github.com/subin-kim-cv/SynCoS",
      "ai_keywords": [
        "text-to-video diffusion models",
        "high-quality short video generation",
        "long video generation",
        "tuning-free approaches",
        "multiple prompts",
        "dynamic content changes",
        "smooth transitions",
        "content drift",
        "semantic coherence",
        "Synchronized Coupled Sampling (SynCoS)",
        "denoising paths",
        "reverse sampling",
        "optimization-based sampling",
        "seamless local transitions",
        "global coherence",
        "grounded timestep",
        "fixed baseline noise",
        "multi-event long video generation",
        "long-range consistency"
      ]
    },
    "publishedAt": "2025-03-11T12:43:45.000Z",
    "title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling",
    "summary": "While recent advancements in text-to-video diffusion models enable\nhigh-quality short video generation from a single prompt, generating real-world\nlong videos in a single pass remains challenging due to limited data and high\ncomputational costs. To address this, several works propose tuning-free\napproaches, i.e., extending existing models for long video generation,\nspecifically using multiple prompts to allow for dynamic and controlled content\nchanges. However, these methods primarily focus on ensuring smooth transitions\nbetween adjacent frames, often leading to content drift and a gradual loss of\nsemantic coherence over longer sequences. To tackle such an issue, we propose\nSynchronized Coupled Sampling (SynCoS), a novel inference framework that\nsynchronizes denoising paths across the entire video, ensuring long-range\nconsistency across both adjacent and distant frames. Our approach combines two\ncomplementary sampling strategies: reverse and optimization-based sampling,\nwhich ensure seamless local transitions and enforce global coherence,\nrespectively. However, directly alternating between these samplings misaligns\ndenoising trajectories, disrupting prompt guidance and introducing unintended\ncontent changes as they operate independently. To resolve this, SynCoS\nsynchronizes them through a grounded timestep and a fixed baseline noise,\nensuring fully coupled sampling with aligned denoising paths. Extensive\nexperiments show that SynCoS significantly improves multi-event long video\ngeneration, achieving smoother transitions and superior long-range coherence,\noutperforming previous approaches both quantitatively and qualitatively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08605.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07891",
      "authors": [
        {
          "_id": "67d108c56bd6c57bab0b6f07",
          "name": "Jinhyuk Lee",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f08",
          "name": "Feiyang Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f09",
          "name": "Sahil Dua",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0a",
          "name": "Daniel Cer",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0b",
          "name": "Madhuri Shanbhogue",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0c",
          "name": "Iftekhar Naim",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0d",
          "name": "Gustavo Hernández Ábrego",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0e",
          "name": "Zhe Li",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0f",
          "name": "Kaifeng Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f10",
          "name": "Henrique Schechter Vera",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f11",
          "name": "Xiaoqi Ren",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f12",
          "name": "Shanfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f13",
          "name": "Daniel Salz",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f14",
          "name": "Michael Boratko",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f15",
          "name": "Jay Han",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f16",
          "name": "Blair Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f17",
          "name": "Shuo Huang",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f18",
          "name": "Vikram Rao",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f19",
          "name": "Paul Suganthan",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1a",
          "name": "Feng Han",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1b",
          "name": "Andreas Doumanoglou",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1c",
          "name": "Nithi Gupta",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1d",
          "name": "Fedor Moiseev",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1e",
          "name": "Cathy Yip",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1f",
          "name": "Aashi Jain",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f20",
          "name": "Simon Baumgartner",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f21",
          "name": "Shahrokh Shahi",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f22",
          "name": "Frank Palma Gomez",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f23",
          "name": "Sandeep Mariserla",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f24",
          "name": "Min Choi",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f25",
          "name": "Parashar Shah",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f26",
          "name": "Sonam Goenka",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f27",
          "name": "Ke Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f28",
          "name": "Ye Xia",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f29",
          "name": "Koert Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2a",
          "name": "Sai Meher Karthik Duddu",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2b",
          "name": "Yichang Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2c",
          "name": "Trevor Walker",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2d",
          "name": "Wenlei Zhou",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2e",
          "name": "Rakesh Ghiya",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2f",
          "name": "Zach Gleicher",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f30",
          "name": "Karan Gill",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f31",
          "name": "Zhe Dong",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f32",
          "name": "Mojtaba Seyedhosseini",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f33",
          "name": "Yunhsuan Sung",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f34",
          "name": "Raphael Hoffmann",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f35",
          "name": "Tom Duerig",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T22:16:45.000Z",
      "title": "Gemini Embedding: ジェミニの拡張性エンベディング",
      "summary": "このレポートでは、Googleの最強な大語言モデルであるGeminiを活用した最先端の埋め込みモデル「Gemini Embedding」を紹介します。Geminiの固有の多言語理解とコード理解能力を活用し、多言語と多様な文脈のテキストに対して高度に一般化可能な埋め込みを生成します。Gemini Embeddingが生成した表現は、分類、類似度、クラスタリング、ランキング、検索などの多様なダウンストリームタスクにも適用できます。Massive Multilingual Text Embedding Benchmark (MMTEB)で評価され、このベンチマークは250以上の言語にわたる100以上のタスクを含むため、Gemini Embeddingは先行の最先端モデルより大幅に優れてい、埋め込みの品質において相当な向上を示します。MMTEBの多言語、英語、コードベンチマークでの最先端の性能を達成し、我々の統一モデルは広い選択のタスクに強い能力を示し、領域専門モデルを超えます。",
      "upvotes": 12,
      "discussionId": "67d108c66bd6c57bab0b6f6e",
      "ai_keywords": [
        "Gemini Embedding",
        "large language model",
        "multilingual",
        "code understanding",
        "representations",
        "downstream tasks",
        "classification",
        "similarity",
        "clustering",
        "ranking",
        "retrieval",
        "Massive Multilingual Text Embedding Benchmark (MMTEB)",
        "embedding quality",
        "specialized domain-specific models"
      ]
    },
    "publishedAt": "2025-03-10T18:16:45.000Z",
    "title": "Gemini Embedding: Generalizable Embeddings from Gemini",
    "summary": "In this report, we introduce Gemini Embedding, a state-of-the-art embedding\nmodel leveraging the power of Gemini, Google's most capable large language\nmodel. Capitalizing on Gemini's inherent multilingual and code understanding\ncapabilities, Gemini Embedding produces highly generalizable embeddings for\ntext spanning numerous languages and textual modalities. The representations\ngenerated by Gemini Embedding can be precomputed and applied to a variety of\ndownstream tasks including classification, similarity, clustering, ranking, and\nretrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark\n(MMTEB), which includes over one hundred tasks across 250+ languages, Gemini\nEmbedding substantially outperforms prior state-of-the-art models,\ndemonstrating considerable improvements in embedding quality. Achieving\nstate-of-the-art performance across MMTEB's multilingual, English, and code\nbenchmarks, our unified model demonstrates strong capabilities across a broad\nselection of tasks and surpasses specialized domain-specific models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07891.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08619",
      "authors": [
        {
          "_id": "67d0eb9cec69694dca382208",
          "name": "Xianfeng Wu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382209",
          "name": "Yajing Bai",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220a",
          "name": "Haoze Zheng",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220b",
          "name": "Harold Haodong Chen",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220c",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220d",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220e",
          "name": "Xuran Ma",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220f",
          "name": "Wen-Jie Shu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382210",
          "name": "Xianzu Wu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382211",
          "name": "Harry Yang",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382212",
          "name": "Ser-Nam Lim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:58:02.000Z",
      "title": "LightGen: 知識転移と直接な好み最適化を通じた効率的な画像生成",
      "summary": "最近の画像生成テキストから画像への変換技術の進歩は、幅広いデータセットとパラメータ豊富なアーキテクチャによって主に進められています。これらの要求は、計算資源が豊富でない研究者や実践者のアクセス性を厳しく制限しています。本論文では、知識の収納（KD）と直接な好み最適化（DPO）を用いた効率的な訓練パラダイムを介して、画像生成モデルのLightGenを紹介します。LightGenは、多タイプ大語言モデル（MLLM）で広く採用されているデータKD技術の成功をモデルとして、最先端（SOTA）のテキストから画像への変換モデルからの知識を、パラメータ数が少ないシンプルなマスクされた自動単語順列モデル（MAR）アーキテクチャに収納します。2Mの高品質画像からなる簡略な合成データセットを使用し、これらのデータの多様性がモデルの性能を決定することにおいてデータ量よりも重要であることを示します。この戦略は、計算資源の要求を大幅に減らし、予約学習時間を数千GPU日を88GPU日に抑えることができます。また、合成データの固有の欠点を解決するために、DPO技術を組み込み、画像のファイドニティと位置精度を改善します。詳細な実験は、LightGenは計算資源の削減と計算資源制限の環境のアクセス性の拡大を伴い、SOTAモデルと同等の画像生成質量を実現したことを確認します。コードは、https://github.com/XianfengWu01/LightGenに公開されています。",
      "upvotes": 11,
      "discussionId": "67d0eba3ec69694dca3823a0",
      "ai_keywords": [
        "knowledge distillation (KD)",
        "Direct Preference Optimization (DPO)",
        "Multi-Modal Large Language Models (MLLMs)",
        "Masked Autoregressive (MAR)",
        "synthetic dataset",
        "data diversity",
        "data volume",
        "model performance",
        "computational demands",
        "pre-training time",
        "synthetic data",
        "high-frequency details",
        "spatial inaccuracies",
        "image fidelity",
        "positional accuracy"
      ]
    },
    "publishedAt": "2025-03-11T12:58:02.000Z",
    "title": "LightGen: Efficient Image Generation through Knowledge Distillation and\n  Direct Preference Optimization",
    "summary": "Recent advances in text-to-image generation have primarily relied on\nextensive datasets and parameter-heavy architectures. These requirements\nseverely limit accessibility for researchers and practitioners who lack\nsubstantial computational resources. In this paper, we introduce \\model, an\nefficient training paradigm for image generation models that uses knowledge\ndistillation (KD) and Direct Preference Optimization (DPO). Drawing inspiration\nfrom the success of data KD techniques widely adopted in Multi-Modal Large\nLanguage Models (MLLMs), LightGen distills knowledge from state-of-the-art\n(SOTA) text-to-image models into a compact Masked Autoregressive (MAR)\narchitecture with only 0.7B parameters. Using a compact synthetic dataset of\njust 2M high-quality images generated from varied captions, we demonstrate\nthat data diversity significantly outweighs data volume in determining model\nperformance. This strategy dramatically reduces computational demands and\nreduces pre-training time from potentially thousands of GPU-days to merely 88\nGPU-days. Furthermore, to address the inherent shortcomings of synthetic data,\nparticularly poor high-frequency details and spatial inaccuracies, we integrate\nthe DPO technique that refines image fidelity and positional accuracy.\nComprehensive experiments confirm that LightGen achieves image generation\nquality comparable to SOTA models while significantly reducing computational\nresources and expanding accessibility for resource-constrained environments.\nCode is available at https://github.com/XianfengWu01/LightGen",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08619.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08686",
      "authors": [
        {
          "_id": "67d0f892a189f3978638e154",
          "name": "Jialv Zou",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e155",
          "name": "Bencheng Liao",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e156",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e157",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e158",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:46.000Z",
      "title": "OmniMamba: 状態スペースモデルをもとにした効率的なユニットモデルでの多模態理解と生成",
      "summary": "最近の統合モノモーダル理解と可視化生成（またはモノモーダル生成）モデルの進展は、オーバーフローの計算複雑さと大規模な訓練データの依存関係により制限されています。私たちは、最初の線形アーキテクチャに基づくモノモーダル生成モデル「OmniMamba」を紹介します。このモデルは、統一的な次のトークン予測パラダイムを通じて、テキストと画像を共に生成します。このモデルは、Mamba-2の高い計算とメモリの効率を最大限に活用し、テキスト生成からモノモーダル生成への拡張を実現しています。現在の統一的なモデルのデータ効率問題を解決するために、私たちは2つのキーイノベーションを提案します：（1）分離されたビオカシフォリィムディーディングモデルの特定のモディュール生成をガイドする、および（2）タスクシーケンスのLoRAをペラメーター効率的な適応に対して。また、2つのタスクのデータ不均衡を軽減するために、私たちは分離された2段階の訓練戦略を導入します。これらの技術を採用して、OmniMambaは、JanusFlowとの比較で相対的に競争的な性能を収め、Show-oのデータ量の1/1000でも、データ量を2Mの画像テキストペアで訓練していることにより、ベンチマークでShow-oを超えます。特に、OmniMambaは、長シーケンス生成においてTransformerベースのコンペナントと比べて、119.2倍のスピードアップと63%のGPUメモリ削減を実現し、推論効率が出色です。コードとモデルは、https://github.com/hustvl/OmniMambaで公開されています。",
      "upvotes": 9,
      "discussionId": "67d0f894a189f3978638e1b7",
      "ai_keywords": [
        "OmniMamba",
        "linear-architecture-based multimodal generation model",
        "unified next-token prediction paradigm",
        "Mamba-2",
        "computational efficiency",
        "memory efficiency",
        "decoupled vocabularies",
        "task-specific LoRA",
        "parameter-efficient adaptation",
        "decoupled two-stage training strategy",
        "data imbalance",
        "Show-o",
        "benchmark",
        "inference efficiency",
        "Transformer-based counterparts"
      ]
    },
    "publishedAt": "2025-03-11T13:59:46.000Z",
    "title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation\n  via State Space Models",
    "summary": "Recent advancements in unified multimodal understanding and visual generation\n(or multimodal generation) models have been hindered by their quadratic\ncomputational complexity and dependence on large-scale training data. We\npresent OmniMamba, the first linear-architecture-based multimodal generation\nmodel that generates both text and images through a unified next-token\nprediction paradigm. The model fully leverages Mamba-2's high computational and\nmemory efficiency, extending its capabilities from text generation to\nmultimodal generation. To address the data inefficiency of existing unified\nmodels, we propose two key innovations: (1) decoupled vocabularies to guide\nmodality-specific generation, and (2) task-specific LoRA for\nparameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage\ntraining strategy to mitigate data imbalance between two tasks. Equipped with\nthese techniques, OmniMamba achieves competitive performance with JanusFlow\nwhile surpassing Show-o across benchmarks, despite being trained on merely 2M\nimage-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba\nstands out with outstanding inference efficiency, achieving up to a 119.2 times\nspeedup and 63% GPU memory reduction for long-sequence generation compared to\nTransformer-based counterparts. Code and models are released at\nhttps://github.com/hustvl/OmniMamba",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08686.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07860",
      "authors": [
        {
          "_id": "67d0e915d0038007e5a75178",
          "user": {
            "_id": "650871aeb44445e9b3625c7b",
            "avatarUrl": "/avatars/c35bd3e4a851389a4b6898a5a51e2219.svg",
            "isPro": false,
            "fullname": "James Burgess",
            "user": "jmhb",
            "type": "user"
          },
          "name": "James Burgess",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:41:30.547Z",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a75179",
          "user": {
            "_id": "65703fab7f50602340d23704",
            "avatarUrl": "/avatars/324c45f5fba9cd8c38a89b30427c06b4.svg",
            "isPro": false,
            "fullname": "Xiaohan Wang",
            "user": "nicholswang",
            "type": "user"
          },
          "name": "Xiaohan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:40:56.564Z",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517a",
          "name": "Yuhui Zhang",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517b",
          "name": "Anita Rau",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517c",
          "name": "Alejandro Lozano",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517d",
          "name": "Lisa Dunlap",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517e",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517f",
          "name": "Serena Yeung-Levy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T21:18:32.000Z",
      "title": "Video Action Differencing を日本語に翻訳します。\n\nビデオアクションディフューセンス",
      "summary": "同じアクションの映像で2人が行動した際にどのように違うのかを調べる方法は何ですか。本稿では、同じアクションの映像の微妙な違いを識別する新しいタスク「Video Action Differencing (VidDiff)」を紹介します。このタスクはコーチンやスキル学習などに多くの応用があります。この新しいタスクの開発を可能にするために、まずVidDiffBenchというベンチマークデータセットを作成します。このデータセットは549の映像ペアを含み、4,469の細分化アクションの違いと2,075の違いが発生する時間スライスの人間注釈を含みます。私たちの実験は、最新の大規模な多モデル（LMMs）のようなGPT-4oとQwen2-VLにおいてVidDiffBenchは大きな挑戦を課していることを示します。LMMsがVidDiffBenchで失敗する場合を分析し、このタスクにおける2つの重要な挑戦を明らかにします：2つの映像の関連するサブアクションの定位と細分化のフレーム比較。これらを克服するために、VidDiffメソッドを提案します。これは、アクションの違いの提案、キーフレームの定位、フレームの比較の3つのステージを構成し、各ステージに特化された基盤モデルを使用します。この新しいタスクの将来の研究を促進するために、ベンチマークはhttps://huggingface.co/datasets/jmhb/VidDiffBenchで、コードはhttp://jmhb0.github.io/viddiffで公開します。",
      "upvotes": 9,
      "discussionId": "67d0e917d0038007e5a751e9",
      "projectPage": "https://jmhb0.github.io/viddiff/",
      "githubRepo": "https://github.com/jmhb0/viddiff",
      "ai_keywords": [
        "Video Action Differencing (VidDiff)",
        "VidDiffBench",
        "multimodal models (LMMs)",
        "GPT-4o",
        "Qwen2-VL",
        "action difference proposal",
        "keyframe localization",
        "frame differencing",
        "agentic workflow",
        "fine-grained action differences",
        "localization timestamps"
      ]
    },
    "publishedAt": "2025-03-10T17:18:32.000Z",
    "title": "Video Action Differencing",
    "summary": "How do two individuals differ when performing the same action? In this work,\nwe introduce Video Action Differencing (VidDiff), the novel task of identifying\nsubtle differences between videos of the same action, which has many\napplications, such as coaching and skill learning. To enable development on\nthis new task, we first create VidDiffBench, a benchmark dataset containing 549\nvideo pairs, with human annotations of 4,469 fine-grained action differences\nand 2,075 localization timestamps indicating where these differences occur. Our\nexperiments demonstrate that VidDiffBench poses a significant challenge for\nstate-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL.\nBy analyzing failure cases of LMMs on VidDiffBench, we highlight two key\nchallenges for this task: localizing relevant sub-actions over two videos and\nfine-grained frame comparison. To overcome these, we propose the VidDiff\nmethod, an agentic workflow that breaks the task into three stages: action\ndifference proposal, keyframe localization, and frame differencing, each stage\nutilizing specialized foundation models. To encourage future research in this\nnew task, we release the benchmark at\nhttps://huggingface.co/datasets/jmhb/VidDiffBench and code at\nhttp://jmhb0.github.io/viddiff.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07860.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07572",
      "authors": [
        {
          "_id": "67d0e38171b6b577dbb8c72c",
          "user": {
            "_id": "6500bbf5e102da55f9ed43fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6500bbf5e102da55f9ed43fc/QZ6EAFV2CStFsILmTJw5D.jpeg",
            "isPro": true,
            "fullname": "Yuxiao Qu",
            "user": "CohenQu",
            "type": "user"
          },
          "name": "Yuxiao Qu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:41:33.926Z",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72d",
          "name": "Matthew Y. R. Yang",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72e",
          "name": "Amrith Setlur",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72f",
          "name": "Lewis Tunstall",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c730",
          "name": "Edward Emanuel Beeching",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c731",
          "name": "Ruslan Salakhutdinov",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c732",
          "name": "Aviral Kumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:40:43.000Z",
      "title": "テスト時の計算を最適化するためのメタ再励勵微調",
      "summary": "テスト時の計算量を効率的に使用するモデルの訓練は、LLMの論理性能の向上に重要です。現在の方法は主に、検索トレースでの微調節または0/1の結果報酬を持つRLを実行していますが、これらのアプローチはテスト時の計算量を効率的に利用しているか？これらのアプローチはマネージメントが改善するときにもスケーリングできるか？本論文では、これらの質問に答えることを試みます。テスト時の計算量を最適化する問題をメタ再帰学習（RL）問題として形式化し、これはテスト時の計算量を使うことを原理的な視点から見ることができます。この視点により、LLMからの長い出力ストリームをテスト時に実行した数え切れたエピソードから構成されていることを見ることができ、出力トークンの累積ロスを使用してテスト時の計算量の効果性を評価することができます。RLアルゴリズムが学習中で最適な探索と利用のバランスを見つけるように、累積ロスの最小化もトークンストリームの探索と利用のバランスを提供します。状態の最先端のモデルがロスを最小化していないことを示し、結果0/1報酬のRLと結合した稠密報酬ボーナスを最大化することでこれを実現することができます。このボーナスは、出力ストリームの各後続ブロックにおける「進歩」を表現し、最終的な成功の確率の変化で定量化されます。これらの見解を使用して、テスト時の計算量を最適化するための新しいクラスの微調節方法であるメタ再帰学習微調節（MRT）を開発します。MRTは、結果報酬のRLと比較して、性能に2-3倍の相対的な効果と数学論理のトークンエフィシェンスに約1.5倍の効果を提供します。",
      "upvotes": 8,
      "discussionId": "67d0e38271b6b577dbb8c7b7",
      "projectPage": "https://cohenqu.github.io/mrt.github.io/",
      "ai_keywords": [
        "meta-reinforcement learning (RL)",
        "cumulative regret",
        "token stream",
        "exploration and exploitation",
        "dense reward bonus",
        "likelihood of eventual success",
        "Meta Reinforcement Fine-Tuning (MRT)"
      ]
    },
    "publishedAt": "2025-03-10T13:40:43.000Z",
    "title": "Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning",
    "summary": "Training models to effectively use test-time compute is crucial for improving\nthe reasoning performance of LLMs. Current methods mostly do so via fine-tuning\non search traces or running RL with 0/1 outcome reward, but do these approaches\nefficiently utilize test-time compute? Would these approaches continue to scale\nas the budget improves? In this paper, we try to answer these questions. We\nformalize the problem of optimizing test-time compute as a meta-reinforcement\nlearning (RL) problem, which provides a principled perspective on spending\ntest-time compute. This perspective enables us to view the long output stream\nfrom the LLM as consisting of several episodes run at test time and leads us to\nuse a notion of cumulative regret over output tokens as a way to measure the\nefficacy of test-time compute. Akin to how RL algorithms can best tradeoff\nexploration and exploitation over training, minimizing cumulative regret would\nalso provide the best balance between exploration and exploitation in the token\nstream. While we show that state-of-the-art models do not minimize regret, one\ncan do so by maximizing a dense reward bonus in conjunction with the outcome\n0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in\nthe output stream, quantified by the change in the likelihood of eventual\nsuccess. Using these insights, we develop Meta Reinforcement Fine-Tuning, or\nMRT, a new class of fine-tuning methods for optimizing test-time compute. MRT\nleads to a 2-3x relative gain in performance and roughly a 1.5x gain in token\nefficiency for math reasoning compared to outcome-reward RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07572.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08588",
      "authors": [
        {
          "_id": "67d125362da8f91f8ef0412f",
          "user": {
            "_id": "6190ab805ca89a28e9f66873",
            "avatarUrl": "/avatars/a677a8401360be473895494e5fb267bb.svg",
            "isPro": false,
            "fullname": "Xin Xu",
            "user": "XinXuNLPer",
            "type": "user"
          },
          "name": "Xin Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:07.130Z",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04130",
          "name": "Wei Xu",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04131",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04132",
          "name": "Julian McAuley",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:25:36.000Z",
      "title": "BiasEdit: 偏見訓練された言語モデルを偏見を除去するためのモデル編集法",
      "summary": "前の研究により、言語モデルがスケート型のバイアスを示すことが明らかになっています。現在のデバイスバイアス戦略では、カンフェクトファクトルデータを用いたモデルの再学習、表現プロジェクション、プロンプティングなどが、バイアスの効率的な除去またはモデルの偏り付き内部表現の直接変更に失敗します。これらの問題に対処するために、私たちは、スケート型のバイアスを除去するための効率的なモデル編集方法、BiasEditを提案します。BiasEditは、軽量ネットワークを用いてモデル編集を行うエディターネットワークを使用し、偏りを除去するためのパラメータ更新を生成します。BiasEditは、デバイスバイアス損失を使用して、言語モデリング能力を保持しながら、言語モデルの一部パラメータを局所的に編集します。StereoSetとCrows-Pairsの実験では、偏りの除去の効果性、効率性と強固性を示し、接点デバイスバイアスベースラインと比較して言語モデルの一般的な能力に少しも影響を与えませんでした。また、バイアストレーラーを行い、各モジュールのバイアスを探査し、言語モデルの異なるコンポーネントにおけるバイアス編集の影響を調査しました。",
      "upvotes": 5,
      "discussionId": "67d125382da8f91f8ef041d0",
      "githubRepo": "https://github.com/zjunlp/BiasEdit",
      "ai_keywords": [
        "language models",
        "counterfactual data",
        "representation projection",
        "prompting",
        "BiasEdit",
        "parameter updates",
        "debiasing loss",
        "retention loss",
        "StereoSet",
        "Crows-Pairs",
        "language modeling abilities",
        "bias tracing"
      ]
    },
    "publishedAt": "2025-03-11T12:25:36.000Z",
    "title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing",
    "summary": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08588.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08689",
      "authors": [
        {
          "_id": "67d0f759cb5bf46c22ac8af1",
          "name": "Yongdong Luo",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af2",
          "name": "Wang Chen",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af3",
          "name": "Xiawu Zheng",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af4",
          "name": "Weizhong Huang",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af5",
          "name": "Shukang Yin",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af6",
          "name": "Haojia Lin",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af7",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af8",
          "name": "Jinfa Huang",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af9",
          "name": "Jiayi Ji",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8afa",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8afb",
          "name": "Rongrong Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:57.000Z",
      "title": "クエリー向けトークン割り当て構造化を長いビデオ理解におけるコンテキストフォーマット化によって実現する",
      "summary": "最近の長いビデオ理解の進歩は、視覚トークンの削減を通じて視覚冗餘を軽減しています。しかし、現在の方法は、デコーダ層で後方論理的な低反応トークンの削減を使用していますが、視覚トークンとインストラクション（クエリ）の入力レベルの語意的関連性を遺漏しています。本論文では、クエリ向けのフレームレベルの重要性評価に基づく視覚トークン割り当てによる現有の大規模なビデオ言語モデル（LVLMs）を拡張するための前方論理的な訓練無制限モジュール「QuoTA」を提案します。クエリ向けのトークン選択は重要であり、視覚処理をタスクの特定の要求に合わせることでトークンバッジの利用を最適化し、語意的に関連する内容を保存します。特に、(i) QuoTAはクエリ関連性に基づいてフレームレベルの重要性スコアを戦略的に割り当て、デコーダ層のクロスモード相互作用前に一度の視覚トークン割り当てを可能にします、(ii) クエリをChain-of-Thoughts推理により分離し、LVLMによるフレーム重要性スコアをより正確に評価することを促進します、(iii) QuoTAは既存のLVLMに拡張するためのポートアンドパイプ機能を提供します。拡張した実験結果は、LLaVA-Video-7Bとの組み合わせで6ベンチマーク（Video-MMEとMLVUを含む）で平均的な性能向上率が3.2%となり、基準と同じ視覚トークンバッジを使用して実行されます。コードは、https://github.com/MAC-AutoML/QuoTAに開放されています。",
      "upvotes": 4,
      "discussionId": "67d0f75bcb5bf46c22ac8b70",
      "githubRepo": "https://github.com/MAC-AutoML/QuoTA",
      "ai_keywords": [
        "QuoTA",
        "ante-hoc",
        "training-free",
        "modular",
        "long video understanding",
        "visual token pruning",
        "attention distribution",
        "decoder layers",
        "input-level semantic correlation",
        "visual tokens",
        "instructions",
        "query",
        "frame-level importance assessment",
        "task-specific requirements",
        "token budget utilization",
        "semantically relevant content",
        "Chain-of-Thoughts reasoning",
        "cross-modal interactions",
        "plug-and-play functionality",
        "LLaVA-Video-7B",
        "Video-MME",
        "MLVU"
      ]
    },
    "publishedAt": "2025-03-11T13:59:57.000Z",
    "title": "QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long\n  Video Comprehension",
    "summary": "Recent advances in long video understanding typically mitigate visual\nredundancy through visual token pruning based on attention distribution.\nHowever, while existing methods employ post-hoc low-response token pruning in\ndecoder layers, they overlook the input-level semantic correlation between\nvisual tokens and instructions (query). In this paper, we propose QuoTA, an\nante-hoc training-free modular that extends existing large video-language\nmodels (LVLMs) for visual token assignment based on query-oriented frame-level\nimportance assessment. The query-oriented token selection is crucial as it\naligns visual processing with task-specific requirements, optimizing token\nbudget utilization while preserving semantically relevant content.\nSpecifically, (i) QuoTA strategically allocates frame-level importance scores\nbased on query relevance, enabling one-time visual token assignment before\ncross-modal interactions in decoder layers, (ii) we decouple the query through\nChain-of-Thoughts reasoning to facilitate more precise LVLM-based frame\nimportance scoring, and (iii) QuoTA offers a plug-and-play functionality that\nextends to existing LVLMs. Extensive experimental results demonstrate that\nimplementing QuoTA with LLaVA-Video-7B yields an average performance\nimprovement of 3.2% across six benchmarks (including Video-MME and MLVU) while\noperating within an identical visual token budget as the baseline. Codes are\nopen-sourced at https://github.com/MAC-AutoML/QuoTA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08689.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08685",
      "authors": [
        {
          "_id": "67d0f7032eaba9be7bf76e0e",
          "user": {
            "_id": "63483629ac5172169929da0e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665676793089-noauth.jpeg",
            "isPro": false,
            "fullname": "Xin Wen",
            "user": "xwen99",
            "type": "user"
          },
          "name": "Xin Wen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:39:00.455Z",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e0f",
          "user": {
            "_id": "62dcd71075e9787ec5aa41ba",
            "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
            "isPro": true,
            "fullname": "Bingchen Zhao",
            "user": "tennant",
            "type": "user"
          },
          "name": "Bingchen Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:38:56.945Z",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e10",
          "name": "Ismail Elezi",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e11",
          "name": "Jiankang Deng",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e12",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:41.000Z",
      "title": "\"主成分分析\" は、画像の新しい言語を可能にします。",
      "summary": "We introduce a novel visual tokenization framework that embeds a provable PCA-like structure into the latent token space. While existing visual tokenizers primarily optimize for reconstruction fidelity, they often neglect the structural properties of the latent space -- a critical factor for both interpretability and downstream tasks. Our method generates a 1D causal token sequence for images, where each successive token contributes non-overlapping information with mathematically guaranteed decreasing explained variance, analogous to principal component analysis. This structural constraint ensures the tokenizer extracts the most salient visual features first, with each subsequent token adding diminishing yet complementary information. Additionally, we identified and resolved a semantic-spectrum coupling effect that causes the unwanted entanglement of high-level semantic content and low-level spectral details in the tokens by leveraging a diffusion decoder. Experiments demonstrate that our approach achieves state-of-the-art reconstruction performance and enables better interpretability to align with the human vision system. Moreover, auto-regressive models trained on our token sequences achieve performance comparable to current state-of-the-art methods while requiring fewer tokens for training and inference.",
      "upvotes": 4,
      "discussionId": "67d0f7052eaba9be7bf76eac",
      "projectPage": "https://visual-gen.github.io/semanticist/",
      "githubRepo": "https://github.com/visual-gen/semanticist",
      "ai_keywords": [
        "visual tokenization framework",
        "PCA-like structure",
        "latent token space",
        "reconstruction fidelity",
        "structural properties",
        "interpretabiliy",
        "1D causal token sequence",
        "explained variance",
        "principal component analysis",
        "salient visual features",
        "semantic-spectrum coupling effect",
        "diffusion decoder",
        "reconstruction performance",
        "human vision system",
        "auto-regressive models",
        "state-of-the-art methods"
      ]
    },
    "publishedAt": "2025-03-11T13:59:41.000Z",
    "title": "\"Principal Components\" Enable A New Language of Images",
    "summary": "We introduce a novel visual tokenization framework that embeds a provable\nPCA-like structure into the latent token space. While existing visual\ntokenizers primarily optimize for reconstruction fidelity, they often neglect\nthe structural properties of the latent space -- a critical factor for both\ninterpretability and downstream tasks. Our method generates a 1D causal token\nsequence for images, where each successive token contributes non-overlapping\ninformation with mathematically guaranteed decreasing explained variance,\nanalogous to principal component analysis. This structural constraint ensures\nthe tokenizer extracts the most salient visual features first, with each\nsubsequent token adding diminishing yet complementary information.\nAdditionally, we identified and resolved a semantic-spectrum coupling effect\nthat causes the unwanted entanglement of high-level semantic content and\nlow-level spectral details in the tokens by leveraging a diffusion decoder.\nExperiments demonstrate that our approach achieves state-of-the-art\nreconstruction performance and enables better interpretability to align with\nthe human vision system. Moreover, auto-regressive models trained on our token\nsequences achieve performance comparable to current state-of-the-art methods\nwhile requiring fewer tokens for training and inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08685.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07699",
      "authors": [
        {
          "_id": "67d114912264403cbf39d0ba",
          "name": "Huiyang Shao",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bb",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bc",
          "name": "Yuhong Yang",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bd",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0be",
          "name": "Xing Wang",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bf",
          "name": "Xuefeng Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:20:52.000Z",
      "title": "RayFlow: インスタンス情報を認識したディフュージョン加速による適応的なフロートラジエクチョン",
      "summary": "Diffusionモデルは、様々な領域で驚異的な成功を収めています。しかし、生成速度の遅さが重要な課題です。現在の加速方法は、ステップを減らすために、サンプルの品質、制御性を失したり、訓練の複雑性を導入します。そこで、RayFlowという新しいDiffusionフレームワークを提案します。RayFlowは、前の方法と異なり、各サンプルをインスタンス特有のターゲット分布への異なるパスによってガイドします。この方法は、生成多様性と安定性を保ちながら、サンプリングステップを最小化します。また、Time Samplerという重要なサンプリング手法を紹介します。これは、重要な時間ステップに焦点を当てて訓練効率を向上させることを目的としています。拡張された実験は、RayFlowが既存の加速手法と比較して、高品質の画像を生成する上で速度、制御性、訓練効率を向上させることを示しています。",
      "upvotes": 3,
      "discussionId": "67d114922264403cbf39d0f8",
      "ai_keywords": [
        "diffusion models",
        "generation speed",
        "RayFlow",
        "instance-specific target distribution",
        "sampling steps",
        "generation diversity",
        "stability",
        "Time Sampler",
        "importance sampling",
        "training efficiency",
        "high-quality images"
      ]
    },
    "publishedAt": "2025-03-10T13:20:52.000Z",
    "title": "RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow\n  Trajectories",
    "summary": "Diffusion models have achieved remarkable success across various domains.\nHowever, their slow generation speed remains a critical challenge. Existing\nacceleration methods, while aiming to reduce steps, often compromise sample\nquality, controllability, or introduce training complexities. Therefore, we\npropose RayFlow, a novel diffusion framework that addresses these limitations.\nUnlike previous methods, RayFlow guides each sample along a unique path towards\nan instance-specific target distribution. This method minimizes sampling steps\nwhile preserving generation diversity and stability. Furthermore, we introduce\nTime Sampler, an importance sampling technique to enhance training efficiency\nby focusing on crucial timesteps. Extensive experiments demonstrate RayFlow's\nsuperiority in generating high-quality images with improved speed, control, and\ntraining efficiency compared to existing acceleration techniques.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07699.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18858",
      "authors": [
        {
          "_id": "67d1080b2264403cbf36b0ad",
          "name": "Jingtao Zhan",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0ae",
          "name": "Jiahao Zhao",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0af",
          "name": "Jiayu Li",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b0",
          "name": "Yiqun Liu",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b1",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b2",
          "name": "Qingyao Ai",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b3",
          "name": "Jiaxin Mao",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b4",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b5",
          "name": "Min Zhang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b6",
          "name": "Shaoping Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T05:59:45.000Z",
      "title": "試験と誤りを通じた知能評価",
      "summary": "知能は、種族が限られた試行錯誤の試行回数で解決策を見つけるための重要な性質です。この考えに基づき、私たちは、試行錯誤プロセスで失敗の回数を基に知能を評価するためのシュールビルバーゲーム（Survival Game）を提案します。失敗の回数が少ないことは、高い知能を示します。失敗の数の期待値と分散が有限であることは、新しい課題について一貫して解決策を見つける能力を示し、これを自律レベル（Autonomous Level）として定義しています。シュールビルバーゲームを用いて、私たちは現在のAIシステムを構成的に評価します。結果として、簡単なタスクではAIシステムが自律レベルを達成しますが、視覚、検索、リカメダリオン、そして言語のような複雑なタスクではまだ遠く離れています。現在のAIテクノロジーをスケール化することは助かるけれども、これは天文学的なコストを伴います。一般的なタスクで自律レベルを達成する予測は、10^26パラメータを必要とします。これを見ると、このような巨大なモデルを読み込むには、H100 GPUの数が多すぎて、その総価格はApple Inc.の市場価値の10^7倍に達します。ムーアの法則（Moore's Law）でも、このパラメータスケールを支えるのは70年かかります。この巨大なコストは、人間の任務の複雑さと現在のAIテクノロジーの不十分さを明らかにします。この現象を進一歩調査するために、私たちはシュールビルバーゲームの理論的な分析および実験結果を行います。私たちの発見は、人間の任務はキャリティー性質（criticality property）を持っていることを示し、これにより、自律レベルは任務の潜在的機構を深く理解する必要があることを示します。しかし、現在のAIシステムはこれらの機構を完全に理解していないので、表面的なミニクリに依存し、自律レベルを達成することが難しいことを示します。私たちは、シュールビルバーゲームはAIの将来的な開発をガイドし、人間の知能に深い見解を提供することができることを信じています。",
      "upvotes": 3,
      "discussionId": "67d108112264403cbf36b1e9",
      "githubRepo": "https://github.com/jingtaozhan/IntelligenceTest"
    },
    "publishedAt": "2025-02-26T00:59:45.000Z",
    "title": "Evaluating Intelligence via Trial and Error",
    "summary": "Intelligence is a crucial trait for species to find solutions within a\nlimited number of trial-and-error attempts. Building on this idea, we introduce\nSurvival Game as a framework to evaluate intelligence based on the number of\nfailed attempts in a trial-and-error process. Fewer failures indicate higher\nintelligence. When the expectation and variance of failure counts are both\nfinite, it signals the ability to consistently find solutions to new\nchallenges, which we define as the Autonomous Level of intelligence. Using\nSurvival Game, we comprehensively evaluate existing AI systems. Our results\nshow that while AI systems achieve the Autonomous Level in simple tasks, they\nare still far from it in more complex tasks, such as vision, search,\nrecommendation, and language. While scaling current AI technologies might help,\nthis would come at an astronomical cost. Projections suggest that achieving the\nAutonomous Level for general tasks would require 10^{26} parameters. To put\nthis into perspective, loading such a massive model requires so many H100 GPUs\nthat their total value is 10^{7} times that of Apple Inc.'s market value.\nEven with Moore's Law, supporting such a parameter scale would take 70 years.\nThis staggering cost highlights the complexity of human tasks and the\ninadequacies of current AI technologies. To further investigate this\nphenomenon, we conduct a theoretical analysis of Survival Game and its\nexperimental results. Our findings suggest that human tasks possess a\ncriticality property. As a result, Autonomous Level requires a deep\nunderstanding of the task's underlying mechanisms. Current AI systems, however,\ndo not fully grasp these mechanisms and instead rely on superficial mimicry,\nmaking it difficult for them to reach an autonomous level. We believe Survival\nGame can not only guide the future development of AI but also offer profound\ninsights into human intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18858.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07639",
      "authors": [
        {
          "_id": "67d0e3ede3afecf451915d0a",
          "name": "Xingyi Yang",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0b",
          "name": "Constantin Venhoff",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0c",
          "name": "Ashkan Khakzar",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0d",
          "name": "Christian Schroeder de Witt",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0e",
          "name": "Puneet K. Dokania",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0f",
          "name": "Adel Bibi",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d10",
          "name": "Philip Torr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T17:40:54.000Z",
      "title": "Mixture of Experts Made Intrinsically Interpretable",
      "summary": "ニューロンが大規模な言語モデルで多意味性を現れ、複数の関係なしの概念を同時にエンコーディングし、解釈性を隠すことが多い。後処理手法を依存せず、我々は、固有の解釈性を持つMixture-of-Experts（MoE）言語モデルを提案します。我々のアプローチは、言語モデルでは、稀疏な活性化を持つ幅広いネットワークが解釈的な要因を捉えやすいことを観察した上で、このことを基にしています。しかし、そのような大きな稀疏なネットワークを直接訓練するのは計算的に禁止されています。MoEアーキテクチャは、与えられた入力に対してその一部のエキスプターを活性化させることでスケーラブルな代替となり、固有の解釈性の目標と一致します。MoE-Xでは、MoE層を等価な稀疏な大きなMLPと書き換えることで、この連携を確立します。このアプローチは、隠れサイズの効率的なスケーリングを可能にし、稀疏性を維持することができます。また、解釈性を進めることを目指し、各エキスプター内で稀疏な活性化を強制し、ルーティング機構を再設計し、最高の活性化稀疏性を持つエキスプターを優先することで、これらの設計は、エキスプターによって路由され、処理されるのは最も鮮明な特徴だけです。チェスおよび自然言語タスクにおいてMoE-Xを評価し、その性能は密なモデルと比較しても相当し、解釈性を大幅に向上させます。MoE-Xは、GPT-2よりもペルミュラティが良く、解釈性は、稀疏自動エンコーダー（SAE）ベースのアプローチよりも上回ります。",
      "upvotes": 2,
      "discussionId": "67d0e3f0e3afecf451915dfa",
      "ai_keywords": [
        "polysemanticity",
        "Mixture-of-Experts (MoE)",
        "interpretable",
        "sparse activations",
        "sparsity",
        "sparse networks",
        "hidden size",
        "sparse activation",
        "routing mechanism",
        "salient features",
        "perplexity",
        "GPT-2",
        "sparse autoencoder (SAE)"
      ]
    },
    "publishedAt": "2025-03-05T12:40:54.000Z",
    "title": "Mixture of Experts Made Intrinsically Interpretable",
    "summary": "Neurons in large language models often exhibit polysemanticity,\nsimultaneously encoding multiple unrelated concepts and obscuring\ninterpretability. Instead of relying on post-hoc methods, we present\nMoE-X, a Mixture-of-Experts (MoE) language model designed to be\nintrinsically interpretable. Our approach is motivated by the\nobservation that, in language models, wider networks with sparse activations\nare more likely to capture interpretable factors. However, directly training\nsuch large sparse networks is computationally prohibitive. MoE architectures\noffer a scalable alternative by activating only a subset of experts for any\ngiven input, inherently aligning with interpretability objectives. In MoE-X, we\nestablish this connection by rewriting the MoE layer as an equivalent sparse,\nlarge MLP. This approach enables efficient scaling of the hidden size while\nmaintaining sparsity. To further enhance interpretability, we enforce sparse\nactivation within each expert and redesign the routing mechanism to prioritize\nexperts with the highest activation sparsity. These designs ensure that only\nthe most salient features are routed and processed by the experts. We evaluate\nMoE-X on chess and natural language tasks, showing that it achieves performance\ncomparable to dense models while significantly improving interpretability.\nMoE-X achieves a perplexity better than GPT-2, with interpretability surpassing\neven sparse autoencoder (SAE)-based approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07639.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08507",
      "authors": [
        {
          "_id": "67d15293e3afecf451aceab7",
          "name": "Qing Jiang",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceab8",
          "name": "Lin Wu",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceab9",
          "name": "Zhaoyang Zeng",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceaba",
          "name": "Tianhe Ren",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabb",
          "name": "Yuda Xiong",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabc",
          "name": "Yihao Chen",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabd",
          "name": "Qin Liu",
          "hidden": false
        },
        {
          "_id": "67d15293e3afecf451aceabe",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T14:57:14.000Z",
      "title": "「どの人にも関係なく」",
      "summary": "人間は確かにコンピュータビジョンの最も重要な参加者であり、自然言語の説明から誰かの個体を検出する能力、これを人間の参照として定義したタスクは実用的な価値を持っている。しかし、現在のモデルは一般的に実世界的な使用可能さを達成していないことを見出し、現在のベンチマークは一対一の参照を焦点にして進歩を妨げている。本稿では、このタスクを仕事定義、データセットデザイン、モデルアーキテクチャの3つの重要な観点から再評価した。まず、参照可能なエンティティの5つの面とこのタスクの3つの特徴を特定し、次に、HumanRefという新しいデータセットを介してこれらの課題を克服し、実世界的なアプリケーションをより真似させることを目的とした。モデルデザインの観点から、多モデル大語言モデルと物体検出フレームワークを統合し、強固な参照モデルをRexSeekとして構築した。実験結果から、通常のベンチマーク（RefCOCO/+/g）でよく表現する最先端のモデルは、HumanRefでは複数の個体を検出できないため困難を見出した。逆に、RexSeekは人間の参照にも優れているのに対して、一般的な物体の参照にも効果的に拡張でき、様々な観測タスクに広く適用できる。コードはhttps://github.com/IDEA-Research/RexSeekに公開されている。",
      "upvotes": 1,
      "discussionId": "67d15294e3afecf451aceb29",
      "ai_keywords": [
        "referable entities",
        "multimodal large language model",
        "object detection framework",
        "HumanRef",
        "RexSeek",
        "RefCOCO/+/g"
      ]
    },
    "publishedAt": "2025-03-11T10:57:14.000Z",
    "title": "Referring to Any Person",
    "summary": "Humans are undoubtedly the most important participants in computer vision,\nand the ability to detect any individual given a natural language description,\na task we define as referring to any person, holds substantial practical value.\nHowever, we find that existing models generally fail to achieve real-world\nusability, and current benchmarks are limited by their focus on one-to-one\nreferring, that hinder progress in this area. In this work, we revisit this\ntask from three critical perspectives: task definition, dataset design, and\nmodel architecture. We first identify five aspects of referable entities and\nthree distinctive characteristics of this task. Next, we introduce HumanRef, a\nnovel dataset designed to tackle these challenges and better reflect real-world\napplications. From a model design perspective, we integrate a multimodal large\nlanguage model with an object detection framework, constructing a robust\nreferring model named RexSeek. Experimental results reveal that\nstate-of-the-art models, which perform well on commonly used benchmarks like\nRefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple\nindividuals. In contrast, RexSeek not only excels in human referring but also\ngeneralizes effectively to common object referring, making it broadly\napplicable across various perception tasks. Code is available at\nhttps://github.com/IDEA-Research/RexSeek",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08507.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08478",
      "authors": [
        {
          "_id": "67d12d0b44be28339053b965",
          "user": {
            "_id": "64a3eb280111d5ff6c4849fd",
            "avatarUrl": "/avatars/3a9000393b8d200418bae5fe7d902e4d.svg",
            "isPro": false,
            "fullname": "Han-Wei Kung",
            "user": "hkung",
            "type": "user"
          },
          "name": "Han-Wei Kung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:50:08.946Z",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b966",
          "name": "Tuomas Varanka",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b967",
          "name": "Terence Sim",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b968",
          "name": "Nicu Sebe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T14:29:37.000Z",
      "title": "NullFace: トレーニング無しの局所的な顔の匿名化",
      "summary": "カメラの数が増え続けることによって、プライバシーの懸念は今日のデジタル時代には増加しています。既存の匿名化方法は、身分情報を隠すことができますが、画像の有用性を保持することは難しく、これを解決するために、本論文では、顔を匿名化しながら、関係ない鍵の属性を保持するための訓練不要の方法を紹介します。我々のアプローチは、最適化や訓練が不要であるための事前学習されたテキストから画像への拡散モデルを使用します。最初に、入力画像を逆算し、初期のノイズを復元します。その後、鍵の属性を保持するための隠れノイズを減込し、隠れノイズを変更した顔の隠れノイズを使用して、隠れノイズを減込します。我々のアプローチは、顔の領域を選択的に匿名化することを支援し、ユーザーが顔の領域を匿名化するか保持するかを制御できます。最先端の方法との詳細な評価により、我々のアプローチは匿名化、鍵の属性の保持、画像の質に優れています。その柔軟性、強固性、実用的な性質により、実世界的なアプリケーションに適しています。コードとデータは、https://github.com/hanweikung/nullface に見つかります。",
      "upvotes": 1,
      "discussionId": "67d12d1044be28339053baab",
      "ai_keywords": [
        "text-to-image diffusion model",
        "identity-conditioned diffusion",
        "identity embeddings",
        "localized anonymization"
      ]
    },
    "publishedAt": "2025-03-11T10:29:37.000Z",
    "title": "NullFace: Training-Free Localized Face Anonymization",
    "summary": "Privacy concerns around ever increasing number of cameras are increasing in\ntoday's digital age. Although existing anonymization methods are able to\nobscure identity information, they often struggle to preserve the utility of\nthe images. In this work, we introduce a training-free method for face\nanonymization that preserves key non-identity-related attributes. Our approach\nutilizes a pre-trained text-to-image diffusion model without requiring\noptimization or training. It begins by inverting the input image to recover its\ninitial noise. The noise is then denoised through an identity-conditioned\ndiffusion process, where modified identity embeddings ensure the anonymized\nface is distinct from the original identity. Our approach also supports\nlocalized anonymization, giving users control over which facial regions are\nanonymized or kept intact. Comprehensive evaluations against state-of-the-art\nmethods show our approach excels in anonymization, attribute preservation, and\nimage quality. Its flexibility, robustness, and practicality make it\nwell-suited for real-world applications. Code and data can be found at\nhttps://github.com/hanweikung/nullface .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08478.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08307",
      "authors": [
        {
          "_id": "67d140378cb4592900a1a75e",
          "user": {
            "_id": "66895b3d41fcf83c026b5dca",
            "avatarUrl": "/avatars/f1104041ee3445024f05d5d0b4d1550b.svg",
            "isPro": false,
            "fullname": "Alex Ergasti",
            "user": "MaverickAlex",
            "type": "user"
          },
          "name": "Alex Ergasti",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:36:03.978Z",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a75f",
          "name": "Giuseppe Gabriele Tarollo",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a760",
          "name": "Filippo Botti",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a761",
          "name": "Tomaso Fontanini",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a762",
          "name": "Claudio Ferrari",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a763",
          "name": "Massimo Bertozzi",
          "hidden": false
        },
        {
          "_id": "67d140378cb4592900a1a764",
          "name": "Andrea Prati",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T11:18:47.000Z",
      "title": "^RFLAV: 無限の音声ビデオ生成に向けたローリングフローマッチング",
      "summary": "ジョイント音声ビデオ（AV）生成は、生成AIではまだ重要な課題であり、主に以下の3つの重要な要求により複雑である：生成されたサンプルの品質、無際だくみのモチューラル同期と時系列的な一貫性、そして音声トラックが視覚データに合わせられ、逆にも同様である、または限らず長いビデオ時間。本論文では、全てのAV生成の重要な課題を解決するための新しいtransformerベースのアーキテクチャを紹介します。3つの異なるクロスモディューラルインタラクションモジュールを検討し、その軽量な時系列的なフュージョンモジュールが音声と視覚モディュールのアラインメントに最も効果的で計算的にも効率的なアプローチとして優れています。実験結果によると、これは現在の最先端のモデルを上回る多モディューラルAV生成タスクで最も優れています。コードとチェックポイントは、https://github.com/ErgastiAlex/R-FLAV から利用できます。",
      "upvotes": 1,
      "discussionId": "67d1403b8cb4592900a1a868",
      "githubRepo": "https://github.com/ErgastiAlex/R-FLAV",
      "ai_keywords": [
        "transformer-based architecture",
        "cross modality interaction modules",
        "lightweight temporal fusion module",
        "audio and visual modalities",
        "multimodal AV generation tasks"
      ]
    },
    "publishedAt": "2025-03-11T07:18:47.000Z",
    "title": "^RFLAV: Rolling Flow matching for infinite Audio Video generation",
    "summary": "Joint audio-video (AV) generation is still a significant challenge in\ngenerative AI, primarily due to three critical requirements: quality of the\ngenerated samples, seamless multimodal synchronization and temporal coherence,\nwith audio tracks that match the visual data and vice versa, and limitless\nvideo duration. In this paper, we present , a novel transformer-based\narchitecture that addresses all the key challenges of AV generation. We explore\nthree distinct cross modality interaction modules, with our lightweight\ntemporal fusion module emerging as the most effective and computationally\nefficient approach for aligning audio and visual modalities. Our experimental\nresults demonstrate that  outperforms existing state-of-the-art models\nin multimodal AV generation tasks. Our code and checkpoints are available at\nhttps://github.com/ErgastiAlex/R-FLAV.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08307.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.08102",
      "authors": [
        {
          "_id": "67d12c32428a3d8d5281f310",
          "name": "Jiale Wei",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f311",
          "name": "Xiang Ying",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f312",
          "name": "Tao Gao",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f313",
          "name": "Felix Tao",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f314",
          "name": "Jingbo Shang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T07:05:52.000Z",
      "title": "AI-native Memory 2.0: Second Me\n\nAI原生メモリー 2.0: セカンド・メ",
      "summary": "人間が外部世界との相互作用は、個人的な記憶の交換に基づいています。これは他の個人、ウェブサイト、アプリケーション、または将来のAIアジェントとの相互作用に含まれます。この相互作用の一部は、ユーザーが異なるコンテキストで同じ情報を再三提供する必要があり、冗長です。現在の解決策として、ブラウザーに保存された認証情報、自動補完機構、または統一認証システムが、この冗長を抑えるために、ユーザーのデータを保存し、取り出す機能を提供しています。大規模な言語モデル（LLM）の発展は、AI原生パラダイムでメモリ管理を再定義する機会を提供します：SECOND ME。SECOND MEは、知識を保持、整理し、ユーザーサイズの知識を動的に利用する智能的な、持続的なメモリロードシステムです。ユーザーの相互作用の中間者として機能し、コンテキストに適合したレスポンスを自動的に生成し、必要な情報を予約補完し、外部システムとの無間のコミュニケーションを促進し、認知負荷と相互作用の摩擦を大幅に減少します。傳統的なメモリ保存解決策に比べ、SECOND MEは、LLMベースのメモリパラメタ化を利用し、静的なデータ保存を超えて、構造的な整理、コンテキストライング、アダプティブな知識検索を可能にし、メモリ管理においてより系統的で智能的なアプローチを促進します。AI駆動のプライベートアジェントのようなSECOND MEが、デジタルエコシステムにさらに統合されることを示しています、SECOND MEは、持続的でコンテキストに適合した、自動調整されたメモリシステムを用いて人間と世界との相互作用を増強する重要なステップとして表現されます。私たちは、GitHubで完全に地域化可能な部署システムをオープンソース化しています：https://github.com/Mindverse/Second-Me。",
      "upvotes": 1,
      "discussionId": "67d12c33428a3d8d5281f346",
      "ai_keywords": [
        "large language models (LLMs)",
        "intelligent, persistent memory offload system",
        "context-aware responses",
        "structured organization",
        "contextual reasoning",
        "adaptive knowledge retrieval",
        "self-optimizing memory systems"
      ]
    },
    "publishedAt": "2025-03-11T03:05:52.000Z",
    "title": "AI-native Memory 2.0: Second Me",
    "summary": "Human interaction with the external world fundamentally involves the exchange\nof personal memory, whether with other individuals, websites, applications, or,\nin the future, AI agents. A significant portion of this interaction is\nredundant, requiring users to repeatedly provide the same information across\ndifferent contexts. Existing solutions, such as browser-stored credentials,\nautofill mechanisms, and unified authentication systems, have aimed to mitigate\nthis redundancy by serving as intermediaries that store and retrieve commonly\nused user data. The advent of large language models (LLMs) presents an\nopportunity to redefine memory management through an AI-native paradigm: SECOND\nME. SECOND ME acts as an intelligent, persistent memory offload system that\nretains, organizes, and dynamically utilizes user-specific knowledge. By\nserving as an intermediary in user interactions, it can autonomously generate\ncontext-aware responses, prefill required information, and facilitate seamless\ncommunication with external systems, significantly reducing cognitive load and\ninteraction friction. Unlike traditional memory storage solutions, SECOND ME\nextends beyond static data retention by leveraging LLM-based memory\nparameterization. This enables structured organization, contextual reasoning,\nand adaptive knowledge retrieval, facilitating a more systematic and\nintelligent approach to memory management. As AI-driven personal agents like\nSECOND ME become increasingly integrated into digital ecosystems, SECOND ME\nfurther represents a critical step toward augmenting human-world interaction\nwith persistent, contextually aware, and self-optimizing memory systems. We\nhave open-sourced the fully localizable deployment system at GitHub:\nhttps://github.com/Mindverse/Second-Me.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08102.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06594",
      "authors": [
        {
          "_id": "67cfd77ff8ee57c14450221b",
          "user": {
            "_id": "6440b38d3e0374802e1acc5e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6440b38d3e0374802e1acc5e/w-ZpW_9gCSHUeDKyGSeMt.jpeg",
            "isPro": false,
            "fullname": "luoyingfeng",
            "user": "luoyingfeng",
            "type": "user"
          },
          "name": "Yingfeng Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:42:33.649Z",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221c",
          "name": "Tong Zheng",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221d",
          "name": "Yongyu Mu",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221e",
          "name": "Bei Li",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c14450221f",
          "name": "Qinghong Zhang",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502220",
          "name": "Yongqi Gao",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502221",
          "name": "Ziqiang Xu",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502222",
          "name": "Peinan Feng",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502223",
          "name": "Xiaoqian Liu",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502224",
          "name": "Tong Xiao",
          "hidden": false
        },
        {
          "_id": "67cfd77ff8ee57c144502225",
          "name": "Jingbo Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T12:54:05.000Z",
      "title": "デコーダーだけでなく：大規模言語モデルは機械翻訳のエンコーダーとしても良い",
      "summary": "神経機械翻訳（NMT）の分野は、大規模言語モデル（LLM）の到来により変容しました。最近の自然言語処理（NLP）の主張は、単一の事前学習されたTransformer解確認器を用いて機械翻訳および他の多くの問題をモデリングすることに焦点を当ててきました。先週のNMTモデルでは標準的なエンコーダー-デコーダーアーキテクチャが相対的に少し注意を受けませんでした。本論文では、LLMとNMTの世界を結びつけることで、一般的で効率的で容易に最適化できる翻訳モデルを検討します。LLMをNMTエンコーダに適用し、NMTデコーダは変更しません。また、LLMをNMTデコーダとより良く協調させるための方法も開発します。また、多くのタスクを含む新しいデータセットを構築し、機械翻訳システムが複数のタスクでの拡張性をどのように示すかを評価します。WMTと私たちのデータセットの評価により、私たちの方法による結果は翻訳品質において基準と比較しても等しいか超えていますが、推論速度は2.4倍～6.5倍のスピードアップとKVキャッシュのメモリフットプレークの75%削減を実現します。また、複数の翻訳関連タスクに強い拡張性を示しています。",
      "upvotes": 1,
      "discussionId": "67cfd780f8ee57c144502268",
      "githubRepo": "https://github.com/NiuTrans/LaMaTE/",
      "ai_keywords": [
        "large language models (LLMs)",
        "neural machine translation (NMT)",
        "natural language processing (NLP)",
        "Transformer decoder",
        "encoder-decoder architectures",
        "pre-trained Transformer decoder",
        "LLMs",
        "NMT encoding",
        "NMT decoder",
        "KV cache"
      ]
    },
    "publishedAt": "2025-03-09T08:54:05.000Z",
    "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
    "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve 2.4 sim 6.5 times inference speedups and a 75% reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06594.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06492",
      "authors": [
        {
          "_id": "67cfe557ad91643b5cb7d2c6",
          "user": {
            "_id": "67cd327432668b04f4555270",
            "avatarUrl": "/avatars/15e2cef976cbe05c4c5858c88dccf4af.svg",
            "isPro": false,
            "fullname": "Yanling Wang",
            "user": "WYLing",
            "type": "user"
          },
          "name": "Yanling Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T16:09:28.071Z",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c7",
          "name": "Yihan Zhao",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c8",
          "name": "Xiaodong Chen",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c9",
          "name": "Shasha Guo",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2ca",
          "name": "Lixin Liu",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cb",
          "name": "Haoyang Li",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cc",
          "name": "Yong Xiao",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cd",
          "name": "Jing Zhang",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2ce",
          "name": "Qi Li",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cf",
          "name": "Ke Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T07:25:32.000Z",
      "title": "VisualSimpleQA: 大規模ビジョン-言語モデルの事実探求問答における離れた評価のためのベンチマーク",
      "summary": "大視覚言語モデル（LVLMs）は驚異的な達成を示していますが、事実的な回答の生成は事実探求問答QAにおいてもまだ多いです。現在の多モデル事実探求ベンチマークは主にモデル出力と真値回答との比較に焦点を当てているため、モデルの性能の詳細な理解には限られています。この隙を埋めるために、VisualSimpleQAという多モデル事実探求ベンチマークを紹介します。これは2つの特徴を持ちます。1. LVLMsの視覚と言語モデルのストリームラインおよびデコープレード評価を可能にします。2. 難易度の定義が明確であり、ヒューマンアノテーションをガイドし、VisualSimpleQA-hardという難しいサブセットの抽出を促進します。15モデルの実験により、GPT-4oなどの最先端モデルもVisualSimpleQAでの多モデル事実探求QAの正確性は60%以上で、VisualSimpleQA-hardでは30%以上です。また、これらのモデルのデコープレード評価は視覚と言語モジュールの両方で大幅な改善の可能性を示しています。データセットはhttps://huggingface.co/datasets/WYLing/VisualSimpleQAから利用できます。",
      "upvotes": 1,
      "discussionId": "67cfe55bad91643b5cb7d3fb",
      "ai_keywords": [
        "Large vision-language models",
        "fact-seeking question answering",
        "multimodal benchmarks",
        "visual modality",
        "linguistic modality",
        "VisualSimpleQA",
        "VisualSimpleQA-hard",
        "GPT-4",
        "multimodal fact-seeking QA",
        "decoupled evaluation"
      ]
    },
    "publishedAt": "2025-03-09T03:25:32.000Z",
    "title": "VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large\n  Vision-Language Models in Fact-Seeking Question Answering",
    "summary": "Large vision-language models (LVLMs) have demonstrated remarkable\nachievements, yet the generation of non-factual responses remains prevalent in\nfact-seeking question answering (QA). Current multimodal fact-seeking\nbenchmarks primarily focus on comparing model outputs to ground truth answers,\nproviding limited insights into the performance of modality-specific modules.\nTo bridge this gap, we introduce VisualSimpleQA, a multimodal fact-seeking\nbenchmark with two key features. First, it enables streamlined and decoupled\nevaluation of LVLMs in visual and linguistic modalities. Second, it\nincorporates well-defined difficulty criteria to guide human annotation and\nfacilitates the extraction of a challenging subset, VisualSimpleQA-hard.\nExperiments on 15 LVLMs show that even state-of-the-art models such as GPT-4o\nachieve merely 60%+ correctness in multimodal fact-seeking QA on VisualSimpleQA\nand 30%+ on VisualSimpleQA-hard. Furthermore, the decoupled evaluation across\nthese models highlights substantial opportunities for improvement in both\nvisual and linguistic modules. The dataset is available at\nhttps://huggingface.co/datasets/WYLing/VisualSimpleQA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06492.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05860",
      "authors": [
        {
          "_id": "67d0a239967ead9b5aff9883",
          "user": {
            "_id": "655a627aab0644b531a02eb1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/9rW6X1idfx1p5omky67D6.jpeg",
            "isPro": false,
            "fullname": "Roham Koohestani",
            "user": "RohamKoohestani",
            "type": "user"
          },
          "name": "Roham Koohestani",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-12T08:41:54.151Z",
          "hidden": false
        },
        {
          "_id": "67d0a239967ead9b5aff9884",
          "user": {
            "_id": "655213d1968a2554a5e8212a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3XM_b9imWk-pwoueJwAZB.jpeg",
            "isPro": false,
            "fullname": "Philippe de Bekker",
            "user": "philippedebekker",
            "type": "user"
          },
          "name": "Philippe de Bekker",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T20:51:06.669Z",
          "hidden": false
        },
        {
          "_id": "67d0a239967ead9b5aff9885",
          "name": "Maliheh Izadi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T18:44:32.000Z",
      "title": "ソフトウェア工学でのAIモデルのベンチマーク：レビュー、検索ツールと拡張プロトコル",
      "summary": "ベンチマークは、一貫性のある評価と再現性の確保に重要です。人工知能（AI）をソフトウェア工学（SE）に統合することにより、コード生成やバグ修正などのタスクに対するベンチマークが極めて多くなりました。しかし、この増加は以下の諸問題を伴います：1. 各タスクにおけるベンチマーク知識の散在性、2. 関連性のあるベンチマークの選択の難しさ、3. ベンチマーク開発の統一標準の欠如、4. 現在のベンチマークの制限性。本論文では、173件の研究を検討し、204件のAI4SEベンチマークを識別しました。これらのベンチマークをクラス分けし、制限性を分析し、実践の欠点を明らかにしました。この論文の基礎上、コンテキストの自動クラスタリングを用いた関連する研究のコンテキストを検索するためのセマンティックサーチツール「BenchScout」を開発しました。22名の参加者とのユーザーステージで、BenchScoutの可用性、有効性、直感性について評価し、5点の平均スコアが4.5、4.0、4.1でした。ベンチマーク標準の向上のために、BenchFrameというユニークな方法を提案しました。ケーススタディとして、HumanEvalベンチマークにBenchFrameを適用し、主な制限を解決しました。これにより、HumanEvalNextが以下の特徴を持つようになりました：1. 誤りの修正、2. 言語変換の改善、3. テストカバレーションの拡大、4. 難易度の向上。その後、HumanEval、HumanEvalPlus、HumanEvalNextにおいて10つの最先端のコード言語モデルを評価しました。HumanEvalNextでは、HumanEvalとHumanEvalPlusに対してポイント1のスコアがそれぞれ31.22%と19.94%減少しました。",
      "upvotes": 1,
      "discussionId": "67d0a23a967ead9b5aff98da",
      "projectPage": "https://evalpro.online/",
      "githubRepo": "https://github.com/AISE-TUDelft/AI4SE-benchmarks",
      "ai_keywords": [
        "AI4SE (Artificial Intelligence in Software Engineering)",
        "BenchScout",
        "semantic search tool",
        "automated clustering",
        "BenchFrame",
        "HumanEval",
        "HumanEvalNext",
        "pass@1 score"
      ]
    },
    "publishedAt": "2025-03-07T13:44:32.000Z",
    "title": "Benchmarking AI Models in Software Engineering: A Review, Search Tool,\n  and Enhancement Protocol",
    "summary": "Benchmarks are essential for consistent evaluation and reproducibility. The\nintegration of Artificial Intelligence into Software Engineering (AI4SE) has\ngiven rise to numerous benchmarks for tasks such as code generation and bug\nfixing. However, this surge presents challenges: (1) scattered benchmark\nknowledge across tasks, (2) difficulty in selecting relevant benchmarks, (3)\nthe absence of a uniform standard for benchmark development, and (4)\nlimitations of existing benchmarks. In this paper, we review 173 studies and\nidentify 204 AI4SE benchmarks. We classify these benchmarks, analyze their\nlimitations, and expose gaps in practices. Based on our review, we created\nBenchScout, a semantic search tool to find relevant benchmarks, using automated\nclustering of the contexts from associated studies. We conducted a user study\nwith 22 participants to evaluate BenchScout's usability, effectiveness, and\nintuitiveness which resulted in average scores of 4.5, 4.0, and 4.1 out of 5.\nTo advance benchmarking standards, we propose BenchFrame, a unified method to\nenhance benchmark quality. As a case study, we applied BenchFrame to the\nHumanEval benchmark and addressed its main limitations. This led to\nHumanEvalNext, featuring (1) corrected errors, (2) improved language\nconversion, (3) expanded test coverage, and (4) increased difficulty. We then\nevaluated ten state-of-the-art code language models on HumanEval,\nHumanEvalPlus, and HumanEvalNext. On HumanEvalNext, models showed a pass@1\nscore reduction of 31.22% and 19.94% compared to HumanEval and HumanEvalPlus,\nrespectively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05860.png",
    "numComments": 1,
    "isAuthorParticipating": true
  }
]