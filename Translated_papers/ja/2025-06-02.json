[
  {
    "paper": {
      "id": "2505.24864",
      "authors": [
        {
          "_id": "683d2d05ae87a04bca311b22",
          "name": "Mingjie Liu",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b23",
          "name": "Shizhe Diao",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b24",
          "name": "Ximing Lu",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b25",
          "name": "Jian Hu",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b26",
          "name": "Xin Dong",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b27",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b28",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "683d2d05ae87a04bca311b29",
          "name": "Yi Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:59:01.000Z",
      "submittedOnDailyAt": "2025-06-02T03:18:21.654Z",
      "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models",
      "submittedOnDailyBy": {
        "_id": "633bd54b00732349209a18fe",
        "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
        "isPro": false,
        "fullname": "Shizhe Diao",
        "user": "shizhediao",
        "type": "user"
      },
      "summary": "最近の論理中心的言語モデルの進展は、強化学習（RL）をモデルと確認可能な報酬に対応させる有望な方法として挙げ出しています。しかし、RLが実際にモデルの論理能力を拡大するか、またはベースモデルの分布における高報酬の出力を強調してしかも、モデルの論理能力を拡大するかどうか、そして、長期的なRL計算のスケーリングが理由論の性能向上に確率的に導くかどうかは、これらの議論にまとまっています。本研究では、これらの主流の仮定を挑戦し、長期的なRL（ProRL）の訓練が、ベースモデルによっては訪問できない新しい論理戦略を発見することができることを示します。ProRLという新しい訓練方法を提案し、KL分散制御、参照ポリシーリセット、さらに多様なタスクのセットを含むものです。実験的な分析により、RLを訓練したモデルは、ベースモデルと比較して、幅広い範囲でパス@k評価によって続続して優れていることが明らかになります。ベースモデルが完全に失敗する場合も、試行回数によらずにも同様の結果を示します。また、論理の境界の改善は、ベースモデルのタスクの実力と訓練時間と強烈に関連し、RLが時間により解決空間の新しい領域を探索し、それらを人々に提供することができることを示します。これらの発見は、RLが言語モデルの論理境界を意味的に拡大する条件について新しいコンプライアンスを提供し、長期的なRLの論理に関する将来の研究の基盤を立てます。モデルの重みを公開し、進める研究にサポートを提供します：https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",
      "upvotes": 44,
      "discussionId": "683d2d08ae87a04bca311bd4",
      "ai_summary": "Prolonged reinforcement learning training (ProRL) uncovers novel reasoning strategies in language models, outperforming base models and suggesting meaningful expansion of reasoning capabilities.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "ProRL",
        "KL divergence control",
        "reference policy resetting",
        "pass@k evaluations",
        "reasoning boundary improvements",
        "task competence",
        "long-horizon RL"
      ]
    },
    "publishedAt": "2025-05-30T13:59:01.000Z",
    "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in\n  Large Language Models",
    "summary": "Recent advances in reasoning-centric language models have highlighted\nreinforcement learning (RL) as a promising method for aligning models with\nverifiable rewards. However, it remains contentious whether RL truly expands a\nmodel's reasoning capabilities or merely amplifies high-reward outputs already\nlatent in the base model's distribution, and whether continually scaling up RL\ncompute reliably leads to improved reasoning performance. In this work, we\nchallenge prevailing assumptions by demonstrating that prolonged RL (ProRL)\ntraining can uncover novel reasoning strategies that are inaccessible to base\nmodels, even under extensive sampling. We introduce ProRL, a novel training\nmethodology that incorporates KL divergence control, reference policy\nresetting, and a diverse suite of tasks. Our empirical analysis reveals that\nRL-trained models consistently outperform base models across a wide range of\npass@k evaluations, including scenarios where base models fail entirely\nregardless of the number of attempts. We further show that reasoning boundary\nimprovements correlates strongly with task competence of base model and\ntraining duration, suggesting that RL can explore and populate new regions of\nsolution space over time. These findings offer new insights into the conditions\nunder which RL meaningfully expands reasoning boundaries in language models and\nestablish a foundation for future work on long-horizon RL for reasoning. We\nrelease model weights to support further research:\nhttps://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633bd54b00732349209a18fe",
      "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
      "fullname": "Shizhe Diao",
      "name": "shizhediao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24863",
      "authors": [
        {
          "_id": "683d0b3de2a7d8d9778bd141",
          "user": {
            "_id": "6719bfd07c6e6c83a388aeae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719bfd07c6e6c83a388aeae/jHxryk04dzHo23TX5F5sz.png",
            "isPro": false,
            "fullname": "Junyu Zhang",
            "user": "jyzhang1208",
            "type": "user"
          },
          "name": "Junyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:59.716Z",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd142",
          "user": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "isPro": false,
            "fullname": "Runpei Dong",
            "user": "RunpeiDong",
            "type": "user"
          },
          "name": "Runpei Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:41:03.079Z",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd143",
          "name": "Han Wang",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd144",
          "name": "Xuying Ning",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd145",
          "name": "Haoran Geng",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd146",
          "name": "Peihao Li",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd147",
          "name": "Xialin He",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd148",
          "name": "Yutong Bai",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd149",
          "name": "Jitendra Malik",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd14a",
          "name": "Saurabh Gupta",
          "hidden": false
        },
        {
          "_id": "683d0b3de2a7d8d9778bd14b",
          "name": "Huan Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/dBNLCtnWBtBclw0ZZsYBU.png"
      ],
      "publishedAt": "2025-05-30T17:58:36.000Z",
      "submittedOnDailyAt": "2025-06-02T00:55:04.615Z",
      "title": "AlphaOne: テストタイムでの速いと遅い思考のモデル",
      "submittedOnDailyBy": {
        "_id": "6201fc5d91d53938a6432fbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
        "isPro": false,
        "fullname": "Runpei Dong",
        "user": "RunpeiDong",
        "type": "user"
      },
      "summary": "この論文では、検証時に大規模な理由論モデル（LRMs）の理由論進捗を調節するための普遍的なフレームワークAlphaOne（alpha1）を紹介します。alpha1は、普遍的なパラメータalphaでスケーリングされた考えの段階を表すalpha momentを初めて紹介します。alpha momentの中では、理由論進捩点トークンの挿入をベルニウス確率過程としてモデル化して、スローな考えの進行を動的にスケジューリングします。alpha momentの後、alpha1は理由論を終了するための理由論終了トークンを使用して、スローな考えを確定的に終了させ、高速な理由論と効率的な答えの生成を促進します。このアプローチは、単調的なスケーリング方法を統一し、柔軟かつ密なスローから高速の理由論の調節を可能にします。数学、コーディング、科学分野の複数の難しいベンチマークでの拡張的な実験研究は、alpha1の優れた理由論能力と効率について示唆します。プロジェクトページ：https://alphaone-project.github.io/",
      "upvotes": 29,
      "discussionId": "683d0b3ee2a7d8d9778bd1ce",
      "projectPage": "https://alphaone-project.github.io/",
      "githubRepo": "https://github.com/ASTRAL-Group/AlphaOne",
      "ai_summary": "AlphaOne dynamically modulates reasoning in large models by introducing $\\alpha$ moment and Bernoulli process for slow thinking, improving efficiency and capability across diverse domains.",
      "ai_keywords": [
        "AlphaOne",
        "$\\alpha$ moment",
        "Bernoulli stochastic process",
        "large reasoning models",
        "reasoning transition tokens",
        "end-of-thinking token",
        "monotonic scaling methods",
        "fast reasoning",
        "efficient answer generation"
      ]
    },
    "publishedAt": "2025-05-30T13:58:36.000Z",
    "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time",
    "summary": "This paper presents AlphaOne (alpha1), a universal framework for\nmodulating reasoning progress in large reasoning models (LRMs) at test time.\nalpha1 first introduces alpha moment, which represents the scaled\nthinking phase with a universal parameter alpha. Within this scaled\npre-alpha moment phase, it dynamically schedules slow thinking transitions\nby modeling the insertion of reasoning transition tokens as a Bernoulli\nstochastic process. After the alpha moment, alpha1 deterministically\nterminates slow thinking with the end-of-thinking token, thereby fostering fast\nreasoning and efficient answer generation. This approach unifies and\ngeneralizes existing monotonic scaling methods by enabling flexible and dense\nslow-to-fast reasoning modulation. Extensive empirical studies on various\nchallenging benchmarks across mathematical, coding, and scientific domains\ndemonstrate alpha1's superior reasoning capability and efficiency. Project\npage: https://alphaone-project.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/dBNLCtnWBtBclw0ZZsYBU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24863.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6201fc5d91d53938a6432fbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
      "fullname": "Runpei Dong",
      "name": "RunpeiDong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24867",
      "authors": [
        {
          "_id": "683d3d6f3f97feb881155aef",
          "user": {
            "_id": "5df7ca7cda6d0311fd3d53f2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5df7ca7cda6d0311fd3d53f2/dtAoDSqgNxeO9AYg9V3na.jpeg",
            "isPro": false,
            "fullname": "Ujjwal Upadhyay",
            "user": "ujjwal9",
            "type": "user"
          },
          "name": "Ujjwal Upadhyay",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-02T05:58:12.617Z",
          "hidden": false
        },
        {
          "_id": "683d3d6f3f97feb881155af0",
          "user": {
            "_id": "65262a396b41932089fd7bae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
            "isPro": true,
            "fullname": "Mukul Ranjan",
            "user": "mukul54",
            "type": "user"
          },
          "name": "Mukul Ranjan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:23.895Z",
          "hidden": false
        },
        {
          "_id": "683d3d6f3f97feb881155af1",
          "name": "Zhiqiang Shen",
          "hidden": false
        },
        {
          "_id": "683d3d6f3f97feb881155af2",
          "name": "Mohamed Elhoseiny",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:59:12.000Z",
      "submittedOnDailyAt": "2025-06-02T04:31:40.253Z",
      "title": "タイムブリンジング：ビデオ言語モデルが人間のように見ることができない理由",
      "submittedOnDailyBy": {
        "_id": "65262a396b41932089fd7bae",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
        "isPro": true,
        "fullname": "Mukul Ranjan",
        "user": "mukul54",
        "type": "user"
      },
      "summary": "最近の視覚言語モデル（VLMs）の進展は、映画の空間時間関係の理解に驚異的な進歩を遂げました。しかし、空間情報が隠された場合、これらのモデルは純粋な時間的パターンを捉えることができません。私たちは、生物的信号化から潜見のコミュニケーションの自然現象を模倣したノイズフレームの時間的配列で情報を符号化したベンチマークを紹介します。興味深いですが、人間はこれらの配列で形状、テキスト、パターンを98%以上の精度で認識できますが、最先端のVLMsは0%の精度を達成しません。この性能間隔は、空間的特徴量の過度依存と時間的コードから意味を抽出できないことを示しています。また、低い空間的SNRを持つデータセットで訓練された場合、モデルの時間的理解は人間の認識よりも急速に低下し、特に細かい時間的理由を必要とするタスクでは特に強くなります。この制限を克服するには、空間的依存関係を時間的処理から離れる新しいアーキテクチャや訓練パラダイムが必要です。我々のシステマティックな分析により、この問題はモデルのスケールやアーキテクチャにもまた延びています。私たちは、SpookyBenchをリリースして、時間的パターン認識の研究を促進し、人間と機械の映画理解の間の隙を埋めることを目指しています。データセットとコードは、プロジェクトウェブサイト（https://timeblindness.github.io/）で提供されています。",
      "upvotes": 23,
      "discussionId": "683d3d743f97feb881155c56",
      "projectPage": "https://timeblindness.github.io",
      "githubRepo": "https://github.com/TimeBlindness/time-blindness",
      "ai_summary": "SpookyBench is a benchmark for temporal pattern recognition in videos that highlights the limitations of vision-language models in processing noise-like frames without spatial information.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "spatio-temporal relationships",
        "temporal sequences",
        "noise-like frames",
        "biological signaling",
        "covert communication",
        "frame-level spatial features",
        "temporal understanding",
        "data sets",
        "low spatial signal-to-noise ratios",
        "SNR",
        "temporal reasoning",
        "novel architectures",
        "training paradigms",
        "systematic analysis"
      ]
    },
    "publishedAt": "2025-05-30T13:59:12.000Z",
    "title": "Time Blindness: Why Video-Language Models Can't See What Humans Can?",
    "summary": "Recent advances in vision-language models (VLMs) have made impressive strides\nin understanding spatio-temporal relationships in videos. However, when spatial\ninformation is obscured, these models struggle to capture purely temporal\npatterns. We introduce SpookyBench, a benchmark where information is\nencoded solely in temporal sequences of noise-like frames, mirroring natural\nphenomena from biological signaling to covert communication. Interestingly,\nwhile humans can recognize shapes, text, and patterns in these sequences with\nover 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance\ngap highlights a critical limitation: an over-reliance on frame-level spatial\nfeatures and an inability to extract meaning from temporal cues. Furthermore,\nwhen trained in data sets with low spatial signal-to-noise ratios (SNR),\ntemporal understanding of models degrades more rapidly than human perception,\nespecially in tasks requiring fine-grained temporal reasoning. Overcoming this\nlimitation will require novel architectures or training paradigms that decouple\nspatial dependencies from temporal processing. Our systematic analysis shows\nthat this issue persists across model scales and architectures. We release\nSpookyBench to catalyze research in temporal pattern recognition and bridge the\ngap between human and machine video understanding. Dataset and code has been\nmade available on our project website: https://timeblindness.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24867.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65262a396b41932089fd7bae",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
      "fullname": "Mukul Ranjan",
      "name": "mukul54",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18842",
      "authors": [
        {
          "_id": "6839543d6451d371f9e834ec",
          "name": "Jiwan Chung",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834ed",
          "user": {
            "_id": "646aecb04c1cd18b497a50ee",
            "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
            "isPro": false,
            "fullname": "Junhyeok Kim",
            "user": "kjunh",
            "type": "user"
          },
          "name": "Junhyeok Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:46:37.442Z",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834ee",
          "user": {
            "_id": "67021743e4d49b157afd8260",
            "avatarUrl": "/avatars/2a22a18cd45f6d115e8a3a5d1e477dcb.svg",
            "isPro": false,
            "fullname": "Siyeol Kim",
            "user": "siyeolkim",
            "type": "user"
          },
          "name": "Siyeol Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:46:34.334Z",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834ef",
          "name": "Jaeyoung Lee",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834f0",
          "name": "Min Soo Kim",
          "hidden": false
        },
        {
          "_id": "6839543d6451d371f9e834f1",
          "name": "Youngjae Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T19:30:47.000Z",
      "submittedOnDailyAt": "2025-06-02T02:58:04.513Z",
      "title": "そのひとつだけ見ないようにしない：選択的な可視的再訪問に向けた多タイプインタラクティブな理由論のため",
      "submittedOnDailyBy": {
        "_id": "646aecb04c1cd18b497a50ee",
        "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
        "isPro": false,
        "fullname": "Junhyeok Kim",
        "user": "kjunh",
        "type": "user"
      },
      "summary": "We present v1, a lightweight extension to Multimodal Large Language Models (MLLMs) that enables selective visual revisitation during inference. While current MLLMs typically consume visual input only once and reason purely over internal memory, v1 introduces a simple point-and-copy mechanism that allows the model to dynamically retrieve relevant image regions throughout the reasoning process. This mechanism augments existing architectures with minimal modifications, enabling contextual access to visual tokens based on the model's evolving hypotheses. To train this capability, we construct v1g, a dataset of 300K multimodal reasoning traces with interleaved visual grounding annotations. Experiments on three multimodal mathematical reasoning benchmarks -- MathVista, MathVision, and MathVerse -- demonstrate that v1 consistently improves performance over comparable baselines, particularly on tasks requiring fine-grained visual reference and multi-step reasoning. Our results suggest that dynamic visual access is a promising direction for enhancing grounded multimodal reasoning. Code, models, and data will be released to support future research.",
      "upvotes": 20,
      "discussionId": "6839543f6451d371f9e83544",
      "githubRepo": "https://github.com/jun297/v1",
      "ai_summary": "v1 enhances Multimodal Large Language Models by enabling selective and dynamic visual region retrieval during inference, improving performance on multimodal reasoning tasks.",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "point-and-copy mechanism",
        "visual tokens",
        "multimodal reasoning traces",
        "visual grounding annotations",
        "MathVista",
        "MathVision",
        "MathVerse",
        "grounded multimodal reasoning"
      ]
    },
    "publishedAt": "2025-05-24T15:30:47.000Z",
    "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with\n  Selective Visual Revisitation",
    "summary": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18842.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646aecb04c1cd18b497a50ee",
      "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
      "fullname": "Junhyeok Kim",
      "name": "kjunh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14752",
      "authors": [
        {
          "_id": "6832c2c8ba29b909f4013a6d",
          "user": {
            "_id": "67569b1860146dd8c9c8008f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
            "isPro": false,
            "fullname": "Yihong Tang",
            "user": "HYTYH",
            "type": "user"
          },
          "name": "Yihong Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-26T08:12:00.941Z",
          "hidden": false
        },
        {
          "_id": "6832c2c8ba29b909f4013a6e",
          "name": "Menglin Kong",
          "hidden": false
        },
        {
          "_id": "6832c2c8ba29b909f4013a6f",
          "name": "Lijun Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/VaA9NxCa0ncxzh03aZE8c.png",
        "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/oEgXPOzTQVTDfuTp5Z575.png"
      ],
      "publishedAt": "2025-05-20T13:35:38.000Z",
      "submittedOnDailyAt": "2025-06-02T02:10:16.659Z",
      "title": "大型データ合成用言語モデル",
      "submittedOnDailyBy": {
        "_id": "67569b1860146dd8c9c8008f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
        "isPro": false,
        "fullname": "Yihong Tang",
        "user": "HYTYH",
        "type": "user"
      },
      "summary": "合成数据生成，以忠实地捕捉现实世界分布的统计结构，是数据建模中的一个基本挑战。传统的方法通常依赖于强烈的参数假设或手动结构设计，在高维或异质领域中往往难以应对。最近在大型语言模型（LLMs）方面的进展揭示了它们作为现实世界分布的灵活、高维先验的潜力。然而，当应用于数据合成时，标准的基于LLM的采样方法效率低下，受限于固定的上下文限制，且无法确保统计对齐。鉴于此，我们提出了LLMSynthor，一个通用的数据合成框架，该框架将LLM转化为由分布反馈引导的结构感知模拟器。LLMSynthor将LLM视为非参数Copula模拟器，用于建模高阶依赖关系，并引入LLM提案采样，以生成基于实际情况的提案分布，从而在没有拒绝采样的情况下提高采样效率。通过最小化摘要统计空间中的差异，迭代合成循环对真实和合成数据进行对齐，同时逐步揭示和细化潜在的生成结构。我们在受控和现实世界环境中使用包含结构化和非结构化格式的异构数据集（例如，电子商务、人口和流动性）对LLMSynthor进行了评估，这些数据集涉及隐私敏感领域。LLMSynthor生成的合成数据显示出高统计保真度、实际效用和跨数据适应性，使其成为经济学、社会科学、城市研究等领域的宝贵工具。",
      "upvotes": 19,
      "discussionId": "6832c2c9ba29b909f4013aea",
      "projectPage": "https://yihongt.github.io/llmsynthor_web/",
      "githubRepo": "https://github.com/YihongT/LLMSynthor",
      "ai_summary": "LLMSynthor enhances LLMs for efficient and statistically accurate data synthesis through distributional feedback and proposal sampling.",
      "ai_keywords": [
        "Large Language Models",
        "LLMSynthor",
        "nonparametric copula simulator",
        "LLM Proposal Sampling",
        "summary statistics space",
        "synthetic data",
        "statistical fidelity",
        "practical utility",
        "cross-data adaptability"
      ]
    },
    "publishedAt": "2025-05-20T09:35:38.000Z",
    "title": "Large Language Models for Data Synthesis",
    "summary": "Generating synthetic data that faithfully captures the statistical structure\nof real-world distributions is a fundamental challenge in data modeling.\nClassical approaches often depend on strong parametric assumptions or manual\nstructural design and struggle in high-dimensional or heterogeneous domains.\nRecent progress in Large Language Models (LLMs) reveals their potential as\nflexible, high-dimensional priors over real-world distributions. However, when\napplied to data synthesis, standard LLM-based sampling is inefficient,\nconstrained by fixed context limits, and fails to ensure statistical alignment.\nGiven this, we introduce LLMSynthor, a general framework for data synthesis\nthat transforms LLMs into structure-aware simulators guided by distributional\nfeedback. LLMSynthor treats the LLM as a nonparametric copula simulator for\nmodeling high-order dependencies and introduces LLM Proposal Sampling to\ngenerate grounded proposal distributions that improve sampling efficiency\nwithout requiring rejection. By minimizing discrepancies in the summary\nstatistics space, the iterative synthesis loop aligns real and synthetic data\nwhile gradually uncovering and refining the latent generative structure. We\nevaluate LLMSynthor in both controlled and real-world settings using\nheterogeneous datasets in privacy-sensitive domains (e.g., e-commerce,\npopulation, and mobility) that encompass both structured and unstructured\nformats. The synthetic data produced by LLMSynthor shows high statistical\nfidelity, practical utility, and cross-data adaptability, positioning it as a\nvaluable tool across economics, social science, urban studies, and beyond.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/VaA9NxCa0ncxzh03aZE8c.png",
      "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/oEgXPOzTQVTDfuTp5Z575.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14752.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67569b1860146dd8c9c8008f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
      "fullname": "Yihong Tang",
      "name": "HYTYH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24098",
      "authors": [
        {
          "_id": "683d2cee5bdbb3803e42bc8a",
          "name": "Zhongmou He",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8b",
          "name": "Yee Man Choi",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8c",
          "name": "Kexun Zhang",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8d",
          "name": "Jiabao Ji",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8e",
          "user": {
            "_id": "65a374a59acab1998092a9bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a374a59acab1998092a9bc/M3s_7bSf9G-6b9nLg7N3Z.jpeg",
            "isPro": false,
            "fullname": "Antonio",
            "user": "JuntingZhou",
            "type": "user"
          },
          "name": "Junting Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:34.926Z",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc8f",
          "name": "Dejia Xu",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc90",
          "name": "Ivan Bercovich",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc91",
          "name": "Aidan Zhang",
          "hidden": false
        },
        {
          "_id": "683d2cee5bdbb3803e42bc92",
          "name": "Lei Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62ee423b4bebb4ab55c674b1/yE3pB5JGaOf-sdyDjpCk6.png"
      ],
      "publishedAt": "2025-05-30T01:00:34.000Z",
      "submittedOnDailyAt": "2025-06-02T03:20:27.903Z",
      "title": "HardTests: 高品質テストケースの合成されたLLMコーディング",
      "submittedOnDailyBy": {
        "_id": "62ee423b4bebb4ab55c674b1",
        "avatarUrl": "/avatars/ce2797937e8225937fc84d6847d50077.svg",
        "isPro": false,
        "fullname": "Kexun Zhang",
        "user": "k1z",
        "type": "user"
      },
      "summary": "バリデータライターは、大規模言語モデル（LLM）の推理に重要な役割を果たし、後学習技術のような強化学習に必要とされる。しかし、難しいコーディング問題では、信頼性のあるバリデータライターが得られにくい。これは、よく隠された間違った解決策は、人間が読んだように書かれたエッジケースによってしか検出されることによる。この問題に対処するために、私たちはHARDTESTGENを提案し、LLMを用いた高品質のテスト合成のパイプラインを提案します。このパイプラインを用いて、私たちは47k問題と合成された高品質のテストを含む詳細なコンペティティングプログラミングデータセットHARDTESTSをカレーレードします。既存のテストと比較して、HARDTESTGENのテストはLLM生成されたコードの評価で精度が11.3パーセント点高く、再現率が17.5パーセント点高く示します。より難しい問題に対しては、精度の向上は最大で40点に達します。HARDTESTSはモデルの訓練にもより効果的であり、これは次世代のコード生成性能によって測定されます。私たちのデータセットと合成パイプラインを公開します。",
      "upvotes": 18,
      "discussionId": "683d2cef5bdbb3803e42bccc",
      "projectPage": "https://leililab.github.io/HardTests/",
      "ai_summary": "HARDTESTGEN creates a large, high-quality competitive programming dataset to enhance the precision and recall of verifiers in evaluating LLM-generated code.",
      "ai_keywords": [
        "LLM reasoning",
        "reinforcement learning",
        "verifiers",
        "test synthesis",
        "LLMs",
        "competitive programming",
        "synthetic tests",
        "precision",
        "recall",
        "code generation performance"
      ]
    },
    "publishedAt": "2025-05-29T21:00:34.000Z",
    "title": "HardTests: Synthesizing High-Quality Test Cases for LLM Coding",
    "summary": "Verifiers play a crucial role in large language model (LLM) reasoning, needed\nby post-training techniques such as reinforcement learning. However, reliable\nverifiers are hard to get for difficult coding problems, because a\nwell-disguised wrong solution may only be detected by carefully human-written\nedge cases that are difficult to synthesize. To address this issue, we propose\nHARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this\npipeline, we curate a comprehensive competitive programming dataset HARDTESTS\nwith 47k problems and synthetic high-quality tests. Compared with existing\ntests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points\nhigher and recall that is 17.5 percentage points higher when evaluating\nLLM-generated code. For harder problems, the improvement in precision can be as\nlarge as 40 points. HARDTESTS also proves to be more effective for model\ntraining, measured by downstream code generation performance. We will\nopen-source our dataset and synthesis pipeline at\nhttps://leililab.github.io/HardTests/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62ee423b4bebb4ab55c674b1/yE3pB5JGaOf-sdyDjpCk6.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24098.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ee423b4bebb4ab55c674b1",
      "avatarUrl": "/avatars/ce2797937e8225937fc84d6847d50077.svg",
      "fullname": "Kexun Zhang",
      "name": "k1z",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24878",
      "authors": [
        {
          "_id": "683d160e51706d12b2c6f79f",
          "name": "Yaxin Luo",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a0",
          "name": "Zhaoyi Li",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a1",
          "name": "Jiacheng Liu",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a2",
          "user": {
            "_id": "683d2ac900c71614bab8ea02",
            "avatarUrl": "/avatars/7cb1a5c2c778774262a7d7cb6d309abe.svg",
            "isPro": false,
            "fullname": "Jiacheng Cui",
            "user": "jiachengcui888",
            "type": "user"
          },
          "name": "Jiacheng Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:52.498Z",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a3",
          "name": "Xiaohan Zhao",
          "hidden": false
        },
        {
          "_id": "683d160e51706d12b2c6f7a4",
          "name": "Zhiqiang Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-02T01:40:24.093Z",
      "title": "Open CaptchaWorld: モノモダルLLMアグエントのテストとベンチマークのためのコンピューターベースのプラットフォーム",
      "submittedOnDailyBy": {
        "_id": "653cb809b424289c5f384a02",
        "avatarUrl": "/avatars/a1565ab5ae51075c75d6857d64c426a8.svg",
        "isPro": true,
        "fullname": "YaxinLuo",
        "user": "YaxinLuo",
        "type": "user"
      },
      "summary": "CAPTCHAsは、実世界のアプリケーションでウェブアグェントの採用に重大なボトルネックとなっている。これらは、終端から終端までの自動化タスクを完了させることを防ぎこむことが多い。しかし、現代の多モデルLLMアグェントは、静的認識タスクでは驚異的な性能を示しているが、CAPTCHAのようなインタラクティブな、多段階推理の挑戦に対する能力は大きく検証されていない。このギャップを解決するために、私たちは、Open CaptchaWorldを紹介します。これは、最初のWebベースベンチマークとプラットフォームで、多様なダイナミックなCAPTCHA謎を通じて、MLLMポーダードアグェントの視覚推理とインタラクション能力を評価するために設計されています。ベンチマークは、20種類の現代CAPTCHAを含む225カウントで、新規のメトリック「CAPTCHA Reasoning Depth」を使用して注釈されています。これは、各謎を解くに必要な認知的および機能的ステップの数を定量化しています。実験結果によると、人間は近極めて完全なスコアを達成し、最先端のMLLMアグェントは显著に難しく、Browser-Use Openai-o3の成功率は最大でも40.0%で、人間レベルの性能の93.3%よりも低いことがわかります。これは、Open CaptchaWorldが現在の多モデルアグェントの限界を診断するための重要なベンチマークとしての役割を明らかにし、より強固な多モデル推理システムの開発をガイドすることを示しています。コードとデータは、このURLで利用できます。",
      "upvotes": 12,
      "discussionId": "683d160f51706d12b2c6f7f4",
      "githubRepo": "https://github.com/MetaAgentX/OpenCaptchaWorld",
      "ai_summary": "Open CaptchaWorld benchmark evaluates MLLM-powered agents on diverse CAPTCHA puzzles, revealing significant performance gaps compared to humans.",
      "ai_keywords": [
        "multimodal LLM",
        "CAPTCHA",
        "visual reasoning",
        "interaction capabilities",
        "CAPTCHA Reasoning Depth",
        "Browser-Use Openai-o3"
      ]
    },
    "publishedAt": "2025-05-30T13:59:55.000Z",
    "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents",
    "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24878.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653cb809b424289c5f384a02",
      "avatarUrl": "/avatars/a1565ab5ae51075c75d6857d64c426a8.svg",
      "fullname": "YaxinLuo",
      "name": "YaxinLuo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24862",
      "authors": [
        {
          "_id": "683d54f364b44c0ccabb9e65",
          "name": "Cailin Zhuang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e66",
          "name": "Ailin Huang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e67",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:49.393Z",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e68",
          "name": "Jingwei Wu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e69",
          "name": "Yaoqi Hu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6a",
          "name": "Jiaqi Liao",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6b",
          "name": "Zhewei Huang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6c",
          "name": "Hongyuan Wang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6d",
          "name": "Xinyao Liao",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6e",
          "name": "Weiwei Cai",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e6f",
          "name": "Hengyuan Xu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e70",
          "name": "Xuanyang Zhang",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e71",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e72",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "683d54f364b44c0ccabb9e73",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ZHmu_F8c4mbhiPtomY6H7.mp4"
      ],
      "publishedAt": "2025-05-30T17:58:21.000Z",
      "submittedOnDailyAt": "2025-06-02T06:09:52.296Z",
      "title": "ViStoryBench: ショートティービジュアライゼーションの総合的なベンチマークシート",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "ストーリービジュアライゼーションは、与えられたノートと参照画像に対して、視覚的に一貫した画像列を生成することを目的としています。最近の生成モデルの進歩により、進展が達しています。実世界的なシナリオでのストーリービジュアライゼーションフレームワークの性能を進めることを目指し、コンプレックスな評価ベンチマーク、ViStoryBenchを導入します。多様なストーリータイプとアーティストスタイルを含む多様なデータセットを集め、モデルの評価をプロット（例：コメディ、ホラー）と視覚的な美術（例：アニメ、3Dレンダリング）などの複数の次元にわたるものとします。ViStoryBenchは、説話構造と視覚的要素のバランスを調整し、単一サテライトと複数サテライトを持つストーリーを挙げ、モデルのキャラクターの一貫性を検証することを目的としています。また、複雑なプロットと複雑な世界設定を含むことで、正確な画像の生成を試みるモデルを課題にします。評価指標の幅広い範囲を持つことで、評価を詳細に行うことを保証し、この構造化された多面性のフレームワークでは、研究者はモデルの強みと弱点を詳細に把握し、目標指向的な改善を促進することができます。",
      "upvotes": 12,
      "discussionId": "683d54f764b44c0ccabb9f60",
      "projectPage": "https://vistorybench.github.io/",
      "githubRepo": "https://github.com/vistorybench/vistorybench"
    },
    "publishedAt": "2025-05-30T13:58:21.000Z",
    "title": "ViStoryBench: Comprehensive Benchmark Suite for Story Visualization",
    "summary": "Story visualization, which aims to generate a sequence of visually coherent\nimages aligning with a given narrative and reference images, has seen\nsignificant progress with recent advancements in generative models. To further\nenhance the performance of story visualization frameworks in real-world\nscenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We\ncollect a diverse dataset encompassing various story types and artistic styles,\nensuring models are evaluated across multiple dimensions such as different\nplots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D\nrenderings). ViStoryBench is carefully curated to balance narrative structures\nand visual elements, featuring stories with single and multiple protagonists to\ntest models' ability to maintain character consistency. Additionally, it\nincludes complex plots and intricate world-building to challenge models in\ngenerating accurate visuals. To ensure comprehensive comparisons, our benchmark\nincorporates a wide range of evaluation metrics assessing critical aspects.\nThis structured and multifaceted framework enables researchers to thoroughly\nidentify both the strengths and weaknesses of different models, fostering\ntargeted improvements.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ZHmu_F8c4mbhiPtomY6H7.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24862.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24196",
      "authors": [
        {
          "_id": "683d29da83edd521f116444c",
          "name": "Longze Chen",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f116444d",
          "name": "Renke Shan",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f116444e",
          "name": "Huiming Wang",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f116444f",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164450",
          "name": "Ziqiang Liu",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164451",
          "name": "Run Luo",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164452",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164453",
          "name": "Hamid Alinejad-Rokny",
          "hidden": false
        },
        {
          "_id": "683d29da83edd521f1164454",
          "name": "Min Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T04:15:06.000Z",
      "submittedOnDailyAt": "2025-06-02T03:23:19.536Z",
      "title": "CLaSp: プロジェクティブ層スキップを用いた自動的な推測処理",
      "submittedOnDailyBy": {
        "_id": "64c7b4d1c547ed5243c07b6c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7b4d1c547ed5243c07b6c/h96CLBj6dcm01soK2UJzr.jpeg",
        "isPro": false,
        "fullname": "Longze Chen",
        "user": "lzchen2001",
        "type": "user"
      },
      "summary": "スペシュライティブデコーディング（SD）は、大規模言語モデル（LLMs）のデコーディングプロセスを加速する有望な方法です。SDの効率性は、草稿モデルと確認モデルの一致性に大きく依存します。しかし、現在の草稿手法は通常、追加のモジュールを訓練する必要があり、これは実装や様々なLLMsの間での相性を確保することが難しいことになります。本論文では、自モデルスペシュライティブデコーディングのためのコンテキスト内の層スキップ戦略として、CLaSpを提案します。先行手法と異なり、CLaSpは追加の草稿モジュールや追加の訓練を必要としません。代りに、確認モデルの中間層をスキップして、コンパクトな草稿モデルを構築するためのプラグインとプレイング機構を使用します。特に、最後の確認ステージからの完全な隠れ状態を目標として、層スキッププロセスを最適化するダイナミックプログラミングアルゴリズムを開発しました。これにより、CLaSpは各確認ステージ後に層スキップ戦略を動的に調整でき、事前最適化されたスキップ層のセットに依存しないようにします。多様な下流タスクの実験結果によると、CLaSpは生成される文章の原動分布を変更しないでも、LLaMA3シリーズモデルで1.3倍～1.7倍のスピードアップを達成します。",
      "upvotes": 10,
      "discussionId": "683d29db83edd521f1164482",
      "ai_summary": "CLaSp, an in-context layer-skipping strategy for self-speculative decoding, accelerates Large Language Model decoding without additional modules or training, achieving a 1.3x to 1.7x speedup on LLaMA3 models.",
      "ai_keywords": [
        "speculative decoding",
        "large language models",
        "draft model",
        "verify model",
        "in-context layer-skipping",
        "dynamic programming algorithm",
        "hidden states",
        "verification stage"
      ]
    },
    "publishedAt": "2025-05-30T00:15:06.000Z",
    "title": "CLaSp: In-Context Layer Skip for Self-Speculative Decoding",
    "summary": "Speculative decoding (SD) is a promising method for accelerating the decoding\nprocess of Large Language Models (LLMs). The efficiency of SD primarily hinges\non the consistency between the draft model and the verify model. However,\nexisting drafting approaches typically require additional modules to be\ntrained, which can be challenging to implement and ensure compatibility across\nvarious LLMs. In this paper, we propose CLaSp, an in-context layer-skipping\nstrategy for self-speculative decoding. Unlike prior methods, CLaSp does not\nrequire additional drafting modules or extra training. Instead, it employs a\nplug-and-play mechanism by skipping intermediate layers of the verify model to\nconstruct a compressed draft model. Specifically, we develop a dynamic\nprogramming algorithm that optimizes the layer-skipping process by leveraging\nthe complete hidden states from the last verification stage as an objective.\nThis enables CLaSp to dynamically adjust its layer-skipping strategy after each\nverification stage, without relying on pre-optimized sets of skipped layers.\nExperimental results across diverse downstream tasks demonstrate that CLaSp\nachieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the\noriginal distribution of the generated text.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24196.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "64c7b4d1c547ed5243c07b6c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7b4d1c547ed5243c07b6c/h96CLBj6dcm01soK2UJzr.jpeg",
      "fullname": "Longze Chen",
      "name": "lzchen2001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23941",
      "authors": [
        {
          "_id": "683cf4405810d395f0a3788b",
          "name": "An Vo",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788c",
          "name": "Khai-Nguyen Nguyen",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788d",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Reza Taesiri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-02T00:52:37.933Z",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788e",
          "name": "Vy Tuong Dang",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a3788f",
          "user": {
            "_id": "653194a4c8da3465f4701ad1",
            "avatarUrl": "/avatars/6682164fcaf1d339ce9ac82ba131af5e.svg",
            "isPro": true,
            "fullname": "Khai-Nguyen Nguyen",
            "user": "knguyennguyen",
            "type": "user"
          },
          "name": "Anh Totti Nguyen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-02T00:45:56.803Z",
          "hidden": false
        },
        {
          "_id": "683cf4405810d395f0a37890",
          "name": "Daeyoung Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T18:47:58.000Z",
      "submittedOnDailyAt": "2025-06-02T03:28:19.444Z",
      "title": "ビジョン・ラングラウジュモデルは偏見を持つ",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、インターネットからの大量の先行知識を記憶し、それが下流タスクに役立つものの、それも偏りや間違った回答にも影響を与えることもある。本研究では、ビジョン言語モデル（VLMs）が標準的、客観的な視覚タスク（数え上げと識別）において、ネットワークからの流行する主題についての知識がどのように精度を損なっているかを調査した。これにより、最先端のVLMsは強烈に偏りを持っていることが明らかになり、例えば、アドバイドスロゴに4つのストライブが追加されたことを認識できないことを示し、7種類の違う領域（動物、ロゴ、チェス、ゲーム、光学騙り、パターン付きグリッド）での平均計数精度は17.05%であることがわかった。テキスト（例えば「アドバイドス」）を事実的な画像に追加することはVLMの精度をさらに低下させる。VLMsの偏りは強く、モデルを結果をチェックするように指示したり、画像の詳細だけを信頼するように指示することで、平均でそれほど精度を向上させることはできない。本研究はVLMsの有趣な失敗モードを示し、VLMの偏りを検査する自動化フレームワークを提供した。コードとデータはvlmsarebiased.github.ioに公開されている。",
      "upvotes": 10,
      "discussionId": "683cf4445810d395f0a37983",
      "projectPage": "https://vlmsarebiased.github.io/",
      "githubRepo": "https://github.com/anvo25/vlms-are-biased",
      "ai_summary": "Vision language models exhibit strong biases in counting and identification tasks, demonstrating a failure mode that persist even with additional instructions or context.",
      "ai_keywords": [
        "large language models",
        "vision language models",
        "downstream tasks",
        "popular subjects",
        "accuracy",
        "visual tasks",
        "counting",
        "identification",
        "biases",
        "counterfactual image",
        "automated framework"
      ]
    },
    "publishedAt": "2025-05-29T14:47:58.000Z",
    "title": "Vision Language Models are Biased",
    "summary": "Large language models (LLMs) memorize a vast amount of prior knowledge from\nthe Internet that help them on downstream tasks but also may notoriously sway\ntheir outputs towards wrong or biased answers. In this work, we test how the\nknowledge about popular subjects hurt the accuracy of vision language models\n(VLMs) on standard, objective visual tasks of counting and identification. We\nfind that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a\nfourth stripe has been added to a 3-stripe Adidas logo) scoring an average of\n17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)\nacross 7 diverse domains from animals, logos, chess, board games, optical\nillusions, to patterned grids. Insert text (e.g., \"Adidas\") describing the\nsubject name into the counterfactual image further decreases VLM accuracy. The\nbiases in VLMs are so strong that instructing them to double-check their\nresults or rely exclusively on image details to answer improves counting\naccuracy by only +2 points, on average. Our work presents an interesting\nfailure mode in VLMs and an automated framework for testing VLM biases. Code\nand data are available at: vlmsarebiased.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 84
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24521",
      "authors": [
        {
          "_id": "683d11d1495f0b58f2fd49a9",
          "name": "Yang-Tian Sun",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49aa",
          "name": "Xin Yu",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ab",
          "name": "Zehuan Huang",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ac",
          "name": "Yi-Hua Huang",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ad",
          "name": "Yuan-Chen Guo",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49ae",
          "name": "Ziyi Yang",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49af",
          "name": "Yan-Pei Cao",
          "hidden": false
        },
        {
          "_id": "683d11d1495f0b58f2fd49b0",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T12:31:59.000Z",
      "submittedOnDailyAt": "2025-06-02T01:25:45.570Z",
      "title": "UniGeo: 統一的一致的ジェネリックのビデオディフューションの制御",
      "submittedOnDailyBy": {
        "_id": "6375d136dee28348a9c63cbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
        "isPro": false,
        "fullname": "zehuan-huang",
        "user": "huanngzh",
        "type": "user"
      },
      "summary": "最近、ディフュージョンモデルの先駆を利用してモノラムの構造的計測（例：深さと正規線）を助ける方法が強い一般化能力を持っているため、それらの注目が増加しています。しかし、多くの既存の研究は個々のビデオフレームのカメラ座標系内の構造的性質の計測を焦点としていて、ディフュージョンモデルが固有の構造内のフレーム間の対応関係を決定する能力を無視しています。本稿では、適切な設計と調整を通じて、ビデオ生成モデルの固有の一貫性を構造的な計測に効果的に利用できることを示します。特に、1) 全体の座標系で共通の対応関係を持つビデオフレームと同じ予測ターゲットとした構造的属性を選択し、2) 位置データの再利用をもとに新しいコンディショナリング方法を導入し、3) 同一の対応関係を持つ複数の構造的属性の併用訓練を通じて性能を向上させます。我々の結果は、ビデオ内の全体的な構造的属性の予測に優れた性能を達成し、直接的に再構成タスクに適用できます。静的なビデオデータだけで訓練されても、我々のアプローチは動的なビデオシーンに対して一般化することができることを示しています。",
      "upvotes": 8,
      "discussionId": "683d11d3495f0b58f2fd4a95",
      "projectPage": "https://sunyangtian.github.io/UniGeo-web/",
      "githubRepo": "https://github.com/SunYangtian/UniGeo",
      "ai_summary": "Video generation models leveraging diffusion priors achieve superior global geometric attribute estimation and reconstructions, benefiting from inter-frame consistency and joint training on shared attributes.",
      "ai_keywords": [
        "diffusion models",
        "monocular geometric estimation",
        "depth",
        "normal",
        "camera coordinate system",
        "intrinsic consistency",
        "video generation models",
        "global coordinate system",
        "positional encodings",
        "joint training",
        "static video data",
        "dynamic video scenes"
      ]
    },
    "publishedAt": "2025-05-30T08:31:59.000Z",
    "title": "UniGeo: Taming Video Diffusion for Unified Consistent Geometry\n  Estimation",
    "summary": "Recently, methods leveraging diffusion model priors to assist monocular\ngeometric estimation (e.g., depth and normal) have gained significant attention\ndue to their strong generalization ability. However, most existing works focus\non estimating geometric properties within the camera coordinate system of\nindividual video frames, neglecting the inherent ability of diffusion models to\ndetermine inter-frame correspondence. In this work, we demonstrate that,\nthrough appropriate design and fine-tuning, the intrinsic consistency of video\ngeneration models can be effectively harnessed for consistent geometric\nestimation. Specifically, we 1) select geometric attributes in the global\ncoordinate system that share the same correspondence with video frames as the\nprediction targets, 2) introduce a novel and efficient conditioning method by\nreusing positional encodings, and 3) enhance performance through joint training\non multiple geometric attributes that share the same correspondence. Our\nresults achieve superior performance in predicting global geometric attributes\nin videos and can be directly applied to reconstruction tasks. Even when\ntrained solely on static video data, our approach exhibits the potential to\ngeneralize to dynamic video scenes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24521.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6375d136dee28348a9c63cbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
      "fullname": "zehuan-huang",
      "name": "huanngzh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24858",
      "authors": [
        {
          "_id": "683d2a3651706d12b2cc8ace",
          "name": "Gabrielle Kaili-May Liu",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8acf",
          "name": "Gal Yona",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad0",
          "name": "Avi Caciularu",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad1",
          "name": "Idan Szpektor",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad2",
          "name": "Tim G. J. Rudner",
          "hidden": false
        },
        {
          "_id": "683d2a3651706d12b2cc8ad3",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:54:08.000Z",
      "submittedOnDailyAt": "2025-06-02T03:13:37.735Z",
      "title": "MetaFaith: LLMでの忠実な自然言語的不確実性表現",
      "submittedOnDailyBy": {
        "_id": "64f1ca1d5b8a6a5d39d75771",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1ca1d5b8a6a5d39d75771/Caq_Ahp7Qkm1nHyhBcztE.jpeg",
        "isPro": false,
        "fullname": "John Chih Liu",
        "user": "johncliu",
        "type": "user"
      },
      "summary": "LLMsの信頼性の重要な成分としては、信頼性のある不確実性の伝達がありますが、LLMsは偽の主張を伝達する際に確信的な言葉を使用し、過度信頼と信頼の崩壊につながります。私たちは、LLMsが固有の不確実性を忠実に反映する言語表現を使用する能力をベンチマークする最初の系統的な研究を提案します。これは、様々なモデル、データセット、プロンプト戦略の幅広い範囲で行います。私たちの結果は、LLMsはこの任務に大きく失敗し、既存の対策が十分ではないことを示しています：標準的なプロンプトアプローチはみえるだけの効果を提供し、既存の事実性ベースの調整手法は忠実な調整に害を与えることもあります。この重要な欠陥を解決するために、私たちは、人間のメタ認知をモデルとした新しいプロンプトベースの調整アプローチを提案します。MetaFaithは、多様なモデルとタスク領域で忠実な調整を強固に改善し、忠実性については最大61%の向上を実現し、人間による評価で元の生成より83%の勝率を達成します。",
      "upvotes": 7,
      "discussionId": "683d2a3751706d12b2cc8b0a",
      "ai_summary": "A study reveals that Large Language Models (LLMs) struggle with expressing uncertainty accurately and introduces MetaFaith, a prompt-based method that enhances their calibration significantly.",
      "ai_keywords": [
        "faithful confidence calibration",
        "linguistic expressions of uncertainty",
        "intrinsic uncertainty",
        "prompting strategies",
        "metacognition"
      ]
    },
    "publishedAt": "2025-05-30T13:54:08.000Z",
    "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs",
    "summary": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of faithful confidence calibration of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\nfaithfully reflect their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24858.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f1ca1d5b8a6a5d39d75771",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1ca1d5b8a6a5d39d75771/Caq_Ahp7Qkm1nHyhBcztE.jpeg",
      "fullname": "John Chih Liu",
      "name": "johncliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24417",
      "authors": [
        {
          "_id": "683d0b6c5810d395f0a9a49e",
          "name": "Runnan Lu",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a49f",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a4a0",
          "name": "Jailing Liu",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a4a1",
          "name": "Haifa Wang",
          "hidden": false
        },
        {
          "_id": "683d0b6c5810d395f0a9a4a2",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T09:55:39.000Z",
      "submittedOnDailyAt": "2025-06-02T00:55:24.254Z",
      "title": "EasyText: 多言語テキストの制御可能なDiffusion Transformer\nレンダリング",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "拡散モデルを用いて多言語の正確なテキストを生成することは長年望まれていましたが、現在までには難しいとされています。最近の方法は、同じ言語でのテキストの描画に進展していますが、任意の言語の描画はまだ調査されていません。本論文では、DiT（Diffusion Transformer）に基づくEasyText（エジーテキスト）のテキスト描画フレームワークを介して、ノイズの除去ラテントと多言語の文字トークンを文字トークンとしてエンコードしたものを結びつけることで、テキスト描画を制御可能で正確に行うことを目指します。また、文字の位置情報を表現する文字位置情報エンコーディングと位置情報エンコーディングのインタープレーティング手法を提案し、これらを用いてテキスト描画を制御可能で正確に行うことを目指しています。また、100万の多言語の画像-テキストのアノテーションを含む大規模な合成テキスト画像データセットと、20Kのアノテーションされた高品質の画像データセットを構築し、これらをプレトレーニングとファインチューニングに用います。拡散モデルを用いて多言語のテキスト描画、可視化の品質、ラウプワードに関するテキスト統合の効果性と進展を示すために、詳細な実験と評価を行いました。",
      "upvotes": 7,
      "discussionId": "683d0b6f5810d395f0a9a57b",
      "ai_summary": "The paper presents EasyText, a multilingual text rendering framework using DiT that enhances rendering precision and visual quality with large datasets.",
      "ai_keywords": [
        "DiT (Diffusion Transformer)",
        "denoising latents",
        "multilingual character tokens",
        "character positioning encoding",
        "position encoding interpolation",
        "synthetic text image dataset",
        "pretraining",
        "fine-tuning",
        "multilingual text rendering",
        "visual quality",
        "layout-aware text integration"
      ]
    },
    "publishedAt": "2025-05-30T05:55:39.000Z",
    "title": "EasyText: Controllable Diffusion Transformer for Multilingual Text\n  Rendering",
    "summary": "Generating accurate multilingual text with diffusion models has long been\ndesired but remains challenging. Recent methods have made progress in rendering\ntext in a single language, but rendering arbitrary languages is still an\nunexplored area. This paper introduces EasyText, a text rendering framework\nbased on DiT (Diffusion Transformer), which connects denoising latents with\nmultilingual character tokens encoded as character tokens. We propose character\npositioning encoding and position encoding interpolation techniques to achieve\ncontrollable and precise text rendering. Additionally, we construct a\nlarge-scale synthetic text image dataset with 1 million multilingual image-text\nannotations as well as a high-quality dataset of 20K annotated images, which\nare used for pretraining and fine-tuning respectively. Extensive experiments\nand evaluations demonstrate the effectiveness and advancement of our approach\nin multilingual text rendering, visual quality, and layout-aware text\nintegration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20873",
      "authors": [
        {
          "_id": "68395a548ead63ba096bba45",
          "user": {
            "_id": "6770efb5b673f241332fc4a7",
            "avatarUrl": "/avatars/7aca599492233e6a51c2d6a8f52c644e.svg",
            "isPro": false,
            "fullname": "Chaeyoung Jung",
            "user": "Chae0",
            "type": "user"
          },
          "name": "Chaeyoung Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:46:20.597Z",
          "hidden": false
        },
        {
          "_id": "68395a548ead63ba096bba46",
          "name": "Youngjoon Jang",
          "hidden": false
        },
        {
          "_id": "68395a548ead63ba096bba47",
          "name": "Jongmin Choi",
          "hidden": false
        },
        {
          "_id": "68395a548ead63ba096bba48",
          "name": "Joon Son Chung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T08:22:56.000Z",
      "submittedOnDailyAt": "2025-06-02T06:33:06.810Z",
      "title": "フォークマージデコーディング：音声ビデオの多模構造理解を向上させる大規模言語モデル",
      "submittedOnDailyBy": {
        "_id": "6770efb5b673f241332fc4a7",
        "avatarUrl": "/avatars/7aca599492233e6a51c2d6a8f52c644e.svg",
        "isPro": false,
        "fullname": "Chaeyoung Jung",
        "user": "Chae0",
        "type": "user"
      },
      "summary": "この研究の目的は、モデルがモデルのバウンスを引き起こすようなものを調整して、オーディオとビデオの大規模な言語モデル（AV-LLMs）でのバランスづけされた多モーダル理解を向上させることである。現在のAV-LLMsでは、デコーダでオーディオとビデオの特徴量は通常一緒に処理されています。この戦略は統一的な多モーダル理解を促進しますが、モデルが不均衡なトレーニングシグナルによって一つのモデルに過度に依存するようなモデルのバウンスを引き起こす可能性があります。これを軽減するために、モデルのバウンスを引き起こすようなものを調整して、オーディオとビデオの大規模な言語モデル（AV-LLMs）でのバランスづけされた多モーダル理解を向上させることである。現在のAV-LLMsでは、デコーダでオーディオとビデオの特徴量は通常一緒に処理されています。この戦略は統一的な多モーダル理解を促進しますが、モデルが不均衡なトレーニングシグナルによって一つのモデルに過度に依存するようなモデルのバウンスを引き起こす可能性があります。これを軽減するために、Fork-Merge Decoding（FMD）という簡単で効果的な推論時の戦略を提案しています。FMDは、オーディオだけの入力とビデオだけの入力を早期のデコーダ層で処理してモデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起こすようなものを調整して、モデルのバウンスを引き起",
      "upvotes": 6,
      "discussionId": "68395a558ead63ba096bba7b",
      "ai_summary": "The Fork-Merge Decoding strategy improves balanced multimodal understanding in audio-visual large language models by separating and then combining modality-specific reasoning.",
      "ai_keywords": [
        "fork-merge decoding",
        "AU-LLMs",
        "modality bias",
        "audio-visual large language models",
        "VideoLLaMA2",
        "video-SALMONN",
        "benchmark datasets",
        "audio-visual reasoning"
      ]
    },
    "publishedAt": "2025-05-27T04:22:56.000Z",
    "title": "Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual\n  Large Language Models",
    "summary": "The goal of this work is to enhance balanced multimodal understanding in\naudio-visual large language models (AV-LLMs) by addressing modality bias\nwithout requiring additional training. In current AV-LLMs, audio and video\nfeatures are typically processed jointly in the decoder. While this strategy\nfacilitates unified multimodal understanding, it may introduce modality bias,\nwhere the model tends to over-rely on one modality due to imbalanced training\nsignals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet\neffective inference-time strategy that requires no additional training or\narchitectural modifications. FMD first performs modality-specific reasoning by\nprocessing audio-only and video-only inputs through the early decoder layers (a\nfork phase), and then merges the resulting hidden states for joint reasoning in\nthe remaining layers (a merge phase). This approach promotes balanced modality\ncontributions and leverages complementary information across modalities. We\nevaluate our method on two representative AV-LLMs, VideoLLaMA2 and\nvideo-SALMONN, using three benchmark datasets. Experimental results demonstrate\nconsistent performance improvements on tasks focused on audio, video, and\ncombined audio-visual reasoning, demonstrating the effectiveness of\ninference-time interventions for robust multimodal understanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20873.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6770efb5b673f241332fc4a7",
      "avatarUrl": "/avatars/7aca599492233e6a51c2d6a8f52c644e.svg",
      "fullname": "Chaeyoung Jung",
      "name": "Chae0",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24850",
      "authors": [
        {
          "_id": "683d0ffbe41c42faceda19b2",
          "user": {
            "_id": "6587e5a4b2177de3967ff434",
            "avatarUrl": "/avatars/f2dfbc44eb2bff8d8d66d26db8539708.svg",
            "isPro": false,
            "fullname": "Shuyao Xu",
            "user": "Tim-Xu",
            "type": "user"
          },
          "name": "Shuyao Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:56.229Z",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b3",
          "name": "Cheng Peng",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b4",
          "name": "Jiangxuan Long",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b5",
          "name": "Weidi Xu",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b6",
          "name": "Wei Chu",
          "hidden": false
        },
        {
          "_id": "683d0ffbe41c42faceda19b7",
          "name": "Yuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T17:47:17.000Z",
      "submittedOnDailyAt": "2025-06-02T02:07:38.924Z",
      "title": "ネガティブシグナルの採用：教師からの強化熱帯化　LLMの理由論理におけるデータ",
      "submittedOnDailyBy": {
        "_id": "66e83ec5deb449d8d856e78d",
        "avatarUrl": "/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg",
        "isPro": false,
        "fullname": "Tongyan Hu",
        "user": "entropyhu",
        "type": "user"
      },
      "summary": "最近のモデルの経験転移の進展により、高度な理由論モデル（例：DeepSeek-R1、OpenAIのo1）からのデータは、複雑な理由論能力を小さくて効率的な学生モデルに有効に伝えることができることが示されています。しかし、標準的な実践は、不正な理由論例を捨てる拒否サンプリングを用いていますが、それらは有價であるが、よく利用されていません。本論文は、どのように正のと負の転移された理由論トレースを効果的に活用して、オフライン設定でLLMの理由論性能を最大化するかという重要な質問を解決しようとしています。そのために、私たちは、Reinforcement Distillation（REDI）という2段階フレームワークを提案しています。第1ステージは、Supervised Fine-Tuning（SFT）により正のトレースから学習します。第2ステージは、我々が提案したREDIの目的関数を用いて、正のと負のトレースを両方とも用いてモデルを進歩的に補間します。この新しい目的関数は、DPOやSimPOといった既存の方法よりも優れている簡単な、参照無しの損失関数です。私たちの実験評価により、REDIは基準となる拒否サンプリングSFTまたはSFTとDPO/SimPOの組み合わせに対して、数学的な理由論テスト上の優れた性能を示しています。特に、Qwen-REDI-1.5Bモデルは、Open-R1データセットからの131kの正のと負の例をみながら補間した後に、MATH-500（pass@1）で83.1%のスコアを達成しました。その性能は、DeepSeek-R1-Distill-Qwen-1.5B（800kのプロピエタリーデータで補間されたモデル）と同じであるかもしくはより良く、数学的な理由論ベンチマーク上での性能が一致したり、超えたりし、公開的なデータで補間された1.5Bモデルの新しい最先端となっています。",
      "upvotes": 5,
      "discussionId": "683d0ffce41c42faceda19da",
      "githubRepo": "https://github.com/Tim-Siu/reinforcement-distillation",
      "ai_summary": "Reinforcement Distillation (REDI) leverages both positive and negative traces to enhance large language model reasoning performance offline, outperforming traditional methods and achieving state-of-the-art results with limited open data.",
      "ai_keywords": [
        "model distillation",
        "DeepSeek-R1",
        "OpenAI's o1",
        "Reinforcement Distillation (REDI)",
        "Supervised Fine-Tuning (SFT)",
        "REDI objective",
        "DPO",
        "SimPO",
        "mathematical reasoning tasks",
        "MATH-500",
        "Qwen-REDI-1.5B",
        "DeepSeek-R1-Distill-Qwen-1.5B"
      ]
    },
    "publishedAt": "2025-05-30T13:47:17.000Z",
    "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning",
    "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24850.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66e83ec5deb449d8d856e78d",
      "avatarUrl": "/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg",
      "fullname": "Tongyan Hu",
      "name": "entropyhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24293",
      "authors": [
        {
          "_id": "683d01ec446fd0c8ff323010",
          "user": {
            "_id": "6658f863ce1b283888625af3",
            "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
            "isPro": false,
            "fullname": "James Golden",
            "user": "jamesgolden1",
            "type": "user"
          },
          "name": "James R. Golden",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-02T02:38:10.635Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6658f863ce1b283888625af3/uulwAnV1EYXXSdsC-eDMW.png"
      ],
      "publishedAt": "2025-05-30T07:08:33.000Z",
      "submittedOnDailyAt": "2025-06-02T01:02:58.980Z",
      "title": "大語言モデルは局所的に線形なマッピングです。",
      "submittedOnDailyBy": {
        "_id": "6658f863ce1b283888625af3",
        "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
        "isPro": false,
        "fullname": "James Golden",
        "user": "jamesgolden1",
        "type": "user"
      },
      "summary": "We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.",
      "upvotes": 3,
      "discussionId": "683d01ee446fd0c8ff323087",
      "githubRepo": "https://github.com/jamesgolden1/llms-are-llms/",
      "ai_summary": "LLMs can be approximated as linear systems for inference, offering insights into their internal representations and semantic structures without altering the models or their predictions.",
      "ai_keywords": [
        "large language models (LLMs)",
        "inference operations",
        "linear system",
        "gradient computation",
        "Jacobian",
        "singular value decomposition",
        "low-dimensional subspaces",
        "semantic concepts",
        "attention components",
        "MLP components",
        "locally linear decompositions"
      ]
    },
    "publishedAt": "2025-05-30T03:08:33.000Z",
    "title": "Large Language Models are Locally Linear Mappings",
    "summary": "We demonstrate that the inference operations of several open-weight large\nlanguage models (LLMs) can be mapped to an exactly equivalent linear system for\nan input sequence without modifying the model weights or altering output\npredictions. Extending techniques from image diffusion models that exhibit\nlocal or piecewise linearity, we strategically alter the gradient computation\nwith respect to a given input sequence for a next-token prediction such that\nthe Jacobian of the model nearly exactly reproduces the forward prediction with\na linear system. We demonstrate this approach across models (Llama 3, Gemma 3,\nQwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show\nthrough the singular value decomposition of the detached Jacobian that these\nLLMs operate in extremely low-dimensional subspaces where many of the largest\nsingular vectors decode to concepts related to the most-likely output token.\nThis approach also allows us to examine the operation of each successive layer\n(and its attention and MLP components) as nearly-exact linear systems and\nobserve the emergence of semantic concepts. Despite their expressive power and\nglobal nonlinearity, modern LLMs can be interpreted through nearly-exact\nlocally linear decompositions that provide insights into their internal\nrepresentations and reveal interpretable semantic structures in the next-token\nprediction process.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6658f863ce1b283888625af3/uulwAnV1EYXXSdsC-eDMW.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24293.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6658f863ce1b283888625af3",
      "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
      "fullname": "James Golden",
      "name": "jamesgolden1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23926",
      "authors": [
        {
          "_id": "683d33be277ad05e5a672f79",
          "name": "Xuweiyi Chen",
          "hidden": false
        },
        {
          "_id": "683d33be277ad05e5a672f7a",
          "name": "Wentao Zhou",
          "hidden": false
        },
        {
          "_id": "683d33be277ad05e5a672f7b",
          "name": "Aruni RoyChowdhury",
          "hidden": false
        },
        {
          "_id": "683d33be277ad05e5a672f7c",
          "name": "Zezhou Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T18:21:47.000Z",
      "submittedOnDailyAt": "2025-06-02T03:49:08.677Z",
      "title": "Point-MoE: 3D セマンティクス分割のクロスドメイン一般化に向けてのミックスオブエキスパート",
      "submittedOnDailyBy": {
        "_id": "634632aaac1cb29fb2ac9f14",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634632aaac1cb29fb2ac9f14/nGZ2TzKOOcKMAR_NFYKkR.jpeg",
        "isPro": false,
        "fullname": "Xuweiyi Chen",
        "user": "Xuweiyi",
        "type": "user"
      },
      "summary": "このテキストは、3Dポイントクラスターの理解がスケーリングラーズに達していない理由を説明し、その解決策を提案しています。以下は、このテキストの日本語翻訳です：\n\nスケーリングラーズは、自然言語処理とコンピュータビジョンに影響を与えましたが、3Dポイントクラスターの理解はまだそのステージに達していません。これは、3Dデータセットの比較的に小さなスケールおよびデータの異なるソースによることを原因としています。ポイントクラスターは、様々なセンサー（例：深さカメラ、LiDAR）から、室内から室外まで様々な領域で撮影されます。各センサーは、独自のスキャンパターン、サンプリング密度、セマンティックバイアスを導入します。この領域の異なりは、スケールでの統一モデルの訓練に重大な障壁となります。特に、領域ラベルが推論時に通常アクセスできない実際的な制約の下でこれが特に問題になります。本稿では、大規模で、領域間の一般化を可能にするMixture-of-Expertsアーキテクチャを提案しています。Point-MoEは、標準のポイントクラスターバックボーンは、混合領域データで訓練されると性能が大幅に低下することを示しますが、Point-MoEは簡単なtop-kルーティングスタラテジにより、データラベルがアクセスできなくても、エキスパートを自動的に特殊化することができます。実験は、Point-MoEは強力な多領域ベースラインティングを超え、見たことのない領域への一般化も良くなっていることを示します。この研究は、3D理解のスケーラブルなパスを示し、モデルが多様な3Dデータの構造を自動的に発見することを優先し、手動の編集や領域のサブプロバイジングによるものを代わりに行うことを示しています。",
      "upvotes": 3,
      "discussionId": "683d33c4277ad05e5a67310e",
      "ai_summary": "Point-MoE, a Mixture-of-Experts architecture, enables large-scale, cross-domain generalization in 3D perception by automatically specializing experts without domain labels.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "Point-MoE",
        "point cloud backbones",
        "3D perception",
        "domain heterogeneity",
        "domain labels",
        "top-k routing",
        "multi-domain baselines"
      ]
    },
    "publishedAt": "2025-05-29T14:21:47.000Z",
    "title": "Point-MoE: Towards Cross-Domain Generalization in 3D Semantic\n  Segmentation via Mixture-of-Experts",
    "summary": "While scaling laws have transformed natural language processing and computer\nvision, 3D point cloud understanding has yet to reach that stage. This can be\nattributed to both the comparatively smaller scale of 3D datasets, as well as\nthe disparate sources of the data itself. Point clouds are captured by diverse\nsensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor,\noutdoor), each introducing unique scanning patterns, sampling densities, and\nsemantic biases. Such domain heterogeneity poses a major barrier towards\ntraining unified models at scale, especially under the realistic constraint\nthat domain labels are typically inaccessible at inference time. In this work,\nwe propose Point-MoE, a Mixture-of-Experts architecture designed to enable\nlarge-scale, cross-domain generalization in 3D perception. We show that\nstandard point cloud backbones degrade significantly in performance when\ntrained on mixed-domain data, whereas Point-MoE with a simple top-k routing\nstrategy can automatically specialize experts, even without access to domain\nlabels. Our experiments demonstrate that Point-MoE not only outperforms strong\nmulti-domain baselines but also generalizes better to unseen domains. This work\nhighlights a scalable path forward for 3D understanding: letting the model\ndiscover structure in diverse 3D data, rather than imposing it via manual\ncuration or domain supervision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23926.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634632aaac1cb29fb2ac9f14",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634632aaac1cb29fb2ac9f14/nGZ2TzKOOcKMAR_NFYKkR.jpeg",
      "fullname": "Xuweiyi Chen",
      "name": "Xuweiyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24615",
      "authors": [
        {
          "_id": "683d4295c31058e5bf2e2b0b",
          "name": "Yan Liu",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0c",
          "user": {
            "_id": "646a11791556443f24b582e9",
            "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
            "isPro": false,
            "fullname": "Zonglin Yang",
            "user": "ZonglinY",
            "type": "user"
          },
          "name": "Zonglin Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:15.714Z",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0d",
          "name": "Soujanya Poria",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0e",
          "name": "Thanh-Son Nguyen",
          "hidden": false
        },
        {
          "_id": "683d4295c31058e5bf2e2b0f",
          "name": "Erik Cambria",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T14:08:13.000Z",
      "submittedOnDailyAt": "2025-06-02T04:51:23.329Z",
      "title": "大語言モデルを科学の新しさ検出に活用する",
      "submittedOnDailyBy": {
        "_id": "646a11791556443f24b582e9",
        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
        "isPro": false,
        "fullname": "Zonglin Yang",
        "user": "ZonglinY",
        "type": "user"
      },
      "summary": "指数的学の成長の時代において、新しい研究アイデアの特定は学術界で重要でも難しい。潜在的な可能性にもとっても、適切なベンチマークデータセットの欠如は新機能検出の研究に妨げている。さらに重要なのは、現在のNLPテクノロジーの単なる採用、例えば検索と確認の両方を行うことは、文の類似性とアイデアの概念化の間の隙間により、一貫して適用できないことである。本論文では、科学の新機能検出（ND）において大規模言語モデル（LLMs）を活用することを提案し、市場とNLP領域に関連する2つの新しいデータセットを備える。NDに適したデータセットを構築するためには、論文の関係に基づいて閉じられたセットを抽出し、LLMsによって主なアイデアを要約することを提案する。アイデアの概念化を捉えるためには、LLMsからアイデアレベルの知識を徹講して、類似な概念化のアイデアを対応させることで、効率的で正確なアイデア検索を実現するために軽量の検索モデルを訓練することを提案する。実験は、提案されたベンチマークデータセット上でのアイデア検索とNDの仕事において、他の方法を一貫して超えることを示している。コードとデータは、https://anonymous.4open.science/r/NoveltyDetection-10FB/ から利用できる。",
      "upvotes": 2,
      "discussionId": "683d4296c31058e5bf2e2b63",
      "ai_summary": "A method utilizing large language models to detect scientific novelty by distilling idea-level knowledge and constructing specialized datasets in marketing and NLP domains.",
      "ai_keywords": [
        "large language models",
        "scientific novelty detection",
        "closure sets",
        "idea retrieval",
        "idea conception",
        "lightweight retriever",
        "knowledge distillation"
      ]
    },
    "publishedAt": "2025-05-30T10:08:13.000Z",
    "title": "Harnessing Large Language Models for Scientific Novelty Detection",
    "summary": "In an era of exponential scientific growth, identifying novel research ideas\nis crucial and challenging in academia. Despite potential, the lack of an\nappropriate benchmark dataset hinders the research of novelty detection. More\nimportantly, simply adopting existing NLP technologies, e.g., retrieving and\nthen cross-checking, is not a one-size-fits-all solution due to the gap between\ntextual similarity and idea conception. In this paper, we propose to harness\nlarge language models (LLMs) for scientific novelty detection (ND), associated\nwith two new datasets in marketing and NLP domains. To construct the\nconsiderate datasets for ND, we propose to extract closure sets of papers based\non their relationship, and then summarize their main ideas based on LLMs. To\ncapture idea conception, we propose to train a lightweight retriever by\ndistilling the idea-level knowledge from LLMs to align ideas with similar\nconception, enabling efficient and accurate idea retrieval for LLM novelty\ndetection. Experiments show our method consistently outperforms others on the\nproposed benchmark datasets for idea retrieval and ND tasks. Codes and data are\navailable at https://anonymous.4open.science/r/NoveltyDetection-10FB/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24615.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646a11791556443f24b582e9",
      "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
      "fullname": "Zonglin Yang",
      "name": "ZonglinY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.24517",
      "authors": [
        {
          "_id": "683d3f3100c71614babecb8c",
          "user": {
            "_id": "64395702bb7ded0a0fee8889",
            "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
            "isPro": false,
            "fullname": "Yinqi Li",
            "user": "yinqi",
            "type": "user"
          },
          "name": "Yinqi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:40:19.037Z",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb8d",
          "name": "Jiahe Zhao",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb8e",
          "name": "Hong Chang",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb8f",
          "name": "Ruibing Hou",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb90",
          "name": "Shiguang Shan",
          "hidden": false
        },
        {
          "_id": "683d3f3100c71614babecb91",
          "name": "Xilin Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T12:29:38.000Z",
      "submittedOnDailyAt": "2025-06-02T04:55:28.985Z",
      "title": "un^2CLIP: CLIPの視覚的な詳細を捉える能力を向上させるための逆引き手法",
      "submittedOnDailyBy": {
        "_id": "64395702bb7ded0a0fee8889",
        "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
        "isPro": false,
        "fullname": "Yinqi Li",
        "user": "yinqi",
        "type": "user"
      },
      "summary": "Contrastive Language-Image Pre-training (CLIP)は、基盤モデルとして採用され、視覚および多タイプモデルタスクに様々な応用が行われています。しかし、最近の研究は、CLIPが画像の詳細な違いを区別する能力が不足し、密集予測および視覚中心的な多タイプモデルタスクでは最適な性能を示していないことを示しています。そこで、本稿は、既存のCLIPモデルを改良し、画像の詳細な視覚情報を可能な限り捉えることを目的としています。私たちは、特定の種類の生成モデルであるunCLIPが、この目的を達成する適切なフレームワークを提供していることを見出しました。特に、unCLIPはCLIP画像埋め込みに基づいた画像ジェネレータを訓練しています。つまり、CLIP画像エンコーダーを逆転させています。判別的なモデルとしてのCLIPと比較して、生成モデルは画像の詳細を捉えることができるほどの性能を示すことができることを見出しました。また、unCLIPの条件付き入力空間は、CLIPの元の画像-テキスト埋め込み空間と一致しています。そこで、私たちは、unCLIPを逆転させること（un^2CLIPと呼ばれる）でCLIPモデルを改善することを提案しています。このように、改善された画像エンコーダーは、unCLIPの視覚詳細を捉える能力を得ながらも、元のテキストエンコーダーとの一致を保っています。私たちは、CLIPが應用されている多様なタスクで、特に難しいMMVP-VLMベンチマーク、密集予測の開放ボックスラベルセグメンテーションタスク、および多タイプモデル大語言モデルタスクで、改善されたCLIPを評価しています。実験は、un^2CLIPが元のCLIPおよび前回のCLIP改善手法を大幅に改善したことを示しています。コードとモデルは、https://github.com/LiYinqi/un2CLIP から利用可能です。",
      "upvotes": 2,
      "discussionId": "683d3f3200c71614babecbe3",
      "githubRepo": "https://github.com/LiYinqi/un2CLIP",
      "ai_summary": "A generative model framework, unCLIP, is inverted to improve CLIP's ability to capture detailed visual information while maintaining text alignment.",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training",
        "CLIP",
        "unCLIP",
        "image generator",
        "image encoding",
        "data distribution",
        "dense-prediction",
        "vision-centric",
        "multimodal",
        "open-vocabulary segmentation",
        "multimodal large language model",
        "MMVP-VLM benchmark"
      ]
    },
    "publishedAt": "2025-05-30T08:29:38.000Z",
    "title": "un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via\n  Inverting unCLIP",
    "summary": "Contrastive Language-Image Pre-training (CLIP) has become a foundation model\nand has been applied to various vision and multimodal tasks. However, recent\nworks indicate that CLIP falls short in distinguishing detailed differences in\nimages and shows suboptimal performance on dense-prediction and vision-centric\nmultimodal tasks. Therefore, this work focuses on improving existing CLIP\nmodels, aiming to capture as many visual details in images as possible. We find\nthat a specific type of generative models, unCLIP, provides a suitable\nframework for achieving our goal. Specifically, unCLIP trains an image\ngenerator conditioned on the CLIP image embedding. In other words, it inverts\nthe CLIP image encoder. Compared to discriminative models like CLIP, generative\nmodels are better at capturing image details because they are trained to learn\nthe data distribution of images. Additionally, the conditional input space of\nunCLIP aligns with CLIP's original image-text embedding space. Therefore, we\npropose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this\nway, the improved image encoder can gain unCLIP's visual detail capturing\nability while preserving its alignment with the original text encoder\nsimultaneously. We evaluate our improved CLIP across various tasks to which\nCLIP has been applied, including the challenging MMVP-VLM benchmark, the\ndense-prediction open-vocabulary segmentation task, and multimodal large\nlanguage model tasks. Experiments show that un^2CLIP significantly improves\nthe original CLIP and previous CLIP improvement methods. Code and models will\nbe available at https://github.com/LiYinqi/un2CLIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24517.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64395702bb7ded0a0fee8889",
      "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
      "fullname": "Yinqi Li",
      "name": "yinqi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23009",
      "authors": [
        {
          "_id": "683916c60df60182c0dee89d",
          "user": {
            "_id": "66958c29d4ca2767b9c41005",
            "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
            "isPro": true,
            "fullname": "Ruskin Raj Manku",
            "user": "ruskinmanku",
            "type": "user"
          },
          "name": "Ruskin Raj Manku",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:47:12.390Z",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee89e",
          "name": "Yuzhi Tang",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee89f",
          "name": "Xingjian Shi",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee8a0",
          "name": "Mu Li",
          "hidden": false
        },
        {
          "_id": "683916c60df60182c0dee8a1",
          "name": "Alex Smola",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T02:36:24.000Z",
      "submittedOnDailyAt": "2025-06-02T01:24:21.995Z",
      "title": "EmergentTTS-Eval: 複雑な読み気、表現力と言語的課題に対するTTSモデルの評価をModel-as-a-Judgeを使用して行う",
      "submittedOnDailyBy": {
        "_id": "66958c29d4ca2767b9c41005",
        "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
        "isPro": true,
        "fullname": "Ruskin Raj Manku",
        "user": "ruskinmanku",
        "type": "user"
      },
      "summary": "Text-to-Speech (TTS) ベンチマークは、モデルがより複雑なテキストをよりよく処理する方法を捉えることが難しいことを多く含みます。EmergentTTS に基づいて、EmergentTTS-Eval を紹介します。これは、感情、パラリンギス、外国語、句法複雑性、複雑な発音（例：URL、公式）および質問を含む6つの難しい TTS シナリオを掲載しています。重要なポイントとして、フレームワークはテストケースの生成と評価を自動化し、ベンチマークの拡張が容易になります。最初に、人間が書いたエッジプロンプトを小さなセットにして、LLM を用いて特定の構文的、音響的、プロニード的な挑戦に向けてイテレーション的に拡張します。これにより、1,645 種類の多様なテストケースが生成されます。また、モデルがジャッジの構成を用いて、Large Audio Language Model (LALM) を使用して、表情の感情、プロニード的、音調的、発音の正確性を評価します。EmergentTTS-Eval 上で最先端のオープンソースおよびプロプライターの TTS システム（例：11Labs、Deepgram、OpenAI の 4o-mini-TTS）を評価し、その性能の細かい違いを示す能力を明らかにします。結果は、モデルがジャッジの構成を用いた方法が強力的な TTS 評価を提供し、人間の好みとの高い相関を示します。EmergentTTS-Eval の評価コードとデータセットは、https://github.com/boson-ai/EmergentTTS-Eval-public{code} と https://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset} で公開されています。",
      "upvotes": 2,
      "discussionId": "683916c70df60182c0dee8dc",
      "ai_summary": "A comprehensive TTS benchmark, EmergentTTS-Eval, automates test-case generation and evaluation using LLMs and LALM to assess nuanced and semantically complex text in speech outputs.",
      "ai_keywords": [
        "EmergentTTS-Eval",
        "LLMs",
        "Large Audio Language Model (LALM)",
        "expressed emotion",
        "prosodic",
        "intonational",
        "pronunciation accuracy",
        "TTS systems",
        "model-as-a-judge"
      ]
    },
    "publishedAt": "2025-05-28T22:36:24.000Z",
    "title": "EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,\n  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge",
    "summary": "Text-to-Speech (TTS) benchmarks often fail to capture how well models handle\nnuanced and semantically complex text. Building on EmergentTTS, we\nintroduce EmergentTTS-Eval, a comprehensive benchmark covering six\nchallenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic\ncomplexity, complex pronunciation (e.g. URLs, formulas), and questions.\nCrucially, our framework automates both test-case generation and evaluation,\nmaking the benchmark easily extensible. Starting from a small set of\nhuman-written seed prompts, we iteratively extend them using LLMs to target\nspecific structural, phonetic and prosodic challenges, resulting in 1,645\ndiverse test cases. Moreover, we employ a model-as-a-judge approach, using a\nLarge Audio Language Model (LALM) to assess the speech across multiple\ndimensions such as expressed emotion, prosodic, intonational, and pronunciation\naccuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,\nsuch as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,\ndemonstrating its ability to reveal fine-grained performance differences.\nResults show that the model-as-a-judge approach offers robust TTS assessment\nand a high correlation with human preferences. We open source the evaluation\nhttps://github.com/boson-ai/EmergentTTS-Eval-public{code} and the\nhttps://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66958c29d4ca2767b9c41005",
      "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
      "fullname": "Ruskin Raj Manku",
      "name": "ruskinmanku",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23844",
      "authors": [
        {
          "_id": "683d0ac47852d920b7dc3dc5",
          "name": "Zhenglun Kong",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc6",
          "name": "Zheng Zhan",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc7",
          "name": "Shiyue Hou",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc8",
          "name": "Yifan Gong",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dc9",
          "name": "Xin Meng",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dca",
          "name": "Pengwei Sui",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcb",
          "name": "Peiyan Dong",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcc",
          "name": "Xuan Shen",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcd",
          "name": "Zifeng Wang",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dce",
          "name": "Pu Zhao",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dcf",
          "name": "Hao Tang",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dd0",
          "name": "Stratis Ioannidis",
          "hidden": false
        },
        {
          "_id": "683d0ac47852d920b7dc3dd1",
          "name": "Yanzhi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T16:24:50.000Z",
      "submittedOnDailyAt": "2025-06-02T00:52:55.266Z",
      "title": "フレックスフレームワークを実現して、スケーラブルな知識の集約を可能にする",
      "submittedOnDailyBy": {
        "_id": "5f2c36551ebc8c6ede2f0e53",
        "avatarUrl": "/avatars/e3ddbd15f50b86958377b5fc2460a57e.svg",
        "isPro": false,
        "fullname": "Tony Kong",
        "user": "TonyK",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、卓越した可能性を示しているが、傳統的な微調節による連続的な向上は難しい、特に、他の専門的なLLMsの機能を統合する場合。エンサーブと重み統合などの人気の方法は、記憶量の多く必要となり、変化するデータ環境に適応するには難しい。最近の努力は、複数のLLMsから知識を収録して1つのターゲットモデルに転移しているが、タスク間での知識干渉と機能の低下に苦しむ、主に候補選択とトレーニングパイプラインの有限な柔軟性によるからである。これらの問題に対処するために、私たちは、多様なLLMsからの知識を適応的に選択し、集約して1つの強いモデルを構築するフレームワークを提案している。これは、エンサーブと重み統合の高い記憶オーバーヘッドを避けることと、選択の柔軟性の欠陥を軽減することを目指している。特に、適応的な選択ネットワークを設計し、スコアに基づいて最も関連のあるソースLLMsを特定し、知識干渉を減らすことを図る。さらに、候補LLMsの固有の強みを考慮した動的な重み付け統合戦略を提案し、選択器が単一のソースのセットに収束しないよう、フュードバック駆動ロス関数を提案している。実験結果は、我々の方法が、既存のアプローチと比較して知識干渉を50%減らし、安定したかつスケーラブルな知識集約プロセスを可能にすることを示している。コードは、https://github.com/ZLKong/LLM_Integrationにアクセス可能です。",
      "upvotes": 2,
      "discussionId": "683d0ac57852d920b7dc3e20",
      "projectPage": "https://github.com/ZLKong/LLM_Integration/tree/main",
      "githubRepo": "https://github.com/ZLKong/LLM_Integration/tree/main",
      "ai_summary": "A framework for adaptive selection and dynamic weighted fusion of knowledge from multiple LLMs reduces interference and improves scalability in knowledge aggregation.",
      "ai_keywords": [
        "large language models",
        "fine-tuning",
        "ensemble",
        "weight merging",
        "adaptive selection network",
        "dynamic weighted fusion",
        "feedback-driven loss function",
        "knowledge interference"
      ]
    },
    "publishedAt": "2025-05-28T12:24:50.000Z",
    "title": "Enabling Flexible Multi-LLM Integration for Scalable Knowledge\n  Aggregation",
    "summary": "Large language models (LLMs) have shown remarkable promise but remain\nchallenging to continually improve through traditional finetuning, particularly\nwhen integrating capabilities from other specialized LLMs. Popular methods like\nensemble and weight merging require substantial memory and struggle to adapt to\nchanging data environments. Recent efforts have transferred knowledge from\nmultiple LLMs into a single target model; however, they suffer from\ninterference and degraded performance among tasks, largely due to limited\nflexibility in candidate selection and training pipelines. To address these\nissues, we propose a framework that adaptively selects and aggregates knowledge\nfrom diverse LLMs to build a single, stronger model, avoiding the high memory\noverhead of ensemble and inflexible weight merging. Specifically, we design an\nadaptive selection network that identifies the most relevant source LLMs based\non their scores, thereby reducing knowledge interference. We further propose a\ndynamic weighted fusion strategy that accounts for the inherent strengths of\ncandidate LLMs, along with a feedback-driven loss function that prevents the\nselector from converging on a single subset of sources. Experimental results\ndemonstrate that our method can enable a more stable and scalable knowledge\naggregation process while reducing knowledge interference by up to 50% compared\nto existing approaches. Code is avaliable at\nhttps://github.com/ZLKong/LLM_Integration",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23844.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f2c36551ebc8c6ede2f0e53",
      "avatarUrl": "/avatars/e3ddbd15f50b86958377b5fc2460a57e.svg",
      "fullname": "Tony Kong",
      "name": "TonyK",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21864",
      "authors": [
        {
          "_id": "683b8af5091615f46fabadde",
          "user": {
            "_id": "655a50a850b9a14799165d53",
            "avatarUrl": "/avatars/6bb37ac0b6840771ff7a6a6da4192ace.svg",
            "isPro": false,
            "fullname": "Mengda Xu",
            "user": "mengdaxu",
            "type": "user"
          },
          "name": "Mengda Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:42:10.223Z",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabaddf",
          "name": "Han Zhang",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade0",
          "name": "Yifan Hou",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade1",
          "name": "Zhenjia Xu",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade2",
          "name": "Linxi Fan",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade3",
          "name": "Manuela Veloso",
          "hidden": false
        },
        {
          "_id": "683b8af5091615f46fabade4",
          "name": "Shuran Song",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655a50a850b9a14799165d53/IaKGx3B7I2pvk3nOzX9s2.mp4"
      ],
      "publishedAt": "2025-05-28T01:25:27.000Z",
      "submittedOnDailyAt": "2025-06-02T06:21:16.447Z",
      "title": "DexUMI: 人の手を普遍的な操作インターフェースとして使用したデキューラスな操作",
      "submittedOnDailyBy": {
        "_id": "655a50a850b9a14799165d53",
        "avatarUrl": "/avatars/6bb37ac0b6840771ff7a6a6da4192ace.svg",
        "isPro": false,
        "fullname": "Mengda Xu",
        "user": "mengdaxu",
        "type": "user"
      },
      "summary": "DexUMI を紹介します。これは、人手を自然なインターフェースとして、さまざまなロボット手にディテクスな操作スキルを伝授するデータ収集と政策学習フレームワークです。DexUMI は、人手とさまざまなロボット手の体像間違いを最小限に抑えるための硬体とソフトウェアの調整を含みます。硬体調整は、可動性間違いを閉じるための可着された手のエクソスケルツーンを使用します。これにより、操作データの収集に直接のハピックフィードバックを提供し、人間の動作を可能なロボット手の動作に適用します。ソフトウェア調整は、映像データの中で人手を高品質のロボット手のインパインティングに置き換えることで視覚的な間違いを閉じます。DexUMI の能力を示すために、2種類のディテクスなロボット手の硬体プラットフォームに対して詳細な実世界実験を行い、平均的なタスク成功率が86%を達成しました。",
      "upvotes": 1,
      "discussionId": "683b8af8091615f46fabaf00",
      "projectPage": "https://dex-umi.github.io/",
      "githubRepo": "https://github.com/real-stanford/DexUMI",
      "ai_summary": "DexUMI framework utilizes a wearable hand exoskeleton and high-fidelity robot hand inpainting to transfer dexterous manipulation skills from human hands to robot hands, achieving high task success rates.",
      "ai_keywords": [
        "wearable hand exoskeleton",
        "haptic feedback",
        "robot hand inpainting",
        "dexterous manipulation",
        "kinematics",
        "visual gap"
      ]
    },
    "publishedAt": "2025-05-27T21:25:27.000Z",
    "title": "DexUMI: Using Human Hand as the Universal Manipulation Interface for\n  Dexterous Manipulation",
    "summary": "We present DexUMI - a data collection and policy learning framework that uses\nthe human hand as the natural interface to transfer dexterous manipulation\nskills to various robot hands. DexUMI includes hardware and software\nadaptations to minimize the embodiment gap between the human hand and various\nrobot hands. The hardware adaptation bridges the kinematics gap using a\nwearable hand exoskeleton. It allows direct haptic feedback in manipulation\ndata collection and adapts human motion to feasible robot hand motion. The\nsoftware adaptation bridges the visual gap by replacing the human hand in video\ndata with high-fidelity robot hand inpainting. We demonstrate DexUMI's\ncapabilities through comprehensive real-world experiments on two different\ndexterous robot hand hardware platforms, achieving an average task success rate\nof 86%.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655a50a850b9a14799165d53/IaKGx3B7I2pvk3nOzX9s2.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655a50a850b9a14799165d53",
      "avatarUrl": "/avatars/6bb37ac0b6840771ff7a6a6da4192ace.svg",
      "fullname": "Mengda Xu",
      "name": "mengdaxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.13157",
      "authors": [
        {
          "_id": "683b58eb84fbd4b28d8d891e",
          "user": {
            "_id": "6469408ab2321e47d3294414",
            "avatarUrl": "/avatars/05621d33f9f6337bc66e59d9e81d05ef.svg",
            "isPro": false,
            "fullname": "Yassine El Boudouri",
            "user": "yelboudouri",
            "type": "user"
          },
          "name": "Yassine El Boudouri",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:42:17.658Z",
          "hidden": false
        },
        {
          "_id": "683b58eb84fbd4b28d8d891f",
          "name": "Walter Nuninger",
          "hidden": false
        },
        {
          "_id": "683b58eb84fbd4b28d8d8920",
          "name": "Julian Alvarez",
          "hidden": false
        },
        {
          "_id": "683b58eb84fbd4b28d8d8921",
          "name": "Yvan Peter",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T14:18:16.000Z",
      "submittedOnDailyAt": "2025-06-02T07:13:19.898Z",
      "title": "ロールプレイティング評価モデルの大規模言語モデルにおける評価",
      "submittedOnDailyBy": {
        "_id": "6469408ab2321e47d3294414",
        "avatarUrl": "/avatars/05621d33f9f6337bc66e59d9e81d05ef.svg",
        "isPro": false,
        "fullname": "Yassine El Boudouri",
        "user": "yelboudouri",
        "type": "user"
      },
      "summary": "大型言語モデル（LLMs）は、ポートライターや役業プレイに適用する能力を示す。しかし、この能力を評価するには、人間の評価は資源により負担が大きく、自動評価は偏りを含む可能性がある。これに対処するために、私たちは、感情理解、決定策、倫理的な合意性、キャラクターの一貫性の4つの主な次元でLLMの役業プレイ能力を評価するための新しいベンチマーク「Role-Playing Eval（RPEval）」を紹介します。本文では、RPEvalの構築と基準評価を詳しく説明します。私たちのコードとデータセットは、https://github.com/yelboudouri/RPEval から利用可能です。",
      "upvotes": 1,
      "discussionId": "683b58ec84fbd4b28d8d8935",
      "githubRepo": "https://github.com/yelboudouri/RPEval",
      "ai_summary": "A benchmark called Role-Playing Eval assesses Large Language Models in role-playing across emotional understanding, decision-making, moral alignment, and in-character consistency.",
      "ai_keywords": [
        "Large Language Models",
        "Role-Playing Eval",
        "emotional understanding",
        "decision-making",
        "moral alignment",
        "in-character consistency"
      ]
    },
    "publishedAt": "2025-05-19T10:18:16.000Z",
    "title": "Role-Playing Evaluation for Large Language Models",
    "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13157.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6469408ab2321e47d3294414",
      "avatarUrl": "/avatars/05621d33f9f6337bc66e59d9e81d05ef.svg",
      "fullname": "Yassine El Boudouri",
      "name": "yelboudouri",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23832",
      "authors": [
        {
          "_id": "683d67343f97feb881211cf8",
          "name": "Chaeeun Kim",
          "hidden": false
        },
        {
          "_id": "683d67343f97feb881211cf9",
          "name": "Jinu Lee",
          "hidden": false
        },
        {
          "_id": "683d67343f97feb881211cfa",
          "name": "Wonseok Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T09:02:41.000Z",
      "submittedOnDailyAt": "2025-06-02T07:26:28.412Z",
      "title": "LegalSearchLM: 法律要素を再考した法律事例検索の再考",
      "submittedOnDailyBy": {
        "_id": "614c9487cbb5e52274a4024d",
        "avatarUrl": "/avatars/a923db5ea27c4184ed2ce84738860203.svg",
        "isPro": false,
        "fullname": "Chaeeun Kim",
        "user": "Chaeeun-Kim",
        "type": "user"
      },
      "summary": "Legal Case Retrieval (LCR), キーウィードケースから関連するケースを検索するものは、研究と判断における法律専門家の基本的な任務です。しかし、現在の研究には2つの大きな制限があります。1つは、評価に使用される検索コRPORAは相対的に小さいサイズであり（例：100-55Kのケース）、刑事のキーウィードタイプの範囲が狭いため、本当に実世界的な法務検索シナリオの複雑性を十分に反映できないことです。2つ目は、埋め込みベースや語彙マッチング方法の依存関係が、法務関連の限りの表現と法務関連の無関係なマッチングにより、制限があることです。これらの問題に対処するために、LEGAR BENCHとLegalSearchLMを提出します。LEGAR BENCHは、120万の法務ケースを超える1.2Mのケースを検索する411種類の多様な犯罪タイプを対象とした最初の大規模な韓国のLCRベンチマークです。LegalSearchLMは、検索ケースに対して法要素の理由論を行い、目標ケースに基づく内容を制約付きのデコーディングを通じて直接生成します。実験結果によると、LEGAR BENCHではベースラインを6-20%より上回り、最先端の性能を達成し、オートマトプライムジェニームモデルでのデータによる学習によるオートマトプライムジェニームモデルを15%より上回ります。",
      "upvotes": 0,
      "discussionId": "683d67353f97feb881211d6a"
    },
    "publishedAt": "2025-05-28T05:02:41.000Z",
    "title": "LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements\n  Generation",
    "summary": "Legal Case Retrieval (LCR), which retrieves relevant cases from a query case,\nis a fundamental task for legal professionals in research and decision-making.\nHowever, existing studies on LCR face two major limitations. First, they are\nevaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and\nuse a narrow range of criminal query types, which cannot sufficiently reflect\nthe complexity of real-world legal retrieval scenarios. Second, their reliance\non embedding-based or lexical matching methods often results in limited\nrepresentations and legally irrelevant matches. To address these issues, we\npresent: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering\n411 diverse crime types in queries over 1.2M legal cases; and (2)\nLegalSearchLM, a retrieval model that performs legal element reasoning over the\nquery case and directly generates content grounded in the target cases through\nconstrained decoding. Experimental results show that LegalSearchLM outperforms\nbaselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It\nalso demonstrates strong generalization to out-of-domain cases, outperforming\nnaive generative models trained on in-domain data by 15%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23832.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "614c9487cbb5e52274a4024d",
      "avatarUrl": "/avatars/a923db5ea27c4184ed2ce84738860203.svg",
      "fullname": "Chaeeun Kim",
      "name": "Chaeeun-Kim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]