[
  {
    "paper": {
      "id": "2505.03318",
      "authors": [
        {
          "_id": "681aac4fd31f567552f0cc0e",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc0f",
          "name": "Zhimin Li",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc10",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:33.194Z",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc11",
          "name": "Chunyu Wang",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc12",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc13",
          "name": "Cheng Jin",
          "hidden": false
        },
        {
          "_id": "681aac4fd31f567552f0cc14",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T08:46:41.000Z",
      "submittedOnDailyAt": "2025-05-07T00:19:35.245Z",
      "title": "強化学習による統合多モデルコンシェント報酬モデルの微調節",
      "submittedOnDailyBy": {
        "_id": "654c6845bac6e6e49895a5b5",
        "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
        "isPro": false,
        "fullname": "Yibin Wang",
        "user": "CodeGoat24",
        "type": "user"
      },
      "summary": "最近の多モデルの報酬モデル（RM）の進展は、視覚モデルと人間の好みを一致させるための報酬信号の提供において显著な望ましいを示しています。しかし、現在のRMは、直接的な応答を提供するか、浅い理由過程を通じて深さに限られていますが、これは報酬信号の不正確さを招くことが多いです。私たちは、報酬理由過程に長い連鎖の思い（CoT）を明示的に挟むことで、その信頼性と強固性を大幅に強化できることを主張しています。また、RMがCoT理由を内部化した後、直接的な応答の正確性も、隠れた理由能力によって改善されることができると信じています。この点について、本論文では、最初の統一的な多モデルCoTベースの報酬モデル、UnifiedReward-Thinkを提案します。このモデルは、視覚理解と生成報酬タスクの両方において、多次元でステップごとの長い連鎖の理由を行うことができます。特に、私たちは、探索を駆動する強化学習の微調節アプローチを採用し、モデルの潜在的な複雑な理由能力を引き出し、報酬を促すことを目指します：（1）まず、GPT-4oの理由過程を学習させるために、少量の画像生成の好みデータを使用し、これをモデルの冷や冷えの始めに使用し、CoT理由の形式と構造を学習することを目指します。（2）その後、モデルの先行知識と一般化能力を活用し、大規模な統一的な多モデルの好みデータを準備し、視覚タスクの幅広い範囲でモデルの理由過程を引き出します。このフェーズでは、正しい理由出力を保持し、拒否サンプリングを用いてモデルを精練します。（3）一方で、不正確な予測サンプルは、最終的にグループ相対ポリシー最適化（GRPO）に基づく強化学習の微調節に使用され、多様な理由パスを探索し、正しいおよび強固な解決策を最適化することを可能にします。視覚報酬タスクの幅広い範囲での拡大的な実験は、本論文のモデルの上位性を示しています。",
      "upvotes": 50,
      "discussionId": "681aac50d31f567552f0cc5d",
      "projectPage": "https://codegoat24.github.io/UnifiedReward/think",
      "githubRepo": "https://github.com/CodeGoat24/UnifiedReward",
      "ai_keywords": [
        "Multimodal Reward Models (RMs)",
        "long chains of thought (CoT)",
        "UnifiedReward-Think",
        "exploration-driven reinforcement fine-tuning",
        "small amount of image generation preference data",
        "GPT-4o",
        "large-scale unified multimodal preference data",
        "rejection sampling",
        "Group Relative Policy Optimization (GRPO)",
        "reinforcement fine-tuning"
      ]
    },
    "publishedAt": "2025-05-06T04:46:41.000Z",
    "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning",
    "summary": "Recent advances in multimodal Reward Models (RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallow reasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the reward reasoning process can significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt an\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill the reasoning process of GPT-4o,\nwhich is then used for the model's cold start to learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model's reasoning process across various vision\ntasks. During this phase, correct reasoning outputs are retained for rejection\nsampling to refine the model (3) while incorrect predicted samples are finally\nused for Group Relative Policy Optimization (GRPO) based reinforcement\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\nfor correct and robust solutions. Extensive experiments across various vision\nreward tasks demonstrate the superiority of our model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03318.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654c6845bac6e6e49895a5b5",
      "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
      "fullname": "Yibin Wang",
      "name": "CodeGoat24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03335",
      "authors": [
        {
          "_id": "681ab9b8f43603c60cab87a3",
          "name": "Andrew Zhao",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a4",
          "user": {
            "_id": "647eb24118274bce0308b2b8",
            "avatarUrl": "/avatars/463e49a89b61164ccfad85ced10658b2.svg",
            "isPro": false,
            "fullname": "Yiran Wu",
            "user": "kevinwyr",
            "type": "user"
          },
          "name": "Yiran Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:25.939Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a5",
          "user": {
            "_id": "649d475111592b1a765ac1a3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
            "isPro": false,
            "fullname": "Yang Yue",
            "user": "Yang130",
            "type": "user"
          },
          "name": "Yang Yue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:23.436Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a6",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a7",
          "name": "Quentin Xu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a8",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87a9",
          "name": "Matthieu Lin",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87aa",
          "user": {
            "_id": "6486dde1f74857df3f1a5828",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
            "isPro": false,
            "fullname": "Shenzhi Wang",
            "user": "shenzhi-wang",
            "type": "user"
          },
          "name": "Shenzhi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:28.623Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ab",
          "name": "Qingyun Wu",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ac",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-07T01:39:06.052Z",
          "hidden": false
        },
        {
          "_id": "681ab9b8f43603c60cab87ad",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T09:08:00.000Z",
      "submittedOnDailyAt": "2025-05-07T00:10:48.130Z",
      "title": "「ゼロデータでの強化サイトプレイ推論」",
      "submittedOnDailyBy": {
        "_id": "630482fbce6b12280b18971d",
        "avatarUrl": "/avatars/b07f31fd970d736bdf574d56da7a5634.svg",
        "isPro": false,
        "fullname": "Andrew Zhao",
        "user": "andrewzh",
        "type": "user"
      },
      "summary": "実現可能な報酬に基づく強化学習（RLVR）は、結果ベースの報酬から直接学習することで、大規模な言語モデルの理由論能力を向上させることができることを示しています。最近のRLVRの研究は、ゼロ設定で操作することで、理由論プロセスのラベル付けにサブジェクションを避けることができますが、まだまだ手動で編集された質優な質問と答えのコレクションをテストデータに依存しています。高品質、人間が作り出した例の不足は、長期的なスケーラビリティに関する人間のサブジェクションを依存することについての懸念を引き起こします。また、将来的にAIが人間の知能を超える場合、人間が提供するタスクは、超知能システムに限られた学習ポテンシャルを提供する可能性が限られると考えられます。これらの懸念に対処するために、私たちは、一つのモデルが自ら学習進歩を最大化するタスクを提案し、それらを解決することで理由論能力を向上させることを目的とした新しいRLVRパラダイム「Absolute Zero」を提案しています。このパラダイムの下で、私たちは、コードエクセカプターを用いて提案されたコード理由論タスクを検証し、答えを確認することで、理由論能力と訓練コースを自己進化させるシステム「Absolute Zero Reasoner（AZR）」を紹介しています。AZRは、外部データを完全に依存しないものの、コーディングと数学的な理由論タスクにおいて最先端の性能を達成し、現在のゼロ設定モデルを超えています。また、AZRは、異なるモデルスケールとモデルクラスとの適用性を示し、効果的に応用できることを示しています。",
      "upvotes": 43,
      "discussionId": "681ab9baf43603c60cab881a",
      "projectPage": "https://andrewzh112.github.io/absolute-zero-reasoner/",
      "githubRepo": "https://github.com/LeapLabTHU/Absolute-Zero-Reasoner",
      "ai_keywords": [
        "Reinforcement learning with verifiable rewards (RLVR)",
        "zero setting",
        "outcome-based rewards",
        "manually curated collections",
        "superintelligent system",
        "Absolute Zero",
        "AzR (Absolute Zero Reasoner)",
        "training curriculum",
        "code executor",
        "verifiable reward",
        "open-ended yet grounded learning",
        "SOTA performance",
        "coding and mathematical reasoning tasks"
      ]
    },
    "publishedAt": "2025-05-06T05:08:00.000Z",
    "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03335.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630482fbce6b12280b18971d",
      "avatarUrl": "/avatars/b07f31fd970d736bdf574d56da7a5634.svg",
      "fullname": "Andrew Zhao",
      "name": "andrewzh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03730",
      "authors": [
        {
          "_id": "681abf3759155282c1cb2306",
          "name": "Shiyi Zhang",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2307",
          "user": {
            "_id": "64970d3d9c3b29dca8633f87",
            "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
            "isPro": false,
            "fullname": "JunhaoZhuang",
            "user": "JunhaoZhuang",
            "type": "user"
          },
          "name": "Junhao Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:58.214Z",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2308",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb2309",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "681abf3759155282c1cb230a",
          "name": "Yansong Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:58:02.000Z",
      "submittedOnDailyAt": "2025-05-07T00:37:06.442Z",
      "title": "FlexiAct: ヘテロジネイシャルなシナリオにおける柔軟なアクション制御へ",
      "submittedOnDailyBy": {
        "_id": "6315d306a9456afe2b9bf34a",
        "avatarUrl": "/avatars/7285b4e7d84b528d1a50f8ee4eb10727.svg",
        "isPro": false,
        "fullname": "ElevenZ",
        "user": "shiyi0408",
        "type": "user"
      },
      "summary": "アクションカスタマイズメントは、シェープガイドされたまたはグローバルモーションカスタマイズメントを使用して、シェープガイドされたまたはグローバルモーションカスタマイズメントを使用して、入力制御シグナルによって指示されたアクションを実行するビデオを生成することである。現在の方法は、スペクトラル構造の厳格な制約（例えば、ラウタイ、スケルタル、視点の一貫性）により、多様な主題とシナリオに対する適応性が低下している。これらの制限を克服するために、私たちはFlexiActを提案しています。FlexiActは、参照ビデオからアクションをターゲット画像に転送することを目的としています。現在の方法と違い、FlexiActは参照ビデオの主題とターゲット画像の間にスペクトラル構造（ラウタイ、スケルタル、視点）の変化を許容し、アクションの一貫性を維持することができます。これを実現するには、アクションの精密な制御、スペクトラル構造の適応、アクションの一貫性の維持が必要です。これに向けて、私たちはRefAdapterを導入しています。RefAdapterは、スペクトラル適応と一貫性の維持に特化し、現在の方法を超えて、アプエアンコンシステンシーと構造的な柔軟性のバランスを実現することができます。また、私たちの観察から、デノイズプロセスは、時間ステップごとに動き（低周波）とアプエアン詳細（高周波）に対する注意が異なることがわかりました。そこで、私たちはFAE（周波数達知アクション抽出）を提案しています。FAEは、現在の方法と違い、別々のスペクトラル・タイムステップアーキテクチャを使用しないで、デノイズプロセス中でアクション抽出を直接実現します。実験は、我々の方法が、多様なラウタイ、スケルタル、視点の主題にアクションを有効に転送することを示しています。我々は、コードとモデル重みを公開して、さらなる研究のためのサポートを提供します。https://shiyi-zh0408.github.io/projectpages/FlexiAct/",
      "upvotes": 17,
      "discussionId": "681abf3859155282c1cb23fa",
      "projectPage": "https://shiyi-zh0408.github.io/projectpages/FlexiAct/",
      "githubRepo": "https://github.com/shiyi-zh0408/FlexiAct",
      "ai_keywords": [
        "FlexiAct",
        "RefAdapter",
        "image-conditioned adapter",
        "spatial adaptation",
        "consistency preservation",
        "FAE",
        "Frequency-aware Action Extraction",
        "denoising process",
        "spatial-temporal architectures",
        "action extraction"
      ]
    },
    "publishedAt": "2025-05-06T13:58:02.000Z",
    "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios",
    "summary": "Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03730.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6315d306a9456afe2b9bf34a",
      "avatarUrl": "/avatars/7285b4e7d84b528d1a50f8ee4eb10727.svg",
      "fullname": "ElevenZ",
      "name": "shiyi0408",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02922",
      "authors": [
        {
          "_id": "681abdbef8feaef23b543877",
          "name": "Yaoqi Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543878",
          "name": "Jinkai Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543879",
          "user": {
            "_id": "667135bdcca06def1c2599a6",
            "avatarUrl": "/avatars/d94ab99265e1970852605d344b4d69e9.svg",
            "isPro": false,
            "fullname": "Baotong Lu",
            "user": "baotonglu",
            "type": "user"
          },
          "name": "Baotong Lu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-07T01:56:16.240Z",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387a",
          "user": {
            "_id": "66e96f58de9ebeee86f5e27f",
            "avatarUrl": "/avatars/e7d692aa47f02f1858186f47186166ad.svg",
            "isPro": false,
            "fullname": "Qianxi Zhang",
            "user": "qianxizhang",
            "type": "user"
          },
          "name": "Qianxi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:00.341Z",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387b",
          "name": "Chengruidong Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387c",
          "name": "Jingjia Luo",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387d",
          "name": "Di Liu",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387e",
          "name": "Huiqiang Jiang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b54387f",
          "name": "Qi Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543880",
          "name": "Jing Liu",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543881",
          "name": "Bailu Ding",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543882",
          "name": "Xiao Yan",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543883",
          "name": "Jiawei Jiang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543884",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543885",
          "name": "Mingxing Zhang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543886",
          "name": "Yuqing Yang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543887",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "681abdbef8feaef23b543888",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T18:01:17.000Z",
      "submittedOnDailyAt": "2025-05-07T00:36:48.274Z",
      "title": "RetroInfer: スケーラブルな長文脈LLMの推論におけるベクトル保存アプローチ",
      "submittedOnDailyBy": {
        "_id": "6278bd42541f3d2dfa77ea70",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
        "isPro": true,
        "fullname": "Huiqiang Jiang",
        "user": "iofu728",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）のコンテキスト長が増加していることは、GPUメモリとバンデッドの制約によって効率的な推論に重大な課題をもたらしています。我々は、長コンテキストLLMの推論を加速するために、固有の注意の稀薄性を利用したベクトルストレージシステムとしてのキーバリュー（KV）キャッシュを再概念化した新しいシステムRetroInferを紹介します。その核心は、wave indexです。wave indexは、三分注意近似、精度制限付き注意推定、分離されたクラスタリングなどの手法を用いて、重要なトークンの効率的かつ正確な検索を可能にします。wave bufferとの組み合わせにより、KVキャッシュの配置を協調し、GPUとCPUの計算とデータ転送を重なり合わせ、高いトランスポートを維持します。先行の稀薄性ベースの方法と違って、RetroInferはトークン選択とハードウェアの協調に難しくないように、モデルの精度を犠牲にしないように強固な性能を提供します。長コンテキストベンチマーク上の実験は、GPUメモリの制限内で全注意より4.5倍のスピードアップを収め、KVキャッシュをCPUメモリに拡張すると、稀疏注意ベースラインに対して10.5倍のスピードアップを収め、全注意レベルの精度を維持します。",
      "upvotes": 16,
      "discussionId": "681abdc0f8feaef23b543926",
      "ai_keywords": [
        "RetrInfer",
        "key-value (KV) cache",
        "vector storage system",
        "wave index",
        "Attention-aWare VEctor index",
        "tripartite attention approximation",
        "accuracy-bounded attention estimation",
        "segmented clustering",
        "wave buffer",
        "GPU memory",
        "token selection",
        "hardware coordination"
      ]
    },
    "publishedAt": "2025-05-05T14:01:17.000Z",
    "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
    "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02922.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6278bd42541f3d2dfa77ea70",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg",
      "fullname": "Huiqiang Jiang",
      "name": "iofu728",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03005",
      "authors": [
        {
          "_id": "681ac8a09f4ed2ece10fac18",
          "user": {
            "_id": "647f4bac45baf21ad709fcd0",
            "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
            "isPro": false,
            "fullname": "Dan Goldstein",
            "user": "SmerkyG",
            "type": "user"
          },
          "name": "Daniel Goldstein",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:53.268Z",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac19",
          "name": "Eric Alcaide",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac1a",
          "name": "Janna Lu",
          "hidden": false
        },
        {
          "_id": "681ac8a09f4ed2ece10fac1b",
          "name": "Eugene Cheah",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/sfcDfWZfka9sy8siQC6MV.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/yXcUOgY48NS_nfIZN0Kqq.png"
      ],
      "publishedAt": "2025-05-05T20:03:28.000Z",
      "submittedOnDailyAt": "2025-05-07T01:19:36.031Z",
      "title": "RADLADS: スケールでの線形アタチンの急速なアテンションディスタイル化",
      "submittedOnDailyBy": {
        "_id": "647f4bac45baf21ad709fcd0",
        "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
        "isPro": false,
        "fullname": "Dan Goldstein",
        "user": "SmerkyG",
        "type": "user"
      },
      "summary": "ラプラチナエフセンション変換をライナーアテンションデコーダーに拡大してもらうRADLADS（Rapid Attention Distillation to Linear Attention Decoders at Scale）を紹介します。これはソフトマックスアテンショントランスフォーマーを迅速にライナーアテンションデコーダーモデルに変換するプロトコルで、2つの新しいRWKV-variantアーキテクチャと、プロフェッショナルなQwen2.5オープンソースモデル（7B、32B、72Bサイズ）からの変換モデルを含みます。変換プロセスには、元の教師モデルのトークンカウントの0.005%程度の350-700Mトークンが必要です。現在の価格で72Bライナーアテンションモデルの変換費用は2,000 USD未満ですが、推論時の質問は元のトランスフォーマーと近いです。これらのモデルは、ライナーアテンションモデルのサイズの標準ベンチマークで最先端の下流性能を達成します。すべてのモデルはApache 2.0ライセンスのもとでHuggingFaceでリリースされ、72BモデルはQwen License Agreementによっても同様の制限を受けています。\n\nモデルは以下のURLで公開されています：\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\n\n訓練コードは以下のURLで公開されています：\nhttps://github.com/recursal/RADLADS-paper",
      "upvotes": 14,
      "discussionId": "681ac8a19f4ed2ece10fac75",
      "ai_keywords": [
        "Rapid Attention Distillation",
        "RADLADS",
        "softmax attention transformers",
        "linear attention decoder models",
        "RWKV-variant",
        "Qwen2.5",
        "token count",
        "linear attention models",
        "HuggingFace",
        "Apache 2.0 license",
        "Qwen License Agreement"
      ]
    },
    "publishedAt": "2025-05-05T16:03:28.000Z",
    "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at\n  Scale",
    "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/sfcDfWZfka9sy8siQC6MV.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f4bac45baf21ad709fcd0/yXcUOgY48NS_nfIZN0Kqq.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03005.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f4bac45baf21ad709fcd0",
      "avatarUrl": "/avatars/14c04cdda95de676aeefa9ae3e7c19ba.svg",
      "fullname": "Dan Goldstein",
      "name": "SmerkyG",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02872",
      "authors": [
        {
          "_id": "681b03a33bf166767107c74b",
          "name": "Cfir Avraham Hadar",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74c",
          "name": "Omer Shubi",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74d",
          "name": "Yoav Meiri",
          "hidden": false
        },
        {
          "_id": "681b03a33bf166767107c74e",
          "name": "Yevgeni Berzak",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T13:23:48.000Z",
      "submittedOnDailyAt": "2025-05-07T05:24:49.077Z",
      "title": "ロードイング 読み込み時の瞳の移動からの開放的な情報探求の目的の解明",
      "submittedOnDailyBy": {
        "_id": "60ef001bed64a34082bfa0dd",
        "avatarUrl": "/avatars/78e4daeac169edbf4dc42fbed9b50d59.svg",
        "isPro": false,
        "fullname": "Omer Shubi",
        "user": "scaperex",
        "type": "user"
      },
      "summary": "読み込みの際には、よく特定の情報が興味を持っていることがあります。例えば、この論文を読み込み、LLMs（大脳連結モデル）について興味を持っている場合、実験設計について興味を持っている場合、または「これは効果的か？」という問いについて興味を持っている場合があります。広範囲において、日常生活では、読み込みに対する複数のテキスト特有の目標が読み込み行動を指導しています。この研究では、初めて、開放的な読み込み目標が読み込み時の眼球移動から自動的に解釈できるかどうかを調べることを試みます。この問題を解決するために、目標分類および目標構成タスクと評価フレームワークを提案し、英語の読み込みデータにおいて、数百のテキスト特有の情報探求タスクを含む大規模な眼球追跡データを使用します。目標分類と目標構成に向けた見分けや生成的な多モーダルLLMsを開発し、比較します。これらの実験は、両タスクにおいて相当の成功を示し、LLMsが読み込み時の眼球移動から読み手のテキスト特有の目標について有價値な情報を抽出できることを示しています。",
      "upvotes": 11,
      "discussionId": "681b03a43bf166767107c787",
      "ai_keywords": [
        "goal classification",
        "goal reconstruction",
        "discriminative models",
        "generative models",
        "multimodal LLMs",
        "large-scale eye tracking",
        "text-specific information seeking"
      ]
    },
    "publishedAt": "2025-05-04T09:23:48.000Z",
    "title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in\n  Reading",
    "summary": "When reading, we often have specific information that interests us in a text.\nFor example, you might be reading this paper because you are curious about LLMs\nfor eye movements in reading, the experimental design, or perhaps you only care\nabout the question ``but does it work?''. More broadly, in daily life, people\napproach texts with any number of text-specific goals that guide their reading\nbehavior. In this work, we ask, for the first time, whether open-ended reading\ngoals can be automatically decoded from eye movements in reading. To address\nthis question, we introduce goal classification and goal reconstruction tasks\nand evaluation frameworks, and use large-scale eye tracking for reading data in\nEnglish with hundreds of text-specific information seeking tasks. We develop\nand compare several discriminative and generative multimodal LLMs that combine\neye movements and text for goal classification and goal reconstruction. Our\nexperiments show considerable success on both tasks, suggesting that LLMs can\nextract valuable information about the readers' text-specific goals from eye\nmovements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02872.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60ef001bed64a34082bfa0dd",
      "avatarUrl": "/avatars/78e4daeac169edbf4dc42fbed9b50d59.svg",
      "fullname": "Omer Shubi",
      "name": "scaperex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02214",
      "authors": [
        {
          "_id": "6819905519b420a2f91aa231",
          "user": {
            "_id": "64612660933afb0106a9dee3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
            "isPro": false,
            "fullname": "Xingyu Zheng",
            "user": "Xingyu-Zheng",
            "type": "user"
          },
          "name": "Xingyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:43.859Z",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa232",
          "name": "Yuye Li",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa233",
          "user": {
            "_id": "68164883244ee586cb159268",
            "avatarUrl": "/avatars/856ca8731ca156f616529e427d8fd76a.svg",
            "isPro": false,
            "fullname": "初浩然",
            "user": "HaoranChu",
            "type": "user"
          },
          "name": "Haoran Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:09:41.155Z",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa234",
          "name": "Yue Feng",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa235",
          "name": "Xudong Ma",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa236",
          "name": "Jie Luo",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa237",
          "name": "Jinyang Guo",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa238",
          "name": "Haotong Qin",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa239",
          "name": "Michele Magno",
          "hidden": false
        },
        {
          "_id": "6819905519b420a2f91aa23a",
          "name": "Xianglong Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64612660933afb0106a9dee3/uQV5mzdTqoNatTz5fk6xC.png"
      ],
      "publishedAt": "2025-05-04T18:43:44.000Z",
      "submittedOnDailyAt": "2025-05-07T02:59:22.920Z",
      "title": "Qwen3 の実験的な減量研究",
      "submittedOnDailyBy": {
        "_id": "64612660933afb0106a9dee3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
        "isPro": false,
        "fullname": "Xingyu Zheng",
        "user": "Xingyu-Zheng",
        "type": "user"
      },
      "summary": "Qwen シリーズは、開放ソースの大規模言語モデル（LLMs）のリーディングフamillyとして現れ、自然言語理解タスクでの卓越した能力を示しています。最近の Qwen3 のリリースでは、多様なベンチマークで上位の性能を発挥し、リソース制限された環境でこれらのモデルを効率的に扱うことに対しての興味が増加しています。低ビット数の量子化は有望な解決策として提案されていますが、Qwen3 の性能に及ぼす影響はまだ詳しく調査されていません。本研究では、Qwen3 の量子化設定による強固性をシステマティックに評価し、この最先端のモデルの圧縮における機会と課題を明らかにすることを目指しています。Qwen3 に対して 5 つの現在のクラシック的な後学量子化技術を厳密に評価し、1〜8ビットのビット幅を範囲にわたし、複数のデータセットでその効果を評価しました。私たちの発見は、Qwen3 は中度のビット幅では相対的に競争的な性能を維持しますが、超低精度での言語タスクでは顕著な低下を認め、LLM の圧縮における持続する課題を明らかにしています。これらの結果は、極端な量子化シナリオでの性能損失を軽減するための進める研究の必要性を強調しています。私たちは、この実験的な分析が Qwen3 および将来の LLMs に適した量子化方法の進歩につながる具体的なエインサイドを提供することを期待しています。本プロジェクトは、https://github.com/Efficient-ML/Qwen3-Quantization と https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b でリリースされています。",
      "upvotes": 7,
      "discussionId": "6819905519b420a2f91aa27c",
      "githubRepo": "https://github.com/Efficient-ML/Qwen3-Quantization",
      "ai_keywords": [
        "Low-bit quantization",
        "Post-training quantization techniques",
        "Bit-widths",
        "Linguistic tasks",
        "LLM compression",
        "Quantization methods"
      ]
    },
    "publishedAt": "2025-05-04T14:43:44.000Z",
    "title": "An Empirical Study of Qwen3 Quantization",
    "summary": "The Qwen series has emerged as a leading family of open-source Large Language\nModels (LLMs), demonstrating remarkable capabilities in natural language\nunderstanding tasks. With the recent release of Qwen3, which exhibits superior\nperformance across diverse benchmarks, there is growing interest in deploying\nthese models efficiently in resource-constrained environments. Low-bit\nquantization presents a promising solution, yet its impact on Qwen3's\nperformance remains underexplored. This study conducts a systematic evaluation\nof Qwen3's robustness under various quantization settings, aiming to uncover\nboth opportunities and challenges in compressing this state-of-the-art model.\nWe rigorously assess 5 existing classic post-training quantization techniques\napplied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their\neffectiveness across multiple datasets. Our findings reveal that while Qwen3\nmaintains competitive performance at moderate bit-widths, it experiences\nnotable degradation in linguistic tasks under ultra-low precision, underscoring\nthe persistent hurdles in LLM compression. These results emphasize the need for\nfurther research to mitigate performance loss in extreme quantization\nscenarios. We anticipate that this empirical analysis will provide actionable\ninsights for advancing quantization methods tailored to Qwen3 and future LLMs,\nultimately enhancing their practicality without compromising accuracy. Our\nproject is released on https://github.com/Efficient-ML/Qwen3-Quantization and\nhttps://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64612660933afb0106a9dee3/uQV5mzdTqoNatTz5fk6xC.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02214.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64612660933afb0106a9dee3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
      "fullname": "Xingyu Zheng",
      "name": "Xingyu-Zheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03735",
      "authors": [
        {
          "_id": "681acb9c285636e830d37c8e",
          "user": {
            "_id": "6625ce8074ae2df4e3effa92",
            "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
            "isPro": false,
            "fullname": "Jiayuan Rao 饶珈源",
            "user": "Homie0609",
            "type": "user"
          },
          "name": "Jiayuan Rao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:50.372Z",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c8f",
          "name": "Zifeng Li",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c90",
          "user": {
            "_id": "632c7a0d1d303f5f9acf01b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
            "isPro": false,
            "fullname": "Haoning Wu",
            "user": "haoningwu",
            "type": "user"
          },
          "name": "Haoning Wu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-07T02:56:09.106Z",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c91",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c92",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "681acb9c285636e830d37c93",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:59:31.000Z",
      "submittedOnDailyAt": "2025-05-07T01:26:13.756Z",
      "title": "多エージェントシステムでの全面的なサッカー理解",
      "submittedOnDailyBy": {
        "_id": "6625ce8074ae2df4e3effa92",
        "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
        "isPro": false,
        "fullname": "Jiayuan Rao 饶珈源",
        "user": "Homie0609",
        "type": "user"
      },
      "summary": "最近、AI駆動のサッカー理解における進展は速いことが明らかですが、現在の研究は主に孤立したものから狭いタスクに焦点を当てています。この隙を埋めるために、サッカー全体の理解に関する一つの厳密なフレームワークを提案します。具体的に、この論文では以下のような貢献を行います: (i) SoccerWikiを構築します。これは最初の大規模な多モデルのサッカー知識ベースで、プレーヤー、チーム、裁判、ビーンスプートなどの豊富な領域知識を統合し、知識を基にした推理を可能にします。 (ii) SoccerBenchを紹介します。これは最大で最も厳密なサッカー特有のベンチマークで、13種類の異なる理解タスクにおける約10Kの標準化された多モデル（テキスト、画像、映像）の多選挙問題ペアを掲載し、自動プイルチンと手動の確認を通じてカレーテッドします。 (iii) SoccerAgentを紹介します。これは新しい多エージェントシステムで、複雑なサッカー問題を協力した推理により分解し、SoccerWikiからの領域専門知識を活用し、強固な性能を達成します。 (iv) 極めて広範囲な評価と消去試験を行い、SoccerBenchにおける最先端のMLLMをベンチマークし、提案されたエージェントシステムの優越性を明らかにします。すべてのデータとコードは以下のURLで公開されています: https://jyrao.github.io/SoccerAgent/",
      "upvotes": 5,
      "discussionId": "681acb9e285636e830d37d4b",
      "projectPage": "https://jyrao.github.io/SoccerAgent/",
      "githubRepo": "https://github.com/jyrao/SoccerAgent",
      "ai_keywords": [
        "multimodal",
        "knowledge base",
        "domain knowledge",
        "multimodal (text, image, video) multi-choice QA pairs",
        "multi-agent system",
        "collaborative reasoning",
        "state-of-the-art MLLMs"
      ]
    },
    "publishedAt": "2025-05-06T13:59:31.000Z",
    "title": "Multi-Agent System for Comprehensive Soccer Understanding",
    "summary": "Recent advancements in AI-driven soccer understanding have demonstrated rapid\nprogress, yet existing research predominantly focuses on isolated or narrow\ntasks. To bridge this gap, we propose a comprehensive framework for holistic\nsoccer understanding. Specifically, we make the following contributions in this\npaper: (i) we construct SoccerWiki, the first large-scale multimodal soccer\nknowledge base, integrating rich domain knowledge about players, teams,\nreferees, and venues to enable knowledge-driven reasoning; (ii) we present\nSoccerBench, the largest and most comprehensive soccer-specific benchmark,\nfeaturing around 10K standardized multimodal (text, image, video) multi-choice\nQA pairs across 13 distinct understanding tasks, curated through automated\npipelines and manual verification; (iii) we introduce SoccerAgent, a novel\nmulti-agent system that decomposes complex soccer questions via collaborative\nreasoning, leveraging domain expertise from SoccerWiki and achieving robust\nperformance; (iv) extensive evaluations and ablations that benchmark\nstate-of-the-art MLLMs on SoccerBench, highlighting the superiority of our\nproposed agentic system. All data and code are publicly available at:\nhttps://jyrao.github.io/SoccerAgent/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6625ce8074ae2df4e3effa92",
      "avatarUrl": "/avatars/686cd85cf8d3d8f0beb0737811144294.svg",
      "fullname": "Jiayuan Rao 饶珈源",
      "name": "Homie0609",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.03368",
      "authors": [
        {
          "_id": "681b05ab8e325b4097318969",
          "user": {
            "_id": "661e841972c7030f3fd57eb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661e841972c7030f3fd57eb9/PysszxrOl__sZ-IToZfJU.jpeg",
            "isPro": false,
            "fullname": "Stef De Sabbata",
            "user": "sdesabbata",
            "type": "user"
          },
          "name": "Stef De Sabbata",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:24.829Z",
          "hidden": false
        },
        {
          "_id": "681b05ab8e325b409731896a",
          "name": "Stefano Mizzaro",
          "hidden": false
        },
        {
          "_id": "681b05ab8e325b409731896b",
          "name": "Kevin Roitero",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T09:40:06.000Z",
      "submittedOnDailyAt": "2025-05-07T05:33:25.973Z",
      "title": "ジオスペクトル機械的説明性の大規模言語モデル",
      "submittedOnDailyBy": {
        "_id": "620cca6f06a4320dbf3b50d8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654011487582-620cca6f06a4320dbf3b50d8.png",
        "isPro": false,
        "fullname": "Kevin Roitero",
        "user": "kevinr",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、多様な自然言語処理タスクで前所未聞の能力を示しています。それらがテキストとコードを処理し生成する能力は、複数の分野でもっとも広く使用されていますが、知識ベースと「理由論」ツールとしての採用は、継続的な研究の領域です。地理学においては、LLMsの地理的知識と空間理由論の能力に関する評価に焦点を当てている文献は増えておりますが、これらのモデルの内部機能については、特に地理的データをどのように処理するかについては、まだ知られていることはありません。\n\nこの章では、地理空間機構的説明可能性の研究に新しいフレームワークを構築します。それは、空間分析を用いて、LLMsが地理的データをどのように処理するかを逆転工学するために使用します。我々の目的は、これらの複雑なモデルが地理的データを処理して生成する内部表現を理解することを進めることです。これらのモデルが地理的データにどのように思い出すかというものを、このフレームワークが必要とするようなものとして考えることができますが、それは過度な人間化ではありません。\n\nまず、LLMsの内部構造を明らかにするためのプローブングの使用を説明します。次に、機構的説明可能性の分野を介して、スペクトラディオンハピオン論とスパースオートエンコーダーの役割について議論します。これらのモデルの多意味的な内部表現を解明し、説明可能な、単意味的な特徴に分解するためにその役割を説明します。実験では、空間自相関を用いて、地名に関連付けられた特徴が空間パターンに関連していることを示し、これらのモデルが地理的データをどのように処理するかについてのより深い理解を提供します。最終的に、我々のフレームワークが地理学の基盤モデルの研究と使用にどのように役立つかについて議論します。",
      "upvotes": 3,
      "discussionId": "681b05ad8e325b4097318a3d",
      "ai_keywords": [
        "probing",
        "mechanistic interpretability",
        "superposition hypothesis",
        "sparse autoencoders",
        "polysemantic",
        "monosemantic",
        "spatial autocorrelation",
        "placenames",
        "geospatial patterns"
      ]
    },
    "publishedAt": "2025-05-06T05:40:06.000Z",
    "title": "Geospatial Mechanistic Interpretability of Large Language Models",
    "summary": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03368.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620cca6f06a4320dbf3b50d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654011487582-620cca6f06a4320dbf3b50d8.png",
      "fullname": "Kevin Roitero",
      "name": "kevinr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03164",
      "authors": [
        {
          "_id": "681ac5d2626f5a368b2a7103",
          "name": "Ji Won Chung",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7104",
          "name": "Tongyu Zhou",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7105",
          "name": "Ivy Chen",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7106",
          "name": "Kevin Hsu",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7107",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7108",
          "name": "Alexa Siu",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a7109",
          "name": "Shunan Guo",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710a",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-07T08:08:55.782Z",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710b",
          "name": "James Tompkin",
          "hidden": false
        },
        {
          "_id": "681ac5d2626f5a368b2a710c",
          "name": "Jeff Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T04:18:42.000Z",
      "submittedOnDailyAt": "2025-05-07T01:00:46.546Z",
      "title": "InfoVids: ビデオの視聴者体験を再考し、オルタナティブな可視化とプレゼンター関係を活用して再設計する。",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "傳統のデータ表示は、プレゼンターとビジュアライゼーションを2つの異なるスペースに分けています—3次元の世界と2次元のスクリーン—これにより、ビジュアライゼーションセンチャリスト的な物語を強化しています。ヒューマンセンチャリスト的な視聴体験を作るために、我々はInfoVidsを用いてビジュアライゼーションとプレゼンターの関係をより公平にします。これらのインフォグラフィックスビデオは、プレゼンターとビジュアライゼーションの関係を再定義するために作成されています。InfoVidsを設計する過程で、ラウォー、フォーム、インタラクションの使用によって視聴体験がどのように変化するかを調査しています。30名の参加者と9ポイント準備で、基準的な2次元スライドとの比較を行い、自伝的な視点から実用的な長期的なインサイトを提供します。混合方法の分析により、このパラダイムは視聴者の注意分裂を減らし、ビジュアライゼーションからプレゼンターへの焦点を変え、視聴者によりインタラクティブで自然で楽しむ全身のデータパフォーマンスを実現しました。最終的に、InfoVidsは視聴者にプレゼンターとビジュアライゼーションの伝統的な動機を再考させました。",
      "upvotes": 3,
      "discussionId": "681ac5d4626f5a368b2a71f8"
    },
    "publishedAt": "2025-05-06T00:18:42.000Z",
    "title": "InfoVids: Reimagining the Viewer Experience with Alternative\n  Visualization-Presenter Relationships",
    "summary": "Traditional data presentations typically separate the presenter and\nvisualization into two separate spaces--the 3D world and a 2D screen--enforcing\nvisualization-centric stories. To create a more human-centric viewing\nexperience, we establish a more equitable relationship between the\nvisualization and the presenter through our InfoVids. These\ninfographics-inspired informational videos are crafted to redefine\nrelationships between the presenter and visualizations. As we design InfoVids,\nwe explore how the use of layout, form, and interactions affects the viewer\nexperience. We compare InfoVids against their baseline 2D `slides' equivalents\nacross 9 metrics with 30 participants and provide practical, long-term insights\nfrom an autobiographical perspective. Our mixed methods analyses reveal that\nthis paradigm reduced viewer attention splitting, shifted the focus from the\nvisualization to the presenter, and led to more interactive, natural, and\nengaging full-body data performances for viewers. Ultimately, InfoVids helped\nviewers re-imagine traditional dynamics between the presenter and\nvisualizations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03164.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03739",
      "authors": [
        {
          "_id": "681b232d7167935b1e979e64",
          "name": "Zuwei Long",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e65",
          "name": "Yunhang Shen",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e66",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e67",
          "name": "Heting Gao",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e68",
          "name": "Lijiang Li",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e69",
          "name": "Peixian Chen",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6a",
          "name": "Mengdan Zhang",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6b",
          "name": "Hang Shao",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6c",
          "name": "Jian Li",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6d",
          "name": "Jinlong Peng",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6e",
          "name": "Haoyu Cao",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e6f",
          "name": "Ke Li",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e70",
          "name": "Rongrong Ji",
          "hidden": false
        },
        {
          "_id": "681b232d7167935b1e979e71",
          "name": "Xing Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T17:59:53.000Z",
      "submittedOnDailyAt": "2025-05-07T08:07:03.177Z",
      "title": "VITA-Audio: 効率的な大規模の音声・言語モデルにおける高速交差モードルトークン生成",
      "submittedOnDailyBy": {
        "_id": "6483143902f98c3f05aff915",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6483143902f98c3f05aff915/ZhWFFgrlRsQf4MXiInh5p.jpeg",
        "isPro": true,
        "fullname": "沈云航 Yunhang Shen",
        "user": "shenyunhang",
        "type": "user"
      },
      "summary": "自然な人間コンピュータインタラクションの要求が増加している中、スピーチベースのシステムは日常的なコミュニケーションの最も一般的な形であるスピーチを利用して、注目を集めています。しかし、現在のスピーチモデルは、ストリーミング時に最初の音声トークンを生成する際に高いラテンシーを認め、採用において重大なボトルネックとなっています。この問題を解決するために、VITA-Audioという端末から端末までの大規模なスピーチモデルを提案します。特に、ライトウェイトなMultiple Cross-modal Token Prediction（MCTP）モジュールを導入し、一度のモデルのフォワードパスで複数の音声トークンを効率的に生成することで、推論を加速し、ストリーミングシナリオで最初の音声の生成ラテンシーを大幅に減少します。また、4ステップの進歩的な訓練戦略を検討し、音声の品質の最小限の損失でモデルの加速を実現します。私たちの知識によると、VITA-Audioは最初の多モデルの大規模な言語モデルで、最初のフォワードパスで音声出力を生成することができ、最小限のラテンシーで実時間のコンバットファイル能力を許可します。VITA-Audioは完全に再現可能で、データセットはオープンソースです。実験結果によると、我々のモデルはパラメーターサイズ7Bで3~5倍の推論スピードアップを達成し、自動的言語識別（ASR）、文脈生成（TTS）、口語質問回答（SQA）の複数のベンチマークで類似サイズのオープンソースモデルよりも显著に優れています。",
      "upvotes": 2,
      "discussionId": "681b232e7167935b1e979ec6",
      "ai_keywords": [
        "end-to-end large speech model",
        "Multiple Cross-modal Token Prediction (MCTP)",
        "model forward pass",
        "inference",
        "streaming scenarios",
        "four-stage progressive training strategy",
        "multi-modal large language model",
        "real-time conversational capabilities",
        "inference speedup",
        "automatic speech recognition (ASR)",
        "text-to-speech (TTS)",
        "spoken question answering (SQA)"
      ]
    },
    "publishedAt": "2025-05-06T13:59:53.000Z",
    "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model",
    "summary": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03739.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6483143902f98c3f05aff915",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6483143902f98c3f05aff915/ZhWFFgrlRsQf4MXiInh5p.jpeg",
      "fullname": "沈云航 Yunhang Shen",
      "name": "shenyunhang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02311",
      "authors": [
        {
          "_id": "681a06459c99f4b5ce5f2838",
          "user": {
            "_id": "658e85bb5b7553ca5c29ba89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
            "isPro": false,
            "fullname": "Jihao Zhao",
            "user": "Robot2050",
            "type": "user"
          },
          "name": "Jihao Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-06T16:51:29.234Z",
          "hidden": false
        },
        {
          "_id": "681a06459c99f4b5ce5f2839",
          "name": "Chunlai Zhou",
          "hidden": false
        },
        {
          "_id": "681a06459c99f4b5ce5f283a",
          "name": "Biao Qin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T01:45:56.000Z",
      "submittedOnDailyAt": "2025-05-07T00:06:14.265Z",
      "title": "インバーカーインターフェイスは必要に応じてだけ呼び出す：問題解答における大規模言語モデルのアダプティブな呼び出し",
      "submittedOnDailyBy": {
        "_id": "658e85bb5b7553ca5c29ba89",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
        "isPro": false,
        "fullname": "Jihao Zhao",
        "user": "Robot2050",
        "type": "user"
      },
      "summary": "大規模と小規模の言語モデル（LM）の協議パラダイムは、性能とコストの調整を効果的に行うが、小規模のLMでのハウラシングの発生時点を正確に特定するのが重要な課題である。先行の最適化の努力は主に後処理手法に焦点を当て、これらはLMの説明過程と分離し、高い計算コストと限定的な効果性を伴っていた。本論文では、小規模のLMの生成過程でのハウラシングの蓄積と伝播を計算するための実用的な呼出評価指標「AttenHScore」を提案し、これを用いて潜在的な説明誤りを連続的に拡大させることで、大規模のLMの実時間呼出を更に正確に行うことができる。また、小規模のLMの説明能力の限界を考慮し、不確実性に関する知識再組織化を活用し、異なるテキストチャンクからの重要な情報のより効果的な掴み取りを促すことができる。拡張的な実験は、複数のQAデータセットでの実時間ハウラシング検出能力の向上において、AttenHScoreが基準となるものよりも優れていることを明らかにし、特に複雑なクエリを対処する場合には特に効果的であることを示している。また、本論文の戦略は追加のモデル訓練の必要性を除去し、多様なTransformerベースのLMに対する適応性を示している。",
      "upvotes": 2,
      "discussionId": "681a06469c99f4b5ce5f28a3",
      "ai_keywords": [
        "large language models (LMs)",
        "small language models (LMs)",
        "hallucinations",
        "post-processing techniques",
        "AttenHScore",
        "generation process",
        "reasoning errors",
        "dynamic adjustment",
        "real-time invocation",
        "reasoning capacity",
        "uncertainty-aware knowledge reorganization",
        "QA datasets",
        "complex queries",
        "transformer-based LMs"
      ]
    },
    "publishedAt": "2025-05-04T21:45:56.000Z",
    "title": "Invoke Interfaces Only When Needed: Adaptive Invocation for Large\n  Language Models in Question Answering",
    "summary": "The collaborative paradigm of large and small language models (LMs)\neffectively balances performance and cost, yet its pivotal challenge lies in\nprecisely pinpointing the moment of invocation when hallucinations arise in\nsmall LMs. Previous optimization efforts primarily focused on post-processing\ntechniques, which were separate from the reasoning process of LMs, resulting in\nhigh computational costs and limited effectiveness. In this paper, we propose a\npractical invocation evaluation metric called AttenHScore, which calculates the\naccumulation and propagation of hallucinations during the generation process of\nsmall LMs, continuously amplifying potential reasoning errors. By dynamically\nadjusting the detection threshold, we achieve more accurate real-time\ninvocation of large LMs. Additionally, considering the limited reasoning\ncapacity of small LMs, we leverage uncertainty-aware knowledge reorganization\nto assist them better capture critical information from different text chunks.\nExtensive experiments reveal that our AttenHScore outperforms most baseline in\nenhancing real-time hallucination detection capabilities across multiple QA\ndatasets, especially when addressing complex queries. Moreover, our strategies\neliminate the need for additional model training and display flexibility in\nadapting to various transformer-based LMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02311.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658e85bb5b7553ca5c29ba89",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
      "fullname": "Jihao Zhao",
      "name": "Robot2050",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.21650",
      "authors": [
        {
          "_id": "681b258f23dbf6b59c5fc735",
          "name": "Haiyang Zhou",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc736",
          "user": {
            "_id": "63f095be6309c84d5f48848a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f095be6309c84d5f48848a/pL2CKi-r-0mMfIGhYSAsm.jpeg",
            "isPro": false,
            "fullname": "Wangbo Yu",
            "user": "Drexubery",
            "type": "user"
          },
          "name": "Wangbo Yu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-07T09:19:13.250Z",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc737",
          "name": "Jiawen Guan",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc738",
          "name": "Xinhua Cheng",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc739",
          "name": "Yonghong Tian",
          "hidden": false
        },
        {
          "_id": "681b258f23dbf6b59c5fc73a",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-30T13:55:28.000Z",
      "submittedOnDailyAt": "2025-05-07T07:52:52.459Z",
      "title": "ホロタイム：フィードバックモデルを使ってフラットディープラーニングに対してパノラマ4Dスケーンの生成を実現する方法",
      "submittedOnDailyBy": {
        "_id": "66eeda3676a8038cb448f11d",
        "avatarUrl": "/avatars/8d6e61f4c9354c6720ccaa7be0fe1d9f.svg",
        "isPro": false,
        "fullname": "Haiyang Zhou",
        "user": "Marblueocean",
        "type": "user"
      },
      "summary": "ディフュージョンモデルの急速な進歩は、VRとARテクノロジーの適用に革命的な可能性を持つことを示しています。通常、これらの技術は、ユーザー体験にとっては、スケーンレベルの4Dアセットが必要となります。しかし、現在のディフュージョンモデルは主に静的な3Dスケーンやオブジェクトレベルの動作をモデル化しているため、真に満喫的な体験を提供することができる限界があります。この問題に対処するために、私たちはHoloTimeを提案しています。このフレームワークは、ビデオディフュージョンモデルを統合して、一つのプロンプトまたは参照画像からパノラマビデオを生成し、360度の4Dスケーン再構成手法を組み合わせて、生成されたパノラマビデオを4Dアセットに変換し、ユーザーに完全に満喫的な4D体験を提供することができるものです。特に、ビデオディフュージョンモデルをパノラマビデオの生成に適用するために、私たちは360Worldデータセットを紹介しています。これは、4Dスケーン再構成タスクに適したパノラマビデオの最初の詳細な集まりです。このコレクションをもとに、私たちはPanoramic Animatorを提案しています。これは2ステップの画像からビデオディフュージョンモデルで、パノラマ画像を高品質のパノラマビデオに変換することができるものです。次に、Panoramic Space-Time Reconstructionを紹介しています。これは、スペースタイムの深さ推定手法を使用して、生成されたパノラマビデオを4Dポイントクラスターに変換し、時間的に一致した4Dスケーンの再構成を可能にします。私たちの方法の効果を評価するために、現在の手法との比較分析を行い、パノラマビデオの生成と4Dスケーン再構成の両方で優位性を示しました。これは、我々の方法が、より関心を引き続けるようにして、実感的な満喫的な環境を作成する能力を示し、VRとARアプリケーションのユーザー体験を向上させることを示しています。",
      "upvotes": 1,
      "discussionId": "681b259123dbf6b59c5fc7cf",
      "projectPage": "https://zhouhyocean.github.io/holotime/",
      "githubRepo": "https://github.com/PKU-YuanGroup/HoloTime",
      "ai_keywords": [
        "diffusion models",
        "HoloTime",
        "video diffusion models",
        "panoramic videos",
        "360-degree 4D scene reconstruction",
        "360World dataset",
        "Panoramic Animator",
        "image-to-video diffusion model",
        "Panoramic Space-Time Reconstruction",
        "space-time depth estimation",
        "4D point clouds",
        "Gaussian Splatting",
        "4D scenes",
        "spatially and temporally consistent"
      ]
    },
    "publishedAt": "2025-04-30T09:55:28.000Z",
    "title": "HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene\n  Generation",
    "summary": "The rapid advancement of diffusion models holds the promise of\nrevolutionizing the application of VR and AR technologies, which typically\nrequire scene-level 4D assets for user experience. Nonetheless, existing\ndiffusion models predominantly concentrate on modeling static 3D scenes or\nobject-level dynamics, constraining their capacity to provide truly immersive\nexperiences. To address this issue, we propose HoloTime, a framework that\nintegrates video diffusion models to generate panoramic videos from a single\nprompt or reference image, along with a 360-degree 4D scene reconstruction\nmethod that seamlessly transforms the generated panoramic video into 4D assets,\nenabling a fully immersive 4D experience for users. Specifically, to tame video\ndiffusion models for generating high-fidelity panoramic videos, we introduce\nthe 360World dataset, the first comprehensive collection of panoramic videos\nsuitable for downstream 4D scene reconstruction tasks. With this curated\ndataset, we propose Panoramic Animator, a two-stage image-to-video diffusion\nmodel that can convert panoramic images into high-quality panoramic videos.\nFollowing this, we present Panoramic Space-Time Reconstruction, which leverages\na space-time depth estimation method to transform the generated panoramic\nvideos into 4D point clouds, enabling the optimization of a holistic 4D\nGaussian Splatting representation to reconstruct spatially and temporally\nconsistent 4D scenes. To validate the efficacy of our method, we conducted a\ncomparative analysis with existing approaches, revealing its superiority in\nboth panoramic video generation and 4D scene reconstruction. This demonstrates\nour method's capability to create more engaging and realistic immersive\nenvironments, thereby enhancing user experiences in VR and AR applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.21650.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66eeda3676a8038cb448f11d",
      "avatarUrl": "/avatars/8d6e61f4c9354c6720ccaa7be0fe1d9f.svg",
      "fullname": "Haiyang Zhou",
      "name": "Marblueocean",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.18373",
      "authors": [
        {
          "_id": "681af7a58f41e99d8ca05074",
          "user": {
            "_id": "6454d2ee1a543cf97b1ba671",
            "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
            "isPro": false,
            "fullname": "shen",
            "user": "lorashen",
            "type": "user"
          },
          "name": "Lei Shen",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-07T06:09:12.039Z",
          "hidden": false
        },
        {
          "_id": "681af7a58f41e99d8ca05075",
          "name": "Xiaoyu Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-25T14:17:47.000Z",
      "submittedOnDailyAt": "2025-05-07T04:49:41.738Z",
      "title": "Auto-SLURP: スマートプライベートアシスタントでの多エージェントフレームワークの評価用ベンチマークデータセット",
      "submittedOnDailyBy": {
        "_id": "6454d2ee1a543cf97b1ba671",
        "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
        "isPro": false,
        "fullname": "shen",
        "user": "lorashen",
        "type": "user"
      },
      "summary": "近年、大規模言語モデル（LLMs）を基盤とした多エージェントフレームワークが急速に進歩しています。この進歩の中でも、LLMベースの多エージェントフレームワークの性能評価に特に適したベンチマークデータセットは欠けていました。この空間を埋めるために、我々は、智能なパーソナルアシスタントの背景でLLMベースの多エージェントフレームワークの性能を評価するためのベンチマークデータセット「Auto-SLURP」を紹介します。Auto-SLURPは、自然言語理解タスクに初期的に開発された元のSLURPデータセットを延長し、データの再ラベリングと計算機サーバーと外部サービスの統合を通じて作成されました。この拡張では、言語理解、タスク実行、レスポンス生成の経由全体を評価するための一様的なエンドエンド評価プロセスを実現します。私たちの実験は、現在の最先端のフレームワークに対してもっとも重要な挑戦を象徴し、真実に信頼できるようになる智能な多エージェントパーソナルアシスタントはまだ進行中であることを明らかにします。データセットと関連するコードは、https://github.com/lorashen/Auto-SLURP/ に公開されています。",
      "upvotes": 0,
      "discussionId": "681af7a68f41e99d8ca050ba",
      "githubRepo": "https://github.com/lorashen/Auto-SLURP/",
      "ai_keywords": [
        "multi-agent frameworks",
        "large language models (LLMs)",
        "benchmark datasets",
        "natural language understanding tasks",
        "simulated servers",
        "external services",
        "end-to-end evaluation pipeline",
        "language understanding",
        "task execution",
        "response generation"
      ]
    },
    "publishedAt": "2025-04-25T10:17:47.000Z",
    "title": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant",
    "summary": "In recent years, multi-agent frameworks powered by large language models\n(LLMs) have advanced rapidly. Despite this progress, there is still a notable\nabsence of benchmark datasets specifically tailored to evaluate their\nperformance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset\naimed at evaluating LLM-based multi-agent frameworks in the context of\nintelligent personal assistants. Auto-SLURP extends the original SLURP dataset\n-- initially developed for natural language understanding tasks -- by\nrelabeling the data and integrating simulated servers and external services.\nThis enhancement enables a comprehensive end-to-end evaluation pipeline,\ncovering language understanding, task execution, and response generation. Our\nexperiments demonstrate that Auto-SLURP presents a significant challenge for\ncurrent state-of-the-art frameworks, highlighting that truly reliable and\nintelligent multi-agent personal assistants remain a work in progress. The\ndataset and related code are available at\nhttps://github.com/lorashen/Auto-SLURP/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18373.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6454d2ee1a543cf97b1ba671",
      "avatarUrl": "/avatars/30edc30cf3684bf685aabffcec8a3c30.svg",
      "fullname": "shen",
      "name": "lorashen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]