[
  {
    "paper": {
      "id": "2502.17157",
      "authors": [
        {
          "_id": "67bd3285ac4a596a43b53205",
          "user": {
            "_id": "646efd223dd912a539e0bd46",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
            "isPro": true,
            "fullname": "Canyu Zhao",
            "user": "Canyu",
            "type": "user"
          },
          "name": "Canyu Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:20.829Z",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53206",
          "name": "Mingyu Liu",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53207",
          "user": {
            "_id": "64d60375d7e30889c65e8cf4",
            "avatarUrl": "/avatars/640f7c570fc45194557ce7931bdfe87f.svg",
            "isPro": false,
            "fullname": "Huanyi Zheng",
            "user": "zhyya",
            "type": "user"
          },
          "name": "Huanyi Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:18.731Z",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53208",
          "user": {
            "_id": "632179745fc60c44fd91fc33",
            "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
            "isPro": false,
            "fullname": "zhumuzhi",
            "user": "Z-MU-Z",
            "type": "user"
          },
          "name": "Muzhi Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:11.968Z",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b53209",
          "name": "Zhiyue Zhao",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320a",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320b",
          "name": "Tong He",
          "hidden": false
        },
        {
          "_id": "67bd3285ac4a596a43b5320c",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T13:51:06.000Z",
      "title": "ディセプション：視覚認識タスク向けの一般的なディフォーションモデル",
      "summary": "本記事の主な目標は、計算資源と訓練データの制限を考慮して、複数のタスクを対応できる良い一般的な認識モデルを作成することです。これを達成するために、億数千枚の画像で預けされたテキストから画像への拡散モデルを利用します。我々の検証指標では、DICEPTIONは複数の認識タスクを効果的に対応でき、最先端のモデルと同等の性能を達成していることが示されています。SAM-vit-hのデータの0.06%だけ（例えば、600K画像対して1B画像のピクセルレベルの標識画像）で同じ結果を達成しました。Wang et al.によるインスピレーションを受け、DICEPTIONは複数の認識タスクの出力を色エンコーディングで構成し、異なるインスタンスにランダムな色を割り当てる戦略は実体分割および語義分割にも非常に効果的であることを示しています。複数の認識タスクを条件付き画像生成として統一することで、事前学習されたテキストから画像へのモデルを最大限に活用できます。このように、DICEPTIONは従来のモデルと比べて、さまざまなタスクに適応する際には、それほど多くのデータやパラメータを必要としないことで、効率的に訓練できます。その他のタスクにモデルを適用する場合、50枚の画像だけでファイナルチューニングが必要となります。DICEPTIONは、視覚的な一般的なモデルにおける有効なインサイトとより望ましい解決策を提供します。",
      "upvotes": 35,
      "discussionId": "67bd328aac4a596a43b532ae"
    },
    "publishedAt": "2025-02-24T22:39:29.837Z",
    "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17157.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "646efd223dd912a539e0bd46",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
      "fullname": "Canyu Zhao",
      "name": "Canyu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17129",
      "authors": [
        {
          "_id": "67bd37cb0d41e01cca99aa8b",
          "user": {
            "_id": "64f033ef82c6eea604c4da8b",
            "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
            "isPro": false,
            "fullname": "Liu Xiaoran",
            "user": "LiuXR",
            "type": "user"
          },
          "name": "Xiaoran Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:07.298Z",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8c",
          "name": "Ruixiao Li",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8d",
          "name": "Mianqiu Huang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8e",
          "name": "Zhigeng Liu",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa8f",
          "name": "Yuerong Song",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa90",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa91",
          "name": "Siyang He",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa92",
          "name": "Qiqi Wang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa93",
          "name": "Linlin Li",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa94",
          "name": "Qun Liu",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa95",
          "name": "Yaqian Zhou",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa96",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "67bd37cb0d41e01cca99aa97",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T13:19:33.000Z",
      "title": "このテキストは日本語に翻訳されますが、タイトル部分は日本語ではないので、それを変更します。\n\n**タイトル:** 長文長大言語モデルの言葉\n\n**本文:**\n\n長文長大言語モデルは、長文を読み込んで理解することができるように設計されています。このモデルは、長い文脈を考慮してより正確な回答を提供することができます。この能力は、多くの応用において重要であり、テキスト分析、翻訳、システムの設計などの領域で活用されています。",
      "summary": "長文脈は、自然言語処理（NLP）の重要な課題であり、NLPアーキテクチャの開発を通じて構成されており、大規模言語モデル（LLMs）にとっては、人類の生涯学習の可能性を持つ巨大な機会を提供しています。不幸ながら、長文脈の追求は複数の障壁を伴います。しかし、長文脈はLLMsの核心的な競争優位であることに残されています。過去の2年間、LLMsの文脈長は数百万トークンに達して突破的な拡大を遂げました。また、長文脈のLLMsの研究は、長さの外挿にかけられてきたものではなく、アーキテクチャ、インフラ、トレーニング、評価技術の全面的な焦点に移りました。\n\nサマーフィールドの交響詩「そうは言ったザラテストラ」をモチーフに、LLMの文脈拡大の旅と人類が彼の死後性を超えようとする努力との類似性を引き出します。この調査では、LLMが長文脈のための巨大な必要と、最終的に有限であることを受け入れる必要との衝突を説明します。これを達成するために、アーキテクチャ、インフラ、トレーニング、評価の4つの観点から長文脈LLMsの生命周期の全体像を提供し、長文脈技術の全貌を展示します。この調査の最終的には、現在の長文脈LLMsに直面する10つの未解決の質問を提示します。この調査は、長文脈LLMsの研究についてのシステム的な介紹として役立つことを望む。",
      "upvotes": 25,
      "discussionId": "67bd37cc0d41e01cca99ab1e"
    },
    "publishedAt": "2025-02-24T22:27:11.566Z",
    "title": "Thus Spake Long-Context Large Language Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f033ef82c6eea604c4da8b",
      "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
      "fullname": "Liu Xiaoran",
      "name": "LiuXR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15814",
      "authors": [
        {
          "_id": "67bd3972f077ddf1f98bacda",
          "user": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "isPro": false,
            "fullname": "Gallil Maimon",
            "user": "gallilmaimon",
            "type": "user"
          },
          "name": "Gallil Maimon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:36.258Z",
          "hidden": false
        },
        {
          "_id": "67bd3972f077ddf1f98bacdb",
          "user": {
            "_id": "644662145004f2cb3af08b27",
            "avatarUrl": "/avatars/5f2af24c7410a5db46374d0b84fb479d.svg",
            "isPro": false,
            "fullname": "Avishai Elmakies",
            "user": "avishai-elmakies",
            "type": "user"
          },
          "name": "Avishai Elmakies",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:33.712Z",
          "hidden": false
        },
        {
          "_id": "67bd3972f077ddf1f98bacdc",
          "name": "Yossi Adi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T17:21:15.000Z",
      "title": "スラム：1日間1カーネーションで言語モデルを学習する",
      "summary": "私たちは、Slamを紹介します。Slamは、単一の学術用GPU上で24時間以内に高品質の言語モデル（SLMs）を訓練するためのレシピです。これを実験的に行うために、モデルの初期化とアーキテクチャ、合成データ、合成データを用いた好み最適化、そして他のすべてのコンポーネントの調整を行います。実験的に示しましたが、この訓練レシピは、より多くの計算量を使用してもスケーリングでき、先進的なSLMsと同等の結果を得るための計算コストの少ない方法です。これらのインサイトを提供して、SLMの訓練および研究がより容易になることを望んでいます。SLMのスケーリング法則の背景で、我々の結果は予測された計算最適性能を大幅に超え、SLMの可能性においてもより諧和的な観点を与えます。コード、データ、モデル、サンプルは以下のURLから参照できます：https://pages.cs.huji.ac.il/adiyoss-lab/slamming。",
      "upvotes": 19,
      "discussionId": "67bd3973f077ddf1f98bacf9"
    },
    "publishedAt": "2025-02-24T23:14:12.363Z",
    "title": "Slamming: Training a Speech Language Model on One GPU in a Day",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/t93GkoiYRplnXH1Go0MmY.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15814.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b9bc2dacdbc1d0b39c3b50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
      "fullname": "Gallil Maimon",
      "name": "gallilmaimon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16584",
      "authors": [
        {
          "_id": "67bd42386959e61abd265a9b",
          "name": "Liumeng Xue",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9c",
          "name": "Ziya Zhou",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9d",
          "name": "Jiahao Pan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9e",
          "name": "Zixuan Li",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265a9f",
          "name": "Shuai Fan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa0",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa1",
          "name": "Sitong Cheng",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa2",
          "name": "Dongchao Yang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa3",
          "name": "Haohan Guo",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa4",
          "name": "Yujia Xiao",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa5",
          "name": "Xinsheng Wang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa6",
          "name": "Zixuan Shen",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa7",
          "name": "Chuanbo Zhu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa8",
          "name": "Xinshen Zhang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aa9",
          "name": "Tianchi Liu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aaa",
          "name": "Ruibin Yuan",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aab",
          "name": "Zeyue Tian",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aac",
          "name": "Haohe Liu",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aad",
          "name": "Emmanouil Benetos",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aae",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265aaf",
          "name": "Yike Guo",
          "hidden": false
        },
        {
          "_id": "67bd42386959e61abd265ab0",
          "name": "Wei Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T14:24:15.000Z",
      "title": "Audio-FLAN: プレリミューラルリリース",
      "summary": "最近の音声トークナイゼーションの進歩は、大規模言語モデル（LLMs）における音声機能の統合を大幅に向上させました。しかし、音声理解と生成は通常に異なるタスクとして扱われ、真の統合された音声言語モデルの開発に軽かくなります。インストラクションチューニングは、文と画像の間での一般化とゼロショット学習において驚異的な成功を示しましたが、音声に対する応用はまだ大きく探索されていません。主な障害は、音声理解と生成を統合するための詳細なデータセットの欠如です。これに対して、Audio-FLANという大規模なインストラクションチューニングデータセットを紹介します。Audio-FLANは、80種類の多様なタスクを収め、音声、音楽、音の領域にわたり、100万以上のインスタンスを含みます。Audio-FLANは、音声理解（例：翻訳、理解）と生成（例：音声、音楽、音）の両方をセミアナスに処理できる統合された音声言語モデルの基盤を築きます。Audio-FLANデータセットは、HuggingFaceとGitHubで提供され、定期的に更新されます。",
      "upvotes": 18,
      "discussionId": "67bd423b6959e61abd265b88"
    },
    "publishedAt": "2025-02-24T23:14:20.487Z",
    "title": "Audio-FLAN: A Preliminary Release",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fd6f670053c8345eddc1b68",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd6f670053c8345eddc1b68/cuTsu2krRYHC6zYGD2dpQ.jpeg",
      "fullname": "Ruibin Yuan",
      "name": "a43992899",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17435",
      "authors": [
        {
          "_id": "67bd6b4b8edd1ce8ad5603a0",
          "name": "Chen-Wei Chang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a1",
          "name": "Cheng-De Fan",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a2",
          "name": "Chia-Che Chang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a3",
          "name": "Yi-Chen Lo",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a4",
          "name": "Yu-Chee Tseng",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a5",
          "name": "Jiun-Long Huang",
          "hidden": false
        },
        {
          "_id": "67bd6b4b8edd1ce8ad5603a6",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:59:54.000Z",
      "title": "GCC: カラーチェッカーを拡散して生成的な色の正規化",
      "summary": "色調恒常性法は、様々なカメラセンサーの間で一般化することが難しいことを多くの場合に認識しています。我々は、ディフュージョンモデルを活用したGCC（Global Color Constancy）を紹介します。GCCは、照明計測に向けて画像に色チェッカーを補間することで、色調恒常性を実現します。我々の主な革新的点は、(1) 場景の照明を反映する色チェッカーの補間を行う一ステップで確定的な推論アプローチ、(2) チェッカーの構造を保持しながら照明依存性の色調適応を可能にするラプラシアン分解手法、(3) 不正確な色チェッカーアノテーションを対処するマスクベースのデータ拡張戦略です。GCCは、クロスカメラシナリオで最も優れた強固性を示し、バイデリクション評価で最悪の25%の誤差率が5.15°と4.32°であることを示します。これらの結果は、カメラの特徴を考慮しなくても、センサー特にトレーニングを必要としないことを示し、実世界的なアプリケーションに対しての広範囲的な解決策としての適応性を示しています。",
      "upvotes": 16,
      "discussionId": "67bd6b4d8edd1ce8ad560401"
    },
    "publishedAt": "2025-02-25T02:06:00.809Z",
    "title": "GCC: Generative Color Constancy via Diffusing a Color Checker",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/gDAYQUcbNE2Ps2pQFxg_m.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17435.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6459d5da3b6fafd9664807ab",
      "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
      "fullname": "Yu-Lun Liu",
      "name": "yulunliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16614",
      "authors": [
        {
          "_id": "67bd36334a9a04b9ca9bbb68",
          "name": "Alexander Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb69",
          "name": "Marcus Dong",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6a",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6b",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6c",
          "name": "Yejie Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6d",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6e",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb6f",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb70",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb71",
          "name": "Yingshui Tan",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb72",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb73",
          "name": "Zhexu Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb74",
          "name": "Weixun Wang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb75",
          "name": "Yancheng He",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb76",
          "name": "Ken Deng",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb77",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb78",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67bd36334a9a04b9ca9bbb79",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T15:36:43.000Z",
      "title": "CodeCriticBench: 大語言モデルの全体的なコード評価ベンチマーク",
      "summary": "LLMsの批判能力は、理由能力を提供するために重要であり、必要な提案（例：詳細な分析と実用的なフィードバック）を提供することができます。そこで、LLMsの批判能力を評価する方法は、大きな注目を集め、数々の批判ベンチマークが提案されています。しかし、現在の批判ベンチマークは以下のような制限があります： (1) 一般領域での多様な理由タスクを焦点とし、コードタスクにおいて評価が不足しています（例：コード生成タスクだけを対象としている）。これらのタスクの難易度は相対的に簡単です（例：CriticBenchのコードクエリはHumanevalとMBPPから）。 (2) 複数の次元からの詳細な評価が欠けています。これらの制限に対処するために、我々はLLMsの批判能力を評価するためのハードルベースベンチマークを介して、CodeCriticBenchという名前で紹介します。特に、我々のCodeCriticBenchは、違う難易度の2つの主流コードタスク（コード生成とコードQA）を含み、評価プロトコルは、基本的な批判評価と進歩的な批判評価を含み、進歩的な設定に適した細分化評価チェックリストを設計しています。最終的に、我々は現在のLLMsの様々な実験結果を実施し、CodeCriticBenchの効果性を示しています。",
      "upvotes": 13,
      "discussionId": "67bd36354a9a04b9ca9bbc16"
    },
    "publishedAt": "2025-02-24T22:17:28.937Z",
    "title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65377c30e48353201e6fdda0",
      "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
      "fullname": "Jiaheng Liu",
      "name": "CheeryLJH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16033",
      "authors": [
        {
          "_id": "67bd31d0d055a27740b16a30",
          "name": "Qianqi Yan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a31",
          "name": "Yue Fan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a32",
          "name": "Hongquan Li",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a33",
          "name": "Shan Jiang",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a34",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a35",
          "name": "Xinze Guan",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a36",
          "name": "Ching-Chen Kuo",
          "hidden": false
        },
        {
          "_id": "67bd31d0d055a27740b16a37",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-22T01:52:37.000Z",
      "title": "多タイプ不適切性推理（MMIR）：多タイプ推理モデルの新たなベンチマーク",
      "summary": "現在の多モデル大語言モデル（MLLM）は主に一貫した画像-文字入力を用いて訓練され、検証されていますが、実世界的な、ラウプーム豊富な内容での不適切性を処理できるかどうかの問題が残されています。この隙間を埋めるために、私たちは、MLLMのセマンティックの不適切性を検出し、理由を与える能力を評価するためのMultimodal Inconsistency Reasoning（MMIR）ベンチマークを提案します。MMIRは534の難しいサンプルを含み、Factual Contradiction、Identity Misattribution、Contextual Mismatch、Quantitative Discrepancy、Temporal/Spatial Incoherenceの5つの理由的な領域における合成的に挿入されたエラーを含みます。私たちは6つの最先端のMLLMを評価し、多モデル理由能力を持つモデル（例えばo1）が、そのコンテラプライズモデルに比べて大幅に優れていることを示しますが、開源モデルは特に不適切性エラーに脆弱です。詳細なエラー分析は、セマンティックの不適切性を検出できることを特にテキストではなく、クロスモデルの衝突と複雑なラウプームに対しては困難を感じることを示します。調査実験は、Chain-of-Thought（CoT）やSet-of-Mark（SoM）メソッドを含む単一モデルのプロンプティングでは微視的な効果だけを示し、クロスモデル理由のキーバックロックを示します。我々の発見は、進捗的な多モデル理由の必要性を示し、将来の研究の方向を多モデルの不適切性に向けて示します。",
      "upvotes": 11,
      "discussionId": "67bd31d2d055a27740b16ad9"
    },
    "publishedAt": "2025-02-24T21:59:50.456Z",
    "title": "Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16033.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16894",
      "authors": [
        {
          "_id": "67bd396ea06bae99f3866911",
          "user": {
            "_id": "641aa5e391e3376a057bbd4c",
            "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
            "isPro": false,
            "fullname": "Chenghao Fan",
            "user": "Facico",
            "type": "user"
          },
          "name": "Chenghao Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:38.942Z",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866912",
          "name": "Zhenyi Lu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866913",
          "name": "Sichen Liu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866914",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866915",
          "name": "Wei Wei",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866916",
          "name": "Chengfeng Gu",
          "hidden": false
        },
        {
          "_id": "67bd396ea06bae99f3866917",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T06:48:13.000Z",
      "title": "LoRAを再び強化する：適応的な固有値とエキスパートの混雑最適化アライメントを用いたLoRAの向上",
      "summary": "Low-Rank Adaptation (LoRA)は、Large Language Models (LLMs)に対してパラメーター効率的な微調節を可能にしますが、その性能は通常Full Fine-Tuning (Full FT)に比べて低いです。現在の方法は、静的単位分解（SVD）のサブセットで初期化してLoRAを最適化していますが、これは事前学習された知識の最適な利用につながりません。LoRAを改善するさらなる方法として、Mixture-of-Experts (MoE)アーキテクチャを採用することが考えられますが、重みの不適切な位置付けと複雑な勾配動力学がSVDをMoEアーキテクチャに導入することにつながりません。これらの問題を解決するために、私たちはGreat LoRA Mixture-of-Expert (GOAT)フレームワークを提案しています。このフレームワークは、(1) SVD構造化されたMoEを用いて適応的に関連する先驅を統合し、(2) 理論的なスケーリング因子を計算してFull Fine-Tuned MoEとの最適化を対応させます。この方法では、アーキテクチャやトレーニングアルゴリズムを変更しないでもLoRA MoEの効率と性能を向上させることができます。自然言語理解、一般知識推理、画像分類、自然言語生成の25データセットでの実験は、GOATの最先端の性能を示し、Full FTとの間の隙間を狭めました。",
      "upvotes": 10,
      "discussionId": "67bd396fa06bae99f3866964"
    },
    "publishedAt": "2025-02-24T22:35:41.042Z",
    "title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16894.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641aa5e391e3376a057bbd4c",
      "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg",
      "fullname": "Chenghao Fan",
      "name": "Facico",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17407",
      "authors": [
        {
          "_id": "67bd48d4becb766415a5d19d",
          "name": "Guijin Son",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d19e",
          "name": "Jiwoo Hong",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d19f",
          "user": {
            "_id": "63e087b6a98d931aa90c1b9c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e087b6a98d931aa90c1b9c/96c6IT3f1pWGLbRdRDB2U.png",
            "isPro": false,
            "fullname": "Hyunwoo Ko",
            "user": "Cartinoe5930",
            "type": "user"
          },
          "name": "Hyunwoo Ko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:12.933Z",
          "hidden": false
        },
        {
          "_id": "67bd48d4becb766415a5d1a0",
          "name": "James Thorne",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:36:15.000Z",
      "title": "測定時のスケーリングの言語学一般化性について",
      "summary": "スケーリング予習計算は、多言語性の達成に効果的であることが証明されていますが、テスト時スケーリングにおいては同様の効果があるかどうかは不明です。本論文では、MCLM（Multilingual Math Benchmark）を紹介します。MCLMは55言語でのコンペティションレベルの問題を扱う多言語バージョンです。Qwen2.5-1.5B MathとMR1-1.5Bにおいて、Outcome Reward Modeling（ORM）、Process Reward Modeling（ORM）、Budget Forcing（BF）の3つのテスト時スケーリング方法を検証しました。Qwen2.5-1.5B MathとORMを使用することでMCLMでのスコアは35.8点に達し、BFをMR1-1.5Bに適用することでは35.2点に達しました。「思考LLMs」は最近注目を集めていますが、推論FLOPsの同じレベルに制限された場合、その性能は最良のNを選ぶような伝統的なスケーリング方法と比較しても比較的です。また、BFは英語のAIMEで20点の向上を示しましたが、その他の言語では平均1.94点の向上しか示しません。このパターンは他のテスト時スケーリング方法においても同様で、テスト時スケーリングは多言語タスクに対してはより効果的に拡がりません。進展のために、MCLM、MR1-1.5B、および評価結果を公開します。",
      "upvotes": 9,
      "discussionId": "67bd48d5becb766415a5d1e9"
    },
    "publishedAt": "2025-02-24T23:37:53.138Z",
    "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17407.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d3e619b8448e1785bbda2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg",
      "fullname": "GUIJIN SON",
      "name": "amphora",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17110",
      "authors": [
        {
          "_id": "67bd3936daef22cbce6d7ef2",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef3",
          "user": {
            "_id": "645b10e80c73ea27d13f7aca",
            "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
            "isPro": false,
            "fullname": "xuhaiyang",
            "user": "xhyandwyy",
            "type": "user"
          },
          "name": "Haiyang Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:41.528Z",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef4",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef5",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef6",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef7",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67bd3936daef22cbce6d7ef8",
          "name": "Jitao Sang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T12:51:23.000Z",
      "title": "Mobile-Agent-V: ビデオガイドロードでの多Agent協調によるモバイルデバイスの操作学習",
      "summary": "モバイルデバイスの使用量の急激な増加に伴い、無間断なタスク管理のための改善された自動化が必要となります。しかし、多くのAI駆動フレームワークは、操作知識の不足により困難を見出しています。手動で書かれた知識は役立ちますが、労力費用が高く、効率が悪いです。これらの課題に対処するために、Mobile-Agent-Vというフレームワークを紹介します。このフレームワークは、ビデオガイドを利用して、モバイル自動化に向けて豊富なそしてコスト効率的な操作知識を提供します。Mobile-Agent-Vは、特訓や前処理が不要で、ビデオ入力を利用してタスク実行能力を向上させます。Mobile-Agent-Vは、スライディングウィンドウ戦略を採用し、ビデオアグエントと深いリフレクションアグエントを組み合わせて、アクションがユーザーの指示に合わせるようにします。この革新的なアプローチにより、ユーザーはガイドされてタスクプロセスを記録でき、システムが自動的に学習し、効率的にタスクを実行することができます。実験結果から、Mobile-Agent-Vは現在のフレームワークと比較して30%の性能向上を達成しています。",
      "upvotes": 8,
      "discussionId": "67bd3938daef22cbce6d7f9d"
    },
    "publishedAt": "2025-02-24T22:31:17.771Z",
    "title": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/mshxtP77rrnN07f6ux6_0.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17110.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16922",
      "authors": [
        {
          "_id": "67bd3d6b60186d7478467208",
          "user": {
            "_id": "6643261b8876db14227eeb19",
            "avatarUrl": "/avatars/67428c9e37a2273697c0547e1783ec6b.svg",
            "isPro": false,
            "fullname": "Zhenglin Wang",
            "user": "wzl0228",
            "type": "user"
          },
          "name": "Zhenglin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:15.633Z",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d7478467209",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720a",
          "name": "Pengfei LI",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720b",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "67bd3d6b60186d747846720c",
          "name": "Deyu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T07:27:54.000Z",
      "title": "中国歴代の時系列理由とアライメントのベンチマーク",
      "summary": "時間推論は人類の認知の基盤であり、多様な実世界のアプリケーションにおいて重要である。最近の大規模言語モデルの進展は、時間推論における有望な能力を示したが、現在のベンチマークは主にルールベースの構築に依存し、コンテキストの深さが不足し、時間的なエンティティの範囲が限定されている。これらの制限を解決するために、私たちは中国時間推論（CTM）を紹介します。CTMは、中国の時代順序の広い範囲での時間推論を評価するために設計されたベンチマークです。CTMは、クロスエンティティ関係、パースウィスエンド時間のアラインメント、コンテキスト化されたおよび文化的に基づいた推論を強調し、詳細な評価を提供します。拡大した実験結果は、CTMによる課題を明らかにし、改善の可能性のある道を示しています。",
      "upvotes": 7,
      "discussionId": "67bd3d6c60186d7478467249"
    },
    "publishedAt": "2025-02-24T22:48:30.357Z",
    "title": "Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16922.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15894",
      "authors": [
        {
          "_id": "67bd3bd26faf9f04b2170f61",
          "name": "Min Zhao",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f62",
          "name": "Guande He",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f63",
          "name": "Yixiao Chen",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f64",
          "user": {
            "_id": "64c269a52d73768f07ac266c",
            "avatarUrl": "/avatars/d497a960f8aef6a974907b68ed750c1c.svg",
            "isPro": false,
            "fullname": "Zhu Hongzhou",
            "user": "zhuhz22",
            "type": "user"
          },
          "name": "Hongzhou Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:23.502Z",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f65",
          "name": "Chongxuan Li",
          "hidden": false
        },
        {
          "_id": "67bd3bd26faf9f04b2170f66",
          "name": "Jun Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T19:28:05.000Z",
      "title": "RIFLEx: 映像ディフュージョンでの長さ推計の無料ランチ\nTransformers",
      "summary": "最近の映像生成の進歩は、モデルが高品質の1分間の映像を合成することができるようになりました。しかし、長い映像を生成し、時系列的な連続性を維持することは大きな課題で、現在の長さ推計法は時系列的な再現または動きの減速による問題を生じます。本論文では、位置付け埋めの頻率成分の役割をシステマティックに分析し、長さ推計行為を主に制御する固有の頻率を特定しました。この洞察に基づき、RIFLExという最小でありもともとの効果的なアプローチを提案しました。RIFLExは、再現を抑制しながら動きの一貫性を維持するために固有の頻率を低減し、追加の変更が必要とならないものです。RIFLExは、最新の映像ディフュージョントランスフォーマーで最高品質の2倍長さ推計を完全な訓練無制限で実現します。また、最小限の微調節で3倍長さ推計を可能にし、長い映像も不要です。プロジェクトページとコードは以下のURLにあります。",
      "upvotes": 6,
      "discussionId": "67bd3bd66faf9f04b21710d1"
    },
    "publishedAt": "2025-02-25T00:09:04.483Z",
    "title": "RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15894.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6205
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16707",
      "authors": [
        {
          "_id": "67bd3bcc797e4d53ce0bc70d",
          "user": {
            "_id": "64f8fbd95515d7dcceb906b1",
            "avatarUrl": "/avatars/1c7d034de408930b166592465e65fc31.svg",
            "isPro": false,
            "fullname": "Yunhai Feng",
            "user": "yunhaif",
            "type": "user"
          },
          "name": "Yunhai Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:31.085Z",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc70e",
          "user": {
            "_id": "62318c0386753f5f41d0e261",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
            "isPro": false,
            "fullname": "Jiaming Han",
            "user": "csuhan",
            "type": "user"
          },
          "name": "Jiaming Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:39:28.772Z",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc70f",
          "name": "Zhuoran Yang",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc710",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc711",
          "name": "Sergey Levine",
          "hidden": false
        },
        {
          "_id": "67bd3bcc797e4d53ce0bc712",
          "name": "Jianlan Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T20:42:15.000Z",
      "title": "リフレクシブルプランニング：ビジョン-言語モデルを用いた多段階長期ロボット操作",
      "summary": "解決複雑な長期ロボット操作問題には、高度な計画能力、物理世界の理解能力、そして長期予測の誤差累積問題を解決するための長期予測能力が必要です。ビジョン言語モデル（VLMs）は、インターネットデータにプレトレーンされていることにより、これらの問題を解決するフレームワークを提供することが理論的に可能です。しかし、現在の形では、VLMsは、機械手操作に必要な複雑な物理の説明の複雑な理解を持っていないことや、長期予測の誤差累積問題を解決するための長期予測能力を持っていません。本論文では、VLMsの物理的な説明能力を強化するための新しいテスト時計算フレームワークを介して、多段階操作タスクを解決することを報告します。その核心は、「反射」機構を用いて、学習済みVLMをイテレーティブに改善するアプローチです。これは、将来の世界状態を想像する生成モデルを使用し、これらの予測を動作選択にガイドし、潜在的な不適切性について批判的に説明を改善します。実験結果は、私たちの方法が数々の最先端の商業VLMsやモンテカルロ木探索（MCTS）などの後学習アプローチを大幅に超えることを示しています。ビデオは、https://reflect-vlm.github.io から利用できます。",
      "upvotes": 5,
      "discussionId": "67bd3bcf797e4d53ce0bc7ff"
    },
    "publishedAt": "2025-02-25T01:02:05.395Z",
    "title": "Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16707.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f8cb8ed04a890f5380d9a4",
      "avatarUrl": "/avatars/d6fdfdbb0c10141aa3b4c832d928121b.svg",
      "fullname": "Jianlan Luo",
      "name": "jianlanluo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15987",
      "authors": [
        {
          "_id": "67bd46ea3e090b402d70f1f4",
          "user": {
            "_id": "64dfbcb18e2084e1d7b51b46",
            "avatarUrl": "/avatars/fafe30beea2d7e8eec3f3ba985c582f7.svg",
            "isPro": false,
            "fullname": "Kushal Raj Bhandari",
            "user": "KBhandari11",
            "type": "user"
          },
          "name": "Kushal Raj Bhandari",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-25T04:30:32.676Z",
          "hidden": false
        },
        {
          "_id": "67bd46ea3e090b402d70f1f5",
          "name": "Pin-Yu Chen",
          "hidden": false
        },
        {
          "_id": "67bd46ea3e090b402d70f1f6",
          "name": "Jianxi Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T22:52:19.000Z",
      "title": "Hugging FaceのOpen-Weight AIモデルの成長予測",
      "summary": "開放重みのAIランドスケープが進化し続け、モデル開発、大規模な投資およびユーザーの興味が増加している中、モデルが最終的にイノベーションを駆動し、AIエコシステムを形成するものを予測することが重要になります。科学文献の引用動態との類似性を基に、開放重みモデルの影響がどのように進化するかを定量化するためのフレームワークを提案します。特に、Wang et al.が導入した科学引用のモデルを適用し、開放重みモデルの微調節モデルの累積数を追跡するための3つの要因— immediacy、longevity、relative fitness—を使用します。我々の調査結果から、引用ごとのアプローチは開放重みモデルの採用の多様な軌跡を有効に捉え、モデルの多くはこの軌跡にフィットし、アウタージャーは特徴的なパターンや使用量の急激な跳躍を示します。",
      "upvotes": 4,
      "discussionId": "67bd46ee3e090b402d70f317"
    },
    "publishedAt": "2025-02-24T23:30:36.556Z",
    "title": "Forecasting Open-Weight AI Model Growth on Hugging Face",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e67bdd61009063689407479/kQHArNjaT0CM1KCujtDc1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15987.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "5e67bdd61009063689407479",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857146757-5e67bdd61009063689407479.jpeg",
      "fullname": "Clem 🤗",
      "name": "clem",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isMod": false,
      "followerCount": 2052
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16701",
      "authors": [
        {
          "_id": "67bd31d6bf6d46017e515a58",
          "user": {
            "_id": "62543749b777cd32720675c2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658760912583-62543749b777cd32720675c2.jpeg",
            "isPro": false,
            "fullname": "Irene Solaiman",
            "user": "irenesolaiman",
            "type": "user"
          },
          "name": "Irene Solaiman",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-25T03:43:21.348Z",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a59",
          "name": "Rishi Bommasani",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5a",
          "name": "Dan Hendrycks",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5b",
          "name": "Ariel Herbert-Voss",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5c",
          "name": "Yacine Jernite",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5d",
          "name": "Aviya Skowron",
          "hidden": false
        },
        {
          "_id": "67bd31d6bf6d46017e515a5e",
          "name": "Andrew Trask",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-23T20:06:12.000Z",
      "title": "Releaseよりも遠くに：生成AIシステムのアクセス検討",
      "summary": "生成型AIのリリース決定は、システムのコンポーネントが利用可能になるかどうかを決定するが、リリースは、ユーザーと利益関係者がシステムとどのように取り組むことができるように変化した複数の要素を対処しない。リリースよりも、システムのコンポーネントにアクセスが与えられることは、潜在的リスクと利益に関する情報を提供する。アクセスは、コンポーネントの利用に必要な実用的な要望、インフラ構造的、技術的、ソシエティーズ的な条件を含む。アクセスを3つの軸で分解しています：リソース、技術的な利用可能度、有用性。各カテゴリ内では、システムのコンポーネントに対して設定された一連の変数が、補損を説明する。例えば、リソースはモデルの重みをサーバーするための計算インフラにアクセスする必要がある。また、4つの高性能の言語モデルのアクセス性を比較し、2つの開放ウェイトモデルと2つのクローズドウェイトモデルを含むもので、すべてのモデルに対して同様の考慮をしていることを示しています。アクセス変数は、ユーザーにアクセスを拡大または増加させることができるような構造を作成するための基盤となり、アクセスのスケールとそのスケールがリスクの管理とリスクの対処にどのような影響を与えるかを調査する。このフレームワークは、システムのリリースのリスクと利益の補損をよりよく理解することを目的として、システムのリリース決定、研究、および政策に情報を提供します。",
      "upvotes": 4,
      "discussionId": "67bd31d7bf6d46017e515a7e"
    },
    "publishedAt": "2025-02-24T21:59:15.571Z",
    "title": "Beyond Release: Access Considerations for Generative AI Systems",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62543749b777cd32720675c2/LwZmJUoXiJriC_c1DZ7qM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16701.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62543749b777cd32720675c2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658760912583-62543749b777cd32720675c2.jpeg",
      "fullname": "Irene Solaiman",
      "name": "irenesolaiman",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 79
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17258",
      "authors": [
        {
          "_id": "67bd515c0417e7f92283d3b8",
          "name": "Xiangpeng Yang",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3b9",
          "name": "Linchao Zhu",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3ba",
          "name": "Hehe Fan",
          "hidden": false
        },
        {
          "_id": "67bd515c0417e7f92283d3bb",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T15:39:14.000Z",
      "title": "VideoGrain: 時間と空間のアテンションを調節した多粒度ビデオ編集",
      "summary": "最近のディフェーションモデルの進歩は、映画の生成と編集能力を大幅に向上させました。しかし、クラスレベル、インスタンスレベル、パートレベルの改変を含む多グリーンド映画編集は、難しい挑戦です。多グリーンド編集の主な難点は、文字から領域への制御の語意的な調整と、ディフェーションモデル内の特徴量の耦合です。これらの難点を解決するために、私たちはVideoGrainを提案します。VideoGrainは、空間時間（交差および自己）注意機構を調節し、映画内容に細かな制御を実現する零ショットアプローチです。私たちは、文字から領域への制御を強化し、交差注意での無関係領域との相互作用を最小化します。また、自己注意での領域間の干渉を減少し、領域内の認識を強化します。広範囲の実験は、私たちの方法が実世界的なシナリオで最先端の性能を達成していることを示します。私たちのコード、データ、デモは、https://knightyxp.github.io/VideoGrain_project_page/ に公開されています。",
      "upvotes": 3,
      "discussionId": "67bd51620417e7f92283d4e9"
    },
    "publishedAt": "2025-02-25T00:13:12.214Z",
    "title": "VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17258.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6205
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14132",
      "authors": [
        {
          "_id": "67b86819d00e69f10c1f31b9",
          "user": {
            "_id": "6231d3ce86753f5f41d39c6f",
            "avatarUrl": "/avatars/9b18f368e5f80cfc935b2e339d42a85f.svg",
            "isPro": false,
            "fullname": "Nadav Borenstein",
            "user": "Nadav",
            "type": "user"
          },
          "name": "Nadav Borenstein",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-25T09:40:52.278Z",
          "hidden": false
        },
        {
          "_id": "67b86819d00e69f10c1f31ba",
          "user": {
            "_id": "6698cffdb2ebada9f4a7e7d7",
            "avatarUrl": "/avatars/e66d946c14595d3b008185f2be8d2f57.svg",
            "isPro": false,
            "fullname": "Greta Warren",
            "user": "gretawarren",
            "type": "user"
          },
          "name": "Greta Warren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T14:42:45.791Z",
          "hidden": false
        },
        {
          "_id": "67b86819d00e69f10c1f31bb",
          "name": "Desmond Elliott",
          "hidden": false
        },
        {
          "_id": "67b86819d00e69f10c1f31bc",
          "name": "Isabelle Augenstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T22:26:39.000Z",
      "title": "コミュニティのノートが専門的な事実検証者を置き換えることができるか？",
      "summary": "ソーシャルメディアで誤信頼の増加を防ぐ通常に使用される2つの戦略は、(i) 専門的な組織が行う事実チェックと、(ii) 平台ユーザが行うコミュニティモデレーションです。Twitter/XとMetaの政策変更は、事実チェック組織とのパートナーシップから、コミュニティノートのコラボレーションに向けた依存関係の増加に向けて移行しています。しかし、事実チェックとコミュニティノートの間の依存関係の程度および性質は明確ではありません。これらの質問を解決するために、言語モデルを使用して、トピック、引用されたソース、ノートが広範囲的な誤信頼の説明に関連付けられた主張を否定するかどうかなどの属性をタイプ化したTwitter/Xの大規模なコーパスを記録しました。分析結果から、コミュニティノートが事実チェックソースを引用した回数は、以前の報告よりも5倍以上になっています。事実チェックは、広範囲的な説明に関連付けられたポストのノートにおいて特に重要で、他のソースよりも2倍以上の確率で事実チェックソースを参照しています。結論として、我々の結果は、成功したコミュニティモデレーションが専門的な事実チェックにより強く依存していることを示しています。",
      "upvotes": 2,
      "discussionId": "67b8681bd00e69f10c1f3267"
    },
    "publishedAt": "2025-02-25T04:11:18.915Z",
    "title": "Can Community Notes Replace Professional Fact-Checkers?",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6231d3ce86753f5f41d39c6f/CwWaf1c9-jOzJ-gD5lvCH.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/6231d3ce86753f5f41d39c6f/WrrBClUkuDsXHcfxP_N8B.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14132.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6231d3ce86753f5f41d39c6f",
      "avatarUrl": "/avatars/9b18f368e5f80cfc935b2e339d42a85f.svg",
      "fullname": "Nadav Borenstein",
      "name": "Nadav",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15122",
      "authors": [
        {
          "_id": "67bbd6d5ba0bb31293e11210",
          "user": {
            "_id": "675f68e3074ff89c5c078bf3",
            "avatarUrl": "/avatars/e3b78d90f032659d411761f47c3cf43e.svg",
            "isPro": false,
            "fullname": "Angus",
            "user": "angus924",
            "type": "user"
          },
          "name": "Angus Dempster",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-24T02:18:57.914Z",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11211",
          "name": "Navid Mohammadi Foumani",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11212",
          "name": "Chang Wei Tan",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11213",
          "name": "Lynn Miller",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11214",
          "name": "Amish Mishra",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11215",
          "name": "Mahsa Salehi",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11216",
          "name": "Charlotte Pelletier",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11217",
          "name": "Daniel F. Schmidt",
          "hidden": false
        },
        {
          "_id": "67bbd6d5ba0bb31293e11218",
          "name": "Geoffrey I. Webb",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T00:54:40.000Z",
      "title": "モンスター: モナススケーラブルなタイムシリーズ評価リポジトリ",
      "summary": "モンスター（MONSTER）-MONASHスカラブルタイムシリーズ評価リポジトリ-、時系列分類のための大規模データセットのコレクションを紹介します。時系列分類分野は、UCRとUEAの時系列分類リポジトリによって設定された共通ベンチマークのもとでフィードバックを受けています。しかし、これらのベンチマークに含まれるデータセットは小さいで、それぞれの中央値が217と255例です。これにより、広範囲の小さなデータセットでの低い分類誤差を達成するモデルの狭いスペクトルを優しむようになっています。それは、分散を最小化し、計算問題その他のスケーラブルさに少しの重みをつけないモデルを優先しています。私たちの望みは、これらのベンチマークを大規模データセットを使用して構築することで、分野を多様化します。私たちは、大規模なデータから学習する理論的と実用的な挑戦に取り組むことで、分野に新たな進展の巨大なポテンシャルがあると信じています。",
      "upvotes": 2,
      "discussionId": "67bbd6d6ba0bb31293e11258"
    },
    "publishedAt": "2025-02-25T00:37:53.138Z",
    "title": "MONSTER: Monash Scalable Time Series Evaluation Repository",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15122.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "675f68e3074ff89c5c078bf3",
      "avatarUrl": "/avatars/e3b78d90f032659d411761f47c3cf43e.svg",
      "fullname": "Angus",
      "name": "angus924",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17414",
      "authors": [
        {
          "_id": "67bd526001d5bfa0abfcc5ba",
          "name": "Zeyuan Chen",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bb",
          "name": "Hongyi Xu",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bc",
          "name": "Guoxian Song",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bd",
          "name": "You Xie",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5be",
          "name": "Chenxu Zhang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5bf",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c0",
          "name": "Chao Wang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c1",
          "name": "Di Chang",
          "hidden": false
        },
        {
          "_id": "67bd526001d5bfa0abfcc5c2",
          "name": "Linjie Luo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:47:54.000Z",
      "title": "X-Dancer: 表現的音楽と人間の踊りの映像生成",
      "summary": "X-Dancerは、一枚の静的画像から多様な長距離的な生れ生きた人間の踊り映像を作成する新しいゼロショット音楽駆動画像アニメーションパイプラインです。その核心となるのは、一つの統合チャンネル・ディフュージョンフレームワークで、自動復元チャンネルモデルを採用して、2Dの体、頭、手のポーズを構成する拡大された音楽同期ターミノールシーケンスを合成します。これらのシーケンスは、ディフュージョンモデルを導き、一致している現実的な踊り映像フレームを生成します。従来の方法と違って、3Dの人間の動きを主に生成するものではなく、X-Dancerはデータの制限を解決し、2Dの舞の動きの広い範囲をモデル化し、モノクロビデオから音楽の拍子との微妙な一致を捉えることでスケーラビリティを向上させます。これを実現するために、まずは、2Dの人間のポーズラベルとキーポイントの信頼度を結合した空間的な構成的なターミノール表現を構築します。そして、音楽から動きへのチャンネルモデルを設計し、音楽同期された踊りポーズターミノールシーケンスを自動復元的に生成し、音楽のスタイルと先行の動きコンテキストを含むグローバルアテンションを採用します。最後に、これらの合成されたポーズターミノールをアダインを用いて参照画像にアニメーションを加え、完全に微分可能な終端的なフレームワークを構築します。実験結果によると、X-Dancerは多様な特徴付きの踊り映像を生成でき、多様性、表現力、リアリティにおいて最先端の方法を大幅に超えることが証明されました。コードとモデルは研究のために利用可能です。",
      "upvotes": 2,
      "discussionId": "67bd526101d5bfa0abfcc62c"
    },
    "publishedAt": "2025-02-25T00:17:51.431Z",
    "title": "X-Dancer: Expressive Music to Human Dance Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17414.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6205
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13074",
      "authors": [
        {
          "_id": "67bd8759fdecc637bd621e6b",
          "name": "Omer Angel",
          "hidden": false
        },
        {
          "_id": "67bd8759fdecc637bd621e6c",
          "name": "Emmanuel Jacob",
          "hidden": false
        },
        {
          "_id": "67bd8759fdecc637bd621e6d",
          "name": "Brett Kolesnik",
          "hidden": false
        },
        {
          "_id": "67bd8759fdecc637bd621e6e",
          "name": "Grégory Miermont",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T17:21:44.000Z",
      "title": "ブラウンの球の中の蛇",
      "summary": "ブローナースプフェアは、二次元球面と同型なランダムメトリックス空間で、複数種類のランダム平面マップの普遍的スケーリング極限として現れる。ブローナースプフェアの直接構成は、Cori--Vauquelin--Schaeffer (CVS) 対応の連続的なアナロジーによって行われる。CVS 対応は標準された木を平面マップに対応させ、連続的なバーノンラベル付きのアルドウスの連続時間ランダム木（ブローナースネーク）をブローナースプフェアに対応させる。本稿では、連続的なCVS対応の逆変換を説明するために、ブローナースプフェアを測度関数としてのブローナースネークを構築する。ブローナースプフェアの向きの処理に特別な注意が必要です。",
      "upvotes": 0,
      "discussionId": "67bd875afdecc637bd621e95"
    },
    "publishedAt": "2025-02-25T04:03:39.758Z",
    "title": "The snake in the Brownian sphere",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13074.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636d12455aaed143cd665607",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679399015950-636d12455aaed143cd665607.png",
      "fullname": "ZLW",
      "name": "ZarkLngeW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15167",
      "authors": [
        {
          "_id": "67bc7ea06f88ef9a2b8283d3",
          "name": "Chuan Cui",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d4",
          "name": "Kejiang Chen",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d5",
          "name": "Zhihua Wei",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d6",
          "name": "Wen Shen",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d7",
          "name": "Weiming Zhang",
          "hidden": false
        },
        {
          "_id": "67bc7ea06f88ef9a2b8283d8",
          "name": "Nenghai Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T03:05:45.000Z",
      "title": "M3-AGIQA: マルチモード、マルチラウンド、マルチアスペクト ビジョンAI生成画像の品質評価",
      "summary": "AI生成画像（AGI）モデルの急速な進歩は、その質の評価に重大な課題を引き起こしています。これらの課題は、視覚的質、プロンプト対応、アクセプタビリティなど複数の次元を考慮する必要があります。これらの課題に対処するために、私たちはM3-AGIQAという、Multimodal、Multi-Round、Multi-Aspectの評価を行うための一構成の評価フレームワークを提案します。私たちのアプローチは、Multimodal Large Language Models（MLLMs）の能力を活用し、ジョイントテキストと画像エンコーダーとして機能させ、Low-Rank Adaptation（LoRA）の微調節でオンラインMLLMsの高度なカプチング能力をローカルモデルに収納します。このフレームワークは、構造化された多ターン評価機構を含み、中間的な画像説明を生成し、質、対応、アクセプタビリティの面での深い見通しを提供します。人間の視覚的判断に合わせる予測には、xLSTMとリジェスションヘッドを構成した予測器を採用し、順番のロジットを処理し、Mean Opinion Scores（MOSs）を予測します。複数のベンチマークデータセット上での拡張ならびに、クロスデータバリデーションでの強い一般化性能を示します。コードは、https://github.com/strawhatboy/M3-AGIQAに公開されています。",
      "upvotes": 0,
      "discussionId": "67bc7ea26f88ef9a2b828473"
    },
    "publishedAt": "2025-02-25T03:36:50.480Z",
    "title": "M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 771
    },
    "isAuthorParticipating": false
  }
]