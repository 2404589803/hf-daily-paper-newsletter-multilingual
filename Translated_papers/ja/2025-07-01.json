[
  {
    "paper": {
      "id": "2506.23044",
      "authors": [
        {
          "_id": "686347cd588cea0da970c87a",
          "user": {
            "_id": "636f4c6b5d2050767e4a1491",
            "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
            "isPro": false,
            "fullname": "Guo-Hua Wang",
            "user": "Flourish",
            "type": "user"
          },
          "name": "Guo-Hua Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:51.516Z",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87b",
          "name": "Shanshan Zhao",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87c",
          "name": "Xinjie Zhang",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87d",
          "name": "Liangfu Cao",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87e",
          "name": "Pengxin Zhan",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c87f",
          "name": "Lunhao Duan",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c880",
          "name": "Shiyin Lu",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c881",
          "name": "Minghao Fu",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c882",
          "name": "Xiaohao Chen",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c883",
          "name": "Jianshan Zhao",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c884",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "686347cd588cea0da970c885",
          "name": "Qing-Guo Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-29T00:40:17.000Z",
      "submittedOnDailyAt": "2025-07-01T04:30:09.856Z",
      "title": "オヴィス-U1 技術報告書",
      "submittedOnDailyBy": {
        "_id": "636f4c6b5d2050767e4a1491",
        "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
        "isPro": false,
        "fullname": "Guo-Hua Wang",
        "user": "Flourish",
        "type": "user"
      },
      "summary": "このレポートでは、Ovis-U1という3バイリオンパラメータの統合モデルを紹介します。このモデルは、多タイプ理解、テキストから画像生成、画像編集機能を統合しています。Ovisシリーズの基盤に基づいて、Ovis-U1は、ディフュージョンベースの可視デコーダとバイデリクショントークンリファイナーを組み合わせ、GPT-4oと同じレベルの画像生成タスクを可能にします。前のモデルと違って、生成タスクに対してMLLMをフリーズして使用したものではありません。Ovis-U1は、言語モデルから始まる新しい統合訓練アプローチを使用しています。言語理解や生成タスクのみで訓練されたものに比べ、統合訓練はより良い性能を示し、これらの両タスクの統合による効果を示しています。Ovis-U1は、OpenCompass Multi-modal Academic Benchmarkで69.6のスコアを達成し、Ristretto-3BやSAIL-VL-1.5-2Bといった最近の最先端モデルを超えます。テキストから画像生成では、DPG-BenchとGenEvalベンチマークではそれぞれ83.72と0.89のスコアを達成し、画像編集ではImgEdit-BenchとGEdit-Bench-ENそれぞれで4.00と6.42のスコアを達成します。Ovis統合モデルシリーズの初期版であるOvis-U1は、多タイプ理解、生成、編集の境界を超えています。",
      "upvotes": 30,
      "discussionId": "686347cd588cea0da970c886",
      "githubRepo": "https://github.com/AIDC-AI/Ovis-U1",
      "ai_summary": "Ovis-U1, a 3-billion-parameter model, combines multimodal understanding, text-to-image generation, and image editing, achieving state-of-the-art performance in various benchmarks.",
      "ai_keywords": [
        "diffusion-based visual decoder",
        "bidirectional token refiner",
        "unified training",
        "OpenCompass",
        "DPG-Bench",
        "GenEval",
        "ImgEdit-Bench",
        "GEdit-Bench-EN"
      ],
      "githubStars": 137
    },
    "publishedAt": "2025-06-28T20:40:17.000Z",
    "title": "Ovis-U1 Technical Report",
    "summary": "In this report, we introduce Ovis-U1, a 3-billion-parameter unified model\nthat integrates multimodal understanding, text-to-image generation, and image\nediting capabilities. Building on the foundation of the Ovis series, Ovis-U1\nincorporates a diffusion-based visual decoder paired with a bidirectional token\nrefiner, enabling image generation tasks comparable to leading models like\nGPT-4o. Unlike some previous models that use a frozen MLLM for generation\ntasks, Ovis-U1 utilizes a new unified training approach starting from a\nlanguage model. Compared to training solely on understanding or generation\ntasks, unified training yields better performance, demonstrating the\nenhancement achieved by integrating these two tasks. Ovis-U1 achieves a score\nof 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent\nstate-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In\ntext-to-image generation, it excels with scores of 83.72 and 0.89 on the\nDPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves\n4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the\ninitial version of the Ovis unified model series, Ovis-U1 pushes the boundaries\nof multimodal understanding, generation, and editing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23044.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "636f4c6b5d2050767e4a1491",
      "avatarUrl": "/avatars/630c2ae12937fdb16ccd3280bc05729d.svg",
      "fullname": "Guo-Hua Wang",
      "name": "Flourish",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23858",
      "authors": [
        {
          "_id": "686347d3588cea0da970c888",
          "name": "Jianzong Wu",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c889",
          "user": {
            "_id": "64560a2aaaaf85a98fa9a4b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560a2aaaaf85a98fa9a4b9/2Kp0S0sMVpKqo81s-l_Yt.png",
            "isPro": false,
            "fullname": "Liang Hou",
            "user": "lianghou",
            "type": "user"
          },
          "name": "Liang Hou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:49.312Z",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88a",
          "name": "Haotian Yang",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88b",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88c",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88d",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88e",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "686347d3588cea0da970c88f",
          "name": "Yunhai Tong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T13:52:31.000Z",
      "submittedOnDailyAt": "2025-07-01T00:59:37.837Z",
      "title": "VMoBA: ブロックの混ぜ合わせ注意力を用いたビデオディフューションモデル",
      "submittedOnDailyBy": {
        "_id": "657a6eed1ccc3c2a5ea7b585",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RIQIF-JJdNI0SwJEq_9z7.jpeg",
        "isPro": true,
        "fullname": "Jianzong Wu",
        "user": "jianzongwu",
        "type": "user"
      },
      "summary": "全注意力机制的二次复杂度对旨在生成长时间、高分辨率视频的视频扩散模型（VDMs）构成了显著的瓶颈。尽管提出了各种稀疏注意力方法，但许多方法被设计为训练无关的推理加速器，或者在未经训练的原生视频数据中未能最优地捕捉独特的时空特征。本文介绍了视频块混合注意力（VMoBA），这是一种专门为VDMs设计的新型稀疏注意力机制。基于对预训练视频变压器中注意力模式的深入分析，该分析揭示了强烈的时空局部性、变化的查询重要性和头部特定的集中水平，VMoBA通过三个关键修改增强了原始MoBA框架：（1）逐层递归块划分方案（1D-2D-3D），以动态适应多样的时空注意力模式并提高效率；（2）全局块选择，优先考虑整个注意力头中查询-键块交互的最显著部分；（3）基于阈值的块选择，根据累积相似性动态确定参与的块数。广泛的实验表明，VMoBA显著加速了VDMs在长序列上的训练，实现了2.92倍的FLOPs和1.48倍的延迟加速，同时达到了与全注意力相当甚至更优的生成质量。此外，VMoBA在训练无关的推理中表现出竞争性能，为高分辨率视频生成提供了2.40倍的FLOPs和1.35倍的延迟加速。",
      "upvotes": 22,
      "discussionId": "686347d3588cea0da970c890",
      "projectPage": "https://github.com/KwaiVGI/VMoBA",
      "githubRepo": "https://github.com/KwaiVGI/VMoBA",
      "ai_summary": "VMoBA, a novel sparse attention mechanism for Video Diffusion Models (VDMs), accelerates training and inference by addressing the quadratic complexity of full attention mechanisms while maintaining or improving video generation quality.",
      "ai_keywords": [
        "quadartic complexity",
        "full attention mechanisms",
        "Video Diffusion Models",
        "VDMs",
        "sparse attention methods",
        "in-depth analysis",
        "attention patterns",
        "video transformers",
        "spatio-temporal locality",
        "query importance",
        "head-specific concentration",
        "layer-wise recurrent block partition scheme",
        "global block selection",
        "threshold-based block selection",
        "FLOPs",
        "latency",
        "high-res video generation"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-06-30T09:52:31.000Z",
    "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models",
    "summary": "The quadratic complexity of full attention mechanisms poses a significant\nbottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,\nhigh-resolution videos. While various sparse attention methods have been\nproposed, many are designed as training-free inference accelerators or do not\noptimally capture the unique spatio-temporal characteristics inherent in video\ndata when trained natively. This paper introduces Video Mixture of Block\nAttention (VMoBA), a novel sparse attention mechanism specifically adapted for\nVDMs. Motivated by an in-depth analysis of attention patterns within\npre-trained video transformers, which revealed strong spatio-temporal locality,\nvarying query importance, and head-specific concentration levels, VMoBA\nenhances the original MoBA framework with three key modifications: (1) a\nlayer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to\ndiverse spatio-temporal attention patterns and improve efficiency; (2) global\nblock selection to prioritize the most salient query-key block interactions\nacross an entire attention head; and (3) threshold-based block selection to\ndynamically determine the number of attended blocks based on their cumulative\nsimilarity. Extensive experiments demonstrate that VMoBA significantly\naccelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and\n1.48x latency speedup, while attaining comparable or even superior generation\nquality to full attention. Furthermore, VMoBA exhibits competitive performance\nin training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for\nhigh-res video generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23858.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a6eed1ccc3c2a5ea7b585",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RIQIF-JJdNI0SwJEq_9z7.jpeg",
      "fullname": "Jianzong Wu",
      "name": "jianzongwu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.24123",
      "authors": [
        {
          "_id": "68634673588cea0da970c862",
          "name": "Yue Ma",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c863",
          "name": "Qingyan Bai",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c864",
          "name": "Hao Ouyang",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c865",
          "name": "Ka Leong Cheng",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c866",
          "name": "Qiuyu Wang",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c867",
          "name": "Hongyu Liu",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c868",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c869",
          "name": "Haofan Wang",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c86a",
          "user": {
            "_id": "6478a982256b62e219917d67",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PUJ-N2cQxgEmDGfyjajyA.jpeg",
            "isPro": false,
            "fullname": "JingyeChen22",
            "user": "JingyeChen22",
            "type": "user"
          },
          "name": "Jingye Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:14:05.849Z",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c86b",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "68634673588cea0da970c86c",
          "name": "Qifeng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T17:59:06.000Z",
      "submittedOnDailyAt": "2025-07-01T01:00:32.385Z",
      "title": "カラクラ家：フリースタイルテキスト画像カスタマイズ",
      "submittedOnDailyBy": {
        "_id": "63f0baf66309c84d5f4a2226",
        "avatarUrl": "/avatars/a122f7d92441bd2feef7d4eda993fab7.svg",
        "isPro": false,
        "fullname": "Meme155",
        "user": "Meme145",
        "type": "user"
      },
      "summary": "カリグラファー、新しいディフュージョンベースのフレームワークを紹介します。これは、高度なテキストカスタマイズと芸術的なテキストデザインを進化的に統合し、デジタルカリグラフィーおよびデザインアプリケーションに適用するための新しいアイデアを提案しています。テキストデザインの汎用性とデータ依存性の課題を解決するために、フレームワークは3つの主な技術貢献を実現しています。最初に、自動的にスタイルセンチャリーテキストベンチマークを構築するために、事前学習されたテキストから画像への生成モデルと大規模な言語モデルを活用した自己煉煉機構を開発します。次に、トレーナブルなスタイルエンコーダーを通じて局所的なスタイルインジェクションフレームを導入し、参照画像から強力なスタイル特徴を抽出します。このフレームワークは、Qformerと線形レイヤーを構成しています。また、このフレームワークは、参照画像を直接ディズィフェインジャンスプロセスに挿入するインコンテキスト生成機構を使用し、目標スタイルの精練な一致を進めます。多様なフォントとデザインコンテキストの幅広い範囲での極めて詳細な定量的および質的な評価を通じて、カリグラファーは複雑なスタイリッシュの詳細を正確に再現し、グリフの位置を精密に決定します。高品質、視覚的に一貫したテキストデザインを自動化することで、カリグラファーは傳統的なモデルを超え、デジタルアート、ブランディング、およびコンテキストテキストデザインのコンティニューアルな創造的な実践者に力を与えます。",
      "upvotes": 20,
      "discussionId": "68634673588cea0da970c86d",
      "projectPage": "https://calligrapher2025.github.io/Calligrapher/",
      "githubRepo": "https://github.com/Calligrapher2025/Calligrapher",
      "ai_summary": "Calligrapher uses a diffusion-based framework with self-distillation and localized style injection to generate high-quality, stylistically consistent digital typography.",
      "ai_keywords": [
        "diffusion-based framework",
        "self-distillation mechanism",
        "text-to-image generative model",
        "large language model",
        "localized style injection",
        "Qformer",
        "linear layers",
        "style encoder",
        "in-context generation mechanism",
        "denoising process",
        "stylistic details",
        "glyph positioning"
      ],
      "githubStars": 21
    },
    "publishedAt": "2025-06-30T13:59:06.000Z",
    "title": "Calligrapher: Freestyle Text Image Customization",
    "summary": "We introduce Calligrapher, a novel diffusion-based framework that\ninnovatively integrates advanced text customization with artistic typography\nfor digital calligraphy and design applications. Addressing the challenges of\nprecise style control and data dependency in typographic customization, our\nframework incorporates three key technical contributions. First, we develop a\nself-distillation mechanism that leverages the pre-trained text-to-image\ngenerative model itself alongside the large language model to automatically\nconstruct a style-centric typography benchmark. Second, we introduce a\nlocalized style injection framework via a trainable style encoder, which\ncomprises both Qformer and linear layers, to extract robust style features from\nreference images. An in-context generation mechanism is also employed to\ndirectly embed reference images into the denoising process, further enhancing\nthe refined alignment of target styles. Extensive quantitative and qualitative\nevaluations across diverse fonts and design contexts confirm Calligrapher's\naccurate reproduction of intricate stylistic details and precise glyph\npositioning. By automating high-quality, visually consistent typography,\nCalligrapher surpasses traditional models, empowering creative practitioners in\ndigital art, branding, and contextual typographic design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.24123.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f0baf66309c84d5f4a2226",
      "avatarUrl": "/avatars/a122f7d92441bd2feef7d4eda993fab7.svg",
      "fullname": "Meme155",
      "name": "Meme145",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22832",
      "authors": [
        {
          "_id": "6863989c588cea0da970c985",
          "user": {
            "_id": "6192657ba9638054a9818f04",
            "avatarUrl": "/avatars/35f6f6eb2e8f6b283034632a141c2670.svg",
            "isPro": false,
            "fullname": "Alexander Gambashidze",
            "user": "alexgambashidze",
            "type": "user"
          },
          "name": "Alexander Gambashidze",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:12:55.879Z",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c986",
          "name": "Li Pengyi",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c987",
          "user": {
            "_id": "6626c5d0a329de26e7eb16fa",
            "avatarUrl": "/avatars/124f389f768fb666efd8b5a9b54c3b3c.svg",
            "isPro": false,
            "fullname": "Matvey Skripkin",
            "user": "barracuda049",
            "type": "user"
          },
          "name": "Matvey Skripkin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:12:51.866Z",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c988",
          "name": "Andrey Galichin",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c989",
          "name": "Anton Gusarov",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c98a",
          "name": "Konstantin Sobolev",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c98b",
          "name": "Andrey Kuznetsov",
          "hidden": false
        },
        {
          "_id": "6863989c588cea0da970c98c",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6192657ba9638054a9818f04/IpysW0QkLzIgzWognSV7n.png"
      ],
      "publishedAt": "2025-06-28T09:53:17.000Z",
      "submittedOnDailyAt": "2025-07-01T06:44:35.487Z",
      "title": "Listener-Rewarded Thinking in VLMs for Image Preferences",
      "submittedOnDailyBy": {
        "_id": "6192657ba9638054a9818f04",
        "avatarUrl": "/avatars/35f6f6eb2e8f6b283034632a141c2670.svg",
        "isPro": false,
        "fullname": "Alexander Gambashidze",
        "user": "alexgambashidze",
        "type": "user"
      },
      "summary": "訓練強健且可擴展的報酬モデル以適合人類の視覚好みが、文から画像および文から動画の生成モデルを人類の意図に合わせるために重要です。しかし、現在の報酬モデルは通常擴散性が低く、規範的調整は記憶化により複雑な注釈パイプラインを必要とします。強化学習（RL）に特にGroup Relative Policy Optimization（GRPO）を使用して擴散性を向上させることができますが、私たちはキー的な失敗モードを発見しました：モデルの理由の跡が独立した、冷やめた視覚言語モデル（「聴き手」）の理由と矛盾した場合、理由の正確性が大幅に低下します。これを解決するために、私たちは「聴き手」を追加したGRPOフレームワークを提案します。ここで、「聴き手」は理由の連鎖オフソースを再評価し、稠密な補正された信頼度スコアを提供し、RLの報酬信号を形成します。これは理由の正確性だけでなく、独立したモデルに説得力のある解説を生成するよう理由のモデルを奨励します。私たちの「聴き手」をもとめた報酬スキームは、ImageRewardベンチマークで最も高い精度（67.4%）を達成し、大規模な人類の好みデータセット（1.2Mの投票、ナイフルな理由のモデルより最大+6%の向上）での外分布性能を大幅に向上させ、強いGRPOとSFTベンチマークに比べて理由の矛盾を減少します。これらの結果は、「聴き手」ベースの報酬が視覚言語モデルと複雑な人類の好みを合わせるためのスケーラブルでデータ効率的なパスを提供することを示しています。私たちの理由のモデルを公開します：https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner。",
      "upvotes": 13,
      "discussionId": "6863989c588cea0da970c98d",
      "ai_summary": "A listener-augmented Group Relative Policy Optimization framework improves reward models by re-evaluating reasoning processes, leading to enhanced accuracy and out-of-distribution performance in aligning vision-language models with human preferences.",
      "ai_keywords": [
        "reinforcement learning",
        "Group Relative Policy Optimization (GRPO)",
        "listener-augmented GRPO",
        "vision-language model",
        "chain-of-thought",
        "image preference reasoning",
        "ImageReward benchmark",
        "out-of-distribution (OOD) performance"
      ]
    },
    "publishedAt": "2025-06-28T05:53:17.000Z",
    "title": "Listener-Rewarded Thinking in VLMs for Image Preferences",
    "summary": "Training robust and generalizable reward models for human visual preferences\nis essential for aligning text-to-image and text-to-video generative models\nwith human intent. However, current reward models often fail to generalize, and\nsupervised fine-tuning leads to memorization, demanding complex annotation\npipelines. While reinforcement learning (RL), specifically Group Relative\nPolicy Optimization (GRPO), improves generalization, we uncover a key failure\nmode: a significant drop in reasoning accuracy occurs when a model's reasoning\ntrace contradicts that of an independent, frozen vision-language model\n(\"listener\") evaluating the same output. To address this, we introduce a\nlistener-augmented GRPO framework. Here, the listener re-evaluates the\nreasoner's chain-of-thought to provide a dense, calibrated confidence score,\nshaping the RL reward signal. This encourages the reasoner not only to answer\ncorrectly, but to produce explanations that are persuasive to an independent\nmodel. Our listener-shaped reward scheme achieves best accuracy on the\nImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)\nperformance on a large-scale human preference dataset (1.2M votes, up to +6%\nover naive reasoner), and reduces reasoning contradictions compared to strong\nGRPO and SFT baselines. These results demonstrate that listener-based rewards\nprovide a scalable, data-efficient path to aligning vision-language models with\nnuanced human preferences. We will release our reasoning model here:\nhttps://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6192657ba9638054a9818f04/IpysW0QkLzIgzWognSV7n.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22832.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6192657ba9638054a9818f04",
      "avatarUrl": "/avatars/35f6f6eb2e8f6b283034632a141c2670.svg",
      "fullname": "Alexander Gambashidze",
      "name": "alexgambashidze",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.24119",
      "authors": [
        {
          "_id": "68634850588cea0da970c892",
          "user": {
            "_id": "635e3a76106f984574c36409",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
            "isPro": false,
            "fullname": "Bo Liu",
            "user": "Benjamin-eecs",
            "type": "user"
          },
          "name": "Bo Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:46.740Z",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c893",
          "name": "Leon Guertler",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c894",
          "user": {
            "_id": "636681feaa6a4af6073ba73e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636681feaa6a4af6073ba73e/u_0moYNu6as9Sszp-ej95.png",
            "isPro": true,
            "fullname": "Simon Yu",
            "user": "simonycl",
            "type": "user"
          },
          "name": "Simon Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:44.863Z",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c895",
          "user": {
            "_id": "65f5392c68b8e0cb3c9977a2",
            "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
            "isPro": false,
            "fullname": "Zichen",
            "user": "lkevinzc",
            "type": "user"
          },
          "name": "Zichen Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:42.588Z",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c896",
          "name": "Penghui Qi",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c897",
          "name": "Daniel Balcells",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c898",
          "name": "Mickel Liu",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c899",
          "name": "Cheston Tan",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89a",
          "name": "Weiyan Shi",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89b",
          "name": "Min Lin",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89c",
          "name": "Wee Sun Lee",
          "hidden": false
        },
        {
          "_id": "68634850588cea0da970c89d",
          "name": "Natasha Jaques",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T17:58:13.000Z",
      "submittedOnDailyAt": "2025-07-01T01:11:49.104Z",
      "title": "SPIRAL: ゼロ和のゲームでの自分自身のゲームをやりながら理由を深めるための多エージェント多段階強化学習",
      "submittedOnDailyBy": {
        "_id": "635e3a76106f984574c36409",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
        "isPro": false,
        "fullname": "Bo Liu",
        "user": "Benjamin-eecs",
        "type": "user"
      },
      "summary": "最近の強化学習の進展は、言語モデルが可証明可能な報酬を持つタスクでの訓練によって複雑な理由論を開発することができることを示していますが、これらのアプローチは人間が編集した問題解答ペアとドライン専門的な報酬工学に依存しています。私たちは、SPIRAL（Self-Play Framework for Incremental Learning and Adaptation）を紹介します。SPIRALは、モデルが自分自身の進化していくバージョンとの多ターン、ゼロサムゲームをプレイして学習するフレームワークです。これにより、人間の監督が不要となります。SPIRALは、モデルが強い相手に対して常に適応する必要があるため、進歩的に難しい問題の無限なカレクリウムを生成します。このようなサイクリックな学習を実現するために、モデルの学習を大規模に実現するために、全てのオンラインでの、多ターン、多エージェントの強化学習システムを実装し、ロール条件付きの優位度評価（RAE）を提案します。SPIRALを用いることで、ゼロサムゲームでの自分自身のプレイによって、理由論能力が広範囲に移行します。Qwen3-4B-Baseの単一のKuhn Pokerでの学習では、数学について8.6%の向上、一般的な理由論について8.4%の向上が実現され、25,000のエファクティブなゲームトラジェクトでのSFTを超えます。分析によると、この移行は、システム的な分解、期待値計算、ケースバーケース分析の3つの認知パターンによって起こることがわかります。多ゲームの学習（TicTacToe、Kuhn Poker、Simple Negotiation）は、各ゲームが異なる理由論力を開発することで、性能を進めます。SPIRALを強い理由論モデル（DeepSeek-R1-Distill-Qwen-7B）に適用することで、平均2.0%の向上が見られます。これらの結果は、ゼロサムゲームが自然に移行可能な理由論能力を開発することを示し、自動認知開発の有望な方向を明らかにします。",
      "upvotes": 11,
      "discussionId": "68634850588cea0da970c89e",
      "githubRepo": "https://github.com/spiral-rl/spiral",
      "ai_summary": "Self-play in zero-sum games using SPIRAL enhances reasoning capabilities in language models through self-improvement and transfer learning.",
      "ai_keywords": [
        "reinforcement learning",
        "language models",
        "self-play framework",
        "multi-turn games",
        "zero-sum games",
        "reward engineering",
        "role-conditioned advantage estimation",
        "Kuhn Poker",
        "TicTacToe",
        "Simple Negotiation",
        "transferable reasoning"
      ],
      "githubStars": 12
    },
    "publishedAt": "2025-06-30T13:58:13.000Z",
    "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning",
    "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.24119.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635e3a76106f984574c36409",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
      "fullname": "Bo Liu",
      "name": "Benjamin-eecs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.17930",
      "authors": [
        {
          "_id": "68634a4e588cea0da970c8ba",
          "user": {
            "_id": "61e09ec13a1781f66b4e9ae2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
            "isPro": false,
            "fullname": "Jianyu Wang",
            "user": "Jianyu",
            "type": "user"
          },
          "name": "Jianyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:27.023Z",
          "hidden": false
        },
        {
          "_id": "68634a4e588cea0da970c8bb",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:31.161Z",
          "hidden": false
        },
        {
          "_id": "68634a4e588cea0da970c8bc",
          "user": {
            "_id": "6454685a548f22be598414c4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg",
            "isPro": false,
            "fullname": "Lidong Bing",
            "user": "LidongBing",
            "type": "user"
          },
          "name": "Lidong Bing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:29.213Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-22T07:53:07.000Z",
      "submittedOnDailyAt": "2025-07-01T01:09:47.554Z",
      "title": "進化するプロンプトのコンテキスト内：開放的で自動複製する観点",
      "submittedOnDailyBy": {
        "_id": "61e09ec13a1781f66b4e9ae2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
        "isPro": false,
        "fullname": "Jianyu Wang",
        "user": "Jianyu",
        "type": "user"
      },
      "summary": "私たちは、大語言モデル（LLM）のプロンプトにおける習慣的な知識を挑戦する新しいプロンプトデザインパラダイムを提案します。習慣的な知識は、プロンプト学習（ICL）のために、よく作られた指示と示唆を優先していますが、私たちは、ランダムな示唆を「ジベリッシュ」として削減することで、多様なタスクにおいても性能を驚異的に向上させることができることを示します。特に、「ジベリッシュ」は、状態の最先端の自動的なプロンプト最適化手法を超え、LLMのアライメントに関係なく大きな効果を収めます。しかし、効果的な削減策を発見するのは非単純であり、既存の属性方法とプロンプト圧縮アルゴリズムは強固な結果を提供することができないこともあり、それに限らず、人間の直感もまた限界です。この点について、私たちは、自動的に低データレインでの削減策を検索する自己発見プロンプト最適化フレームワーク、PromptQuineを提案します。このフレームワークは、自然のような現象に似たような、共生と自組織化のような現象が資源制約に対する反応で現れるような、非傳統的なものであっても非常に効果的なプロンプトを進化的に鍛錬します。このフレームワークは、分類、多選問答、生成と数学的な理由論のタスクにおいてLLMでも効果的であり、よりよい実行時間効率を達成します。私たちは、この発見が、プロンプト学習における機構的研究にガイドし、より開放的なサーチアルゴリズムのためのより効果的なLLMプロンプトに向けての実験を促進することを望むと考えています。",
      "upvotes": 10,
      "discussionId": "68634a4f588cea0da970c8bd",
      "ai_summary": "A novel prompt design paradigm, PromptQuine, shows that pruning random demonstrations into \"gibberish\" can improve large language model performance across various tasks, surpassing state-of-the-art methods.",
      "ai_keywords": [
        "prompt design paradigm",
        "in-context learning",
        "pruning",
        "demonstrations",
        "gibberish",
        "prompt optimization",
        "self-discover prompt optimization framework",
        "PromptQuine",
        "evolutionary search framework",
        "classification",
        "multi-choice question answering",
        "generation",
        "math reasoning",
        "tokens",
        "emergent complexity",
        "symbiosis",
        "self-organization"
      ]
    },
    "publishedAt": "2025-06-22T03:53:07.000Z",
    "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective",
    "summary": "We propose a novel prompt design paradigm that challenges conventional wisdom\nin large language model (LLM) prompting. While conventional wisdom prioritizes\nwell-crafted instructions and demonstrations for in-context learning (ICL), we\nshow that pruning random demonstrations into seemingly incoherent \"gibberish\"\ncan remarkably improve performance across diverse tasks. Notably, the\n\"gibberish\" always matches or surpasses state-of-the-art automatic prompt\noptimization techniques, achieving substantial gains regardless of LLM\nalignment. Nevertheless, discovering an effective pruning strategy is\nnon-trivial, as existing attribution methods and prompt compression algorithms\nfail to deliver robust results, let alone human intuition. In terms of this, we\npropose a self-discover prompt optimization framework, PromptQuine, an\nevolutionary search framework that automatically searches for the pruning\nstrategy by itself using only low-data regimes. Much like the emergent\ncomplexity in nature--such as symbiosis and self-organization--arising in\nresponse to resource constraints, our framework evolves and refines\nunconventional yet highly effective prompts by leveraging only the tokens\npresent within the context. We demonstrate its effectiveness across\nclassification, multi-choice question answering, generation and math reasoning\ntasks across LLMs, while achieving decent runtime efficiency. We hope our\nfindings can guide mechanistic studies on in-context learning, and provide a\ncall to action, to pave the way for more open-ended search algorithms for more\neffective LLM prompting.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17930.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "61e09ec13a1781f66b4e9ae2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
      "fullname": "Jianyu Wang",
      "name": "Jianyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23542",
      "authors": [
        {
          "_id": "6863479d588cea0da970c86f",
          "user": {
            "_id": "661b9d96c153e4a0a25adc3e",
            "avatarUrl": "/avatars/b3099b51064c8b71a4bce24e2a49b766.svg",
            "isPro": false,
            "fullname": "Weida Wang",
            "user": "weidawang",
            "type": "user"
          },
          "name": "Weida Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:14:03.221Z",
          "hidden": false
        },
        {
          "_id": "6863479d588cea0da970c870",
          "name": "Changyong He",
          "hidden": false
        },
        {
          "_id": "6863479d588cea0da970c871",
          "name": "Jin Zeng",
          "hidden": false
        },
        {
          "_id": "6863479d588cea0da970c872",
          "name": "Di Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T06:29:24.000Z",
      "submittedOnDailyAt": "2025-07-01T01:05:13.898Z",
      "title": "グラフ情報化ジェモトリックアテンションによる一貫的な時間測定深さデノイジング",
      "submittedOnDailyBy": {
        "_id": "6684b284dc7b0ae2cc67660c",
        "avatarUrl": "/avatars/54b3c0c4d808f293d78085d4d504570a.svg",
        "isPro": false,
        "fullname": "liuwanhao",
        "user": "wanhaoliu",
        "type": "user"
      },
      "summary": "時間フライト（ToF）センサーから取得される深さ画像はノイズにもとせることが多いため、信頼性のある次の段階のアプリケーションにおいてはノイズ除去が必要となる。先行研究は、シングルフレーム処理だけであるか、フレーム間処理を行うが、フレーム間の対応するピクセルの深さ変化を考慮しないため、不満足な時間的な不確実性と空間的な不明確性を招く。本論文では、時間的な安定性と空間的なシャープさを同時に向上させるために、動きによらないグラフ統合を活用した新しいToF深さノイズ除去ネットワークを提案します。特に、フレーム間の深さの移動にもかかわらず、グラフ構造は時間的な自己類似性を示し、フレーム間の幾何的なアタンションを行うことができます。次に、結合されたグラフ上の画像の平滑性先頭とToFノイズ分布から得られるデータの忠実性項を用いて、ToFノイズ除去の最大先頭後だけの問題を構成します。最後に、グラフ情報に基づいた幾何的なアタンションから適応的に学習される重みを持つイテレーションフィルターにより、高性能で解釈可能なネットワークを生成します。実験結果によると、提案されたスキームは合成データセットDVToFでの精度と一致性において最先端の性能を収め、実体のKinectv2データセットでは強い一般化性能を示します。ソースコードは以下のURLで公開されます。",
      "upvotes": 8,
      "discussionId": "6863479d588cea0da970c873",
      "ai_summary": "A novel ToF depth denoising network uses motion-invariant graph fusion and adaptive filters to improve temporal stability and spatial sharpness, achieving state-of-the-art performance.",
      "ai_keywords": [
        "ToF depth denoising",
        "motion-invariant graph fusion",
        "graph structures",
        "temporal self-similarity",
        "geometric attention",
        "image smoothness prior",
        "maximum a posterior problem",
        "iterative filters",
        "DVToF dataset",
        "Kinectv2 dataset"
      ]
    },
    "publishedAt": "2025-06-30T02:29:24.000Z",
    "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric\n  Attention",
    "summary": "Depth images captured by Time-of-Flight (ToF) sensors are prone to noise,\nrequiring denoising for reliable downstream applications. Previous works either\nfocus on single-frame processing, or perform multi-frame processing without\nconsidering depth variations at corresponding pixels across frames, leading to\nundesirable temporal inconsistency and spatial ambiguity. In this paper, we\npropose a novel ToF depth denoising network leveraging motion-invariant graph\nfusion to simultaneously enhance temporal stability and spatial sharpness.\nSpecifically, despite depth shifts across frames, graph structures exhibit\ntemporal self-similarity, enabling cross-frame geometric attention for graph\nfusion. Then, by incorporating an image smoothness prior on the fused graph and\ndata fidelity term derived from ToF noise distribution, we formulate a maximum\na posterior problem for ToF denoising. Finally, the solution is unrolled into\niterative filters whose weights are adaptively learned from the graph-informed\ngeometric attention, producing a high-performance yet interpretable network.\nExperimental results demonstrate that the proposed scheme achieves\nstate-of-the-art performance in terms of accuracy and consistency on synthetic\nDVToF dataset and exhibits robust generalization on the real Kinectv2 dataset.\nSource code will be released at\nhttps://github.com/davidweidawang/GIGA-ToF{https://github.com/davidweidawang/GIGA-ToF}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23542.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6684b284dc7b0ae2cc67660c",
      "avatarUrl": "/avatars/54b3c0c4d808f293d78085d4d504570a.svg",
      "fullname": "liuwanhao",
      "name": "wanhaoliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.17417",
      "authors": [
        {
          "_id": "685c8635696820ba1f28f24b",
          "user": {
            "_id": "65d3b7ec8f6b98b34ee6bbe3",
            "avatarUrl": "/avatars/53c2d4e4746147fc2559435d252e8951.svg",
            "isPro": false,
            "fullname": "Mingyuan Wu",
            "user": "Mingyuan1997",
            "type": "user"
          },
          "name": "Mingyuan Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:32:22.703Z",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24c",
          "name": "Meitang Li",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24d",
          "name": "Jingcheng Yang",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24e",
          "name": "Jize Jiang",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f24f",
          "name": "Kaizhuo Yan",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f250",
          "name": "Zhaoheng Li",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f251",
          "name": "Minjia Zhang",
          "hidden": false
        },
        {
          "_id": "685c8635696820ba1f28f252",
          "name": "Klara Nahrstedt",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-20T18:23:48.000Z",
      "submittedOnDailyAt": "2025-07-01T00:36:26.256Z",
      "title": "Aha モーメント再見：VLMs は推論時のスケーリングで本物に自動証明能力を持つか？",
      "submittedOnDailyBy": {
        "_id": "65d3b7ec8f6b98b34ee6bbe3",
        "avatarUrl": "/avatars/53c2d4e4746147fc2559435d252e8951.svg",
        "isPro": false,
        "fullname": "Mingyuan Wu",
        "user": "Mingyuan1997",
        "type": "user"
      },
      "summary": "最近の大規模言語モデル（LLMs）の進展は、推論時の計算手法（例えば、解確定時スケーリングと自己修正）が、外部知識を依存しないで理由論の能力を大幅に向上させることを示しています。この成功の鍵となるのは、自己補正と自己確認の行動の現れ、通常は強化学習（RL）によって引き起こされるものです。本論文では、これらの推論時の手法が、視覚言語モデル（VLMs）にも有効に適用できるかどうかを調査します。また、RLで訓練されたものに特に注目します。私たちは、多数決とNの中の最良を選択したような解確定戦略がすべてVLMの理由論性能を向上させ、生成に依存する方法（例えば、前述のもの）が、確認に依存する方法（例えば、後述のもの）よりも大幅に効果的であることを見出しました。また、RL調整されたモデルに関連付けられる自己修正の行動（例えば、「その瞬間」）は、測定可能な効果を示すことはありません。推論時のスケーリングフレームワーク内での拡張的な実験を通じて、主な原因を特定するために、RL訓練されたVLMsは、両方視覚と文字的なモデルでも強固な自己確認能力を欠いていることを示します。",
      "upvotes": 8,
      "discussionId": "685c8636696820ba1f28f253",
      "ai_summary": "Inference-time techniques like decoding-time scaling and self-refinement enhance reasoning in vision-language models, with generation-based methods providing greater improvement than verification-based methods, despite RL-trained models not showing self-correction benefits.",
      "ai_keywords": [
        "large language models",
        "inference-time computation",
        "decoding-time scaling",
        "self-refinement",
        "self-correction",
        "self-verification",
        "reinforcement learning",
        "vision-language models",
        "majority voting",
        "best-of-N selection",
        "aha moment"
      ]
    },
    "publishedAt": "2025-06-20T14:23:48.000Z",
    "title": "Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in\n  Inference-time Scaling?",
    "summary": "Recent advances in large language models (LLMs) have demonstrated that\ninference-time computation techniques, such as decoding-time scaling and\nself-refinement, can significantly enhance reasoning capabilities without\nrelying on external knowledge. A key driver of this success is the emergence of\nself-correction and self-verification behaviors, often elicited through\nreinforcement learning (RL). In this paper, we investigate whether these\ninference-time techniques extend effectively to vision-language models (VLMs),\nparticularly those trained with RL. We find that while decoding strategies such\nas majority voting and best-of-N selection with self-verification all improve\nVLM reasoning performance, generation-reliant methods such as the former\nachieve significantly higher gains versus verification-reliant methods such as\nthe latter. Additionally, the self-correction behavior often associated with\nRL-tuned models, such as aha moment, does not lead to measurable gains. We show\nvia extensive experimentation within the inference-time scaling framework to\nidentify a key root cause: RL-trained VLMs still lack robust self-verification\ncapabilities across both visual and textual modalities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d3b7ec8f6b98b34ee6bbe3",
      "avatarUrl": "/avatars/53c2d4e4746147fc2559435d252e8951.svg",
      "fullname": "Mingyuan Wu",
      "name": "Mingyuan1997",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16500",
      "authors": [
        {
          "_id": "6858a7f0c0c8e29df8ea3c06",
          "user": {
            "_id": "64b38bc2a248169796fec4fa",
            "avatarUrl": "/avatars/1371021f2ef0ce2197cc13627c9e03c9.svg",
            "isPro": false,
            "fullname": "Samir Khaki",
            "user": "Skhaki",
            "type": "user"
          },
          "name": "Samir Khaki",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:14:44.911Z",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c07",
          "name": "Xiuyu Li",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c08",
          "name": "Junxian Guo",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c09",
          "name": "Ligeng Zhu",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0a",
          "name": "Chenfeng Xu",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0b",
          "name": "Konstantinos N. Plataniotis",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0c",
          "name": "Amir Yazdanbakhsh",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0d",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0e",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "6858a7f0c0c8e29df8ea3c0f",
          "user": {
            "_id": "650dac79b959b0e1d41d7378",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dac79b959b0e1d41d7378/mzbN0MFk3k8b94FQ40I7L.jpeg",
            "isPro": false,
            "fullname": "Zhijian Liu",
            "user": "zhijianliu",
            "type": "user"
          },
          "name": "Zhijian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:14:42.873Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T17:53:34.000Z",
      "submittedOnDailyAt": "2025-07-01T03:16:07.666Z",
      "title": "SparseLoRA: LLMの微調節をコンテキスト関数のスパース性によって加速する",
      "submittedOnDailyBy": {
        "_id": "64b38bc2a248169796fec4fa",
        "avatarUrl": "/avatars/1371021f2ef0ce2197cc13627c9e03c9.svg",
        "isPro": false,
        "fullname": "Samir Khaki",
        "user": "Skhaki",
        "type": "user"
      },
      "summary": "LLMの微調節は、計算量とメモリ使用量にも強い要望を持つ。QLoRAとDoRAのようなパラメータ効率的な微調節方法は、学習可能なパラメータの数を減らし、メモリ使用量を下げることができますが、計算コストを減らすことはできません。このような場合、それらは微調節を遅らせることもあります。本論文では、コンテキストのスパース性を用いてLLMの微調節を加速するSparseLoRAの方法を紹介します。これにより、計算コストを最大で2.2倍減らし、精度を維持する同時に、計算速度を最大で1.6倍高速化することができます。",
      "upvotes": 7,
      "discussionId": "6858a7f0c0c8e29df8ea3c10",
      "projectPage": "https://z-lab.ai/projects/sparselora/",
      "githubRepo": "https://github.com/z-lab/sparselora",
      "ai_summary": "SparseLoRA reduces computational cost and speeds up fine-tuning of LLMs by dynamically selecting a sparse subset of weights for loss and gradient computation.",
      "ai_keywords": [
        "QLoRA",
        "DoRA",
        "parameter-efficient fine-tuning",
        "SparseLoRA",
        "contextual sparsity",
        "SVD sparsity estimator",
        "computational cost",
        "commonsense reasoning",
        "arithmetic reasoning",
        "code generation",
        "instruction following"
      ],
      "githubStars": 20
    },
    "publishedAt": "2025-06-19T13:53:34.000Z",
    "title": "SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity",
    "summary": "Fine-tuning LLMs is both computationally and memory-intensive. While\nparameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the\nnumber of trainable parameters and lower memory usage, they do not decrease\ncomputational cost. In some cases, they may even slow down fine-tuning. In this\npaper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning\nthrough contextual sparsity. We propose a lightweight, training-free SVD\nsparsity estimator that dynamically selects a sparse subset of weights for loss\nand gradient computation. Also, we systematically analyze and address\nsensitivity across layers, tokens, and training steps. Our experimental results\nshow that SparseLoRA reduces computational cost by up to 2.2 times and a\nmeasured speedup of up to 1.6 times while maintaining accuracy across various\ndownstream tasks, including commonsense and arithmetic reasoning, code\ngeneration, and instruction following.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16500.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b38bc2a248169796fec4fa",
      "avatarUrl": "/avatars/1371021f2ef0ce2197cc13627c9e03c9.svg",
      "fullname": "Samir Khaki",
      "name": "Skhaki",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23151",
      "authors": [
        {
          "_id": "686399bc588cea0da970c98f",
          "name": "Vladislav Bargatin",
          "hidden": false
        },
        {
          "_id": "686399bc588cea0da970c990",
          "name": "Egor Chistov",
          "hidden": false
        },
        {
          "_id": "686399bc588cea0da970c991",
          "user": {
            "_id": "663692c75f67f8da32723bf8",
            "avatarUrl": "/avatars/258264afe2ea5048a4a7a8e9945d2f5b.svg",
            "isPro": false,
            "fullname": "Alexander Yakovenko",
            "user": "a-yakovenko",
            "type": "user"
          },
          "name": "Alexander Yakovenko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:12:16.036Z",
          "hidden": false
        },
        {
          "_id": "686399bc588cea0da970c992",
          "name": "Dmitriy Vatolin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-29T09:01:42.000Z",
      "submittedOnDailyAt": "2025-07-01T08:44:33.443Z",
      "title": "メモリー効率的多フレーム光学流推定のための高解像度訓練",
      "submittedOnDailyBy": {
        "_id": "663692c75f67f8da32723bf8",
        "avatarUrl": "/avatars/258264afe2ea5048a4a7a8e9945d2f5b.svg",
        "isPro": false,
        "fullname": "Alexander Yakovenko",
        "user": "a-yakovenko",
        "type": "user"
      },
      "summary": "最近の光学フロー推定の進展は、精度を優先したが、GPUメモリ消費量が増加していることに注意されています。特に、高解像度（FullHD）の入力に対しては、この問題が厳しいです。ここでは、MEMFOF（Memory-Efficient Multi-Frame Optical Flow）を紹介します。MEMFOFは、多フレーム推定とGPUメモリ使用量の間の有利なトレードオフを見つけるメモリ効率的な多フレーム光学フロー方法です。特に、1080pの入力では、実行時には2.09 GBのGPUメモリを必要とし、学習時には28.5 GBのGPUメモリを必要とします。これにより、1080pのネイティブスケールで学習することが可能で、カットアウトやダウンサンプリングが不要です。RAFT-likeアーキテクチャからデザイン選択をシステマチックに再評価し、減少された相関ボリュームと高解像度トレーニングプロトコルを多フレーム推定と組み合わせて、複数のベンチマークで最先端の性能を達成しながら、大幅にメモリオーバーヘッドを減少します。我々の方法は、精度と実行時間効率の両方で、よりリソース鉱石なアルタナティブよりも優れています。これにより、高解像度でのフロー推定の強固性を証明します。提出時点では、我々の方法は、1ピクセル（1px）のオフライアー率が3.289で、Springベンチマークで1位に立ち、Sintel（clean）では端点エラー（EPE）が0.963で、KITTI-2015ではFl-allエラーが2.94%で最善を収めています。コードは、https://github.com/msu-video-group/memfof で提供されています。",
      "upvotes": 5,
      "discussionId": "686399bd588cea0da970c993",
      "projectPage": "https://msu-video-group.github.io/memfof/",
      "githubRepo": "https://github.com/msu-video-group/memfof",
      "ai_summary": "MEMFOF is a memory-efficient multi-frame optical flow method that achieves state-of-the-art performance on high-resolution inputs with reduced GPU memory usage.",
      "ai_keywords": [
        "MEMFOF",
        "optical flow estimation",
        "memory-efficient",
        "multi-frame",
        "correlation volumes",
        "RAFT-like architectures",
        "high-resolution training",
        "Spring benchmark",
        "Sintel benchmark",
        "KITTI-2015 benchmark",
        "outlier rate",
        "endpoint error",
        "Fl-all error"
      ]
    },
    "publishedAt": "2025-06-29T05:01:42.000Z",
    "title": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame\n  Optical Flow Estimation",
    "summary": "Recent advances in optical flow estimation have prioritized accuracy at the\ncost of growing GPU memory consumption, particularly for high-resolution\n(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical\nflow method that identifies a favorable trade-off between multi-frame\nestimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU\nmemory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely\npositions our method to be trained at native 1080p without the need for\ncropping or downsampling. We systematically revisit design choices from\nRAFT-like architectures, integrating reduced correlation volumes and\nhigh-resolution training protocols alongside multi-frame estimation, to achieve\nstate-of-the-art performance across multiple benchmarks while substantially\nreducing memory overhead. Our method outperforms more resource-intensive\nalternatives in both accuracy and runtime efficiency, validating its robustness\nfor flow estimation at high resolutions. At the time of submission, our method\nranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,\nleads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the\nbest Fl-all error on KITTI-2015 at 2.94%. The code is available at\nhttps://github.com/msu-video-group/memfof.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23151.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "663692c75f67f8da32723bf8",
      "avatarUrl": "/avatars/258264afe2ea5048a4a7a8e9945d2f5b.svg",
      "fullname": "Alexander Yakovenko",
      "name": "a-yakovenko",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.22992",
      "authors": [
        {
          "_id": "686370bd588cea0da970c90e",
          "user": {
            "_id": "662a75287181150b857245fb",
            "avatarUrl": "/avatars/6a489bf6d3d9cd26ba88f17b35c6ecb5.svg",
            "isPro": false,
            "fullname": "Yulun Jiang",
            "user": "yljblues",
            "type": "user"
          },
          "name": "Yulun Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:16.902Z",
          "hidden": false
        },
        {
          "_id": "686370bd588cea0da970c90f",
          "name": "Yekun Chai",
          "hidden": false
        },
        {
          "_id": "686370bd588cea0da970c910",
          "name": "Maria Brbić",
          "hidden": false
        },
        {
          "_id": "686370bd588cea0da970c911",
          "user": {
            "_id": "6438d1d843d932c462404500",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
            "isPro": false,
            "fullname": "Michael Moor",
            "user": "mdmoor",
            "type": "user"
          },
          "name": "Michael Moor",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:19.043Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6438d1d843d932c462404500/FI8PgTG5g_uZ1q4y-c_Jd.png"
      ],
      "publishedAt": "2025-06-28T19:44:32.000Z",
      "submittedOnDailyAt": "2025-07-01T05:06:51.086Z",
      "title": "マーベル：多モーダルスペース認識と計画の厳しいベンチマーク",
      "submittedOnDailyBy": {
        "_id": "6438d1d843d932c462404500",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
        "isPro": false,
        "fullname": "Michael Moor",
        "user": "mdmoor",
        "type": "user"
      },
      "summary": "情報の多様なモデライズマルデータからの処理能力とステップごとに理由をつくる能力は、人工知能の進歩において重要な課題です。しかし、現在の理由のベンチマークは、テキストだけの理由を焦点にしているか、モデライズマルデータから直接情報を検索できるような多様な質問を使用しています。これにより、複雑な理由の理解は、多様なモデライズマルデータ領域ではよりもより低いレベルで調査されています。ここで、私たちは、複雑な多様なモデライズマルデータ問題と環境をステップごとに理由をつくる能力を評価するために設計された挑戦的な多様なモデライズマルデータ理由のベンチマークを紹介します。これは、複雑なスペース的、視覚的、物理的な制約の下で複数ステップの計画の作成と理解を求める2つの高度な挑戦的なタスク、M-PortalとM-Cubeを構成しています。私たちは、現在の多様なモデライズマルデータモデル（MLLMs）は、MARBLEでは悪い性能を示しています。12つの先進モデルは、すべてのM-Portalで近似ランダムな性能を示し、M-Cubeでは0%の精度を示しています。そのほか、簡略化されたサブタスクでは、一部のモデルはランダムベースラインを超えていることがわかり、複雑な理由の理解は、現在のMLLMsではまだ挑戦的な課題であることが示されています。また、私たちは、視覚的入力から情報を抽出することが困難な場合があることを示し、視覚的認識はボトルネックとなっていることを示しています。これにより、複雑な理由の理解の限界を明らかにして、私たちはMARBLEが、複数の多様な理由のステップを通じて理由をつくり計画を立てる能力を持つ次世代のモデルの開発に促しています。",
      "upvotes": 3,
      "discussionId": "686370be588cea0da970c912",
      "ai_summary": "MARBLE is a challenging multimodal reasoning benchmark that highlights the limitations of existing multimodal language models in step-by-step reasoning and plan crafting under spatial and visual constraints.",
      "ai_keywords": [
        "multimodal reasoning",
        "multimodal language models",
        "M-Portal",
        "M-Cube",
        "multistep plans",
        "spatial constraints",
        "visual constraints"
      ]
    },
    "publishedAt": "2025-06-28T15:44:32.000Z",
    "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning",
    "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6438d1d843d932c462404500/FI8PgTG5g_uZ1q4y-c_Jd.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22992.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6438d1d843d932c462404500",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
      "fullname": "Michael Moor",
      "name": "mdmoor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23394",
      "authors": [
        {
          "_id": "686350a9588cea0da970c8d4",
          "user": {
            "_id": "645dbaa6f5760d1530d7580d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
            "isPro": true,
            "fullname": "Simeon Emanuilov",
            "user": "s-emanuilov",
            "type": "user"
          },
          "name": "Simeon Emanuilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:22.926Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-29T20:47:27.000Z",
      "submittedOnDailyAt": "2025-07-01T01:39:51.669Z",
      "title": "ツールの言語を話すことを学ぶ言語モデルの教え方",
      "submittedOnDailyBy": {
        "_id": "645dbaa6f5760d1530d7580d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
        "isPro": true,
        "fullname": "Simeon Emanuilov",
        "user": "s-emanuilov",
        "type": "user"
      },
      "summary": "外部ツールの連携を関数呼び出しで実践的な言語モデルアプリケーションには重要ですが、多言語モデルは多くの非英語では信頼性のあるツール使用能力を持っていません。状態の最先端の多言語モデルも、ツールを使用する時期を決定し、関数呼び出しに必要な構造化された出力を生成することに苦戦し、低リソース言語でプロンプトされた場合には言語混同を示すこともあります。本論文では、ブルガリアを場合研究として、現在の言語モデルを応用して任意のターゲット言語で強固なツール使用を可能にする方法を提案します。アプローチは、MCP（モデルコンテキストプロトコル）などの標準化プロトコルをサポートするための新しいバイリンガルデータセット（10,035例）でのBgGPTモデルシリーズ（2.6B、9B、27Bパラメータ）の継続的訓練を含みます。本研究では、TUCAN（ツール使用可能なアシスタントナビゲーター）を紹介し、基礎モデルに対して関数呼び出し精度が28.75%以上向上し、ブルガリアの既定ベンチマークで確認された核心的な言語理解を保持します。精度の向上よりも、TUCANモデルは、基礎モデルの冗長で不一致的な出力と対照し、セミプライムで整形された関数呼び出しを示し、生産用の準備が完了しています。モデル、評価フレームワーク、データセットは、他の言語の再現を可能にすることを目指して公開されています。本論文では、ツール追加の機能を英語中心的なシステムを超える実用的なアプローチを示しています。",
      "upvotes": 2,
      "discussionId": "686350aa588cea0da970c8d5",
      "ai_summary": "A methodology is presented to adapt language models for robust tool use across languages, specifically improving function-calling accuracy in Bulgarian.",
      "ai_keywords": [
        "function-calling",
        "multilingual models",
        "tool-use capabilities",
        "language confusion",
        "BgGPT",
        "bilingual dataset",
        "MCP (Model Context Protocol)",
        "TUCAN (Tool-Using Capable Assistant Navigator)",
        "function-calling accuracy",
        "production-ready response formatting"
      ]
    },
    "publishedAt": "2025-06-29T16:47:27.000Z",
    "title": "Teaching a Language Model to Speak the Language of Tools",
    "summary": "External tool integration through function-calling is essential for practical\nlanguage model applications, yet most multilingual models lack reliable\ntool-use capabilities in non-English languages. Even state-of-the-art\nmultilingual models struggle with determining when to use tools and generating\nthe structured outputs required for function calls, often exhibiting language\nconfusion when prompted in lower-resource languages. This work presents a\nmethodology for adapting existing language models to enable robust tool use in\nany target language, using Bulgarian as a case study. The approach involves\ncontinued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a\nnovel bilingual dataset of 10,035 function-calling examples designed to support\nstandardized protocols like MCP (Model Context Protocol). The research\nintroduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to\n28.75% improvement in function-calling accuracy over base models while\npreserving core language understanding, as verified on established Bulgarian\nbenchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready\nresponse formatting with clean, parsable function calls, contrasting with the\nverbose and inconsistent outputs of base models. The models, evaluation\nframework, and dataset are released to enable replication for other languages.\nThis work demonstrates a practical approach for extending tool-augmented\ncapabilities beyond English-centric systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23394.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645dbaa6f5760d1530d7580d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg",
      "fullname": "Simeon Emanuilov",
      "name": "s-emanuilov",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 39
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.23219",
      "authors": [
        {
          "_id": "686376f6588cea0da970c92b",
          "user": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "isPro": false,
            "fullname": "Jie Feng",
            "user": "JJ-TMT",
            "type": "user"
          },
          "name": "Jie Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:13:14.367Z",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92c",
          "name": "Shengyuan Wang",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92d",
          "name": "Tianhui Liu",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92e",
          "name": "Yanxin Xi",
          "hidden": false
        },
        {
          "_id": "686376f6588cea0da970c92f",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/RBLtf0_JPAX9A8msse6c0.jpeg"
      ],
      "publishedAt": "2025-06-29T13:04:27.000Z",
      "submittedOnDailyAt": "2025-07-01T04:22:03.888Z",
      "title": "都市LLaVA: 都市情報のための空間論理と理解を含む多模態大語言モデル",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "都市研究には、多様なシナリオとタスクが含まれ、多タイプデータの理解が必要です。現在の方法は、特定のデータタイプに焦点を当て、都市分野での収まり合わせたフレームワークを持っていません。最近の多タイプ大語言モデル（MLLM）の成功は、この制限を克服する可能性を示しています。本論文では、都市LLaVAという多タイプ大語言モデルを紹介します。これは、4つのデータタイプを同時に処理し、一般的なMLLMと比較して多様な都市タスクで強い性能を達成することを目指しています。都市LLaVAでは、まず、シングルモーダルとクロスモーダルの都市データを含む多様な都市インストラクションデータセットを編集します。また、空間的な理由の向上と領域知識の学習を区別した多段階の訓練フレームワークを提案し、都市LLaVAの適用性とダウンストリーム性能を改善します。最後に、都市研究の現在のベンチマークを拡張し、MLLMの多様な都市タスクでの性能を評価することを試みます。3つの都市からの実験結果は、都市LLaVAが開放ソースおよび所有権のMLLMを上回り、シングルモーダルタスクと複雑なクロスモーダルタスクでの性能を示し、都市間での強固な一般化能力を示していることを明らかにします。ソースコードとデータは、https://github.com/tsinghua-fib-lab/UrbanLLaVAから公開的にアクセス可能です。",
      "upvotes": 2,
      "discussionId": "686376f6588cea0da970c930",
      "githubRepo": "https://github.com/tsinghua-fib-lab/UrbanLLaVA",
      "ai_summary": "UrbanLLaVA, a multi-modal large language model, effectively processes urban datasets for various tasks, outperforming existing models in both single-modal and complex cross-modal scenarios.",
      "ai_keywords": [
        "multi-modal large language models",
        "spatial reasoning",
        "domain knowledge learning",
        "urban instruction dataset",
        "single-modal",
        "cross-modal",
        "benchmark",
        "urban research"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-06-29T09:04:27.000Z",
    "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence\n  with Spatial Reasoning and Understanding",
    "summary": "Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce UrbanLLaVA, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\nUrbanLLaVA, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of UrbanLLaVA across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that UrbanLLaVA outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/RBLtf0_JPAX9A8msse6c0.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23219.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.22694",
      "authors": [
        {
          "_id": "68637193588cea0da970c914",
          "name": "Raghavv Goel",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c915",
          "name": "Sudhanshu Agrawal",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c916",
          "name": "Mukul Gagrani",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c917",
          "name": "Junyoung Park",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c918",
          "name": "Yifan Zao",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c919",
          "name": "He Zhang",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91a",
          "name": "Tian Liu",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91b",
          "name": "Yiping Yang",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91c",
          "name": "Xin Yuan",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91d",
          "name": "Jiuyan Lu",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91e",
          "name": "Chris Lott",
          "hidden": false
        },
        {
          "_id": "68637193588cea0da970c91f",
          "name": "Mingu Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T00:26:40.000Z",
      "submittedOnDailyAt": "2025-07-01T04:07:47.278Z",
      "title": "VOCABTRIM: ボキャブラリープラウィングでの効率的な推測的解確語言処理モデルのプログラミング",
      "submittedOnDailyBy": {
        "_id": "649b6eb2f7cc759ab756adaf",
        "avatarUrl": "/avatars/a0f7c7345dd653887122200fbe375c2b.svg",
        "isPro": false,
        "fullname": "Raghavv Goel",
        "user": "RaghavvGoel",
        "type": "user"
      },
      "summary": "この論文では、スペシャルデコーディング（SpD）メソッドの性能向上に対して、スタンディングフリーの簡単な手法を紹介します。この手法は、スペシャルデコーディングのスタートプロセスにおいて言語モデルヘッド（LM head）を組み込むことで、スペシャルデコーディングを効果的に改善します。スペシャルデコーディングは、1つ以上の小さな言語モデル（ドラフターやドラフモデル）を利用して、複数のトークンからなるドラフトシーケンスや木をサンプリングし、その後、ベースLLM（ターゲットモデル）で確認し、一部をベースLLMの有効な生成として受け入れます。通常、スペシャルデコーディングはターゲットモデルとドラフモデルのビオカラブリィーの1対1マッピングが必要と考えられるため、ビオカラブリィーを共有したり、EAGLEやMedusaのようにLM headを共有することが自然なことになります。まず、このドラフトトークンサンプリングスキームが、ドラフト時に不必要な推論オーバーヘッドを含んでいることを認識します。特に、ターゲットLLMのビオカラブリィーが非常に大きい場合は、このオーバーヘッドが特に大きくなります。そこで、VocabTrimという簡単な手法を提案し、メモリバネット環境での生成速度を向上させるためにドラフトオーバーヘッドを軽減します。VocabTrimは、ドラフターのLM headを、ターゲットモデルのビオカラブリィーから最も頻繁にサンプリングされるトークンの有限集合に限定します。ドラフト時にビオカラブリィーを制限することは、受容率を少し下げるだけですが、メモリバネットプロセスのラテンシーを大幅に減少させ、エッジデバイスでの通常の場合、メモリバネットスピードアップ（MBSU）を上げます。また、我々の方法は、Spec-BenchでLlama-3モデルのメモリバネットスピードアップを16%程度に上げ、特にLlama-3.2-3B-Instructでは16%程度に上げることを示しています。",
      "upvotes": 2,
      "discussionId": "68637193588cea0da970c920",
      "ai_summary": "A technique called VocabTrim improves drafter-based speculative decoding by reducing the vocabulary of the drafter language model, thus decreasing drafting latency in memory-bound environments.",
      "ai_keywords": [
        "drafter-based speculative decoding",
        "speculative decoding",
        "LM head",
        "drafters",
        "token sampling",
        "inference overhead",
        "target LLM",
        "vocabulary sharing",
        "EAGLE",
        "Medusa",
        "VocabTrim",
        "acceptance rate",
        "drafting latency",
        "memory-bound speed up",
        "Spec-Bench",
        "Llama-3"
      ]
    },
    "publishedAt": "2025-06-27T20:26:40.000Z",
    "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs",
    "summary": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649b6eb2f7cc759ab756adaf",
      "avatarUrl": "/avatars/a0f7c7345dd653887122200fbe375c2b.svg",
      "fullname": "Raghavv Goel",
      "name": "RaghavvGoel",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23135",
      "authors": [
        {
          "_id": "68638235588cea0da970c966",
          "name": "Yu Shang",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c967",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c968",
          "name": "Yinzhou Tang",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c969",
          "name": "Lei Jin",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c96a",
          "name": "Chen Gao",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c96b",
          "name": "Wei Wu",
          "hidden": false
        },
        {
          "_id": "68638235588cea0da970c96c",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/oiWNtmiOwH-sWe0ewE2YQ.jpeg"
      ],
      "publishedAt": "2025-06-29T08:19:45.000Z",
      "submittedOnDailyAt": "2025-07-01T05:11:31.002Z",
      "title": "RoboScape: 物理情報をもつ具象化ワールドモデル",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "世界モデルは、体化された知能の不可欠なツールとして機能しています。これらは、実際のロボットの写真を生成するための強力なシミュレーターであり、重要なデータ不足の問題を解決しています。しかし、現在の体化された世界モデルは、3Dジェネリックおよび動作ダイナミクスの物理的な認識が限られているため、接触エン富みのロボットスケーナーでは実写的な写真生成に欠陥があります。本論文では、RGBビデオ生成と物理知識を統合したインターグレイフレームワークで学習する物理情報をもつ統一された物理モデルを提出します。2つの物理情報をもつ連結トレーニングタスクを導入します：時系列的なデプス予測は、ビデオの渲染における3Dジェネリックの一致性を高め、キーポイント動力学学習は物理的な属性（例えば、物体の形状と材料の特徴）を隠れにエンコードし、複雑な動作モデルを改善します。拡張された実験は、RoboScapeは多様なロボットスケーナーでもその上で優れた視覚的なフィデリティと物理的な可能性を示します。さらに、生成データを用いたロボットポリシートレーニングとポリシー評価のダウンストラムアプリケーションで実用的な効果を証明します。我々の研究は、物理情報を持つ世界モデルの効率的な構築に新しいヒントを提供し、体化された知能研究の進歩に貢献します。コードは以下のURLで提供されています：https://github.com/tsinghua-fib-lab/RoboScape。",
      "upvotes": 1,
      "discussionId": "68638235588cea0da970c96d",
      "ai_summary": "RoboScape is a unified physics-informed world model that enhances visual fidelity and physical plausibility in robotic video generation by integrating temporal depth prediction and keypoint dynamics learning.",
      "ai_keywords": [
        "world models",
        "embodied intelligence",
        "RGB video generation",
        "physics knowledge",
        "temporal depth prediction",
        "keypoint dynamics learning",
        "visual fidelity",
        "physical plausibility"
      ]
    },
    "publishedAt": "2025-06-29T04:19:45.000Z",
    "title": "RoboScape: Physics-informed Embodied World Model",
    "summary": "World models have become indispensable tools for embodied intelligence,\nserving as powerful simulators capable of generating realistic robotic videos\nwhile addressing critical data scarcity challenges. However, current embodied\nworld models exhibit limited physical awareness, particularly in modeling 3D\ngeometry and motion dynamics, resulting in unrealistic video generation for\ncontact-rich robotic scenarios. In this paper, we present RoboScape, a unified\nphysics-informed world model that jointly learns RGB video generation and\nphysics knowledge within an integrated framework. We introduce two key\nphysics-informed joint training tasks: temporal depth prediction that enhances\n3D geometric consistency in video rendering, and keypoint dynamics learning\nthat implicitly encodes physical properties (e.g., object shape and material\ncharacteristics) while improving complex motion modeling. Extensive experiments\ndemonstrate that RoboScape generates videos with superior visual fidelity and\nphysical plausibility across diverse robotic scenarios. We further validate its\npractical utility through downstream applications including robotic policy\ntraining with generated data and policy evaluation. Our work provides new\ninsights for building efficient physics-informed world models to advance\nembodied intelligence research. The code is available at:\nhttps://github.com/tsinghua-fib-lab/RoboScape.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/oiWNtmiOwH-sWe0ewE2YQ.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23135.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.22753",
      "authors": [
        {
          "_id": "6863b259588cea0da970c9bf",
          "name": "Jianing Zhang",
          "hidden": false
        },
        {
          "_id": "6863b259588cea0da970c9c0",
          "name": "Jiayi Zhu",
          "hidden": false
        },
        {
          "_id": "6863b259588cea0da970c9c1",
          "name": "Feiyu Ji",
          "hidden": false
        },
        {
          "_id": "6863b259588cea0da970c9c2",
          "name": "Xiaokang Yang",
          "hidden": false
        },
        {
          "_id": "6863b259588cea0da970c9c3",
          "user": {
            "_id": "67761e674467879a54b4624a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hbniDcGynaZIUZTNSgc2G.jpeg",
            "isPro": false,
            "fullname": "Xiaoyun Yuan",
            "user": "XiaoyunYuan",
            "type": "user"
          },
          "name": "Xiaoyun Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-01T10:12:11.708Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T04:48:37.000Z",
      "submittedOnDailyAt": "2025-07-01T08:43:39.113Z",
      "title": "デグレーションモデル付きのマルチパスディフュージョンモデルを用いたチューナブルメランスキーフォトグラフィー",
      "submittedOnDailyBy": {
        "_id": "67761e674467879a54b4624a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hbniDcGynaZIUZTNSgc2G.jpeg",
        "isPro": false,
        "fullname": "Xiaoyun Yuan",
        "user": "XiaoyunYuan",
        "type": "user"
      },
      "summary": "金属フレンズは、超小型化の計算機画像において大きな可能性を持つが、複雑な光学の減衰と計算的なリストアバイディションの難関に直面しています。現在の方法は通常、精密な光学の調整または巨大なペアレッドデータセットを依存していますが、実世界的な画像システムにとっては非単純です。また、推論プロセスの制御の欠陥は、望ましくないハウショックされたアーティファクトが生じることがあります。我々は、預習モデルからの強力な自然画像の先驅を活用し、大きなデータセットを代わりに利用することで、Degradation-Modeled Multipath Diffusionを導入し、適応性のある金属フレンズ写真を実現します。我々のフレームワークは、高周波の詳細の生成、構造的な忠実性、金属フレンズ特有の減衰の抑制、そしてファクトローディングとしての補助として、肯定的、中性的、否定的なプロンプトパスを使用して平衡を調整しています。適応性のあるデコーダーは、忠実性と視覚的な品質の間の制御されたトレードオフを可能にします。また、空間的に変化する減衰に関心のあるアテンション（SVDA）モジュールは、複雑な光学とセンサーによる減衰を適応的にモデル化します。最後に、実世界的な検証のために、ミリメートラースケールのMetaCameraを設計して構築しました。拡大的な結果は、我々のアプローチが最先端の方法を超え、高忠実性とシャープな画像の再構築を実現したことを示しています。追加の材料：https://dmdiff.github.io/",
      "upvotes": 1,
      "discussionId": "6863b259588cea0da970c9c4",
      "projectPage": "https://dmdiff.github.io/",
      "githubRepo": "https://github.com/yuanxy92/DMDiff_ICCV2025",
      "ai_summary": "The proposed Degradation-Modeled Multipath Diffusion framework improves metalens image quality by using natural image priors and specific modules to balance detail, fidelity, and perceptual quality while addressing optical degradation.",
      "ai_keywords": [
        "multipath diffusion",
        "natural image priors",
        "positive-prompt paths",
        "neutral-prompt paths",
        "negative-prompt paths",
        "pseudo data augmentation",
        "tunable decoder",
        "spatially varying degradation-aware attention",
        "SVDA module",
        "high-fidelity image reconstruction"
      ],
      "githubStars": 0
    },
    "publishedAt": "2025-06-28T00:48:37.000Z",
    "title": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography",
    "summary": "Metalenses offer significant potential for ultra-compact computational\nimaging but face challenges from complex optical degradation and computational\nrestoration difficulties. Existing methods typically rely on precise optical\ncalibration or massive paired datasets, which are non-trivial for real-world\nimaging systems. Furthermore, a lack of control over the inference process\noften results in undesirable hallucinated artifacts. We introduce\nDegradation-Modeled Multipath Diffusion for tunable metalens photography,\nleveraging powerful natural image priors from pretrained models instead of\nlarge datasets. Our framework uses positive, neutral, and negative-prompt paths\nto balance high-frequency detail generation, structural fidelity, and\nsuppression of metalens-specific degradation, alongside pseudo data\naugmentation. A tunable decoder enables controlled trade-offs between fidelity\nand perceptual quality. Additionally, a spatially varying degradation-aware\nattention (SVDA) module adaptively models complex optical and sensor-induced\ndegradation. Finally, we design and build a millimeter-scale MetaCamera for\nreal-world validation. Extensive results show that our approach outperforms\nstate-of-the-art methods, achieving high-fidelity and sharp image\nreconstruction. More materials: https://dmdiff.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22753.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67761e674467879a54b4624a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hbniDcGynaZIUZTNSgc2G.jpeg",
      "fullname": "Xiaoyun Yuan",
      "name": "XiaoyunYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.21448",
      "authors": [
        {
          "_id": "68636ecd588cea0da970c905",
          "name": "Huadai Liu",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c906",
          "name": "Jialei Wang",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c907",
          "name": "Kaicheng Luo",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c908",
          "name": "Wen Wang",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c909",
          "name": "Qian Chen",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c90a",
          "name": "Zhou Zhao",
          "hidden": false
        },
        {
          "_id": "68636ecd588cea0da970c90b",
          "name": "Wei Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-26T16:32:06.000Z",
      "submittedOnDailyAt": "2025-07-01T03:47:20.627Z",
      "title": "ThinkSound: 多モデル大語言モデルにおけるChain-of-Thought Reasoningによる音声生成と編集",
      "submittedOnDailyBy": {
        "_id": "63d8c0d3da4f72339241c7dd",
        "avatarUrl": "/avatars/c5852fa7d2b8ffb7a76f0143faa453ef.svg",
        "isPro": false,
        "fullname": "liuhuadai",
        "user": "liuhuadai",
        "type": "user"
      },
      "summary": "ビデオから音声の生成は大幅に向上していますが、高品質の音声を生成し、ビデオ内容の微妙な気づきを真実的に捉えることは難しい。ディズニーエンターテインメント業界の専門家と同様に、この生成は視覚的動作、音響環境、時系列関係などの複雑な理由を理解する必要がある。私たちは、Chain-of-Thought (CoT) 理由を利用して、ビデオのステップごとの相互作用的な音声生成と編集を可能にする新しいフレームワーク「ThinkSound」を紹介します。私たちのアプローチは、3つの補間ステージに分解されています：機能的なフォーリー生成、精度の高いユーザーインタラクションを通じたオブジェクトシーンコンフィーン、自然言語指示によるターゲット化編集。各ステージでは、多モデルの大規模な言語モデルがコンテキスト的に一致するCoT理由を生成し、統一的な音声ベースモデルを指導します。また、AudioCoTという、構造化された理由注釈を持つ詳細なデータセットを紹介します。これは、視覚内容、文脈説明、音声合成の間の連携を確立します。実験は、ThinkSoundはビデオから音声の生成において、どちらも音声評価とCoT評価で最先端の性能を達成し、Movie Gen Audioベンチマークでオフライン分布を超えて優れていることを示しています。デモページは、https://ThinkSound-Project.github.io から利用可能です。",
      "upvotes": 1,
      "discussionId": "68636ecd588cea0da970c90c",
      "projectPage": "https://thinksound-project.github.io/",
      "githubRepo": "https://github.com/liuhuadai/ThinkSound",
      "ai_summary": "ThinkSound, a novel framework, uses Chain-of-Thought reasoning with a multimodal large language model to generate high-quality audio from videos, achieving state-of-the-art results in various benchmarks.",
      "ai_keywords": [
        "Chain-of-Thought (CoT) reasoning",
        "multimodal large language model",
        "unified audio foundation model",
        "AudioCoT",
        "video-to-audio generation",
        "foley generation",
        "object-centric refinement",
        "targeted editing",
        "Audio Metrics",
        "CoT metrics",
        "Movie Gen Audio benchmark"
      ],
      "githubStars": 23
    },
    "publishedAt": "2025-06-26T12:32:06.000Z",
    "title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language\n  Models for Audio Generation and Editing",
    "summary": "While end-to-end video-to-audio generation has greatly improved, producing\nhigh-fidelity audio that authentically captures the nuances of visual content\nremains challenging. Like professionals in the creative industries, such\ngeneration requires sophisticated reasoning about items such as visual\ndynamics, acoustic environments, and temporal relationships. We present\nThinkSound, a novel framework that leverages Chain-of-Thought (CoT) reasoning\nto enable stepwise, interactive audio generation and editing for videos. Our\napproach decomposes the process into three complementary stages: foundational\nfoley generation that creates semantically coherent soundscapes, interactive\nobject-centric refinement through precise user interactions, and targeted\nediting guided by natural language instructions. At each stage, a multimodal\nlarge language model generates contextually aligned CoT reasoning that guides a\nunified audio foundation model. Furthermore, we introduce AudioCoT, a\ncomprehensive dataset with structured reasoning annotations that establishes\nconnections between visual content, textual descriptions, and sound synthesis.\nExperiments demonstrate that ThinkSound achieves state-of-the-art performance\nin video-to-audio generation across both audio metrics and CoT metrics and\nexcels in out-of-distribution Movie Gen Audio benchmark. The demo page is\navailable at https://ThinkSound-Project.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d8c0d3da4f72339241c7dd",
      "avatarUrl": "/avatars/c5852fa7d2b8ffb7a76f0143faa453ef.svg",
      "fullname": "liuhuadai",
      "name": "liuhuadai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]