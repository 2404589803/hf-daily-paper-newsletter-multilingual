[
  {
    "paper": {
      "id": "2506.03569",
      "authors": [
        {
          "_id": "6841003e45e7d8a890731765",
          "name": "Xiaomi LLM-Core Team",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731767",
          "name": "Zihao Yue",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731768",
          "name": "Zhenru Lin",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731769",
          "name": "Yifan Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176a",
          "name": "Weikun Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176b",
          "user": {
            "_id": "60d2e681b8448e1785bbda06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
            "isPro": false,
            "fullname": "Shuhuai Ren",
            "user": "ShuhuaiRen",
            "type": "user"
          },
          "name": "Shuhuai Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:00.497Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176c",
          "user": {
            "_id": "642e72cec1b0f8e4e76af16d",
            "avatarUrl": "/avatars/f900811d3c22a114c67283b646949f86.svg",
            "isPro": false,
            "fullname": "shuhao gu",
            "user": "gsh33",
            "type": "user"
          },
          "name": "Shuhao Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:04.948Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176d",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176e",
          "name": "Peidian Li",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073176f",
          "name": "Liang Zhao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731770",
          "user": {
            "_id": "6038d6d0612f5eef3cc05ea9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
            "isPro": false,
            "fullname": "Lei Li",
            "user": "tobiaslee",
            "type": "user"
          },
          "name": "Lei Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:07.044Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731771",
          "name": "Kainan Bao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731772",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731773",
          "name": "Hailin Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731774",
          "name": "Gang Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731775",
          "user": {
            "_id": "64d2fce8129a210e569e0c76",
            "avatarUrl": "/avatars/a79a832dc3a46ece1b9e542369fc4888.svg",
            "isPro": false,
            "fullname": "Dawei Zhu",
            "user": "dwzhu",
            "type": "user"
          },
          "name": "Dawei Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:02.720Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731776",
          "name": "Cici",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731777",
          "name": "Chenhong He",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731778",
          "name": "Bowen Ye",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731779",
          "name": "Bowen Shen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177a",
          "name": "Zihan Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177b",
          "name": "Zihan Jiang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177c",
          "name": "Zhixian Zheng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177d",
          "name": "Zhichao Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177e",
          "name": "Zhenbo Luo",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073177f",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731780",
          "name": "Yudong Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731781",
          "name": "Yuanyuan Tian",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731782",
          "name": "Yu Tu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731783",
          "name": "Yihan Yan",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731784",
          "name": "Yi Huang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731785",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731786",
          "name": "Xinzhe Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731787",
          "name": "Xingchen Song",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731788",
          "name": "Xing Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731789",
          "name": "Xing Yong",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178a",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178b",
          "name": "Xiangwei Deng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178c",
          "name": "Wenyu Yang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178d",
          "name": "Wenhan Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178e",
          "name": "Weiwei Lv",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073178f",
          "name": "Weiji Zhuang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731790",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731791",
          "name": "Sirui Deng",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731792",
          "name": "Shuo Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731793",
          "name": "Shimao Chen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731794",
          "name": "Shihua Yu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731795",
          "name": "Shaohui Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731796",
          "name": "Shande Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731797",
          "name": "Rui Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731798",
          "name": "Qiantong Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a890731799",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179a",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179b",
          "name": "Menghang Zhu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179c",
          "name": "Kangyang Zhou",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179d",
          "name": "Kang Zhou",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179e",
          "name": "Kai Fang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a89073179f",
          "name": "Jun Shi",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a0",
          "name": "Jinhao Dong",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a1",
          "name": "Jiebao Xiao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a2",
          "name": "Jiaming Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a3",
          "user": {
            "_id": "680e9a219e529f779991be0c",
            "avatarUrl": "/avatars/327b945649192b0881fe290298d10e23.svg",
            "isPro": false,
            "fullname": "Huaqiu Liu",
            "user": "Prestonprom",
            "type": "user"
          },
          "name": "Huaqiu Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:58.279Z",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a4",
          "name": "Hongshen Xu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a5",
          "name": "Heng Qu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a6",
          "name": "Haochen Zhao",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a7",
          "name": "Hanglong Lv",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a8",
          "name": "Guoan Wang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317a9",
          "name": "Duo Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317aa",
          "name": "Dong Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ab",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ac",
          "name": "Chong Ma",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ad",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317ae",
          "name": "Can Cai",
          "hidden": false
        },
        {
          "_id": "6841003e45e7d8a8907317af",
          "name": "Bingquan Xia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T04:32:54.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:27.734Z",
      "title": "MiMo-VL 技術報告",
      "submittedOnDailyBy": {
        "_id": "6038d6d0612f5eef3cc05ea9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
        "isPro": false,
        "fullname": "Lei Li",
        "user": "tobiaslee",
        "type": "user"
      },
      "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.",
      "upvotes": 44,
      "discussionId": "6841004145e7d8a890731853",
      "githubRepo": "https://github.com/XiaomiMiMo/MiMo-VL",
      "ai_summary": "MiMo-VL-7B-SFT and MiMo-VL-7B-RL provide state-of-the-art general visual understanding and multimodal reasoning through four-stage pre-training and Mixed On-policy Reinforcement Learning, outperforming models with up to 78B parameters.",
      "ai_keywords": [
        "vision-language models",
        "multimodal reasoning",
        "four-stage pre-training",
        "Mixed On-policy Reinforcement Learning",
        "MORL",
        "Chain-of-Thought",
        "reproducibility"
      ]
    },
    "publishedAt": "2025-06-04T00:32:54.000Z",
    "title": "MiMo-VL Technical Report",
    "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03569.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6038d6d0612f5eef3cc05ea9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
      "fullname": "Lei Li",
      "name": "tobiaslee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04207",
      "authors": [
        {
          "_id": "684117e22db29aa7b403af8d",
          "name": "Shuang Chen",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af8e",
          "name": "Yue Guo",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af8f",
          "user": {
            "_id": "64264095ba51f8a2136946a0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
            "isPro": false,
            "fullname": "Zhaochen Su",
            "user": "Warrieryes",
            "type": "user"
          },
          "name": "Zhaochen Su",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:45.759Z",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af90",
          "name": "Yafu Li",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af91",
          "name": "Yulun Wu",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af92",
          "user": {
            "_id": "65352acb7139c5dd8d9a8590",
            "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
            "isPro": false,
            "fullname": "JiachengChen",
            "user": "JC-Chen",
            "type": "user"
          },
          "name": "Jiacheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:48:38.463Z",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af93",
          "name": "Jiayu Chen",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af94",
          "name": "Weijie Wang",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af95",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "684117e22db29aa7b403af96",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:51:08.000Z",
      "submittedOnDailyAt": "2025-06-05T02:38:24.366Z",
      "title": "進歩する多モデル論理：優化された冷やかなスタートからステージごとの強化学習",
      "submittedOnDailyBy": {
        "_id": "65352acb7139c5dd8d9a8590",
        "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
        "isPro": false,
        "fullname": "JiachengChen",
        "user": "JC-Chen",
        "type": "user"
      },
      "summary": "ディープセキー-R1の複雑な文脈タスクでの卓越した理由能力を受け続けて、多モデル大語言モデル（MLLMs）に類似した能力を奨励するために、直接的に強化学習（RL）を適用しているワークが多くあります。しかし、複雑な理由を活性化することに難しいのです。この論文では、多モデルRLを孤立して見るよりも、現在のトレーニングパイプラインを詳細に調べ、3つの重要な現象を特定します。1）MLLMの理由能力を向上させるには、効果的な初期化が重要です。興味深いことに、コンテキストデータのみでよりもよく選択されたデータで初期化することで、多モデルRLを行う前にも、最近の多モデル理由モデルの多くのものを超える性能を収めることができます。2）標準的なGRPOを多モデルRLに適用した場合、勾配停止現象が発生し、トレーニングの安定性と性能を低下させます。3）多モデルRLの後についての文だけのRLトレーニングは、多モデル理由能力をさらに向上させます。このステージづけされたトレーニングアプローチは、視覚基礎と認知理由の開発をバランスづけることができます。これらのヒントを統合し、多モデルRLの問題を解決することで、ReVisual-R1を紹介し、MathVerse、MathVision、WeMath、LogicVista、DynaMath、そして挑戦的なAIME2024とAIME2025の難しいベンチマークで、開源7B MLLMsの新たな最先端を達成します。",
      "upvotes": 35,
      "discussionId": "684117e32db29aa7b403afc2",
      "githubRepo": "https://github.com/CSfufu/Revisual-R1"
    },
    "publishedAt": "2025-06-04T13:51:08.000Z",
    "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning",
    "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04207.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65352acb7139c5dd8d9a8590",
      "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
      "fullname": "JiachengChen",
      "name": "JC-Chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04089",
      "authors": [
        {
          "_id": "684153cf911d1b3135fa5dfe",
          "name": "Anastasiia Ivanova",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5dff",
          "user": {
            "_id": "661af24d8328f43c6abc2d11",
            "avatarUrl": "/avatars/afe7eaf1f7a378dbcdba5cd3e86adf9c.svg",
            "isPro": false,
            "fullname": "Eva",
            "user": "tenebrissilvam",
            "type": "user"
          },
          "name": "Eva Bakaeva",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T10:00:02.435Z",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5e00",
          "user": {
            "_id": "64198f70ed725fef6442b37e",
            "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
            "isPro": false,
            "fullname": "Alexey Kovalev",
            "user": "AlexeyKov",
            "type": "user"
          },
          "name": "Zoya Volovikova",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T08:22:39.926Z",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5e01",
          "name": "Alexey K. Kovalev",
          "hidden": false
        },
        {
          "_id": "684153cf911d1b3135fa5e02",
          "name": "Aleksandr I. Panov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T15:47:07.000Z",
      "submittedOnDailyAt": "2025-06-05T07:08:16.935Z",
      "title": "AmbiK: キッチン環境での不明確なタスクのデータセット",
      "submittedOnDailyBy": {
        "_id": "64198f70ed725fef6442b37e",
        "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
        "isPro": false,
        "fullname": "Alexey Kovalev",
        "user": "AlexeyKov",
        "type": "user"
      },
      "summary": "LLMsは、機体化アグェントの一部で、ユーザからの自然言語指令に基づいて行動計画を決定するために通常使用されます。しかし、実世界的な環境での構文不明の指令を処理することは、LLMsにとって難問です。様々な構文不明のタスク検出手法が提案されましたが、それらを比較するのは困難であり、データセットや普遍的なベンチマークがないためです。そこで、AmbiK（Kitchen Environmentでの構文不明のタスク）を提案します。AmbiKは、機械に対する構文不明の指示を対処するための完全な文脈データセットで、LLMsの助けを受けてコレクトされ、人間のバリデーションも行われています。AmbiKは、構文不明のタスクとそのカスタムな対応を含む1000ペアを構成し、構文不明の種類（人間の好み、一般知識、安全性）にカテゴリジーされ、環境説明、説明の質問と答え、ユーザの意気込み、タスク計画を含む2000タスクの総合的なデータセットです。AmbiKは、構文不明の検出手法の統一的な比較を可能にしてランダムです。AmbiKは、https://github.com/cog-model/AmbiK-dataset で利用可能です。",
      "upvotes": 31,
      "discussionId": "684153cf911d1b3135fa5e2e",
      "ai_summary": "AmbiK, a textual dataset of ambiguous instructions for kitchen robots, enables unified comparison of ambiguity detection methods.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "behavior planning",
        "ambiguous instructions",
        "task ambiguity detection",
        "AmbiK",
        "dataset",
        "human-validated",
        "ambiguity types",
        "Human Preferences",
        "Common Sense Knowledge",
        "Safety",
        "environment descriptions",
        "clarifying questions",
        "user intents",
        "task plans"
      ]
    },
    "publishedAt": "2025-06-04T11:47:07.000Z",
    "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment",
    "summary": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04089.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64198f70ed725fef6442b37e",
      "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
      "fullname": "Alexey Kovalev",
      "name": "AlexeyKov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02921",
      "authors": [
        {
          "_id": "683ff4dcfbc9041ef7274c51",
          "user": {
            "_id": "657eea68f4f72f2c4c44640d",
            "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
            "isPro": false,
            "fullname": "Yijun YANG",
            "user": "thomasyyj",
            "type": "user"
          },
          "name": "Yijun Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:53:47.455Z",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c52",
          "name": "Zeyu Huang",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c53",
          "name": "Wenhao Zhu",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c54",
          "name": "Zihan Qiu",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c55",
          "name": "Fei Yuan",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c56",
          "name": "Jeff Z. Pan",
          "hidden": false
        },
        {
          "_id": "683ff4dcfbc9041ef7274c57",
          "name": "Ivan Titov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T14:23:06.000Z",
      "submittedOnDailyAt": "2025-06-05T02:04:55.586Z",
      "title": "長文脈言語モデルの制御可能な検査",
      "submittedOnDailyBy": {
        "_id": "657eea68f4f72f2c4c44640d",
        "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
        "isPro": false,
        "fullname": "Yijun YANG",
        "user": "thomasyyj",
        "type": "user"
      },
      "summary": "現在の長文脈言語モデル（LCLM）評価フレームワークは、実世界タスクと合成タスクに分類されます。両方のアプローチは、それぞれの固有の制限を伴います。実世界タスクは、複雑すぎて解釈や特徴化が困難で、データ汚染に脆弱です。一方、合成タスクは、「ネイド・イン・ザ・ハイスタック」（NIAH）フォーマットを採用し、「ネイド」と「ハイスタック」の非連貫性が、実用的なアプリケーションの代理としての正当性を破壊します。これらの挑戦に対応し、理想的な長文脈評価フレームワークは、3つの基本的な特徴を持つ必要があると主張します：無間断なコンテキスト、制御可能な設定、セキュリティの評価。本研究では、人工的に生成されたビジネスログを使用した新しいベンチマーク「LongBioBench」を紹介し、LCLMの理解、推理、信頼性の評価を行います。\n\n実験的評価では、18つのLCLMを含むもので、複数のモデルが検索結果の語意的理解と基本的推理に欠陥を見出し、文脈長を増やすと信頼性が低下します。進めた分析では、現在の合成ベンチマークで使用されるもののように、コンテキストの非連貫性、数値の「ネイド」、デトラクターのなしなどの設計選択肢は、モデルの長文脈能力を検証する脆弱性を持つことを示します。また、長文脈の継続的な予ち練習は、RoPEエンベディングを長文脈長に適応することで主に調整されていることが明らかになりました。まとめて言えば、前の合成ベンチマークと比較して、LongBioBenchは、真実の言語タスクをミラーしながら制御可能性を維持することでより良いバランスを実現し、高度に解釈可能で設定可能です。",
      "upvotes": 25,
      "discussionId": "683ff4ddfbc9041ef7274c73",
      "githubRepo": "https://github.com/Thomasyyj/LongBio-Benchmark",
      "ai_summary": "LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.",
      "ai_keywords": [
        "long-context language models (LCLM)",
        "real-world tasks",
        "synthetic tasks",
        "needle-in-the-haystack (NIAH)",
        "seamless context",
        "controllable setting",
        "sound evaluation",
        "LongBioBench",
        "semantic understanding",
        "elementary reasoning",
        "trustworthiness",
        "long-context continual pretraining",
        "RoPE embedding"
      ]
    },
    "publishedAt": "2025-06-03T10:23:06.000Z",
    "title": "A Controllable Examination for Long-Context Language Models",
    "summary": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: seamless context, controllable setting, and\nsound evaluation. This study introduces LongBioBench, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\nunderstanding, reasoning, and trustworthiness.\nOur experimental evaluation, which includes 18 LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02921.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657eea68f4f72f2c4c44640d",
      "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
      "fullname": "Yijun YANG",
      "name": "thomasyyj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16968",
      "authors": [
        {
          "_id": "683656aefd55e753bf26ed3e",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T08:58:30.760Z",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed3f",
          "user": {
            "_id": "62676a94dacab364889bb36c",
            "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
            "isPro": false,
            "fullname": "SARIM HASHMI",
            "user": "Sarim-Hash",
            "type": "user"
          },
          "name": "Sarim Hashmi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:49:01.879Z",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed40",
          "user": {
            "_id": "62eaadf4086bd1debb30a122",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62eaadf4086bd1debb30a122/wgxsPVnkOuEfq1oqlUhiB.jpeg",
            "isPro": false,
            "fullname": "Gustavo Stahl",
            "user": "GustavoStahl",
            "type": "user"
          },
          "name": "Gustavo Bertolo Stahl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T08:31:48.782Z",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed41",
          "name": "Seung Hun Eddie Han",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed42",
          "name": "Salman Khan",
          "hidden": false
        },
        {
          "_id": "683656aefd55e753bf26ed43",
          "name": "Abdulrahman Mahmoud",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/T4ESSrZsC7163P3I8p17C.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/WF-SJEyKKtpa3Zq0JvBXA.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Hl8Dkgmc4QL_l9YKPhRvD.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/p-io7OU8TtxwvBp4_M1Hd.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/bu6bpeVfonZgrXopd9f79.png"
      ],
      "publishedAt": "2025-05-22T17:48:53.000Z",
      "submittedOnDailyAt": "2025-06-05T06:33:02.615Z",
      "title": "CASS: Nvidia データ、モデル、ベンチマークを用いた AMD へのトランスプレーション",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "CASSを紹介します。これは最初の大規模なデータセットとモデルシステムで、コンピューターアーキテクチャ間のGPUコードのトランスピレーションを目指しています。これは、ソースレベル（CUDA ⇔ HIP）とアセンブリレベル（Nvidia SASS ⇔ AMD RDNA3）の翻訳を対象としています。このデータセットは、70kの検証されたコードペアを含み、ホストとデバイスの両方において、低レベルのGPUコードのポータビリティの重要な欠陥を解決しています。このリソースを活用して、CASSフamillyのドメイン専門言語モデルを訓練し、95%のソース翻訳精度と37.5%のアセンブリ翻訳精度を達成し、GPT-4o、Claude、Hipifyなどの商業ベースラインを大幅に超えました。生成されたコードは、85%以上のテストケースでネーティブ性能を保ち、実行時間とメモリバランスを維持しています。厳密な評価のために、CASS-Benchを紹介します。これは、16ゲームタイプのGPU領域を拡張したカスタマイズされたベンチマークで、真の実行結果を含むものです。すべてのデータ、モデル、評価ツールは、GPUコンパイラツーリング、バイナリーバイアス、LLMガイドされたハードウェア翻訳の進歩を促進するために、オープンソースでリリースされています。データセットとベンチマークは、https://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}}にあり、コードは、https://github.com/GustavoStahl/CASS{blue{GitHub}}にあります。",
      "upvotes": 23,
      "discussionId": "683656b0fd55e753bf26edf7",
      "githubRepo": "https://github.com/GustavoStahl/CASS",
      "ai_summary": "CASS is a dataset and model suite for GPU code transpilation at both source and assembly levels, achieving high accuracy and performance matching with native code.",
      "ai_keywords": [
        "cross-architecture GPU code transpilation",
        "CASS",
        "CUDA",
        "HIP",
        "Nvidia SASS",
        "AMD RDNA3",
        "domain-specific language models",
        "source translation accuracy",
        "assembly translation accuracy",
        "native performance",
        "CASS-Bench",
        "GPU compiler tooling",
        "binary compatibility",
        "LLM-guided hardware translation"
      ]
    },
    "publishedAt": "2025-05-22T13:48:53.000Z",
    "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
    "summary": "We introduce CASS, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level (CUDA\nleftrightarrow HIP) and assembly-level (Nvidia SASS leftrightarrow AMD\nRDNA3) translation. The dataset comprises 70k verified code pairs across host\nand device, addressing a critical gap in low-level GPU code portability.\nLeveraging this resource, we train the CASS family of domain-specific language\nmodels, achieving 95% source translation accuracy and 37.5% assembly\ntranslation accuracy, substantially outperforming commercial baselines such as\nGPT-4o, Claude, and Hipify. Our generated code matches native performance in\nover 85% of test cases, preserving runtime and memory behavior. To support\nrigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16\nGPU domains with ground-truth execution. All data, models, and evaluation tools\nare released as open source to foster progress in GPU compiler tooling, binary\ncompatibility, and LLM-guided hardware translation. Dataset and benchmark are\non\nhttps://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}},\nwith code at\nhttps://github.com/GustavoStahl/CASS{blue{GitHub}}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/T4ESSrZsC7163P3I8p17C.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/WF-SJEyKKtpa3Zq0JvBXA.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Hl8Dkgmc4QL_l9YKPhRvD.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/p-io7OU8TtxwvBp4_M1Hd.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/bu6bpeVfonZgrXopd9f79.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16968.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 39
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04180",
      "authors": [
        {
          "_id": "6840fefb3098ab525906d852",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d853",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d854",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqiang Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:09.305Z",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d855",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "6840fefb3098ab525906d856",
          "name": "Roy Ka-Wei Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:27:42.000Z",
      "submittedOnDailyAt": "2025-06-05T00:58:27.883Z",
      "title": "スーパーライター：反省をもった長文生成モデル",
      "submittedOnDailyBy": {
        "_id": "64ed568ccf6118a9379a61b8",
        "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
        "isPro": false,
        "fullname": "Yushi Bai",
        "user": "bys0318",
        "type": "user"
      },
      "summary": "長文生成は、大規模言語モデル（LLMs）にとって長期的な挑戦であり、特に、長文生成の連貫性の維持、論理的な一貫性の確保、シーケンス長の増加に伴いの文質の保全について重要な課題です。これらの制限を解決するために、私たちはSuperWriter-Agentを提案します。SuperWriter-Agentは、長文生成の品質と一貫性を向上させるために設計されたアガントベースフレームワークです。SuperWriter-Agentは生成パイプラインに明示的な構造化された思考および改善ステージを導入し、モデルをより謹重なおよび認知的に基づいた過程に沿って導くようにします。このフレームワークに基づいて、私たちは7BのSuperWriter-LMを訓練するための規範付き微調節データセットを構築し、また、階層的なDirect Preference Optimization（DPO）プロセスを開発します。これは、モンテカルロ木探索（MCTS）を用いて最終的な品質評価を伝播し、各生成ステップを適切に最適化するものです。多様なベンチマークでの実験結果から、SuperWriter-LMは状態の最先端の性能を達成し、自動的な評価および人間の評価でも、より大規模なベースモデルを超えることが証明されました。また、詳細な消去試験は、階層的なDPOの効果性を示し、長文生成の品質向上において構造化された思考ステップの挙動を含める値を強調します。",
      "upvotes": 20,
      "discussionId": "6840fefc3098ab525906d89c",
      "ai_summary": "SuperWriter-Agent enhances long-form text generation by integrating structured planning and refinement, achieving top performance with a 7B model and hierarchical Direct Preference Optimization.",
      "ai_keywords": [
        "agent-based framework",
        "structured thinking-through planning",
        "refinement stages",
        "SuperWriter-Agent",
        "SuperWriter-LM",
        "hierarchical Direct Preference Optimization",
        "Monte Carlo Tree Search",
        "DPO",
        "automatic evaluation",
        "human evaluation",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-06-04T13:27:42.000Z",
    "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models",
    "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04180.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ed568ccf6118a9379a61b8",
      "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
      "fullname": "Yushi Bai",
      "name": "bys0318",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01320",
      "authors": [
        {
          "_id": "684124368cb0edba3ab8f738",
          "name": "Taehoon Yoon",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f739",
          "name": "Yunhong Min",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f73a",
          "name": "Kyeongmin Yeo",
          "hidden": false
        },
        {
          "_id": "684124368cb0edba3ab8f73b",
          "name": "Minhyuk Sung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T05:02:33.000Z",
      "submittedOnDailyAt": "2025-06-05T03:38:33.152Z",
      "title": "Ψ-Sampler: 初期粒子サンプリングによるSMC基盤での推論時の報酬の調整を行うスコアモデル",
      "submittedOnDailyBy": {
        "_id": "66ee81b676a8038cb42c8caa",
        "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
        "isPro": false,
        "fullname": "Yunhong Min",
        "user": "myhong",
        "type": "user"
      },
      "summary": "Psi-Samplerを紹介します。これは、スコアベースの生成モデルとの推論時の報酬調整に効果的な対応を提供するSMCベースのフレームワークで、pCNLベースの初期粒子サンプリングを採用しています。スコアベースの生成モデルとの推論時の報酬調整は、後学習最適化のプラグライド変更からの広範囲的なパラダイム変更により最近によく取り扱われています。この潮流の核心は、シーケンシャル・モンテカルロ法（SMC）の応用となりますが、現在の方法は通常はガウス先驅から粒子を初期化し、報酬に関係のある領域を不十分に捉え、サンプリング効率を低下させています。報酬に関心のある後駆を初期化することで、調整性能を大幅に向上させることを示します。高次元の潜在空間で後駆サンプリングを可能にするために、preconditioned Crank-Nicolson Langevin（pCNL）アルゴリズムを紹介し、次元に対応した提案と勾配に基づく動力学を組み合わせています。このアプローチは、効率的かつスケーラブルな後駆サンプリングを可能にし、ラウエアーフォーム生成、量覚的な生成、美術的な好み生成など、様々な報酬調整タスクで積極的な性能向上を示します。",
      "upvotes": 15,
      "discussionId": "6841243c8cb0edba3ab8f8bf",
      "ai_summary": "The framework $\\Psi$-Sampler uses SMC with pCNL for efficient posterior sampling and reward alignment in score-based generative models, enhancing performance across various tasks.",
      "ai_keywords": [
        "SMC-based framework",
        "pCNL-based initial particle sampling",
        "inference-time reward alignment",
        "score-based generative model",
        "Sequential Monte Carlo",
        "denoising process",
        "Gaussian prior",
        "reward-aware posterior",
        "preconditioned Crank-Nicolson Langevin",
        "layout-to-image generation",
        "quantity-aware generation",
        "aesthetic-preference generation"
      ]
    },
    "publishedAt": "2025-06-02T01:02:33.000Z",
    "title": "Ψ-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models",
    "summary": "We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based\ninitial particle sampling for effective inference-time reward alignment with a\nscore-based generative model. Inference-time reward alignment with score-based\ngenerative models has recently gained significant traction, following a broader\nparadigm shift from pre-training to post-training optimization. At the core of\nthis trend is the application of Sequential Monte Carlo (SMC) to the denoising\nprocess. However, existing methods typically initialize particles from the\nGaussian prior, which inadequately captures reward-relevant regions and results\nin reduced sampling efficiency. We demonstrate that initializing from the\nreward-aware posterior significantly improves alignment performance. To enable\nposterior sampling in high-dimensional latent spaces, we introduce the\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines\ndimension-robust proposals with gradient-informed dynamics. This approach\nenables efficient and scalable posterior sampling and consistently improves\nperformance across various reward alignment tasks, including layout-to-image\ngeneration, quantity-aware generation, and aesthetic-preference generation, as\ndemonstrated in our experiments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01320.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ee81b676a8038cb42c8caa",
      "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
      "fullname": "Yunhong Min",
      "name": "myhong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04225",
      "authors": [
        {
          "_id": "68413366adeec0116d071af2",
          "user": {
            "_id": "63425d394c9a81858b36aeb5",
            "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
            "isPro": false,
            "fullname": "Tianyu Huang",
            "user": "tyhuang",
            "type": "user"
          },
          "name": "Tianyu Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:41.198Z",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af3",
          "name": "Wangguandong Zheng",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af4",
          "name": "Tengfei Wang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af5",
          "name": "Yuhao Liu",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af6",
          "name": "Zhenwei Wang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af7",
          "name": "Junta Wu",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af8",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071af9",
          "name": "Hui Li",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afa",
          "name": "Rynson W. H. Lau",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afb",
          "name": "Wangmeng Zuo",
          "hidden": false
        },
        {
          "_id": "68413366adeec0116d071afc",
          "name": "Chunchao Guo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63425d394c9a81858b36aeb5/cjZH2kR6B3y9IAmmRHNJS.mp4"
      ],
      "publishedAt": "2025-06-04T17:59:04.000Z",
      "submittedOnDailyAt": "2025-06-05T06:00:26.815Z",
      "title": "Voyager: 長距離と世界一致のビデオディフュージョンを用いた可探索的3Dスケーン生成",
      "submittedOnDailyBy": {
        "_id": "63425d394c9a81858b36aeb5",
        "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
        "isPro": false,
        "fullname": "Tianyu Huang",
        "user": "tyhuang",
        "type": "user"
      },
      "summary": "実世界的アプリケーションでは、ビデオゲームやバーチュアルリアルなどには、ユーザーがカメラのタロイに沿って3Dスケーンを探索できるような3Dスケーンのモデリング能力が求められます。文句からもしくは画像から3Dオブジェクトを生成することには進展がありましたが、長距離的な、3D一致的な、探索可能な3Dスケーンの作成は複雑で難しい問題です。本稿では、ユーザーが定めたカメラのパスを元に1枚の画像から世界一致的な3Dポイントクラスターシーケンスを生成する新しいビデオディフュージョンフレームワークを紹介します。Voyagerという名前をつけています。現在のアプローチと異なり、Voyagerはフレーム毎の固有の一致性を持つ終始一致的なスケーンの生成と再構成を実現し、3D再構成プイプライン（例：構造からの動作や多視点ステレオ）の必要性を削減します。私たちの方法は3つのキーコンポーネントを組み合わせています：1) 世界一致的なビデオディフュージョン：統一的なアーキテクチャで、現存する世界観察に基づいて一致的なRGBと深さのビデオシーケンスを生成し、グローバル的な一致性を確保します。2) 長距離的世界探索：効率的な世界キャッシュと点の削減と自動回帰推論と平滑なビデオサンプリングを用いたイテレーティブ的なスケーン拡大とコンテキストによる一致性を実現します。3) 可換性のあるデータエンジン：任意のビデオに対するカメラの姿勢推定とメトリックの深さ予測を自動化し、大規模的な、多様なトレーニングデータの採集を可能にし、手動の3Dアノテーションを必要とさせません。これらの設計は、現在の方法に比べて視覚質量とジェネリック精度に明らかな向上を実現し、多様なアプリケーションに適用できます。",
      "upvotes": 14,
      "discussionId": "6841336badeec0116d071c2b",
      "projectPage": "https://voyager-world.github.io",
      "githubRepo": "https://github.com/Voyager-World/Voyager",
      "ai_summary": "Voyager is a video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image, enabling long-range, consistent 3D scene exploration with user-defined camera paths.",
      "ai_keywords": [
        "video diffusion",
        "world-consistent video diffusion",
        "3D point-cloud sequences",
        "camera path",
        "end-to-end scene generation",
        "consistent frames",
        "unified architecture",
        "RGB and depth video sequences",
        "world observation",
        "global coherence",
        "long-range world exploration",
        "world cache",
        "point culling",
        "auto-regressive inference",
        "smooth video sampling",
        "scene extension",
        "context-aware consistency",
        "scalable data engine",
        "camera pose estimation",
        "metric depth prediction",
        "large-scale",
        "diverse training data"
      ]
    },
    "publishedAt": "2025-06-04T13:59:04.000Z",
    "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation",
    "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63425d394c9a81858b36aeb5/cjZH2kR6B3y9IAmmRHNJS.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63425d394c9a81858b36aeb5",
      "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
      "fullname": "Tianyu Huang",
      "name": "tyhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04228",
      "authors": [
        {
          "_id": "684103aed45a1fc5540ddc10",
          "name": "Sihui Ji",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc11",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc12",
          "user": {
            "_id": "644a1b6401e18bf93a6f45c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
            "isPro": false,
            "fullname": "xichen",
            "user": "xichenhku",
            "type": "user"
          },
          "name": "Xi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:50.027Z",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc13",
          "name": "Yuanpeng Tu",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc14",
          "name": "Yiyang Wang",
          "hidden": false
        },
        {
          "_id": "684103aed45a1fc5540ddc15",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T17:59:58.000Z",
      "submittedOnDailyAt": "2025-06-05T01:11:16.967Z",
      "title": "LayerFlow: 層に関心のあるビデオ生成のユニットモデル",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "LayerFlowは、レイヤ情報を認識した映像生成の統一ソリューションです。レイヤ毎のプロンプトを与えると、LayerFlowは透明なフォントロック、クリーンなバックグラウンド、およびブレンドされたシーンの映像を生成します。また、ブレンドされた映像の分解や、与えられたフォントロックのバックグラウンドの生成など、多様なバージョンもサポートしています。文脈からビデオディフュージョントランスフォーマーを始め、各レイヤの映像をサブクリップとして整理し、レイヤエンベディングを利用して各クリップとレイヤ毎のプロンプトを区別します。このように、前述のバージョンを一つの統一フレームワークで無難にサポートできます。高品質のレイヤ毎の訓練ビデオの不足により、高品質のレイヤ注釈付きの静的画像に対応するために、多段階訓練戦略を設計します。特に、最初に低品質のビデオデータでモデルを訓練し、次に動作のLoRAを調整して静的のフレームに対応させ、その後、高品質のレイヤ毎の画像とコピーペーストされたビデオデータの混合によるデータで内容のLoRAを訓練します。推論時に、動作のLoRAを削除し、望ましいレイヤの平滑なビデオを生成します。",
      "upvotes": 12,
      "discussionId": "684103b0d45a1fc5540ddca8",
      "ai_summary": "LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.",
      "ai_keywords": [
        "LayerFlow",
        "text-to-video diffusion transformer",
        "layer embeddings",
        "sub-clips",
        "multi-stage training strategy",
        "motion LoRA",
        "content LoRA",
        "layered images",
        "copy-pasted video data",
        "smooth videos"
      ]
    },
    "publishedAt": "2025-06-04T13:59:58.000Z",
    "title": "LayerFlow: A Unified Model for Layer-aware Video Generation",
    "summary": "We present LayerFlow, a unified solution for layer-aware video generation.\nGiven per-layer prompts, LayerFlow generates videos for the transparent\nforeground, clean background, and blended scene. It also supports versatile\nvariants like decomposing a blended video or generating the background for the\ngiven foreground and vice versa. Starting from a text-to-video diffusion\ntransformer, we organize the videos for different layers as sub-clips, and\nleverage layer embeddings to distinguish each clip and the corresponding\nlayer-wise prompts. In this way, we seamlessly support the aforementioned\nvariants in one unified framework. For the lack of high-quality layer-wise\ntraining videos, we design a multi-stage training strategy to accommodate\nstatic images with high-quality layer annotations. Specifically, we first train\nthe model with low-quality video data. Then, we tune a motion LoRA to make the\nmodel compatible with static frames. Afterward, we train the content LoRA on\nthe mixture of image data with high-quality layered images along with\ncopy-pasted video data. During inference, we remove the motion LoRA thus\ngenerating smooth videos with desired layers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04228.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 41
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03139",
      "authors": [
        {
          "_id": "683fb0a7be8421eda3152283",
          "user": {
            "_id": "676b86e79ff0244316f7202f",
            "avatarUrl": "/avatars/3e1d26312a96752356895ab88eeb3ce0.svg",
            "isPro": false,
            "fullname": "chensiqi",
            "user": "xiaoooobai",
            "type": "user"
          },
          "name": "Siqi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:28:25.792Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152284",
          "name": "Xinyu Dong",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152285",
          "user": {
            "_id": "6692aff88db712bad780f02a",
            "avatarUrl": "/avatars/5dc4b1c27c70f6a64864711dbff4910f.svg",
            "isPro": false,
            "fullname": "xhl",
            "user": "zjuxhl",
            "type": "user"
          },
          "name": "Haolei Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:42.705Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152286",
          "name": "Xingyu Wu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152287",
          "name": "Fei Tang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152288",
          "name": "Hang Zhang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda3152289",
          "user": {
            "_id": "64098738342c26884c792c93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
            "isPro": false,
            "fullname": "Yuchen Yan",
            "user": "yanyc",
            "type": "user"
          },
          "name": "Yuchen Yan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T08:56:47.244Z",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228a",
          "name": "Linjuan Wu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228b",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228c",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228d",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228e",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "683fb0a7be8421eda315228f",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:58:57.000Z",
      "submittedOnDailyAt": "2025-06-05T03:44:24.728Z",
      "title": "SVGenius: SVG理解、編集および生成におけるLLMのベンチマーク",
      "submittedOnDailyBy": {
        "_id": "5e1058e9fcf41d740b69966d",
        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
        "isPro": false,
        "fullname": "Yongliang Shen",
        "user": "tricktreat",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）と多模態LLMsは、SVG処理において有望な能力を示していますが、現在のベンチマークは実世界的なカバー面積が限られ、複雑さの分類がなく、評価パラダイムが分断されています。我々は、理解、編集、生成の3つの進歩的な次元にわたる2,377件のクエリを含む、実世界的データから構築された、システマティックな複雑さの分類を備えた、SVGeniusという厳密なベンチマークを紹介します。このベンチマークは8つのタスクカテゴリーと18つのメトリックを用いてモデルを評価します。22つの主流モデル（それぞれのスケール、アーキテクチャ、学習パラダイム、アクセス可能なレベルを異なる）を評価します。分析により、開放ソースモデルに対しては、プロピエートモデルが显著に優位を取りますが、すべてのモデルは複雑さが増加するにつれてシステミックな性能低下を示し、現在のアプローチにおける基本的な制限を示していますが、理由論を強化した学習は、これらの制限を克服するための純粋なスケーリングよりもより効果的ですが、すべてのモデル種類で最も難しい能力としてスタイルトランスファーがあります。SVGeniusは、SVG処理の最初の厳密な評価フレームワークを構築し、ベクターグラフィックモデルの開発と自動化グラフィックデザインアプリケーションの進歩に重要なインサイトを提供します。追加の資料と補足データ（データとコードを含む）は、https://zju-real.github.io/SVGeniusから利用できます。",
      "upvotes": 12,
      "discussionId": "683fb0a7be8421eda31522ca",
      "projectPage": "https://zju-real.github.io/SVGenius/",
      "githubRepo": "https://github.com/ZJU-REAL/SVGenius",
      "ai_summary": "SVGenius evaluates Large Language Models and Multimodal LLMs for SVG processing using a comprehensive benchmark across three dimensions: understanding, editing, and generation, revealing insights into model capabilities and limitations.",
      "ai_keywords": [
        "Large Language Models",
        "Multimodal LLMs",
        "SVG processing",
        "SVGenius",
        "complexity stratification",
        "reasoning-enhanced training",
        "style transfer"
      ]
    },
    "publishedAt": "2025-06-03T13:58:57.000Z",
    "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
    "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03139.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e1058e9fcf41d740b69966d",
      "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
      "fullname": "Yongliang Shen",
      "name": "tricktreat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03295",
      "authors": [
        {
          "_id": "6840e7d81fadbc85ae3bdc0f",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc10",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc11",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc12",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "6840e7d81fadbc85ae3bdc13",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T18:35:52.000Z",
      "submittedOnDailyAt": "2025-06-05T02:48:14.083Z",
      "title": "批判によって、事前学習されたLLMの理由論的な潛力を解放する\n  問題に対しての微調整",
      "submittedOnDailyBy": {
        "_id": "636a35eff8d9af4aea181608",
        "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
        "isPro": false,
        "fullname": "yubo",
        "user": "ubowang",
        "type": "user"
      },
      "summary": "私たちは、強力なLLMs（例：Qwen-Math、MiMo、Phi-4）が学習前階段から継承した巨大な理由論的な可能性を証拠していることを見た。強化学習（RL）を用いることで、これらのモデルは理由論的タスクにおいて大幅に改善することができる。最近の研究によると、単一の問題に対するRLもこのモデルの理由論的能力を解放することができることが示されている。しかし、RLはそれほど費用がかかり、安定していない。それだけシンプルなRLも、数百ヒープウェア時間を必要とする。これにより、重要な質問が生じる：これらの強力な基盤モデルの理由論的の可能性を解放するためには、より効率的な方法があるか？この研究では、そのような方法を示し、単一の問題に対するCritique Fine-Tuning（CFT）がLLMsの理由論的の可能性を効果的に解放することを示している。私たちの方法は、単一の問題に対する多様なモデル生成された解決策を集め、教師モデルLLMsが詳細な批判を提供することにより、批判データを構築する。QwenやLlamaファミリーモデル（パラメータ数：1.5Bから14B）をCFTデータに対して微調節し、多様な理由論的タスクで明らかに性能の向上を見出した。例えば、5ヒープウェア時間の訓練でも、Qwen-Math-7B-CFTは6つの数学ベンチマークで平均15%の向上、3つのロジック理由ベンチマークで16%の向上を示した。これらの結果は、20倍の計算量を減らしたRLの結果と比較的なものであるか、それよりも良く、一つの問題に対するCFTの効果性と一般性を示している。消滅調査により、一つの問題に対するCFTの強固さが異なるプロンプト問題においても示されている。これらの結果は、現代のLLMsの理由論的能力を解放するためには、簡単で計算量に余裕のある手法であることを示している。",
      "upvotes": 10,
      "discussionId": "6840e7d81fadbc85ae3bdc45",
      "ai_summary": "Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.",
      "ai_keywords": [
        "Critique Fine-Tuning",
        "teacher LLMs",
        "Qwen-Math",
        "Llama family models",
        "reasoning tasks",
        "one-shot CFT",
        "performance gains",
        "logic reasoning benchmarks",
        "math benchmarks",
        "prompt problems"
      ]
    },
    "publishedAt": "2025-06-03T14:35:52.000Z",
    "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem",
    "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636a35eff8d9af4aea181608",
      "avatarUrl": "/avatars/d9c5cf3491243d1f2b1c5df1873ee8e7.svg",
      "fullname": "yubo",
      "name": "ubowang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.24500",
      "authors": [
        {
          "_id": "683fb063ef97de05eb2a44cc",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44cd",
          "name": "Xing Gao",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44ce",
          "name": "Yuchuan Wu",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44cf",
          "name": "Xiang Huang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d0",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d1",
          "name": "Zhe Zheng",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d2",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d3",
          "name": "Jialu Du",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d4",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d5",
          "name": "Yongbin Li",
          "hidden": false
        },
        {
          "_id": "683fb063ef97de05eb2a44d6",
          "name": "Weiming Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-30T12:01:06.000Z",
      "submittedOnDailyAt": "2025-06-05T00:45:50.911Z",
      "title": "時間認知ハイパーコーナー強化学習：時系列情報を認知するハイパーコーナー強化学習への応用",
      "submittedOnDailyBy": {
        "_id": "67c03110e8c7d56a8e135ac8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
        "isPro": false,
        "fullname": "Hou",
        "user": "Guiyang1001",
        "type": "user"
      },
      "summary": "最近、大語言モデル（LLMs）は数学やコーディングなどIQ関連の領域で慎重な思考が必要な分野で顕著な進歩を遂げています。しかし、LLMsの認知開発を社会領域に特にトレーニング後の観点から高めることはまだ調査不足しています。\n\n社会の世界は別の時間線を持ち、直感的な反応（System 1）と表面レベルの思考から慎重な思考（System 2）の豊富な組み合わせが必要となることが数学よりも明らかです。数学は主にSystem 2の認知（慎重な、ステップごとの理由論）を依存しているため、これらの認知モードの組み合わせが必要となることがわかります。これに対して、我々は、社会知能を高めるための時間に関する認知の強化学習（TimeHC-RL）を導入します。\n\n実験では、8つのデータセットにおいて5つのトレーニング後のパラダイムと2つのテスト時のインターベーションパラダイムを通じて、LLMsの社会知能を向上させ、TimeHC-RLの方法の効果を評価しました。実験結果から、我々が提案したTimeHC-RL方法が広く採用されているSystem 2 RL方法に比べて上位の性能を示していることが明らかになりました。この方法は7Bベースモデルを翼にし、DeepSeek-R1やOpenAI-O3の先進モデルとの性能を比べることができるようにしました。また、トレーニング後とテスト時のインターベーションから社会知能を向上させるためのシステマティックな探索は、数多くの有價値なフィードバックを提供しました。",
      "upvotes": 10,
      "discussionId": "683fb064ef97de05eb2a452b",
      "ai_summary": "Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.",
      "ai_keywords": [
        "Large Language Models",
        "Temporal-aware Hierarchical Cognitive Reinforcement Learning",
        "TimeHC-RL",
        "System 1",
        "System 2",
        "RL",
        "DeepSeek-R1",
        "OpenAI-O3"
      ]
    },
    "publishedAt": "2025-05-30T08:01:06.000Z",
    "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence",
    "summary": "Recently, Large Language Models (LLMs) have made significant progress in\nIQ-related domains that require careful thinking, such as mathematics and\ncoding. However, enhancing LLMs' cognitive development in social domains,\nparticularly from a post-training perspective, remains underexplored.\nRecognizing that the social world follows a distinct timeline and requires a\nricher blend of cognitive modes (from intuitive reactions (System 1) and\nsurface-level thinking to deliberate thinking (System 2)) than mathematics,\nwhich primarily relies on System 2 cognition (careful, step-by-step reasoning),\nwe introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning\n(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we\nsystematically explore improving LLMs' social intelligence and validate the\neffectiveness of the TimeHC-RL method, through five other post-training\nparadigms and two test-time intervention paradigms on eight datasets with\ndiverse data patterns. Experimental results reveal the superiority of our\nproposed TimeHC-RL method compared to the widely adopted System 2 RL method. It\ngives the 7B backbone model wings, enabling it to rival the performance of\nadvanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic\nexploration from post-training and test-time interventions perspectives to\nimprove LLMs' social intelligence has uncovered several valuable insights.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24500.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67c03110e8c7d56a8e135ac8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
      "fullname": "Hou",
      "name": "Guiyang1001",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04158",
      "authors": [
        {
          "_id": "6840fb71d4e16ff5f95108aa",
          "name": "Yujia Hu",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ab",
          "name": "Songhua Liu",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ac",
          "name": "Zhenxiong Tan",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ad",
          "user": {
            "_id": "634cfebc350bcee9bed20a4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
            "isPro": false,
            "fullname": "Xingyi Yang",
            "user": "adamdad",
            "type": "user"
          },
          "name": "Xingyi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:11.256Z",
          "hidden": false
        },
        {
          "_id": "6840fb71d4e16ff5f95108ae",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/tp0cLz8OZhdI3vs50cxhF.jpeg"
      ],
      "publishedAt": "2025-06-04T16:57:24.000Z",
      "submittedOnDailyAt": "2025-06-05T00:38:20.484Z",
      "title": "Image Editing は、Diffusion Models を使用するプログラムです。",
      "submittedOnDailyBy": {
        "_id": "634cfebc350bcee9bed20a4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
        "isPro": false,
        "fullname": "Xingyi Yang",
        "user": "adamdad",
        "type": "user"
      },
      "summary": "ディフュージョンモデルは、テキストから画像生成において驚異的な成功を収めていますが、インストラクション駆動の画像編集においては、大きな課題を見出します。本研究では、重要な課題を明らかにしています：これらのモデルは、大幅なレイアウト変更を含む構造的に不一致的な編集に特に難しくなります。この隙を補うために、Image Editing As Programs (IEAP) という、Diffusion Transformer (DiT) アーキテクチャに基づく統一された画像編集フレームワークを介しています。IEAPの核心では、インストラクションドライブ編集を簡約主義的な視点から扱い、複雑な編集指示を原子操作の列に分解しています。各操作は、同じDiTバックボーンを共有し、特定の編集の種類に専門化されています。ビジョン-言語モデル(VLM)に基づくアガントがプログラミングされ、これらの操作は、任意的で構造的に不一致的な変換をサポートします。このようにモジュール化し、編集を順序付けすることで、IEAPは、簡単な調整から大幅な構造的変更まで幅広く編集タスクに強固に一般化します。拡張された実験は、標準ベンチマーク上で、IEAPは多様な編集スケーナrioにおいて最先端の方法を大幅に超えることを示しています。これらの評価では、我々のフレームワークは、特に複雑な、多段階の指示に対して、上位の精度と語意的忠実性を提供します。コードは、https://github.com/YujiaHu1109/IEAP にアクセス可能です。",
      "upvotes": 8,
      "discussionId": "6840fb73d4e16ff5f9510950",
      "projectPage": "https://yujiahu1109.github.io/IEAP/",
      "githubRepo": "https://github.com/YujiaHu1109/IEAP",
      "ai_summary": "A unified image editing framework, IEAP, built on Diffusion Transformer (DiT) decomposes complex editing instructions into operations performed by vision-language models for robust editing across various tasks.",
      "ai_keywords": [
        "diffusion models",
        "text-to-image generation",
        "instruction-driven image editing",
        "structurally inconsistent edits",
        "Image Editing As Programs (IEAP)",
        "Diffusion Transformer (DiT)",
        "atomic operations",
        "lightweight adapter",
        "vision-language model (VLM)",
        "modularizing edits"
      ]
    },
    "publishedAt": "2025-06-04T12:57:24.000Z",
    "title": "Image Editing As Programs with Diffusion Models",
    "summary": "While diffusion models have achieved remarkable success in text-to-image\ngeneration, they encounter significant challenges with instruction-driven image\nediting. Our research highlights a key challenge: these models particularly\nstruggle with structurally inconsistent edits that involve substantial layout\nchanges. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a\nunified image editing framework built upon the Diffusion Transformer (DiT)\narchitecture. At its core, IEAP approaches instructional editing through a\nreductionist lens, decomposing complex editing instructions into sequences of\natomic operations. Each operation is implemented via a lightweight adapter\nsharing the same DiT backbone and is specialized for a specific type of edit.\nProgrammed by a vision-language model (VLM)-based agent, these operations\ncollaboratively support arbitrary and structurally inconsistent\ntransformations. By modularizing and sequencing edits in this way, IEAP\ngeneralizes robustly across a wide range of editing tasks, from simple\nadjustments to substantial structural changes. Extensive experiments\ndemonstrate that IEAP significantly outperforms state-of-the-art methods on\nstandard benchmarks across various editing scenarios. In these evaluations, our\nframework delivers superior accuracy and semantic fidelity, particularly for\ncomplex, multi-step instructions. Codes are available at\nhttps://github.com/YujiaHu1109/IEAP.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/tp0cLz8OZhdI3vs50cxhF.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04158.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634cfebc350bcee9bed20a4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
      "fullname": "Xingyi Yang",
      "name": "adamdad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04141",
      "authors": [
        {
          "_id": "684106fc8cb0edba3ab212bb",
          "name": "Kejian Zhu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bc",
          "name": "Zhuoran Jin",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bd",
          "name": "Hongbang Yuan",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212be",
          "name": "Jiachun Li",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212bf",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c0",
          "name": "Pengfei Cao",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c1",
          "name": "Yubo Chen",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c2",
          "name": "Kang Liu",
          "hidden": false
        },
        {
          "_id": "684106fc8cb0edba3ab212c3",
          "name": "Jun Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/_-WD0IU9jQMInqhXgGfUP.jpeg"
      ],
      "publishedAt": "2025-06-04T16:33:41.000Z",
      "submittedOnDailyAt": "2025-06-05T04:38:26.132Z",
      "title": "MMR-V: どれが言わずに残されているのか？ ビデオでの多模構造の深い理由論のベンチマーク",
      "submittedOnDailyBy": {
        "_id": "648c48d8c0ddeee6df5b6d22",
        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
        "isPro": false,
        "fullname": "Shangqing Tu",
        "user": "tsq2000",
        "type": "user"
      },
      "summary": "ビデオの順序的構造は、多モデル大語言モデル（MLLM）が多フレームの証拠を特定し、多モデル論理を行う能力に挑戦しています。しかし、現在のビデオベンチマークは主に理解タスクに焦点を当てているため、モデルが問題に述べられたフレーム（以下「問題フレーム」と呼ばれる）と隣接する少なくとも数フレームを認識するだけで十分であることを求めています。この欠点を解決するために、私たちはMMR-V（ビデオ中の多モデル深い論理のベンチマーク）を提案します。このベンチマークは以下の特徴を持っています。 （1）長距離、多フレーム論理：モデルは、問題フレームから遠くの証拠フレームを推論し、分析する必要があります。 （2）認識よりも遠い：問題は直接の認識では解決できなく、隠された情報を論理する必要があります。 （3）信頼性：すべてのタスクは手動でアノテーションされ、広くの実世界的なユーザーの理解を参照して、一般的な認識に合わせています。 （4）混同性：謹重に設計されたデトラクターアノテーション戦略を用いて、モデルのショートカットを減らすことでした。MMR-Vは317ビデオと1,257タスクから成るものです。私たちの実験により、現在のモデルは多モデル論理に難しいことがわかり、最も優れたモデルであるo4-miniもその精度は52.5%でした。また、現在の論理拡張戦略（Chain-of-Thoughtとテストタイムコンピュートのスケーリング）は限られた効果を示しました。進ける分析により、多モデル論理に必要なChain-of-Thoughtは文字論理のものと異なることがわかり、これが性能の限界などの理由の一部を説明しています。私たちはMMR-Vが多モデル論理能力の向上についての進展を促していただきたいと思います。",
      "upvotes": 7,
      "discussionId": "684106ff8cb0edba3ab21374",
      "ai_summary": "A new benchmark, MMR-V, is proposed to challenge multimodal large language models with long-range, multi-frame reasoning and hidden information processing in videos, revealing their limitations and inspiring further research.",
      "ai_keywords": [
        "multimodal large language models",
        "MMR-V",
        "long-range",
        "multi-frame reasoning",
        "multimodal reasoning",
        "manual annotation",
        "distractor annotation strategy",
        "Chain-of-Thought"
      ]
    },
    "publishedAt": "2025-06-04T12:33:41.000Z",
    "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos",
    "summary": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/_-WD0IU9jQMInqhXgGfUP.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04141.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03930",
      "authors": [
        {
          "_id": "6841090145662bb7d322ecc6",
          "user": {
            "_id": "64de37ee5e192985054be575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
            "isPro": false,
            "fullname": "Yuansheng Ni",
            "user": "yuanshengni",
            "type": "user"
          },
          "name": "Yuansheng Ni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T10:00:04.196Z",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc7",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc8",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecc9",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "6841090145662bb7d322ecca",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T13:24:44.000Z",
      "submittedOnDailyAt": "2025-06-05T05:54:39.464Z",
      "title": "VisCoder: LLMの微調節による実行可能なPython可視化コードの生成",
      "submittedOnDailyBy": {
        "_id": "64de37ee5e192985054be575",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
        "isPro": false,
        "fullname": "Yuansheng Ni",
        "user": "yuanshengni",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、ディアグラムやチャートの描画などの可視化タスクには、コードの正確性と可視的語義の両方を考慮しなければならない。現在のインストラクションチューニングデータセットは、実行に基づくサポートを欠け、反復的なコード修正に限られて、脆弱で不信頼なプロット生成になる。私たちは、Pythonベースの可視化と自己修正に向けた大規模なインストラクションチューニングデータセット「VisCode-200K」を紹介します。これは、2つのソースからの200K例以上を含み、以下のような内容を含む：1）開放ソースリポジトリからの検証された描画コード、自然言語インストラクションと描画されたプロットの組み合わせ、2）Code-Feedbackからの45K回の複数ターンの修正ダイアログ、これらをモデルが運行時のフィードバックを用いてフィラーショルドなコードを修正することを可能にします。Qwen2.5-Coder-InstructをVisCode-200Kに微調節して「VisCoder」を作成し、PandasPlotBenchで評価しました。VisCoderは強力な開放ソースベースの基準と比較して显著に優れ、GPT-4o-miniのようなプロプライティモデルの性能に近づきます。また、反復的な修正を評価するために自らのデバッグ評価プロトコルを採用し、実行可能で可視的に正確なコード生成におけるフィードバック駆動学習のベータを示します。",
      "upvotes": 7,
      "discussionId": "6841090245662bb7d322ed1f",
      "projectPage": "https://tiger-ai-lab.github.io/VisCoder/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VisCoder",
      "ai_summary": "VisCode-200K, a large-scale dataset for visualization, improves plot generation performance by integrating execution-grounded supervision and iterative code correction, outperforming open-source models and rivaling proprietary ones.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "visualization tasks",
        "plot generation",
        "execution-grounded supervision",
        "iterative code correction",
        "VisCode-200K",
        "Python-based visualization",
        "validated plotting code",
        "natural language instructions",
        "rendered plots",
        "correction dialogues",
        "Qwen2.5-Coder-Instruct",
        "VisCoder",
        "PandasPlotBench",
        "self-debug evaluation",
        "feedback-driven learning"
      ]
    },
    "publishedAt": "2025-06-04T09:24:44.000Z",
    "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation",
    "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03930.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64de37ee5e192985054be575",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
      "fullname": "Yuansheng Ni",
      "name": "yuanshengni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03517",
      "authors": [
        {
          "_id": "68412f853c22997c7329f3a0",
          "user": {
            "_id": "62980664ff0acd7e027d6686",
            "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
            "isPro": false,
            "fullname": "Ziyi Wu",
            "user": "Dazitu616",
            "type": "user"
          },
          "name": "Ziyi Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:43.314Z",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a1",
          "name": "Anil Kag",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a2",
          "name": "Ivan Skorokhodov",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a3",
          "name": "Willi Menapace",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a4",
          "name": "Ashkan Mirzaei",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a5",
          "name": "Igor Gilitschenski",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a6",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "68412f853c22997c7329f3a7",
          "name": "Aliaksandr Siarohin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T03:06:08.000Z",
      "submittedOnDailyAt": "2025-06-05T04:18:57.242Z",
      "title": "DenseDPO: ビデオの細かい時間的偏好最適化",
      "submittedOnDailyBy": {
        "_id": "62980664ff0acd7e027d6686",
        "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
        "isPro": false,
        "fullname": "Ziyi Wu",
        "user": "Dazitu616",
        "type": "user"
      },
      "summary": "直接偏好最適化（Direct Preference Optimization, DPO）は、文から映画の拡散モデルのトレーニング後の技術として最近に適用されています。トレーニングデータを得るためには、2つの独立したノイズから生成された映画の好みを提供することを要求されます。しかし、このアプローチは細かい比較を禁止し、動きの少ないクリップにバイアスを与えることを指摘します。この研究では、DenseDPOという方法を導入し、これらの欠点を解決するために3つの貢献を提供します。まず、DPOでは、真の映画のノイズをコーラストしたものを用いてビデオペアを作成します。これにより、運動構造が類似していて、局所的な詳細が異なるようなアラインされたペアを生成し、動きのバイアスを中和します。次に、これによって得られた時間的なアラインメントを利用して、短いセグメントでの好みをラベル付けすることで、より稠密で正確な学習信号を得ます。ラベルされたデータの1/3だけで、DenseDPOは動きの生成を大幅に向上させ、文のアラインメント、視覚的な質、時間的な一貫性についてはベージャDPOと同じ性能を維持します。最後に、DenseDPOはオフショットビジョン言語モデル（VLMs）を使用して自動的な好みラベルを生成することを示します：GPTはタスクに特化された訓練された映画報酬モデルと同様に段階レベルの好みを予測し、これによって訓練されたDenseDPOは人間のラベルを使用した場合と近い性能を達成します。",
      "upvotes": 7,
      "discussionId": "68412f8a3c22997c7329f4ff",
      "projectPage": "https://snap-research.github.io/DenseDPO/"
    },
    "publishedAt": "2025-06-03T23:06:08.000Z",
    "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video\n  Diffusion Models",
    "summary": "Direct Preference Optimization (DPO) has recently been applied as a\npost-training technique for text-to-video diffusion models. To obtain training\ndata, annotators are asked to provide preferences between two videos generated\nfrom independent noise. However, this approach prohibits fine-grained\ncomparisons, and we point out that it biases the annotators towards low-motion\nclips as they often contain fewer visual artifacts. In this work, we introduce\nDenseDPO, a method that addresses these shortcomings by making three\ncontributions. First, we create each video pair for DPO by denoising corrupted\ncopies of a ground truth video. This results in aligned pairs with similar\nmotion structures while differing in local details, effectively neutralizing\nthe motion bias. Second, we leverage the resulting temporal alignment to label\npreferences on short segments rather than entire clips, yielding a denser and\nmore precise learning signal. With only one-third of the labeled data, DenseDPO\ngreatly improves motion generation over vanilla DPO, while matching it in text\nalignment, visual quality, and temporal consistency. Finally, we show that\nDenseDPO unlocks automatic preference annotation using off-the-shelf Vision\nLanguage Models (VLMs): GPT accurately predicts segment-level preferences\nsimilar to task-specifically fine-tuned video reward models, and DenseDPO\ntrained on these labels achieves performance close to using human labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03517.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62980664ff0acd7e027d6686",
      "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
      "fullname": "Ziyi Wu",
      "name": "Dazitu616",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04142",
      "authors": [
        {
          "_id": "684132cb725b7fb67f68ffb8",
          "name": "Kejian Zhu",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffb9",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffba",
          "name": "Zhuoran Jin",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbb",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbc",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "684132cb725b7fb67f68ffbd",
          "name": "Jun Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T16:33:44.000Z",
      "submittedOnDailyAt": "2025-06-05T04:32:19.273Z",
      "title": "信頼性のあるLLM評価を実現するための短絡ニューロン分析",
      "submittedOnDailyBy": {
        "_id": "648c48d8c0ddeee6df5b6d22",
        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
        "isPro": false,
        "fullname": "Shangqing Tu",
        "user": "tsq2000",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）の開発には信頼性のある評価が重要ですが、現在の評価は多くは公開ベンチマークに依存しており、データのコンテマイション問題に陥り、公平性を大きく影響します。先行研究は、コンテマイション問題を解決するために動的なベンチマークの構築に焦点を当てていましたが、新しいベンチマークの連続的な構築は高額で循環的です。本論文では、コンテマイションを解決するために、コンテマイションされたモデルの機構を分析することで問題を解決しようとしています。実験では、コンテマイションされたモデルの過大評価は、トレーニング中にパラメータがショートカット解決策を取得したことによるものであることが見出されました。さらに、比較的アナライザと因果アナライザを用いて短路ニューロンを特定する新しい方法を提案しました。これにより、短路ニューロンを抑制するための評価方法「短路ニューロンパッチ」を導入しました。実験は、このアプローチがコンテマイションを軽減することを証明しました。また、最近公開された信頼性のあるベンチマークMixEvalとの評価結果は強い線形相関を示し、シューパーマンの係数（rho）が0.95を超えました。この高い相関は、このアプローチがモデルの真の能力を真実に示すことを示し、信頼性を持つことを意味します。また、このアプローチの一般化性能を示すために、各種のベンチマークとパラメータ設定の範囲で進めた追加の実験も行いました。コード：https://github.com/GaryStack/Trustworthy-Evaluation",
      "upvotes": 6,
      "discussionId": "684132cc725b7fb67f68fff5",
      "githubRepo": "https://github.com/GaryStack/Trustworthy-Evaluation",
      "ai_summary": "A method called shortcut neuron patching identifies and suppresses shortcut neurons in language models to mitigate data contamination issues in trustworthy evaluations.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "trustworthy evaluation",
        "data contamination",
        "benchmarks",
        "dynamic benchmarks",
        "shortcut solutions",
        "shortcut neurons",
        "comparative analysis",
        "causal analysis",
        "shortcut neuron patching",
        "MixEval",
        "Spearman coefficient"
      ]
    },
    "publishedAt": "2025-06-04T12:33:44.000Z",
    "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
    "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient (rho)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648c48d8c0ddeee6df5b6d22",
      "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
      "fullname": "Shangqing Tu",
      "name": "tsq2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04108",
      "authors": [
        {
          "_id": "684104a16b106ae42f5acc1a",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1b",
          "name": "Tianzhu Ye",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1c",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1d",
          "name": "Yuqing Xia",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1e",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc1f",
          "name": "Yizhao Gao",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc20",
          "name": "Shijie Cao",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc21",
          "name": "Jianyong Wang",
          "hidden": false
        },
        {
          "_id": "684104a16b106ae42f5acc22",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T16:01:48.000Z",
      "submittedOnDailyAt": "2025-06-05T01:16:28.444Z",
      "title": "Rectified Sparse Attention",
      "submittedOnDailyBy": {
        "_id": "6300ef4779c5ddbc6cf83e1a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661005591657-noauth.jpeg",
        "isPro": false,
        "fullname": "Yutao Sun",
        "user": "sunyt32",
        "type": "user"
      },
      "summary": "Efficientな長文頭生成は、大規模言語モデルにとって重要な課題です。最近のスパースデコーディング方法は、効率を向上させることができますが、KVキャッシュの不対称性問題に苦戦し、近似誤差が積み上がり生成質量が低下します。本稿では、ブロックスパースアテンションと周期的な密な補正を組み合わせた簡単で効果的な方法であるRectified Sparse Attention (ReSA)を提案します。ReSAは、固定間隔で密なフローワードパスを使用してKVキャッシュを更新することで、誤差の積み上がりを制限し、予ち学習分布との対応を保持します。数学論理、言語モデリング、検索タスクの各種実験では、ReSAは近似無失誤の生成質量を実現し、大幅に効率向上を収めました。特に、256K文長での解碼では、2.42倍の終端からのスピードアップを達成し、可損失な長文脈推論の実用的な解決策となります。コードは、https://aka.ms/ReSA-LM から利用可能です。",
      "upvotes": 6,
      "discussionId": "684104a26b106ae42f5acc50",
      "ai_summary": "Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.",
      "ai_keywords": [
        "sparse decoding",
        "KV cache misalignment",
        "Rectified Sparse Attention",
        "ReSA",
        "block-sparse attention",
        "dense rectification",
        "pretraining distribution",
        "long-context inference"
      ]
    },
    "publishedAt": "2025-06-04T12:01:48.000Z",
    "title": "Rectified Sparse Attention",
    "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42times end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04108.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6300ef4779c5ddbc6cf83e1a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661005591657-noauth.jpeg",
      "fullname": "Yutao Sun",
      "name": "sunyt32",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03099",
      "authors": [
        {
          "_id": "684107c6142b5c0b4226025f",
          "name": "Chetwin Low",
          "hidden": false
        },
        {
          "_id": "684107c6142b5c0b42260260",
          "name": "Weimin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:29:28.000Z",
      "submittedOnDailyAt": "2025-06-05T01:30:00.782Z",
      "title": "TalkingMachines: 時間帯の音声ドライブフェイチャースタイルのビデオをディフフェレーションモデルで実現する",
      "submittedOnDailyBy": {
        "_id": "62b43ffec624a43b1a1ada46",
        "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
        "isPro": false,
        "fullname": "weimin wang ",
        "user": "weiminwang",
        "type": "user"
      },
      "summary": "この論文では、TalkingMachinesという効率的なフレームワークを提案します。これは、予った学習されたビデオ生成モデルを音声駆動のキャラクターアニメーションモデルに変換するものです。TalkingMachinesは、音声大語言モデル（LLM）と我々のビデオ生成基盤モデルを統合することで、自然な対話的な経験を可能にします。私たちの主な貢献は以下の通りです。\n\n(1) 予ったSOTAの画像からビデオのDiTを音声駆動のアバター生成モデルに適用し、18億パラメーターを持つものを作成しました。\n(2) 対向的な教師モデルからの非対称的な知識の経験を受けることで、誤りの積み上げを防ぎ、無限にビデオストリーミングを可能にしました。\n(3) 高ハイプロフロー、低ラテンシーの推論パイプラインを設計し、以下のようなキーの工学最適化を採用しました。\n(a) DiTとVAEデコーダーを別々のデバイスで分離します。\n(b) CUDAストリームを使用して、デバイス間のコミュニケーションと計算を効率的に重なります。\n(c) 重複の再計算を削減し、フレーム生成のトランフローを最大化します。\n\nデモビデオはこちらで見ることができます - https://aaxwaz.github.io/TalkingMachines/",
      "upvotes": 6,
      "discussionId": "684107c8142b5c0b42260293",
      "projectPage": "https://aaxwaz.github.io/TalkingMachines/",
      "githubRepo": "https://github.com/aaxwaz/TalkingMachines",
      "ai_summary": "TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.",
      "ai_keywords": [
        "DiT",
        "audio large language model",
        "asymmetric knowledge distillation",
        "bidirectional teacher model",
        "sparse causal",
        "autoregressive student model",
        "inference pipeline",
        "CUDA streams",
        "frame-generation throughput"
      ]
    },
    "publishedAt": "2025-06-03T13:29:28.000Z",
    "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models",
    "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03099.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b43ffec624a43b1a1ada46",
      "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
      "fullname": "weimin wang ",
      "name": "weiminwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.02592",
      "authors": [
        {
          "_id": "684104c89ec96d9991484c24",
          "user": {
            "_id": "65309a1d657ae56cdb65e0e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
            "isPro": false,
            "fullname": "Zhi-Yuan Chen",
            "user": "JaxChen",
            "type": "user"
          },
          "name": "Zhi-Yuan Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:45:29.221Z",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c25",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c26",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c27",
          "name": "Enrui Hu",
          "hidden": false
        },
        {
          "_id": "684104c89ec96d9991484c28",
          "name": "Yankai Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T08:12:47.000Z",
      "submittedOnDailyAt": "2025-06-05T01:30:17.419Z",
      "title": "表面より深く：LLMの判断での自らの好みの測定",
      "submittedOnDailyBy": {
        "_id": "65309a1d657ae56cdb65e0e7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
        "isPro": false,
        "fullname": "Zhi-Yuan Chen",
        "user": "JaxChen",
        "type": "user"
      },
      "summary": "最近の研究によると、大規模な言語モデル（LLMs）は、自らの回答を判定者として扱うときに自優位性バイアスを示すことがあることが分かった。これは、他のモデルから生成された回答よりも自らの回答を好ましくする傾向を示していることを意味します。現在の方法は通常、判定者モデルが自らの回答に割り当てるスコアと他のモデルから生成された回答に割り当てるスコアの差を計算してこのバイアスを測定しています。しかし、このアプローチは、判定者モデルが生成した高品質な回答によっても、バイアスがない場合も正のスコアの差が生じることを認めているため、回答の品質とバイアスを混同させています。この問題に対処するために、我々は実際の回答の品質を代理とした「金判定」を介し、判定者モデルが自らの回答に割り当てるスコアと対応する金判定のスコアの差をDBGスコアとして提案します。金判定は真の回答の品質を反映しているため、DBGスコアは回答の品質がバイアス測定において混同させる影響を減らします。DBGスコアを用いて、我々はLLMsの異なるバージョン、サイズ、理由論の能力による自優位性バイアスを評価するための検討を行います。また、我々は自優位性バイアスを影響し、それを緩和するために影響する2つの要因を調査し、回答のテキストスタイルと判定者モデルの後学習データを検討します。最後に、我々は注意ベースの観点から自優位性バイアスの潜在的な機構を調査します。我々のコードとデータは、https://github.com/zhiyuanc2001/self-preference に公開されています。",
      "upvotes": 6,
      "discussionId": "684104c99ec96d9991484c5e",
      "githubRepo": "https://github.com/zhiyuanc2001/self-preference",
      "ai_summary": "The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.",
      "ai_keywords": [
        "large language models",
        "self-preference bias",
        "judge model",
        "gold judgments",
        "DBG score",
        "response quality",
        "attention-based perspective"
      ]
    },
    "publishedAt": "2025-06-03T04:12:47.000Z",
    "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments",
    "summary": "Recent studies show that large language models (LLMs) exhibit self-preference\nbias when serving as judges, meaning they tend to favor their own responses\nover those generated by other models. Existing methods typically measure this\nbias by calculating the difference between the scores a judge model assigns to\nits own responses and those it assigns to responses from other models. However,\nthis approach conflates self-preference bias with response quality, as\nhigher-quality responses from the judge model may also lead to positive score\ndifferences, even in the absence of bias. To address this issue, we introduce\ngold judgments as proxies for the actual quality of responses and propose the\nDBG score, which measures self-preference bias as the difference between the\nscores assigned by the judge model to its own responses and the corresponding\ngold judgments. Since gold judgments reflect true response quality, the DBG\nscore mitigates the confounding effect of response quality on bias measurement.\nUsing the DBG score, we conduct comprehensive experiments to assess\nself-preference bias across LLMs of varying versions, sizes, and reasoning\nabilities. Additionally, we investigate two factors that influence and help\nalleviate self-preference bias: response text style and the post-training data\nof judge models. Finally, we explore potential underlying mechanisms of\nself-preference bias from an attention-based perspective. Our code and data are\navailable at https://github.com/zhiyuanc2001/self-preference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02592.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65309a1d657ae56cdb65e0e7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
      "fullname": "Zhi-Yuan Chen",
      "name": "JaxChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03106",
      "authors": [
        {
          "_id": "684104d9ee7646c073776b2e",
          "name": "Xiaoying Zhang",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b2f",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b30",
          "user": {
            "_id": "666e91b1623133f1ce35acc5",
            "avatarUrl": "/avatars/cc78520e6cfb83817c1d0c1ac867ebdd.svg",
            "isPro": false,
            "fullname": "YipengZhang",
            "user": "YipengZhang",
            "type": "user"
          },
          "name": "Yipeng Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:45:46.851Z",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b31",
          "name": "Kaituo Feng",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b32",
          "name": "Chaochao Lu",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b33",
          "name": "Chao Yang",
          "hidden": false
        },
        {
          "_id": "684104d9ee7646c073776b34",
          "name": "Helen Meng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T17:39:02.000Z",
      "submittedOnDailyAt": "2025-06-05T01:16:53.836Z",
      "title": "Critique-GRPO: 自然言語と数値のフィードバックを用いたLLM論理推理の進歩",
      "submittedOnDailyBy": {
        "_id": "67079840a9bcb7459b8d2a46",
        "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
        "isPro": false,
        "fullname": "Kaituo Feng",
        "user": "KaituoFeng",
        "type": "user"
      },
      "summary": "最近の数値的フィードバック（例：スカラーリュード）による強化学習（RL）の進展は、大規模な言語モデル（LLMs）の複雑な理由論能力を大幅に向上させました。この成功にもかかわらず、我々は、数値フィードバックのみを使用したRLに遭遇する3つの重要な課題を特定しました：性能の平坦化、自己反省の限界性、長期的な失敗。また、RLで微調節されたモデルは、性能の平坦化が現れた後でも、自然言語のフィードバック（評価の形）を利用して、長期的な失敗問題に対して正しい微調節を生成できることを示しました。この見通しに基づき、我々は、自然言語と数値のフィードバックを統合した効果的な政策最適化のオンラインRLフレームワーク、Critique-GRPOを提案しました。Critique-GRPOは、LLMsが初期の回答と評価による改良を同時に学習しながら探索を維持することを可能にします。Qwen2.5-7B-BaseとQwen3-8B-Baseを使用した拡張ならびに、8つの難しい数学、STEM、一般的な理由論タスクでの検証により、Critique-GRPOは、観覧学習ベースとRLベースの微調節アプローチを経由して、平均pass@1スコアを約4.5%と5%に向上させ、これらのベース線に超えました。特に、Critique-GRPOは、オンラインRLによる専門家の示唆を含む強いベース線を超えました。進ける分析から、政策探索についての2つの重要な見通しが明らかになりました：（1）高いエントロピーは探索からの効率的な学習を確保することは常に保証されない、（2）長い回答は探索の効果的さを引き起こすことは常に保証されない。",
      "upvotes": 5,
      "discussionId": "684104daee7646c073776b88",
      "githubRepo": "https://github.com/zhangxy-2019/critique-GRPO",
      "ai_summary": "Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "large language models",
        "LLMs",
        "scalar rewards",
        "performance plateaus",
        "self-reflection",
        "persistent failures",
        "natural language feedback",
        "critiques",
        "policy optimization",
        "Qwen2.5-7B-Base",
        "Qwen3-8B-Base",
        "pass@1",
        "policy exploration",
        "entropy",
        "response length"
      ]
    },
    "publishedAt": "2025-06-03T13:39:02.000Z",
    "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
    "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03106.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67079840a9bcb7459b8d2a46",
      "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
      "fullname": "Kaituo Feng",
      "name": "KaituoFeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21541",
      "authors": [
        {
          "_id": "6840f79ceb249b555b244efc",
          "name": "Zitong Wang",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244efd",
          "name": "Hang Zhao",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244efe",
          "name": "Qianyu Zhou",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244eff",
          "name": "Xuequan Lu",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244f00",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "6840f79ceb249b555b244f01",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T16:08:04.000Z",
      "submittedOnDailyAt": "2025-06-05T00:21:34.798Z",
      "title": "DiffDecompose: アルファ合成画像のレイヤー毎の分解によるディフュージョンツレンマーの分解",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "ディフュージョンモデルは最近、物体除去などの多くの生成タスクで非常に成功しています。しかし、現在の画像分解手法は、マスクプロイダー依存性、静的物体仮定、データセットの不足により、半透明または透明な層の遮蔽を解離することが難しいです。本論文では、新しいタスクに取り組みます：Alpha合成画像の層ごとの分解、半透明/透明なalpha層の非線形遮蔽の状況で、重なった画像から構成要素の層を復元することを目指します。層の不明確性、一般化、データの不足に対処するために、まずAlphaBlendを紹介します。AlphaBlendは、最初の大規模な高品質なデータセットで、透明と半透明な層の分解に対応し、6つの実世界的なサブタスクをサポートします（例：半透明なフレアの除去、半透明なセルの分解、ガラス器具の分解）。このデータセットに基づいて、DiffDecomposeを提出します。DiffDecomposeは、入力画像、セマンティックプロンプト、ブレンディングタイプに基づいて可能な層の分解の後項を学習します。直接アルファマットを予測するより、DiffDecomposeはIn-Context Decompositionを行い、モデルが一つまたは複数の層を予測することを可能にし、Layer Position Encoding Cloningを導入して、層間でピクセルレベルの対応を維持します。提案されたAlphaBlendデータセットと公開されたLOGOデータセットにおいて、DiffDecomposeの効果を評価します。コードとデータセットは、論文の受容後に提供されます。コードは以下のURLから利用できます：https://github.com/Wangzt1121/DiffDecompose。",
      "upvotes": 5,
      "discussionId": "6840f7a1eb249b555b244ffe",
      "ai_summary": "DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.",
      "ai_keywords": [
        "diffusion models",
        "diffusion Transformer",
        "posterior",
        "semantic prompts",
        "blending type",
        "In-Context Decomposition",
        "Layer Position Encoding Cloning",
        "AlphaBlend dataset",
        "translucent flare removal",
        "semi-transparent cell decomposition",
        "glassware decomposition",
        "LOGO dataset"
      ]
    },
    "publishedAt": "2025-05-24T12:08:04.000Z",
    "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers",
    "summary": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21541.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03956",
      "authors": [
        {
          "_id": "6841396eee27975702b57e87",
          "user": {
            "_id": "6759546743971eff5a12a087",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/esJm_83zW1R6NqWltof8P.png",
            "isPro": false,
            "fullname": "Aojun Lu",
            "user": "Kurt1024",
            "type": "user"
          },
          "name": "Aojun Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:29.156Z",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e88",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e89",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:31.411Z",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e8a",
          "name": "Chunhui Ding",
          "hidden": false
        },
        {
          "_id": "6841396eee27975702b57e8b",
          "name": "Yanan Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T13:46:33.000Z",
      "submittedOnDailyAt": "2025-06-05T05:00:41.771Z",
      "title": "Adapt before Continual Learning",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "継続的学習（CL）は、神経ネットワークが新しい知識を増やしながら既存の知識を保持することを目指しています（可塑性）。予ったモデル（PTMs）はCLに重要な役割を果たしていますが、現行のアプローチでは、PTMの本体を固定して安定性を保ち、可塑性を制限し、増分タスクで大きなドメイン間違いを遭遇する場合に特に問題があります。一方で、PTM全体を順次微調節することは、拡散的な知識の忘れ物になるリスクを見せ、安定性と可塑性の間の重要なトレードオフを暴露します。この挑戦に対処するために、私たちは、CLの核心プロセス前にPTM本体を改良する新しいフレームワーク「Adapting PTMs before the core CL process (ACL)」を提案します。ACLは、各新しいタスクを学習する前に、既存のCLアプローチ（例：プロンプトチューニング）を用いてプラッグとパートナーシップのアダプターフェイズでPTM本体を改良します。ACLは、理論的および実験的に示されるように、安定性と可塑性をバランスするために、埋め込みを元のクラスプロトタイプと一致させながら他のクラスから離れることで可塑性を向上させます。拡張された実験は、ACLがベンチマークと統合された方法でCLの性能を大幅に向上させることを示し、PTMベースのCLの広範囲的な解決策を提供します。",
      "upvotes": 4,
      "discussionId": "6841396eee27975702b57eb7",
      "projectPage": "https://github.com/byyx666/ACL_code",
      "githubRepo": "https://github.com/byyx666/ACL_code",
      "ai_summary": "Adapting Pre-trained Models before the core CL process (ACL) improves Continual Learning by enhancing plasticity while maintaining stability.",
      "ai_keywords": [
        "Continual Learning",
        "CL",
        "Pre-trained models",
        "PTMs",
        "plasticity",
        "stability",
        "domain gaps",
        "catastrophic forgetting",
        "prompt tuning",
        "embeddings",
        "class prototypes"
      ]
    },
    "publishedAt": "2025-06-04T09:46:33.000Z",
    "title": "Adapt before Continual Learning",
    "summary": "Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). While pre-trained models (PTMs) have become pivotal in CL,\nprevailing approaches freeze the PTM backbone to preserve stability, limiting\ntheir plasticity, particularly when encountering significant domain gaps in\nincremental tasks. Conversely, sequentially finetuning the entire PTM risks\ncatastrophic forgetting of generalizable knowledge, exposing a critical\nstability-plasticity trade-off. To address this challenge, we propose Adapting\nPTMs before the core CL process (ACL), a novel framework that refines the PTM\nbackbone through a plug-and-play adaptation phase before learning each new task\nwith existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by\naligning embeddings with their original class prototypes while distancing them\nfrom others, theoretically and empirically shown to balance stability and\nplasticity. Extensive experiments demonstrate that ACL significantly improves\nCL performance across benchmarks and integrated methods, offering a versatile\nsolution for PTM-based CL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03956.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03448",
      "authors": [
        {
          "_id": "684105479060432bf302b432",
          "name": "Bimsara Pathiraja",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b433",
          "user": {
            "_id": "622d2ff38d04fd29a9ccf1a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
            "isPro": false,
            "fullname": "Maitreya Patel",
            "user": "mpatel57",
            "type": "user"
          },
          "name": "Maitreya Patel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:48.135Z",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b434",
          "name": "Shivam Singh",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b435",
          "name": "Yezhou Yang",
          "hidden": false
        },
        {
          "_id": "684105479060432bf302b436",
          "name": "Chitta Baral",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T23:20:24.000Z",
      "submittedOnDailyAt": "2025-06-05T01:19:11.023Z",
      "title": "RefEdit: 参照表現に基づくイメージ編集モデルの向上に関するベンチマークと手法",
      "submittedOnDailyBy": {
        "_id": "622d2ff38d04fd29a9ccf1a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
        "isPro": false,
        "fullname": "Maitreya Patel",
        "user": "mpatel57",
        "type": "user"
      },
      "summary": "最近の反転および指示ベースの画像編集技術の進歩にもとづいても、現在の手法は主に単一の主な物体の編集に優れていますが、複雑なスキームに多数の存在を含む場合には大きな問題を抱えます。この隙間を定量化するために、私たちはRefEdit-Benchを紹介します。これはRefCOCOに基づいた厳密な実世界ベンチマークで、萬のサンプルを訓練したベースラインもほとんど優れていません。この制限を克服するために、私たちはRefEditを紹介します。これは、私たちのスケーラブルな合成データ生成パイプラインを用いて訓練された指示ベースの編集モデルです。RefEditは、20,000の編集トライプレットを訓練しても、萬のデータを訓練したFlux/SD3モデルベースのベースラインよりも優れています。複数のベンチマークでの拡張的な評価により、私たちのモデルは参照表現タスクにも優れているだけでなく、伝統的なベンチマークの性能を向上させ、クローズドサースコープの方法と同等の最先端の結果を実現します。私たちは、再現性のためのデータとチェックポイントをリリースします。",
      "upvotes": 4,
      "discussionId": "6841054b9060432bf302b559",
      "projectPage": "https://refedit.vercel.app",
      "githubRepo": "https://github.com/bimsarapathiraja/refedit",
      "ai_summary": "RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.",
      "ai_keywords": [
        "RefEdit-Bench",
        "RefCOCO",
        "instruction-based editing model",
        "scalable synthetic data generation pipeline",
        "Flux/SD3",
        "state-of-the-art results"
      ]
    },
    "publishedAt": "2025-06-03T19:20:24.000Z",
    "title": "RefEdit: A Benchmark and Method for Improving Instruction-based Image\n  Editing Model on Referring Expressions",
    "summary": "Despite recent advances in inversion and instruction-based image editing,\nexisting approaches primarily excel at editing single, prominent objects but\nsignificantly struggle when applied to complex scenes containing multiple\nentities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous\nreal-world benchmark rooted in RefCOCO, where even baselines trained on\nmillions of samples perform poorly. To overcome this limitation, we introduce\nRefEdit -- an instruction-based editing model trained on our scalable synthetic\ndata generation pipeline. Our RefEdit, trained on only 20,000 editing triplets,\noutperforms the Flux/SD3 model-based baselines trained on millions of data.\nExtensive evaluations across various benchmarks demonstrate that our model not\nonly excels in referring expression tasks but also enhances performance on\ntraditional benchmarks, achieving state-of-the-art results comparable to\nclosed-source methods. We release data \\& checkpoint for reproducibility.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622d2ff38d04fd29a9ccf1a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
      "fullname": "Maitreya Patel",
      "name": "mpatel57",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03355",
      "authors": [
        {
          "_id": "68415c5cce09e3eca94e9839",
          "name": "Elias Abad Rocamora",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983a",
          "user": {
            "_id": "6310a6bb0a43f97f6c5567d3",
            "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
            "isPro": false,
            "fullname": "Christian Schlarmann",
            "user": "chs20",
            "type": "user"
          },
          "name": "Christian Schlarmann",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T09:59:48.051Z",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983b",
          "name": "Naman Deep Singh",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983c",
          "name": "Yongtao Wu",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983d",
          "name": "Matthias Hein",
          "hidden": false
        },
        {
          "_id": "68415c5cce09e3eca94e983e",
          "name": "Volkan Cevher",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T19:57:09.000Z",
      "submittedOnDailyAt": "2025-06-05T07:29:30.561Z",
      "title": "ドメインの両方での強固性：CLIPは強固なテキストエンコーダーが必要",
      "submittedOnDailyBy": {
        "_id": "6310a6bb0a43f97f6c5567d3",
        "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
        "isPro": false,
        "fullname": "Christian Schlarmann",
        "user": "chs20",
        "type": "user"
      },
      "summary": "敵対的入力攻撃がCLIPエンベディングを大幅に変化させることができます。これは、プラインワークにCLIPを挟むモデルの下流の強固性に影響を与えます。例えば、文から画像の生成モデルまたは大規模な視覚言語モデルなどです。CLIP画像エンコーダーの強固性に向けては、いくつかの努力が行われていますが、文エンコーダーの強固性は未知の範囲です。本稿では、この文献の欠点を補完します。LEAF（Locally-Enhanced Adversarial Fine-tuning）を提案します。LEAFは、文領域に対して効率的な敵対的微調練習の方法で、大規模なCLIPモデルにもスケール可能です。我々のモデルは、文領域の零ショット敵対的精度を大幅に向上させ、強固な画像エンコーダーが提供する視覚性能を維持することを可能にします。文から画像の拡散モデルと組み合わせて、敵対的ノイズの下での生成質量を向上させます。多タイプレビュー取り扱いタスクで我々の強固なCLIPエンコーダーを使用すると、標準のCLIPモデルに対する敵対的ノイズの下での呼び出し率を向上させます。最後に、強固な文エンコーダーは、直接の最適化による入力文の再構成を改善することを示します。",
      "upvotes": 4,
      "discussionId": "68415c5ece09e3eca94e98e4",
      "ai_summary": "LEAF, an adversarial finetuning method, enhances the robustness of CLIP text encoders, improving zero-shot accuracy and multimodal retrieval performance under adversarial noise.",
      "ai_keywords": [
        "adversarial input attacks",
        "CLIP embeddings",
        "text-to-image generative models",
        "large vision language models",
        "adversarial finetuning",
        "zero-shot adversarial accuracy",
        "text-to-image diffusion models",
        "multimodal retrieval tasks",
        "robust text encoders"
      ]
    },
    "publishedAt": "2025-06-03T15:57:09.000Z",
    "title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder",
    "summary": "Adversarial input attacks can cause a significant shift of CLIP embeddings.\nThis can affect the downstream robustness of models incorporating CLIP in the\npipeline, such as text-to-image generative models or large vision language\nmodels. While some efforts have been done towards making the CLIP image\nencoders robust, the robustness of text encoders remains unexplored. In this\nwork, we cover this gap in the literature. We propose LEAF: an efficient\nadversarial finetuning method for the text domain, with the ability to scale to\nlarge CLIP models. Our models significantly improve the zero-shot adversarial\naccuracy in the text domain, while maintaining the vision performance provided\nby robust image encoders. When combined with text-to-image diffusion models, we\ncan improve the generation quality under adversarial noise. When employing our\nrobust CLIP encoders in multimodal retrieval tasks, we improve the recall under\nadversarial noise over standard CLIP models. Finally, we show that robust text\nencoders facilitate better reconstruction of input text from its embedding via\ndirect optimization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03355.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310a6bb0a43f97f6c5567d3",
      "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
      "fullname": "Christian Schlarmann",
      "name": "chs20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02945",
      "authors": [
        {
          "_id": "6841008f2f66f731bf010feb",
          "name": "Aishwarya Sahoo",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fec",
          "name": "Jeevana Kruthi Karnuthala",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fed",
          "name": "Tushar Parmanand Budhwani",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fee",
          "name": "Pranchal Agarwal",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010fef",
          "name": "Sankaran Vaidyanathan",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff0",
          "name": "Alexa Siu",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff1",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:55.170Z",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff2",
          "name": "Jennifer Healey",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff3",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff4",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff5",
          "name": "Uttaran Bhattacharya",
          "hidden": false
        },
        {
          "_id": "6841008f2f66f731bf010ff6",
          "name": "Branislav Kveton",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-03T14:44:23.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:33.123Z",
      "title": "Quantitative LLM Judges\n\nカンナンタイル・LLM・ジュージェスト",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "LLM-as-a-judgeは、大規模言語モデル（LLM）が他のLLMの出力を自動的に評価するフレームワークです。私たちは、定量的なLLMジャッジを提案します。これは、特定の領域での人間のスコアに一致するように、現在のLLMジャッジの評価スコアを回帰モデルを使用して調整します。これらのモデルは、ジャッジの文脈評価とスコアを使用して元のジャッジのスコアを向上させることを目指して訓練されています。私たちは、異なる種類の絶対的および相対的なフィードバックに対する4つの定量的なジャッジを紹介します。これらは、我々のフレームワークの一般性と多様性を示します。我々のフレームワークは、観覧学習ではなく、訓練されたフィードバックを使用して計算的により効率的であり、人間のフィードバックが限られた場合には統計的により効率的であることが期待されます。私たちのフレームワークの効果を4つのデータセットと2つの基礎ジャッジを使用して実験的に検証しました。私たちの実験は、定量的なジャッジが既存のジャッジの予測力を効果的に向上させることを示しています。",
      "upvotes": 4,
      "discussionId": "684100902f66f731bf01101e",
      "ai_summary": "A framework uses quantitative LLM judges to align existing LLM evaluation scores with human scores, improving predictive power and efficiency through regression models.",
      "ai_keywords": [
        "LLM-as-a-judge",
        "large language model",
        "quantitative LLM judges",
        "regression models",
        "score alignment",
        "predictive power",
        "post-hoc modeling"
      ]
    },
    "publishedAt": "2025-06-03T10:44:23.000Z",
    "title": "Quantitative LLM Judges",
    "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02945.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.00482",
      "authors": [
        {
          "_id": "68402986a50b67f983749710",
          "user": {
            "_id": "6576ace7769f3ee9bd7b1b88",
            "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
            "isPro": false,
            "fullname": "Eunsu Kim",
            "user": "EunsuKim",
            "type": "user"
          },
          "name": "Eunsu Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:27:49.056Z",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749711",
          "name": "Haneul Yoo",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749712",
          "name": "Guijin Son",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749713",
          "name": "Hitesh Patel",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749714",
          "name": "Amit Agarwal",
          "hidden": false
        },
        {
          "_id": "68402986a50b67f983749715",
          "user": {
            "_id": "60e0251ea9b5d8282481f2b7",
            "avatarUrl": "/avatars/43441373af054a6184c22097bfeb97e4.svg",
            "isPro": false,
            "fullname": "Alice Oh",
            "user": "aliceoh",
            "type": "user"
          },
          "name": "Alice Oh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-05T06:37:21.889Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-31T09:24:32.000Z",
      "submittedOnDailyAt": "2025-06-05T04:40:48.000Z",
      "title": "BenchHub: ユニットベンチマークシートです。LLMの全体的でカスタマイズ可能な評価",
      "submittedOnDailyBy": {
        "_id": "6576ace7769f3ee9bd7b1b88",
        "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
        "isPro": false,
        "fullname": "Eunsu Kim",
        "user": "EunsuKim",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）が進化する中、最新のデータセットと整頓されたベンチマークの必要性が日間増しに重要になっています。しかし、現在のデータセットは散らばり、管理が難しく、特定のニーズや分野に合わせた評価を行うことが難しいことがあります。特に、数学やコーディングなどの分野での特定分野モデルの重要性が増加していることを記述しています。この論文では、研究者や開発者がLLMsをより効果的に評価することを可能にするために、ベンチマークデータセットを収集し、自動的にクラスフィードするダイナミックなベンチマークリポジトリ「BenchHub」を紹介します。BenchHubは、38ベンチマークを横断する303Kの質問を統合し、連続的な更新とスケーラブルなデータ管理を支援しています。これにより、多様な分野や使用ケースに合わせた柔軟な評価を可能にします。LLMフamiliesの様々な実験を通じて、分野ごとのサブセットでモデルの性能が显著に異なることを示し、分野に関するベンチマークの重要性を強調しています。BenchHubは、データセットの再利用を促進し、モデルの比較の透明化、既存のベンチマークで表現が不足している分野の容易な識別を可能にし、LLM評価研究の進歩に重要なインフラを提供しています。",
      "upvotes": 3,
      "discussionId": "68402987a50b67f983749746",
      "projectPage": "https://huggingface.co/BenchHub",
      "githubRepo": "https://github.com/rladmstn1714/BenchHub",
      "ai_summary": "BenchHub is a dynamic benchmark repository that aggregates and classifies datasets for large language models, facilitating domain-specific evaluations and improving model comparisons.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "BenchHub",
        "benchmark repository",
        "domain-specific models",
        "benchmark datasets",
        "continuous updates",
        "scalable data management",
        "model performance",
        "domain-aware benchmarking"
      ]
    },
    "publishedAt": "2025-05-31T05:24:32.000Z",
    "title": "BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM\n  Evaluation",
    "summary": "As large language models (LLMs) continue to advance, the need for up-to-date\nand well-organized benchmarks becomes increasingly critical. However, many\nexisting datasets are scattered, difficult to manage, and make it challenging\nto perform evaluations tailored to specific needs or domains, despite the\ngrowing importance of domain-specific models in areas such as math or code. In\nthis paper, we introduce BenchHub, a dynamic benchmark repository that empowers\nresearchers and developers to evaluate LLMs more effectively. BenchHub\naggregates and automatically classifies benchmark datasets from diverse\ndomains, integrating 303K questions across 38 benchmarks. It is designed to\nsupport continuous updates and scalable data management, enabling flexible and\ncustomizable evaluation tailored to various domains or use cases. Through\nextensive experiments with various LLM families, we demonstrate that model\nperformance varies significantly across domain-specific subsets, emphasizing\nthe importance of domain-aware benchmarking. We believe BenchHub can encourage\nbetter dataset reuse, more transparent model comparisons, and easier\nidentification of underrepresented areas in existing benchmarks, offering a\ncritical infrastructure for advancing LLM evaluation research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00482.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6576ace7769f3ee9bd7b1b88",
      "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
      "fullname": "Eunsu Kim",
      "name": "EunsuKim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23807",
      "authors": [
        {
          "_id": "683fec0a9f37285365be6142",
          "user": {
            "_id": "656201912d309fa7e27ddf40",
            "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
            "isPro": false,
            "fullname": "Yuli chen",
            "user": "yulichen",
            "type": "user"
          },
          "name": "Yuli Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-04T15:03:32.826Z",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6143",
          "name": "Bo Cheng",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6144",
          "name": "Jiale Han",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6145",
          "name": "Yingying Zhang",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6146",
          "name": "Yingting Li",
          "hidden": false
        },
        {
          "_id": "683fec0a9f37285365be6147",
          "name": "Shuhao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T07:35:00.000Z",
      "submittedOnDailyAt": "2025-06-05T00:42:31.891Z",
      "title": "DLP: 大語言モデルの層毎の動的縮小化",
      "submittedOnDailyBy": {
        "_id": "656201912d309fa7e27ddf40",
        "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
        "isPro": false,
        "fullname": "Yuli chen",
        "user": "yulichen",
        "type": "user"
      },
      "summary": "プリニングは最近、大規模言語モデル（LLMs）のパラメーターサイズを減らし、推論効率を向上させるために広く採用されています。主流のプリニング手法は通常、一様なレイヤごとのプリニング戦略を依存し、高いスパースレベルでは性能の低下が厳しいです。LLMsの各レイヤの異なる貢献を認識し、最近の研究は、非一様なレイヤごとのプリニングに焦点を当てていますが、これらのアプローチは通常、事前定義された値を依存し、最適な性能を獲得することができません。これらの制限を克服するために、私たちは新しい方法「Dynamic Layerwise Pruning（DLP）」を提案します。このアプローチは、モデルの重みと入力アクティベーション情報を統合して、各レイヤの相対的な重要性を適応的に決定し、プリニングレートを適切に割り当てます。実験結果によると、DLPは高いスパースレベルでモデルの性能を保存し、複数のLLMsにおいて効果的です。特に、70%のスパースレベルでは、DLPはLLaMA2-7Bのperplexityを7.79%減少させ、最先端の方法と比較して平均精度を2.7%向上させます。また、DLPは現在のLLMの圧縮技術と統合可能で、Parameter-Efficient Fine-Tuning（PEFT）にも無難に組み込みます。私たちは、https://github.com/ironartisan/DLPでコードをリリースして、将来の研究を促進することを目的としています。",
      "upvotes": 3,
      "discussionId": "683fec0a9f37285365be617f",
      "ai_summary": "A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.",
      "ai_keywords": [
        "pruning",
        "Large Language Models (LLMs)",
        "uniform layerwise pruning",
        "non-uniform layerwise pruning",
        "Dynamic Layerwise Pruning (DLP)",
        "perplexity",
        "Parameter-Efficient Fine-Tuning (PEFT)"
      ]
    },
    "publishedAt": "2025-05-27T03:35:00.000Z",
    "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
    "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656201912d309fa7e27ddf40",
      "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
      "fullname": "Yuli chen",
      "name": "yulichen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04133",
      "authors": [
        {
          "_id": "6840f32dda736de98e843831",
          "user": {
            "_id": "64d3c16a0553a2522f1aa792",
            "avatarUrl": "/avatars/951e272ffccf2388f138b248e5ef7142.svg",
            "isPro": false,
            "fullname": "Shaina Raza",
            "user": "shainar",
            "type": "user"
          },
          "name": "Shaina Raza",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-05T01:39:24.184Z",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843832",
          "name": "Ranjan Sapkota",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843833",
          "name": "Manoj Karkee",
          "hidden": false
        },
        {
          "_id": "6840f32dda736de98e843834",
          "name": "Christos Emmanouilidis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/7jK4mzUkVjPRUDMAacaCO.jpeg"
      ],
      "publishedAt": "2025-06-04T16:26:11.000Z",
      "submittedOnDailyAt": "2025-06-05T00:02:27.010Z",
      "title": "TRiSM for Agentic AI: 代理型AIの信頼性、リスク、およびセキュリティ管理のレビュー\n\nLLMベースの代理型多エージェントシステムでの信頼性、リスク、およびセキュリティ管理のレビュー",
      "submittedOnDailyBy": {
        "_id": "67ddd80896ac367438d400a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
        "isPro": false,
        "fullname": "Ranjan Sapkota",
        "user": "RanjanSapkota",
        "type": "user"
      },
      "summary": "Agentic AIシステムは、大規模な言語モデル（LLM）に基づいて、多エージェント構成で構築され、企業および社会の領域での脳魄的自主性、協力と決策を再定義しています。本レビューは、LLMベースの多エージェントシステム（AMAS）のコンテキストでの信頼、リスク、およびセキュリティ管理（TRiSM）についての構造化された分析を提供します。まず、Agentic AIの概念的な基礎、フォーラムティック的なAIエージェントとの違い、エージェントの機能を拡大するシステム設計について調査します。次に、Agentic AIフレームワークでのTRiSMは、ジョイニスム、説明性、ModelOps、プライバシー/セキュリティの4つの支柱によって詳細に説明されます。Agentic AIアプリケーションの特徴的なリスクベクトルを特定し、リスクタクロニミーを支える実世界的な脆弱性を示すケーススタディを挙げています。また、信頼ビルディング機構、透明性と監視手法、分散されたLLMエージェントシステムの最先端の説明性戦略を調査しています。また、信頼、説明性、ヒューマンエンジニアリング向けの性能を評価するメトリックも調査され、開放ベンチマークチャレンジを含むオープンベンチマークチャレンジを調査しています。セキュリティとプライバシーは、暗号化、対抗的防御、AI法規の進化に対応することで調査されています。本研究は、責任付きなAgentic AIのマップを提供し、新しい多エージェントシステムを強固なTRiSM原則に合わせた安全、責任付き、透明な採用に向けた研究方向を提案します。",
      "upvotes": 2,
      "discussionId": "6840f32eda736de98e843858",
      "ai_summary": "A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.",
      "ai_keywords": [
        "LLMs",
        "agentic AI",
        "multi-agent systems",
        "TRiSM",
        "governance",
        "explainability",
        "ModelOps",
        "privacy",
        "security",
        "encryption",
        "adversarial defense",
        "compliance",
        "AI regulations",
        "trust-building mechanisms",
        "transparency",
        "oversight",
        "interpretability",
        "human-centered performance",
        "benchmarking",
        "responsible AI",
        "research directions"
      ]
    },
    "publishedAt": "2025-06-04T12:26:11.000Z",
    "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems",
    "summary": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/7jK4mzUkVjPRUDMAacaCO.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04133.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ddd80896ac367438d400a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
      "fullname": "Ranjan Sapkota",
      "name": "RanjanSapkota",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04034",
      "authors": [
        {
          "_id": "6840ff0b535bfb4942b31576",
          "name": "Qing Jiang",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31577",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31578",
          "name": "Zhaoyang Zeng",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b31579",
          "name": "Junzhi Yu",
          "hidden": false
        },
        {
          "_id": "6840ff0b535bfb4942b3157a",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f46b6838ac3601fc89852/0J-cvgz2dA6bVQJJNb_Yz.jpeg"
      ],
      "publishedAt": "2025-06-04T14:56:57.000Z",
      "submittedOnDailyAt": "2025-06-05T00:56:30.698Z",
      "title": "Rex-Thinker: 連鎖コンテキスト論理による基盤に立つ物体参照",
      "submittedOnDailyBy": {
        "_id": "647f46b6838ac3601fc89852",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647f46b6838ac3601fc89852/N5cr1MFEtgKLJ4sVAhS04.jpeg",
        "isPro": true,
        "fullname": "Qing Jiang",
        "user": "Mountchicken",
        "type": "user"
      },
      "summary": "対象物検出の目的は、画像中にあるすべての物体を与えられた自然言語記述と一致したものを検出することです。我々は、強固な対象物検出モデルが、説明可能でビジュアルコンテンツに忠実な予測を行うことが重要であることを主張します。特に、それは以下の2つのキー特徴を満たす必要があります：1）可視性、ビジュアルエビデンスと明確に関連付けされた解釈可能な理由を提供することで、予測を正当化することです；2）信頼性、画像中に見つからない対象の表現に対しても、その表現に対応する物体が存在しない場合にも、その表現を拒否することで、信頼できるものです。しかし、多くの方法は、対象物検出を直接のボウニングボックス予測タスクとして扱い、解釈性が限られ、無対象の表現に対して拒否できない問題を抱えています。本研究では、Rex-Thinkerモデルを提案し、対象物検出を明示的なCoT（コンテキストに基づく推理）タスクとして構成します。対象の表現を与えると、まず、対象の物体カテゴリに対応するすべての候補物体インスタンスを識別します。Rex-Thinkerは、それぞれの候補に対してステップごとの推理を行い、それが与えられた表現に一致しているかどうかを評価し、最終的な予測を行います。このパラダイムを支えるために、HumanRefデータセットに対してGPT-4oをプロンプトし、大規模なCoTスタイルの対象物検出データセットHumanRef-CoTを構築しました。各理由の跡は、構築、行動、要約の構造化フォーマットを見せ、モデルが物体候補に対して分解された、解釈可能な理由を学習することを可能にします。そして、Rex-Thinkerは2段階で訓練されます：冷開始のステップごとの監督学習ファイナルチューニングフェイズで、モデルが構築された理由を行うことを教え、次にGRPOベースのRL学習で精度と一般化能力を向上させます。実験は、我々のアプローチは、領域内評価で精度と解釈性の標準ベースラインよりも上回り、また、偽物語の出力を拒否する能力と領域外設定での強い一般化能力を示しました。",
      "upvotes": 2,
      "discussionId": "6840ff0e535bfb4942b3165f",
      "projectPage": "https://rexthinker.github.io/",
      "githubRepo": "https://github.com/IDEA-Research/Rex-Thinker",
      "ai_summary": "Rex-Thinker is a CoT-based model that enhances object referring by performing step-by-step reasoning over candidate objects, leading to improved interpretability and rejection of mismatched queries.",
      "ai_keywords": [
        "CoT reasoning",
        "HumanRef-CoT",
        "GPT-4o",
        "structured reasoning",
        "cold-start supervised fine-tuning",
        "GRPO-based RL learning"
      ]
    },
    "publishedAt": "2025-06-04T10:56:57.000Z",
    "title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning",
    "summary": "Object referring aims to detect all objects in an image that match a given\nnatural language description. We argue that a robust object referring model\nshould be grounded, meaning its predictions should be both explainable and\nfaithful to the visual content. Specifically, it should satisfy two key\nproperties: 1) Verifiable, by producing interpretable reasoning that justifies\nits predictions and clearly links them to visual evidence; and 2) Trustworthy,\nby learning to abstain when no object in the image satisfies the given\nexpression. However, most methods treat referring as a direct bounding box\nprediction task, offering limited interpretability and struggling to reject\nexpressions with no matching object. In this work, we propose Rex-Thinker, a\nmodel that formulates object referring as an explicit CoT reasoning task. Given\na referring expression, we first identify all candidate object instances\ncorresponding to the referred object category. Rex-Thinker then performs\nstep-by-step reasoning over each candidate to assess whether it matches the\ngiven expression, before making a final prediction. To support this paradigm,\nwe construct a large-scale CoT-style referring dataset named HumanRef-CoT by\nprompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a\nstructured planning, action, and summarization format, enabling the model to\nlearn decomposed, interpretable reasoning over object candidates. We then train\nRex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach\nthe model how to perform structured reasoning, followed by GRPO-based RL\nlearning to improve accuracy and generalization. Experiments show that our\napproach outperforms standard baselines in both precision and interpretability\non in-domain evaluation, while also demonstrating improved ability to reject\nhallucinated outputs and strong generalization in out-of-domain settings.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f46b6838ac3601fc89852/0J-cvgz2dA6bVQJJNb_Yz.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04034.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f46b6838ac3601fc89852",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647f46b6838ac3601fc89852/N5cr1MFEtgKLJ4sVAhS04.jpeg",
      "fullname": "Qing Jiang",
      "name": "Mountchicken",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03951",
      "authors": [
        {
          "_id": "68415a1cce09e3eca94e02ef",
          "user": {
            "_id": "6759546743971eff5a12a087",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/esJm_83zW1R6NqWltof8P.png",
            "isPro": false,
            "fullname": "Aojun Lu",
            "user": "Kurt1024",
            "type": "user"
          },
          "name": "Aojun Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T09:59:50.242Z",
          "hidden": false
        },
        {
          "_id": "68415a1cce09e3eca94e02f0",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T09:59:52.907Z",
          "hidden": false
        },
        {
          "_id": "68415a1cce09e3eca94e02f1",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "68415a1cce09e3eca94e02f2",
          "name": "Yanan Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T13:40:41.000Z",
      "submittedOnDailyAt": "2025-06-05T07:19:58.382Z",
      "title": "継続的学習の構造的観点からの安定性と可塑性のトレードオフの再考",
      "submittedOnDailyBy": {
        "_id": "649d54b314afbb10ce2a9eeb",
        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
        "isPro": false,
        "fullname": "Hangjie Yuan",
        "user": "JacobYuan",
        "type": "user"
      },
      "summary": "カンティニュアル・ラーニング（CL）の課題は、神経ネットワークに学習と進歩的な適応能力を与えることを目指しています。この課題の中心は、「安定性と可塑性の二難課題」を解決することです。これは、前回学習した知識を保存すると新しい知識を獲得するのを両立させることにあります。頻繁にCLの方法は、このトレードオフを達成するために努力していますが、これらはネットワークのアーキテクチャが安定性と可塑性に及ぼす影響を見落としています。この論文では、アーキテクチャレベルでの安定性と可塑性の衝突に焦点を当てます。ここでは、同じパラメーター制約の下でも、深いネットワークはより良い可塑性を示し、幅広いネットワークはより良い安定性を特徴としています。このアーキテクチャレベルの二難課題を解決するために、Dual-Archという新しいフレームワークを提案します。このフレームワークは、CLのプラグインコンポーネントとして機能し、2つの異なる独立なネットワークの補間的な強みを活用します。これらのネットワークは、それぞれの目的に合わせた特殊化されたさまざまなアーキテクチャを持ち、拡張的な実験は、Dual-Archは既存のCL方法の性能を向上させ、パラメーターのコンパクティベティーを87%以上向上させることを示しました。",
      "upvotes": 2,
      "discussionId": "68415a1dce09e3eca94e0314",
      "projectPage": "https://github.com/byyx666/Dual-Arch",
      "githubRepo": "https://github.com/byyx666/Dual-Arch",
      "ai_summary": "A novel framework, Dual-Arch, enhances Continual Learning by addressing the stability-plasticity dilemma at the architectural level using two specialized networks.",
      "ai_keywords": [
        "Continual Learning",
        "stability-plasticity dilemma",
        "deep networks",
        "wide networks",
        "Dual-Arch"
      ]
    },
    "publishedAt": "2025-06-04T09:40:41.000Z",
    "title": "Rethinking the Stability-Plasticity Trade-off in Continual Learning from\n  an Architectural Perspective",
    "summary": "The quest for Continual Learning (CL) seeks to empower neural networks with\nthe ability to learn and adapt incrementally. Central to this pursuit is\naddressing the stability-plasticity dilemma, which involves striking a balance\nbetween two conflicting objectives: preserving previously learned knowledge and\nacquiring new knowledge. While numerous CL methods aim to achieve this\ntrade-off, they often overlook the impact of network architecture on stability\nand plasticity, restricting the trade-off to the parameter level. In this\npaper, we delve into the conflict between stability and plasticity at the\narchitectural level. We reveal that under an equal parameter constraint, deeper\nnetworks exhibit better plasticity, while wider networks are characterized by\nsuperior stability. To address this architectural-level dilemma, we introduce a\nnovel framework denoted Dual-Arch, which serves as a plug-in component for CL.\nThis framework leverages the complementary strengths of two distinct and\nindependent networks: one dedicated to plasticity and the other to stability.\nEach network is designed with a specialized and lightweight architecture,\ntailored to its respective objective. Extensive experiments demonstrate that\nDual-Arch enhances the performance of existing CL methods while being up to 87%\nmore compact in terms of parameters.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d54b314afbb10ce2a9eeb",
      "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
      "fullname": "Hangjie Yuan",
      "name": "JacobYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03614",
      "authors": [
        {
          "_id": "684134ca20ff8abcccb11302",
          "name": "Zhanhui Zhou",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11303",
          "name": "Lingjie Chen",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11304",
          "name": "Chao Yang",
          "hidden": false
        },
        {
          "_id": "684134ca20ff8abcccb11305",
          "name": "Chaochao Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T06:46:06.000Z",
      "submittedOnDailyAt": "2025-06-05T04:41:50.905Z",
      "title": "VLMs は、分散した訓練データポーチを集約できます。",
      "submittedOnDailyBy": {
        "_id": "642e5a7ba0b65dce1f87a7a2",
        "avatarUrl": "/avatars/3ae01c9330a47e98fac9f1eb0ba94073.svg",
        "isPro": false,
        "fullname": "Zhanhui Zhou",
        "user": "ZHZisZZ",
        "type": "user"
      },
      "summary": "ビジョン言語モデル（VLMs）のリスクを軽減する方法の一つは、トレーニングデータから危険なサンプルを削除することです。しかし、有害な画像が小さな、良い見えるパッチに分割され、複数のトレーニングサンプルに散らばれる場合、このデータモデレーションは簡単に回避されることがあります。その結果、VLMsはトレーニング中にこれらのパッチを組み合わせて、推論時に有害な応答を生成することができます。例えば、血のある場面の画像パッチと「安全」という説明を組み合わせてトレーニングされた場合、VLMsは後にその場面の全画像または文脈参照を「安全」と説明することも可能です。\n\nこの攻撃を可能にするVLMsの核心能力を「視覚ステーシング」と定義します。これは、同じ文脈記述を共有する複数のトレーニングサンプルに分散した視覚情報を統合する能力です。私たちの研究では、3つのデータセットで、各画像に独自の合成IDを付与した場合に、VLMsの視覚ステーシング能力を示します。まず、各（画像、ID）ペアを、異なるグランラリティで分割した（パッチ、ID）ペアに変換し、トレーニングを行い、トレーニングされたモデルが全画像または文脈参照から正しいIDを語り出すことができることを示しました。これに基づいて、上記の対策を模倣し、危険な画像のパッチを使用し、IDを「安全」や「不安全」という文脈記述に置き換え、有害な内容がパッチでモデレーションを回避し、後に視覚ステーシングによって再構築されることを示し、VLMsの安全性リスクを厳重に見立てます。コードは、https://github.com/ZHZisZZ/visual-stitching に公開されています。",
      "upvotes": 2,
      "discussionId": "684134cb20ff8abcccb11334",
      "githubRepo": "https://github.com/ZHZisZZ/visual-stitching",
      "ai_summary": "VLMs exhibit visual stitching, an ability to integrate fragmented visual information, which enables harmful content to evade data moderation and be reconstructed during inference.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "visual stitching",
        "data moderation",
        "adversarial data poisoning",
        "image patches",
        "textual descriptions",
        "inference"
      ]
    },
    "publishedAt": "2025-06-04T02:46:06.000Z",
    "title": "VLMs Can Aggregate Scattered Training Patches",
    "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as visual\nstitching -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each (image, ID) pair into {(patch,\nID)} pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03614.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e5a7ba0b65dce1f87a7a2",
      "avatarUrl": "/avatars/3ae01c9330a47e98fac9f1eb0ba94073.svg",
      "fullname": "Zhanhui Zhou",
      "name": "ZHZisZZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.01344",
      "authors": [
        {
          "_id": "6841009bdf863485e04879c8",
          "name": "Manan Suri",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879c9",
          "user": {
            "_id": "65c16444d4c3b8dff2f0d78d",
            "avatarUrl": "/avatars/4ed764c1657bd260d2a12ba61c111062.svg",
            "isPro": false,
            "fullname": "Puneet Mathur",
            "user": "puneetm",
            "type": "user"
          },
          "name": "Puneet Mathur",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T02:27:41.018Z",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879ca",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cb",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-05T07:26:52.205Z",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cc",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879cd",
          "name": "Vivek Gupta",
          "hidden": false
        },
        {
          "_id": "6841009bdf863485e04879ce",
          "name": "Dinesh Manocha",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T06:02:41.000Z",
      "submittedOnDailyAt": "2025-06-05T00:57:44.204Z",
      "title": "フローを追い越し：ニューロシンボリックアグェントによる細かいフローチャートアトリビュート",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "フローチャートは、決済のプロセスを可視化するための重要なツールです。しかし、その非線形な構造と複雑な視覚的・文字的関係が、LLMsを使用してフローチャートを解釈するのに難しいことを生み出します。これにより、物流、健康、工学などの重要な領域での自動化フローチャート処理の信頼性が低下します。私たちは、LLMのレスポンスに基づくフローチャートの特定の成分を踪対象としたFine-grained Flowchart Attributionの仕事を導入します。フローチャート Attributionは、LLMの予測の証明性を確保し、生成されたレスポンスをフローチャートの構造と結びつけることで解釈性を向上させます。私たちは、graph-based reasoningを用いてfine-grained post hoc Attributionを行うneurosymbolic agentとしてFlowPathAgentを提案します。それは、フローチャートを分割し、構造化された符号的グラフに変換し、グラフと動的に相互作用し、Attribution Pathを生成するアグリーンスアプローチを使用します。また、FlowExplainBenchという新しいベンチマークを紹介します。これは、多様なスタイル、領域、質問タイプのフローチャート Attributionを評価するためのものです。実験結果は、FlowPathAgentはフローチャートQAでLLMの回答の視覚的ハローシューマティオンを軽減し、私たちの提案のFlowExplainBenchデータセットで強いベースラインを10-14%超えるように優れています。",
      "upvotes": 2,
      "discussionId": "6841009ddf863485e0487a38",
      "ai_summary": "FlowPathAgent, a neurosymbolic agent, enhances the reliability of LLM predictions for flowchart interpretation by tracing specific components and generating accurate attribution paths.",
      "ai_keywords": [
        "Flowcharts",
        "Fine-grained Flowchart Attribution",
        "FlowPathAgent",
        "graph-based reasoning",
        "symbolic graph",
        "neurosymbolic agent",
        "flowExplainBench",
        "flowchart QA"
      ]
    },
    "publishedAt": "2025-06-02T02:02:41.000Z",
    "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents",
    "summary": "Flowcharts are a critical tool for visualizing decision-making processes.\nHowever, their non-linear structure and complex visual-textual relationships\nmake it challenging to interpret them using LLMs, as vision-language models\nfrequently hallucinate nonexistent connections and decision paths when\nanalyzing these diagrams. This leads to compromised reliability for automated\nflowchart processing in critical domains such as logistics, health, and\nengineering. We introduce the task of Fine-grained Flowchart Attribution, which\ntraces specific components grounding a flowchart referring LLM response.\nFlowchart Attribution ensures the verifiability of LLM predictions and improves\nexplainability by linking generated responses to the flowchart's structure. We\npropose FlowPathAgent, a neurosymbolic agent that performs fine-grained post\nhoc attribution through graph-based reasoning. It first segments the flowchart,\nthen converts it into a structured symbolic graph, and then employs an agentic\napproach to dynamically interact with the graph, to generate attribution paths.\nAdditionally, we present FlowExplainBench, a novel benchmark for evaluating\nflowchart attributions across diverse styles, domains, and question types.\nExperimental results show that FlowPathAgent mitigates visual hallucinations in\nLLM answers over flowchart QA, outperforming strong baselines by 10-14% on our\nproposed FlowExplainBench dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03817",
      "authors": [
        {
          "_id": "68415ee454d7c6b3f9786deb",
          "name": "Julius Gonsior",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786dec",
          "name": "Tim Rieß",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786ded",
          "name": "Anja Reusch",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786dee",
          "name": "Claudio Hartmann",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786def",
          "name": "Maik Thiele",
          "hidden": false
        },
        {
          "_id": "68415ee454d7c6b3f9786df0",
          "name": "Wolfgang Lehner",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T10:41:37.000Z",
      "submittedOnDailyAt": "2025-06-05T07:40:58.805Z",
      "title": "アクティブ学習のパラメータの調査：大規模実験グリッドからのアインサイズ",
      "submittedOnDailyBy": {
        "_id": "637638fa1f0421002b42facb",
        "avatarUrl": "/avatars/ed1e3024cd2ed7284b437db4bbeb2668.svg",
        "isPro": false,
        "fullname": "Julius Gonsior",
        "user": "jgonsior",
        "type": "user"
      },
      "summary": "データの注釈は時間と費用がかかる任務であり、しかし、ホームラインマシン学習に固有の必要性がある。アクティブ・ラーニング（AL）は、最情報量の多い無ラベルサンプルを連続的に選択し、専門家の注釈を要求することで、人間のラベル作業を最小限に抑え、全体の分類性能を向上させるための既に確立された方法である。ALは数十年ほど知られているが、実世界的なアプリケーションではまだ稀に使用されている。NLPコミュニティの2つのウェブ調査で示されたように、ALの使用を妨げる2つの主な理由がある：1. ALの設定の複雑さ、2. その効果性の信頼の欠如。両者の理由は、ALの大きな超パラメータ空間が原因であると仮定している。この超パラメータ空間は、ALの実験結果が誤ったり、再現可能でないことを多くの場合引き起こす。本研究では、1. 460万以上の超パラメータ組み合わせの大きな超パラメータグリッドを作成し、2. それらのすべての組み合わせの性能を記録し、3. 実験結果における各超パラメータの影響を分析した。最終的に、各超パラメータの影響についての推薦を与え、具体的なAL戦略の実装の驚くことの多い影響を示し、最小限の計算努力で再現可能なAL実験の設計を概観し、将来の再現可能で信頼できるAL研究に貢献する。",
      "upvotes": 1,
      "discussionId": "68415ee554d7c6b3f9786e15",
      "githubRepo": "https://github.com/jgonsior/olympic-games-of-active-learning",
      "ai_summary": "The study investigates the impact of hyperparameters on Active Learning performance, providing insights to improve its practical application and reproducibility.",
      "ai_keywords": [
        "Active Learning",
        "hyperparameter space",
        "hyperparameter grid",
        "experimental study design",
        "reproducibility",
        "trustworthiness"
      ]
    },
    "publishedAt": "2025-06-04T06:41:37.000Z",
    "title": "Survey of Active Learning Hyperparameters: Insights from a Large-Scale\n  Experimental Grid",
    "summary": "Annotating data is a time-consuming and costly task, but it is inherently\nrequired for supervised machine learning. Active Learning (AL) is an\nestablished method that minimizes human labeling effort by iteratively\nselecting the most informative unlabeled samples for expert annotation, thereby\nimproving the overall classification performance. Even though AL has been known\nfor decades, AL is still rarely used in real-world applications. As indicated\nin the two community web surveys among the NLP community about AL, two main\nreasons continue to hold practitioners back from using AL: first, the\ncomplexity of setting AL up, and second, a lack of trust in its effectiveness.\nWe hypothesize that both reasons share the same culprit: the large\nhyperparameter space of AL. This mostly unexplored hyperparameter space often\nleads to misleading and irreproducible AL experiment results. In this study, we\nfirst compiled a large hyperparameter grid of over 4.6 million hyperparameter\ncombinations, second, recorded the performance of all combinations in the\nso-far biggest conducted AL study, and third, analyzed the impact of each\nhyperparameter in the experiment results. In the end, we give recommendations\nabout the influence of each hyperparameter, demonstrate the surprising\ninfluence of the concrete AL strategy implementation, and outline an\nexperimental study design for reproducible AL experiments with minimal\ncomputational effort, thus contributing to more reproducible and trustworthy AL\nresearch in the future.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03817.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637638fa1f0421002b42facb",
      "avatarUrl": "/avatars/ed1e3024cd2ed7284b437db4bbeb2668.svg",
      "fullname": "Julius Gonsior",
      "name": "jgonsior",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.03538",
      "authors": [
        {
          "_id": "6841585dd777f13c59460b47",
          "name": "Chengqi Li",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b48",
          "name": "Zhihao Shi",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b49",
          "name": "Yangdi Lu",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b4a",
          "name": "Wenbo He",
          "hidden": false
        },
        {
          "_id": "6841585dd777f13c59460b4b",
          "user": {
            "_id": "634e60454677a5891c0902f4",
            "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
            "isPro": false,
            "fullname": "Xiangyu Xu",
            "user": "xjcvcvxj",
            "type": "user"
          },
          "name": "Xiangyu Xu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-05T08:42:10.367Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T03:40:33.000Z",
      "submittedOnDailyAt": "2025-06-05T07:13:24.079Z",
      "title": "野生の穏やかなニューラルレンダリングに対して、アスミームデュアル3次ガウススプレッティングを用いる",
      "submittedOnDailyBy": {
        "_id": "634e60454677a5891c0902f4",
        "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
        "isPro": false,
        "fullname": "Xiangyu Xu",
        "user": "xjcvcvxj",
        "type": "user"
      },
      "summary": "3D再構造は、不均一な照明条件と瞬時的な分散要因により難しい課題として残されています。現在の方法は、低品質の訓練データを処理するために通常はヒューリスティックな戦略を依存していますが、これらは安定したものと一致性のある再構造を生成することが難しく、視覚的なアーティファクトが発生します。本稿では、これらのアーティファクトの乱数性を利用した新しいフレームワークであるAsymmetric Dual 3DGSを提案します。特に、我々の方法は2つの3Dガウススプレッティング（3DGS）モデルを並列に訓練し、信頼性のあるシーンジェモトリーへの収束を促す一致性制約を強制し、不確実なアーティファクトを抑制します。2つのモデルが同じ失敗モードに収束しないように、我々は異なるマスクを適用することでチェックボイスバイアスによるファイルドムモードによる崩壊を防ぎます。具体的には、多ケースの適応マスクと自動軽マスクを用い、2つのモデルの不均衡な訓練プロセスを実現し、共有エラーモードを減らします。また、モデルの訓練の効率化のためには、Dynamic EMA Proxyという軽量なバージョンを導入し、1つのモデルを動的に更新される指数移動平均（EMA）プロキシに置き換え、交換的なマスクプロセスを用いてディバージェンスを保持します。難しい実世界データセット上での極めて広範囲な実験により、我々の方法は現在のアプローチを経ち越し、高い効率性を達成します。コードと訓練モデルは公開します。",
      "upvotes": 1,
      "discussionId": "68415862d777f13c59460c85",
      "ai_summary": "A novel Asymmetric Dual 3DGS framework improves 3D reconstruction by training dual models with consistency constraints and divergent masking, outperforming existing methods with high efficiency.",
      "ai_keywords": [
        "3D reconstruction",
        "3D Gaussian Splatting (3DGS)",
        "stochastic artifacts",
        "consistency constraint",
        "confirmation bias",
        "divergent masking",
        "multi-cue adaptive mask",
        "self-supervised soft mask",
        "Dynamic EMA Proxy",
        "lightweight variant",
        "Exponential Moving Average (EMA)",
        "alternating masking strategy"
      ]
    },
    "publishedAt": "2025-06-03T23:40:33.000Z",
    "title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian\n  Splatting",
    "summary": "3D reconstruction from in-the-wild images remains a challenging task due to\ninconsistent lighting conditions and transient distractors. Existing methods\ntypically rely on heuristic strategies to handle the low-quality training data,\nwhich often struggle to produce stable and consistent reconstructions,\nfrequently resulting in visual artifacts. In this work, we propose Asymmetric\nDual 3DGS, a novel framework that leverages the stochastic nature of these\nartifacts: they tend to vary across different training runs due to minor\nrandomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)\nmodels in parallel, enforcing a consistency constraint that encourages\nconvergence on reliable scene geometry while suppressing inconsistent\nartifacts. To prevent the two models from collapsing into similar failure modes\ndue to confirmation bias, we introduce a divergent masking strategy that\napplies two complementary masks: a multi-cue adaptive mask and a\nself-supervised soft mask, which leads to an asymmetric training process of the\ntwo models, reducing shared error modes. In addition, to improve the efficiency\nof model training, we introduce a lightweight variant called Dynamic EMA Proxy,\nwhich replaces one of the two models with a dynamically updated Exponential\nMoving Average (EMA) proxy, and employs an alternating masking strategy to\npreserve divergence. Extensive experiments on challenging real-world datasets\ndemonstrate that our method consistently outperforms existing approaches while\nachieving high efficiency. Codes and trained models will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e60454677a5891c0902f4",
      "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
      "fullname": "Xiangyu Xu",
      "name": "xjcvcvxj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.02294",
      "authors": [
        {
          "_id": "6840cd169241913d43af9d28",
          "name": "Niclas Popp",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d29",
          "name": "Kevin Alexander Laube",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d2a",
          "name": "Matthias Hein",
          "hidden": false
        },
        {
          "_id": "6840cd169241913d43af9d2b",
          "name": "Lukas Schott",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T22:15:59.000Z",
      "submittedOnDailyAt": "2025-06-05T00:15:23.870Z",
      "title": "未知の共変動シフトによる知識絞り込みの改善を通じて、信頼性ガイドされたデータアプローディング",
      "submittedOnDailyBy": {
        "_id": "655646baf8a2d3c020546ec8",
        "avatarUrl": "/avatars/4ca8de82745bb5a4fda511569bb6bd94.svg",
        "isPro": false,
        "fullname": "Niclas P",
        "user": "NPBP26",
        "type": "user"
      },
      "summary": "大きなデータセットで訓練された基盤モデルは、複数の領域で強いゼロショット能力を示す。データサイズとモデルサイズが制限されている場合にその成功を再現するために、知識の収納は基盤モデルから小さな学生ネットワークへの知識の収納によるツールとして定着している。しかし、収納の効果は利用可能な訓練データにより緊張的に制限されている。本稿は、知識の収納における共変動変換の通常の実用的な問題を解決し、訓練中に現れるがテスト時には現れないスパイラスな特徴量が出現することを指摘する。スパイラスな特徴量が未知であるが、強固な教師が存在する場合に、学生もそれに対して強固になることができるかという問題を調べる。この問題を解決するために、教師と学生の意見分けを最大化して画像を生成する新しいディフュージョンベースのデータ拡張戦略を導入する。実験は、このアプローチはコバイアントシフトの影響を受けながらもCelebA、SpuCo Birds、そしてスパイラスなImageNetのspurious mAUCにおいて最悪グループと平均グループの精度を大幅に向上させ、最先端のディフュージョンベースのデータ拡張ベースラインナーを超えることを示す。",
      "upvotes": 1,
      "discussionId": "6840cd199241913d43af9dac",
      "ai_summary": "A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.",
      "ai_keywords": [
        "knowledge distillation",
        "diffusion-based data augmentation",
        "covariate shift",
        "teacher-student model",
        "CelebA",
        "SpuCo Birds",
        "spurious ImageNet",
        "mean group accuracy",
        "worst group accuracy",
        "spurious mAUC"
      ]
    },
    "publishedAt": "2025-06-02T18:15:59.000Z",
    "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation",
    "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02294.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655646baf8a2d3c020546ec8",
      "avatarUrl": "/avatars/4ca8de82745bb5a4fda511569bb6bd94.svg",
      "fullname": "Niclas P",
      "name": "NPBP26",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]