[
  {
    "paper": {
      "id": "2502.08590",
      "authors": [
        {
          "_id": "67ad79552fdac6537b43f120",
          "name": "Yujie Zhou",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f121",
          "name": "Jiazi Bu",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f122",
          "name": "Pengyang Ling",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f123",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f124",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f125",
          "name": "Qidong Huang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f126",
          "name": "Jinsong Li",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f127",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f128",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:31.817Z",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f129",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12a",
          "name": "Anyi Rao",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12b",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "67ad79552fdac6537b43f12c",
          "name": "Li Niu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T17:24:19.000Z",
      "title": "Light-A-Video: プログレッシブな光の融合による無学習ビデオ再点光",
      "summary": "最近の画像の光源再設定モデルの進歩は、大規模なデータセットと事前学習された拡散モデルによって駆動されています。これらの進歩は、一貫性のある光源の設定を可能にしましたが、映像の光源再設定は主に過剰な学習コストと多様性のない高品質の映像の光源再設定データセットの不足により遅れています。画像の光源再設定モデルをフレームごとに簡単に適用すると、光源の不連続性と再設定された外観の不連続性が生じ、生成される映像にフリッキングが発生します。本稿では、Light-A-Videoという学習不要のアプローチを提案し、時間的に平滑な映像の光源再設定を実現します。画像の光源再設定モデルから転用されたLight-A-Videoは、光源の一貫性を向上させるために2つの重要な技術を導入しています。まず、自注意層内のフレーム間の相互作用を強化し、背景の光源の生成を安定させるためにConsistent Light Attention (CLA)モジュールを設計します。次に、光の伝播の物理的な原理を利用し、源映像の外観と再設定された外観との線形ブレンディングを適用し、Progressive Light Fusion (PLF)戦略を用いて照明の時間的な遷移を平滑に保証します。実験は、Light-A-Videoは再設定された映像の時間的な一貫性を向上させ、画像の品質を維持することを示し、フレーム間での光源の一貫性の遷移を確保します。プロジェクトページ：https://bujiazi.github.io/light-a-video.github.io/",
      "upvotes": 25,
      "discussionId": "67ad79572fdac6537b43f189"
    },
    "publishedAt": "2025-02-12T23:47:56.223Z",
    "title": "Light-A-Video: Training-free Video Relighting via Progressive Light Fusion",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07870",
      "authors": [
        {
          "_id": "67ad79cb60ec3f444b21cbcb",
          "name": "Alex Jinpeng Wang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcc",
          "name": "Dongxing Mao",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcd",
          "name": "Jiawei Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbce",
          "name": "Weiming Han",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbcf",
          "name": "Zhuobai Dong",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd0",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd1",
          "name": "Yiqi Lin",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd2",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd3",
          "name": "Libo Qin",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd4",
          "name": "Fuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd5",
          "name": "Lijuan Wang",
          "hidden": false
        },
        {
          "_id": "67ad79cb60ec3f444b21cbd6",
          "name": "Min Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:59:19.000Z",
      "title": "TextAtlas5M: デンステキスト画像生成のための大規模データセット",
      "summary": "近年で、テキスト条件付き画像生成は関注を集めており、長いや詳細なテキストプロンプトを処理することができるようになりました。日常生活では、広告、デザイングラフィック、サインなどで密なや複雑なテキストが出現し、テキストとビジュアルの統合が複雑な情報を伝えるために重要です。しかし、これらの進歩の上でも、長文を含む画像の生成は長期的な課題で、現在のデータセットの制限により主な原因です。この空間を填ぐために、TextAtlas5Mという新しいデータセットを紹介します。このデータセットは、長文の描画を評価するために特に設計されています。データセットは500万枚の長文生成された画像からなり、多様なデータタイプを組み合わせて、大規模な生成モデルの長文画像生成を詳細に評価できます。また、3データドメインを横断的に3000枚の人間改善テストセットTextAtlasEvalをカレーライドし、テキスト条件付き生成の最も幅広いベンチマークを構築しました。評価によると、TextAtlasEvalベンチマークは最先端のプロプライエーションモデル（例：GPT4oとDallE-3）にとっても重大な課題を提示しますが、その開放ソースコンタラップはより大きな性能間違いを示しています。これらの証拠により、TextAtlas5Mは将来のテキスト条件付き画像生成モデルの訓練と評価にもとって有効なデータセットとして位置づけられています。",
      "upvotes": 24,
      "discussionId": "67ad79d260ec3f444b21cd1f"
    },
    "publishedAt": "2025-02-12T23:50:07.130Z",
    "title": "TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08639",
      "authors": [
        {
          "_id": "67ad5f25cad644864b436186",
          "name": "Qinghe Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436187",
          "name": "Yawen Luo",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436188",
          "name": "Xiaoyu Shi",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b436189",
          "name": "Xu Jia",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618a",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618b",
          "name": "Tianfan Xue",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618c",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618d",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618e",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "67ad5f25cad644864b43618f",
          "name": "Kun Gai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T18:55:36.000Z",
      "title": "CineMaster: 映画的3D認識と制御可能なフレームワークでのテキストから動画生成",
      "summary": "この研究では、CineMasterという新しいフレームワークを提案します。これは3D認識していて、テキストから動画を生成できるものです。私たちの目標は、プロの映画監督と同じ程度の制御可能性をユーザーに提供することです。シーン内の物体の精確な配置、3D空間での物体とカメラの柔軟な操作、渲染されたフレームの直感的な並び替えの制御を可能にします。これを達成するために、CineMasterは2つのステージで動作します。最初のステージでは、ユーザーが直感的に3D認識した条件付き信号を構築するためのインタラクティブなワークフローを設計します。3D空間で物体のボウンディングボックスを位置づけ、カメラの移動を定義します。2番目のステージでは、これらの制御信号（渲染された深さマップ、カメラの移動軌跡、物体のクラスラベル）がテキストから動画への拡散モデルのガイドになり、ユーザーが希望した動画内容を生成します。また、3D物体の動きとカメラの姿勢の注釈付きのデータセットの不足を克服するために、大規模な動画データから3Dボウンディングボックスとカメラの移動軌跡を抽出するための自動データ注釈プイルインフェーバーを認真に構築します。詳細な質的的および量性的な実験は、CineMasterが現在の方法を大幅に超え、顕著な3D認識しているテキストから動画の生成を実現していることを示します。プロジェクトページ：https://cinemaster-dev.github.io/。",
      "upvotes": 22,
      "discussionId": "67ad5f26cad644864b4361cf"
    },
    "publishedAt": "2025-02-12T21:55:44.479Z",
    "title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6063
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08047",
      "authors": [
        {
          "_id": "67ad92bfbbf3810ab20595c2",
          "name": "Henry Hengyuan Zhao",
          "hidden": false
        },
        {
          "_id": "67ad92bfbbf3810ab20595c3",
          "name": "Difei Gao",
          "hidden": false
        },
        {
          "_id": "67ad92bfbbf3810ab20595c4",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T01:06:10.000Z",
      "title": "WorldGUI: オーバーフローテストのための総合的なデスクトップGUI自動化",
      "summary": "現在のGUIアガントはGUI要素のジョージングで出色な性能を達成しています。しかし、計画が非常に難しい問題に残されています、特に環境の初期状態に対する敏感性によりこれが強く感じられます。具体的には、目標のソフトウェアが開いていないや、インターフェイスがデフォルト状態でないような初期状態の微妙な違いは、計画誤りを引き起こすことが多いです。この問題は実際のユーザーシナリオで広く存在しますが、現在のベンチマークはこれを評価できません。本論文では、WorldGUIという新しいGUIベンチマークを紹介します。このベンチマークは、実際のコンピューターユーザーインタラクションを模倣するために、様々な初期状態を持つGUIタスクを設計しています。このベンチマークは、PowerPoint、VSCode、Adobe Acrobatなど10つの人気のソフトウェアの幅広い範囲のタスクを含みます。また、動的なGUI自動化タスクの挑戦に対処するために、GUI-Thinkerという全体的なフレームワークを提案します。このフレームワークは、評価機構を活用してGUIインタラクションの不確実性と複雑性を効果的に管理することができます。実験結果は、GUI-ThinkerはWorldGUIタスクの成功率でClaude-3.5（Computer Use）に対して14.9%の改善率を収め、我々の批判的思考に基づくフレームワークの効果性を明らかにしています。",
      "upvotes": 19,
      "discussionId": "67ad92c1bbf3810ab205961c"
    },
    "publishedAt": "2025-02-13T01:39:08.775Z",
    "title": "WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08047.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647d7eb9770c299e56f5b39b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647d7eb9770c299e56f5b39b/CC5JJgkyLkXOxw-BeT4G5.jpeg",
      "fullname": "Hengyuan Zhao",
      "name": "hhenryz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07563",
      "authors": [
        {
          "_id": "67ad7929dc2968691c241147",
          "user": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "isPro": false,
            "fullname": "Weigao Sun",
            "user": "weigao266",
            "type": "user"
          },
          "name": "Weigao Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:37.445Z",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c241148",
          "user": {
            "_id": "66ea643899af9ac3463639b1",
            "avatarUrl": "/avatars/252d470e761a57834dee3dbc60dfefed.svg",
            "isPro": false,
            "fullname": "Disen Lan",
            "user": "landisen",
            "type": "user"
          },
          "name": "Disen Lan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:33.746Z",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c241149",
          "name": "Yiran Zhong",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c24114a",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "67ad7929dc2968691c24114b",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T14:01:39.000Z",
      "title": "LASP-2: 線形アテンションの順番並列化の再考とそのハイブリッド",
      "summary": "線形順序モデリングアプローチのように、線形アテンションなどは、順序長さに関係なく線形時間の訓練と常時メモリの推論のような優った点を提供します。しかし、現在の順序平行計算（SP）メソッドは、線形アテンションの右積マーク先の特徴に最適化されていないか、リングスタイルのコミュニケーション戦略を使用しているため、計算平行計算の低下、長い順序の分散システムでのスケーラビリティの制限があります。本論文では、LASP-2という新しいSPメソッドを紹介し、非常に長い入力順序を持つ線形アテンションtransformerモデルの訓練時に通信と計算の平行計算を両方とも向上させることを目指しています。LASPと比較して、LASP-2は線形アテンション層でのSPの最小限のコミュニケーション要求を再考し、LASPの全てのコミュニケーション・計算ワークフローを再組織しています。このように、間接メモリ状態上の一つのAllGatherコミュニケーションのみが必要となり、ショートケースの長さに依存しないサイズで、通信と計算の平行計算およびその重なりによる大幅な向上が実現されます。また、LASP-2HというLASP-2の拡張版を提案し、標準アテンションモジュールに類似のコミュニケーション再設計を適用し、線形アテンションと標準アテンション層をブレンドした混合モデルに対して効率的なSP解決策を提供します。Linear-Llama3モデルの評価では、標準アテンションを線形アテンションに置き換えたLlama3のバージョンで、LASP-2とLASP-2Hの効果性を示します。特に、LASP-2は64グラファイアで2048Kの順序長さでLASPに対して15.2%の訓練スピード向上、Ring Attentionに対して36.6%の訓練スピード向上を実現します。コードは、https://github.com/OpenSparseLLMs/Linear-MoEの一部としてリリースされています。",
      "upvotes": 17,
      "discussionId": "67ad792adc2968691c241173"
    },
    "publishedAt": "2025-02-12T23:47:31.651Z",
    "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07563.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6246bb33da617c00b48e4d92",
      "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
      "fullname": "Weigao Sun",
      "name": "weigao266",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07346",
      "authors": [
        {
          "_id": "67ac4e046b8c86e0cc7988f0",
          "user": {
            "_id": "649d1d4c379eada9a580cf59",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d1d4c379eada9a580cf59/ucXv7KoJDEB3Phgn-Dn5E.png",
            "isPro": false,
            "fullname": "xuhuang",
            "user": "ggdcr",
            "type": "user"
          },
          "name": "Xu Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:25:17.555Z",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f1",
          "name": "Wenhao Zhu",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f2",
          "name": "Hanxu Hu",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f3",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f4",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f5",
          "name": "Shujian Huang",
          "hidden": false
        },
        {
          "_id": "67ac4e046b8c86e0cc7988f6",
          "name": "Fei Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T08:17:19.000Z",
      "title": "BenchMAX: 大規模言語モデルの詳細多言語評価システム",
      "summary": "前の多言語ベンチマークは主に簡単な理解タスクに焦点を当てていましたが、大規模な言語モデル（LLMs）においては、単語従い、理由、長文脈理解、コード生成などの高度な能力を強調しています。しかし、これらの高度な能力を複数の言語で測定する方法は調査が浅いです。この課題を解決するために、BenchMAXという多言語評価ベンチマークを紹介します。これは、重要な能力を複数の言語で公平な比較を可能にします。高品質を維持するために、データを英語から16つの他の言語に機械翻訳した後、3人の母語者が独立して各タスクのサンプルを記録します。また、データセットの構築からの新しい翻訳挑戦も提示します。BenchMAXでの拡大的な実験は、各言語での核心能力の効果性の差異を明らかにし、モデルサイズの増大でも閉ざされない性能の間違いを示します。BenchMAXは、複数の言語での評価のための一様なプラットフォームとして提供され、複数の言語の言語モデルの開発を促進するテストベンチを提供します。データセットとコードは公開的にアクセス可能です。",
      "upvotes": 14,
      "discussionId": "67ac4e056b8c86e0cc798952"
    },
    "publishedAt": "2025-02-13T03:34:47.873Z",
    "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07346.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d1d4c379eada9a580cf59",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d1d4c379eada9a580cf59/ucXv7KoJDEB3Phgn-Dn5E.png",
      "fullname": "xuhuang",
      "name": "ggdcr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.08127",
      "authors": [
        {
          "_id": "67ad5ca29109885ce9b859e4",
          "name": "Lingfei Qian",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e5",
          "name": "Weipeng Zhou",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e6",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e7",
          "name": "Xueqing Peng",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e8",
          "user": {
            "_id": "63b58ed5889aa6707f0bb0f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
            "isPro": true,
            "fullname": "Jimin Huang",
            "user": "jiminHuang",
            "type": "user"
          },
          "name": "Jimin Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T02:44:52.979Z",
          "hidden": false
        },
        {
          "_id": "67ad5ca29109885ce9b859e9",
          "user": {
            "_id": "6479f4317c18dca75e9a9324",
            "avatarUrl": "/avatars/9aa709230b057f57ee4415c04a622c63.svg",
            "isPro": false,
            "fullname": "Xie",
            "user": "QianqianXie1994",
            "type": "user"
          },
          "name": "Qianqian Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:22:01.539Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T05:13:04.000Z",
      "title": "Fino1: 理論的適用性について金融における推論向上型LLMの応用性",
      "summary": "最近の大規模言語モデル（LLMs）の進展は強力な一般的な論理能力を示していますが、金融論理における効果は調査が不足しています。本研究では、金融テキスト、テーブルデータ、方程式を含む3つの複雑な金融タスクにおいて、数値論理、テーブル詳細解釈、金融用語理解、長コンテキスト処理、方程式ベースの問題解決を評価し、16つの強力な論理モデルと一般的なLLMsを採用します。結果として、データセットの改善と事前学習の改善は金融論理における効果を高めることがわかりましたが、一般的な強化としてのCoT調整は常に一致した効果を得ることはありませんでした。また、すべての論理戦略は、長コンテキストや多テーブルタスクにおける性能向上には課題を抱えています。これらの制限を解決するために、Llama-3.1-8B-Instructを基盤にした金融論理向上モデルを開発し、CoT調整とディスクリプション専門的な論理パスを用いた強化学習を行いました。1つの金融データセットでの簡単な調整でも、モデルはすべてのタスクで10%の積極的な性能向上を収め、すべての8Bモデルを超え、さらにLlama3-70B-InstructとLlama3.1-70B-Instructの平均値を超えました。これらの結果は、金融タスクにおけるディスクリプション専門的な変更の必要性を強調し、多テーブル論理、長コンテキスト処理、金融用語理解のような将来の方向性を示しています。すべてのデータセット、モデル、コードは公開的に提供されています。また、将来のデータセットとモデルのベンチマークに向けてリーダブールを導入しました。",
      "upvotes": 13,
      "discussionId": "67ad5ca59109885ce9b85a5b"
    },
    "publishedAt": "2025-02-12T21:45:28.944Z",
    "title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08127.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63b58ed5889aa6707f0bb0f4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
      "fullname": "Jimin Huang",
      "name": "jiminHuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07864",
      "authors": [
        {
          "_id": "67ad5b3a007d78b391946a57",
          "user": {
            "_id": "643f55d4ec817b766686438a",
            "avatarUrl": "/avatars/0feb460432c92ab9ada0d417a7a38f6a.svg",
            "isPro": false,
            "fullname": "mengfanxu",
            "user": "fxmeng",
            "type": "user"
          },
          "name": "Fanxu Meng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:22:03.808Z",
          "hidden": false
        },
        {
          "_id": "67ad5b3a007d78b391946a58",
          "name": "Zengwei Yao",
          "hidden": false
        },
        {
          "_id": "67ad5b3a007d78b391946a59",
          "name": "Muhan Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:20:18.000Z",
      "title": "TransMLA: 多ヘッド潜在注意は、あなたに必要なものです",
      "summary": "現代の大規模言語モデル（LLMs）は、現在のハードウェア上では通信ボトルネックを多く課題に直面し、または純粋な計算制約による制限を受けることが多い。多けットレイエンテーション（MLA）は、この挑戦を解決するために、キー-バリュー（KV）層で低レンキング行列を使用し、これにより圧縮された潜在的なKV状態をキャッシュできるようにする。このアプローチは、伝統的な多けットレイエンテーションに比べてKVキャッシュサイズを大幅に削減し、推論速度を大幅に向上させる。また、MLAは、表現力を向上させるためにアッププロジェクト行列を使用し、追加の計算を通じて通信ボトルネックを減らす。しかし、MLAはDeepseek V2/V3/R1では効率と効果性を示しているが、多くの主要なモデル提供者は、グループクエリアテンション（GQA）を依存し、MLAの採用については任何の計画がない。本論文中、我々は、GQAは同じKVキャッシュボトルネックを保ちながらMLAで表現できることを示し、逆のことは成り立たないことを示す。MLAの広くなる利用を促進するために、我々は、**TransMLA**というトレーニング後の方法を紹介し、広く使用されているGQAベースの事前学習モデル（例：LLaMA、Qwen、Mixtral）をMLAベースのモデルに変換する。変換後、モデルは追加の学習を通じて表現力を向上させることができ、KVキャッシュサイズを増加させない。また、我々は、MLAに特化された推論加速技術を開発し、変換されたモデルでも低ラテンシーを維持することを目指し、Deepseek R1のより効率的なディスティルを可能にする。",
      "upvotes": 13,
      "discussionId": "67ad5b3b007d78b391946a79"
    },
    "publishedAt": "2025-02-12T21:41:19.791Z",
    "title": "TransMLA: Multi-head Latent Attention Is All You Need",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643f55d4ec817b766686438a",
      "avatarUrl": "/avatars/0feb460432c92ab9ada0d417a7a38f6a.svg",
      "fullname": "mengfanxu",
      "name": "fxmeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.08606",
      "authors": [
        {
          "_id": "67ad77f9cd8de299e5049c05",
          "name": "Dan Busbridge",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c06",
          "name": "Amitis Shidani",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c07",
          "name": "Floris Weers",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c08",
          "name": "Jason Ramapuram",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c09",
          "name": "Etai Littwin",
          "hidden": false
        },
        {
          "_id": "67ad77f9cd8de299e5049c0a",
          "name": "Russ Webb",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T17:52:47.000Z",
      "title": "ディスティルーションスケーリングラース",
      "summary": "私たちは、計算バジュードと学生と教師の間の割り当てに基づく簡約化モデルの性能を予測する簡約化スケーリングラーを提供します。我々の見つけたものは、スケールでの簡約化の使用に関連するリスクを減らします；教師および学生モデルの両方の計算割り当ては、学生の性能を最大化することを目指して行われます。教師が存在する場合や、教師が訓練が必要な場合の簡約化の最適計算レシピを提供します。学生が多数の場合や、教師が既に存在する場合、簡約化は計算レベルが学生サイズに予測的に増加するまでに、訓練された予測学習よりも優れています。一方、学生が1人の場合や、教師も訓練が必要な場合、訓練された学習を優先するべきです。また、我々の大規模な簡約化研究の結果から得られたフィードバックを提供し、簡約化の理解を深め、実験設計に情報を提供します。",
      "upvotes": 8,
      "discussionId": "67ad77fccd8de299e5049d06"
    },
    "publishedAt": "2025-02-12T23:41:41.281Z",
    "title": "Distillation Scaling Laws",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08606.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6063
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.08168",
      "authors": [
        {
          "_id": "67ad5f32d1a5243cc4fa38ad",
          "user": {
            "_id": "64a0ed5ed5374ca472cfb0ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
            "isPro": false,
            "fullname": "ZhimingMa",
            "user": "JimmyMa99",
            "type": "user"
          },
          "name": "Zhiming Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:57.239Z",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38ae",
          "name": "Xiayang Xiao",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38af",
          "name": "Sihao Dong",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b0",
          "name": "Peidong Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b1",
          "name": "HaiPeng Wang",
          "hidden": false
        },
        {
          "_id": "67ad5f32d1a5243cc4fa38b2",
          "name": "Qingyun Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T07:19:36.000Z",
      "title": "SARChat-Bench-2M: SAR画像の多タスクビジョン言語ベンチマークにおける解釈",
      "summary": "合成孔径雷达（SAR）遥感画像解釈の分野で、ビジョン言語モデル（VLMs）は自然言語処理と画像理解に関して驚異的な進歩を遂げていますが、プロジェクト領域での応用はドメイン知識が不足しているため限られています。本論文では、初めての大規模な多タイプダイアログデータセットを提案します。このデータセットは、約200万枚の高品質の画像-テキストペアを含み、詳細なターゲット注釈を持ち、多様なシナリオを収録しています。このデータセットは、視覚理解や物体検出などの重要なタスクを支援し、独自の創新性があります：本論文では、SAR領域のビジョン言語データセットとベンチマークを開発し、VLMsのSAR画像解釈能力を評価することができ、多様な遠隔観測領域での多タイプデータセットの構築にフレームワークを提供します。16つの主流のVLMsに対しての実験で、データセットの効果が完全に証明され、SAR領域の最初の多タスクダイアログベンチマークが成功して設立されました。このプロジェクトは、https://github.com/JimmyMa99/SARChatでリリースされ、SARビジョン言語モデルの深い開発と広範囲の応用を促進することを目的としています。",
      "upvotes": 8,
      "discussionId": "67ad5f37d1a5243cc4fa399c"
    },
    "publishedAt": "2025-02-12T21:57:30.420Z",
    "title": "SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/LvHzRQCttMAvKS-LM0ZDH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08168.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64a0ed5ed5374ca472cfb0ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
      "fullname": "ZhimingMa",
      "name": "JimmyMa99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.08524",
      "authors": [
        {
          "_id": "67ad783da2808b57a3cd3316",
          "name": "Jihoon Tack",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3317",
          "name": "Jack Lanchantin",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3318",
          "name": "Jane Yu",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd3319",
          "name": "Andrew Cohen",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331a",
          "name": "Ilia Kulikov",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331b",
          "name": "Janice Lan",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331c",
          "name": "Shibo Hao",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331d",
          "name": "Yuandong Tian",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331e",
          "name": "Jason Weston",
          "hidden": false
        },
        {
          "_id": "67ad783da2808b57a3cd331f",
          "user": {
            "_id": "659a395421a7431643caedda",
            "avatarUrl": "/avatars/c1e0bbcedce68fe3b4fe39e0cf01c65c.svg",
            "isPro": false,
            "fullname": "Xian Li",
            "user": "xlxxl",
            "type": "user"
          },
          "name": "Xian Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T04:42:38.302Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T16:00:11.000Z",
      "title": "LLM の連続概念付き予習練",
      "summary": "次のトークン予測は、大規模言語モデルの事前学習で使用される標準的な訓練目的として歴代沿って使用されています。トークンレベルの構造誤差を最適化することにより、表現を学習しています。私たちは、離散的次のトークン予測と連続的な概念を組み合わせる新しい事前学習フレームワーク「CoCoMix」を提案します。特に、CoCoMixは事前学習された稀疏自動エンコーダーから学習された連続的な概念を予測し、トークンの隠れ表現と交差してモデルの隠れ状態に混ぜます。複数のベンチマーク（言語モデリングおよび下流の理由論タスク）での試験により、CoCoMixはサンプル効率が高く、標準的な次のトークン予測、知識転移とポーストークンの挿入を常に超えることを示します。コンピューター学習のフレームワークでの概念学習と交差の組み合わせが性能の向上に重要であることを見出しました。また、CoCoMixは予測された概念の直接の検討と修正を可能にし、モデルの内部の理由論プロセスを透明にガイドする方法を提供し、解釈性と操作性を向上させます。",
      "upvotes": 6,
      "discussionId": "67ad783ea2808b57a3cd3361"
    },
    "publishedAt": "2025-02-12T23:42:44.287Z",
    "title": "LLM Pretraining with Continuous Concepts",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08524.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6063
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06533",
      "authors": [
        {
          "_id": "67accc647e1fcf03e14b1033",
          "user": {
            "_id": "6637cd3e691043ccb248d0fd",
            "avatarUrl": "/avatars/94cf09cf817327be50ecba75f7f60fa1.svg",
            "isPro": false,
            "fullname": "Jean Vassoyan",
            "user": "supertardigrade",
            "type": "user"
          },
          "name": "Jean Vassoyan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:24:24.993Z",
          "hidden": false
        },
        {
          "_id": "67accc647e1fcf03e14b1034",
          "user": {
            "_id": "63da60458658cbc1cc489bd7",
            "avatarUrl": "/avatars/620ce7ea229de7abe4dc9ea93021f0e4.svg",
            "isPro": false,
            "fullname": "Nathanaël Beau",
            "user": "Nbeau",
            "type": "user"
          },
          "name": "Nathanaël Beau",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-12T16:29:25.829Z",
          "hidden": false
        },
        {
          "_id": "67accc647e1fcf03e14b1035",
          "user": {
            "_id": "66470e227d73a39a342866e4",
            "avatarUrl": "/avatars/cb0746295492044c483a470692b9637c.svg",
            "isPro": false,
            "fullname": "Roman Plaud",
            "user": "lecraquito",
            "type": "user"
          },
          "name": "Roman Plaud",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:24:27.330Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T14:56:25.000Z",
      "title": "無視KLペナルティ！重要トークンの探索を強化してRLの微調節を向上させる",
      "summary": "長期目標を達成する能力は、現在の大規模言語モデル（LLMs）の開発において重要な課題です。これを解決するために、強化学習（RL）を用いた調整学習を実施することができます。しかし、LLMsの探索は難しいです。新しい解決策を発見すると同時に、調整されたモデルとの距離をどころか近づけないことが必要です。これは通常、Kullback-Leibler（KL）ペナルティで制御されます。この論文では、単純な算術任務で小規模な言語モデルの探索ダイナミクスを調査します。また、探索に影響を及ぼす程度の調整学習の影響を示し、最終的な結果に巨大な影響を与える「キリタルトークン」の重要性を示します。その結果、KLペナルティに対する簡単な改良を提案し、RLの調整学習ステージの効率化を図ります。",
      "upvotes": 5,
      "discussionId": "67accc657e1fcf03e14b109e"
    },
    "publishedAt": "2025-02-13T03:47:28.654Z",
    "title": "Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06533.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66470e227d73a39a342866e4",
      "avatarUrl": "/avatars/cb0746295492044c483a470692b9637c.svg",
      "fullname": "Roman Plaud",
      "name": "lecraquito",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06145",
      "authors": [
        {
          "_id": "67ad9fb9731ff0d7da9f40e9",
          "user": {
            "_id": "67ad9f06040354c9105b00bc",
            "avatarUrl": "/avatars/39e9f4c48c93bb33f155390653936fc1.svg",
            "isPro": false,
            "fullname": "LiHu",
            "user": "Hookszdp",
            "type": "user"
          },
          "name": "Li Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:24.286Z",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ea",
          "name": "Guangyuan Wang",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40eb",
          "name": "Zhen Shen",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ec",
          "name": "Xin Gao",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ed",
          "name": "Dechao Meng",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ee",
          "name": "Lian Zhuo",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40ef",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40f0",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "67ad9fb9731ff0d7da9f40f1",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T04:20:11.000Z",
      "title": "アニマット・ニャンニー 2: 環境の補足を用いた高品質なキャラクター画像アニメーション",
      "summary": "最近のディフューションモデルに基づく特徴マップアニメーション手法であるAnimate Anyoneなどが、一致したもので一般化可能な特徴マップアニメーションの生成に関して進歩していることがわかっています。しかし、これらの手法は、特徴マップとその周辺環境の間の合理的な関連性を生成することができないことがあります。この制限を解決するために、我々はAnimate Anyone 2を紹介し、特徴マップを環境の機能性に基づいてアニメーションすることを目指しています。ソースビデオからの動き信号を抽出することを超えて、また環境の表現を条件付き入力として捉えることを実現しています。環境は特徴マップを除いた領域として構成され、この領域に特徴マップを生成しながら、環境のコンテキストとの一致性を維持するようにしています。特徴マップと環境の関係をより効果的に表現するために、形状無関係のマスク戦略を提案しています。また、物体の相互作用の忠実度を向上させるために、物体ガイダーを利用して相互作用する物体の特徴を抽出し、スペクトラルブレンディングを用いて特徴を注入することを実現しています。また、物体の多様な動作パターンを扱うことができるような姿勢調節戦略を提案しています。実験結果は、提案された手法の優れた性能を示しています。",
      "upvotes": 3,
      "discussionId": "67ad9fbb731ff0d7da9f4145"
    },
    "publishedAt": "2025-02-13T03:45:43.646Z",
    "title": "Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06145.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ad9f06040354c9105b00bc",
      "avatarUrl": "/avatars/39e9f4c48c93bb33f155390653936fc1.svg",
      "fullname": "LiHu",
      "name": "Hookszdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06872",
      "authors": [
        {
          "_id": "67ad7da995ff670869168209",
          "name": "Bo Ni",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820a",
          "name": "Zheyuan Liu",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820b",
          "name": "Leyao Wang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820c",
          "name": "Yongjia Lei",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820d",
          "name": "Yuying Zhao",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820e",
          "name": "Xueqi Cheng",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916820f",
          "name": "Qingkai Zeng",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168210",
          "name": "Luna Dong",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168211",
          "name": "Yinglong Xia",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168212",
          "name": "Krishnaram Kenthapadi",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168213",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168214",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:21:27.740Z",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168215",
          "name": "Md Mehrab Tanjim",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168216",
          "name": "Nesreen Ahmed",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168217",
          "name": "Xiaorui Liu",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168218",
          "name": "Wenqi Fan",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff670869168219",
          "name": "Erik Blasch",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821a",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821b",
          "name": "Meng Jiang",
          "hidden": false
        },
        {
          "_id": "67ad7da995ff67086916821c",
          "name": "Tyler Derr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-08T06:50:47.000Z",
      "title": "トラストワーストリーチアウガージャションフォルラージュオングジャイアントラングジュエーションモデル：アサイド",
      "summary": "レタイブルージングアウゲンレーション（RAG）は、人工知能生成コンテンツ（AIGC）の課題を解決するために設計された先進的な技術です。コンテンツ生成にカテゴリーの検索を統合し、RAGは信頼性のある最新の外部知識を提供し、ハロキニングを減らし、幅広いタスクの関連性を確保します。しかし、RAGパラダイムは成功と可能性があるにも関わらず、最近の研究により、強固性問題、プライバシー懸念、対抗攻撃、責任性問題などの新しいリスクが発見されています。これらのリスクを解決することは、RAGシステムの将来的な応用において重要であり、その信頼性に直接影響します。RAG方法の信頼性を向上させるための方法は開発されていますが、この分野の研究における統一的な視点とフレームワークは欠如です。そこで、本論文では、信頼性のあるRAGシステムの開発についての詳細なマップを提供することを目的としています。議論は信頼性、プライバシー、安全性、公平性、説明性、責任性の5つのキー的な観点を中心にしています。各観点について、一般的なフレームワークとタクロロジーを提供し、現在の課題を理解、既存の解決策を評価、未来の研究方向を特定するための構造化されたアプローチを提供します。広範囲な採用とイノベーションを促進するために、信頼性のあるRAGシステムの影響を大きく持つ下流のアプリケーションも特に挙げます。",
      "upvotes": 3,
      "discussionId": "67ad7daa95ff670869168251"
    },
    "publishedAt": "2025-02-13T00:06:04.056Z",
    "title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05167",
      "authors": [
        {
          "_id": "67aa583c3a878652daeae02e",
          "user": {
            "_id": "60e4738a8c0ddd18fc27ff88",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e4738a8c0ddd18fc27ff88/lpLeeIW8r85RTY4fGZTva.jpeg",
            "isPro": false,
            "fullname": "Ali Modarressi",
            "user": "amodaresi",
            "type": "user"
          },
          "name": "Ali Modarressi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T07:55:42.560Z",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae02f",
          "name": "Hanieh Deilamsalehy",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae030",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:26:01.327Z",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae031",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae032",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae033",
          "name": "Seunghyun Yoon",
          "hidden": false
        },
        {
          "_id": "67aa583c3a878652daeae034",
          "name": "Hinrich Schütze",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-07T18:49:46.000Z",
      "title": "NoLiMa: 長文脈評価を超えた直訳的な対応よりも長文脈評価",
      "summary": "最近の大規模言語モデル（LLMs）は、128Kから1Mトークンの長いコンテキストをサポートしています。これらの機能を評価するための人気の方法は、「ネイル・イン・アップ・ライオン」（NIAH）テストです。これは、長い無関係のコンテキスト（「サック」）から「ネイル」（関連情報）を検索することにより行われます。この方法の拡張は、干渉者の増加、事実の連鎖、テキスト内の理由論理などが含まれています。しかし、これらのベンチマークでは、モデルは「ネイル」と「サック」の既存の文字的なマッチングを利用して課題を簡単にすることができます。これを解決するために、ネイル・イン・アップ・ライオンを拡張したベンチマーク「NoLiMa」を導入しています。これには、問題と「ネイル」が最小限の語彙重複を持つように設計されており、モデルが「サック」内で「ネイル」を位置するために潜在的な連鎖を推論する必要があることを求めています。12つの人気のLLMsを評価しています。これらのモデルは、短いコンテキスト（<1K）ではよく動作しますが、コンテキストの長さが増加すると性能が显著に低下します。例えば、32Kでは、10モデルが強い短コンテキストベースラインの50%を下回ります。さらに、GPT-4oは、最も優れた例外の一つであり、近乎完全なベースライン（99.3%）から69.7%に減少します。我々の分析は、文字的なマッチングがない場合に長いコンテキストでのアタション機構の難易度が増加し、関連情報の検索に難しくなることが原因と考えられています。",
      "upvotes": 3,
      "discussionId": "67aa583d3a878652daeae06c"
    },
    "publishedAt": "2025-02-13T00:04:29.194Z",
    "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07737",
      "authors": [
        {
          "_id": "67ad5d2f8436e8ea7abb7a15",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a16",
          "name": "Shuming Ma",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a17",
          "name": "Xu Sun",
          "hidden": false
        },
        {
          "_id": "67ad5d2f8436e8ea7abb7a18",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T17:57:53.000Z",
      "title": "次ブロック予測：半自動回帰モデリングによるビデオ生成",
      "summary": "次のトークン予測（NTP）は、自動復元（AR）ビデオ生成において事実上の手法ですが、片方向的な依存関係が最適ではあり、推論速度が遅い問題を伴います。本稿では、ビデオ生成に対して半自動復元（semi-AR）フレームワークを提案し、Next-Block Prediction（NBP）としています。ビデオ内容を等だけのブロックに等しく分解し、現在のブロック内の各トークンが次のブロックの対応トークンを同時に予測できるように、生成単位を変更します。傳統的なARモデリングと異なり、各ブロック内でバイデリクシャルアテンションを使用し、トークンが強力な空間依存関係を捉えることができます。複数のトークンを並行に予測することで、NBPモデルは生成ステップ数を大幅に減少させ、高速かつ効率的な推論を実現します。モデルはUCF101でFVDスコア103.3、K600で25.5を達成し、平均4.4点以上のNTPモデルを上回ります。また、推論ステップ数の減少により、NBPモデルは1秒に128x128解像度の8.89フレームを生成し、11倍のスピードアップを実現します。また、モデルサイズが700Mから3Bパラメータの範囲で調査し、生成質量の大幅な向上が見られ、UCF101ではFVDスコアが103.3から55.3、K600では25.5から19.5に落ち、アプローチのスケーラビリティを示します。",
      "upvotes": 3,
      "discussionId": "67ad5d308436e8ea7abb7a3d"
    },
    "publishedAt": "2025-02-12T21:48:00.325Z",
    "title": "Next Block Prediction: Video Generation via Semi-Autoregressive Modeling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07737.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d2e681b8448e1785bbda06",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
      "fullname": "Shuhuai Ren",
      "name": "ShuhuaiRen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07599",
      "authors": [
        {
          "_id": "67ad5bd2ac32a8e230fc8996",
          "name": "Xiliang Yang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8997",
          "name": "Feng Jiang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8998",
          "name": "Qianen Zhang",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc8999",
          "name": "Lei Zhao",
          "hidden": false
        },
        {
          "_id": "67ad5bd2ac32a8e230fc899a",
          "name": "Xiao Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T14:49:44.000Z",
      "title": "DPO-Shift: 直接偏好最適化の分布の変換",
      "summary": "直接偏好最適化（Direct Preference Optimization, DPO）およびその変体は、言語モデルと人間の好みに合わせるために逐次に人気を増しています。これらの方法は、モデルを選択された（または好みのある）と拒否された（または好みのない）の区別をより良くすることを目指しています。しかし、先行研究では、選択されたの際の確率が訓練中に減少することが見出され、この現象は「頻度置換」と呼ばれています。この課題を解決するために、本研究では、選択された確率の分布を制御的に変形する方法を介しています。そして、この方法は、理論的な分析と実験的な検証により、選択確率を改善すると同時に報酬の境界を失わずに行うための基本的な転換として、その重要性が明らかになりました。また、この方法は、MT-Benchや設計された勝率試験のような下流タスクでDPOよりも優れていることを示しています。私たちは、この研究を通じて、DPOの頻度置換問題が理論的に基づいた簡単な解決策で効果的に軽減できることを信じています。コードは、https://github.com/Meaquadddd/DPO-Shift にアクセスできます。",
      "upvotes": 3,
      "discussionId": "67ad5bd3ac32a8e230fc89a7"
    },
    "publishedAt": "2025-02-12T21:43:42.404Z",
    "title": "DPO-Shift: Shifting the Distribution of Direct Preference Optimization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07599.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66270fcef7cf69d4223a8a3f",
      "avatarUrl": "/avatars/115db0326737e65318c92a7b8dc5ed6a.svg",
      "fullname": "Xiao Li",
      "name": "xli0982",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04411",
      "authors": [
        {
          "_id": "67adad972883187d78409a7a",
          "name": "Kunfeng Lai",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7b",
          "name": "Zhenheng Tang",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7c",
          "name": "Xinglin Pan",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7d",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7e",
          "user": {
            "_id": "63024676056ec3a2a8714b24",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
            "isPro": false,
            "fullname": "Xiang Liu",
            "user": "Dominic789654",
            "type": "user"
          },
          "name": "Xiang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-13T08:45:17.030Z",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a7f",
          "name": "Haolan Chen",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a80",
          "name": "Li Shen",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a81",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67adad972883187d78409a82",
          "name": "Xiaowen Chu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T11:26:30.000Z",
      "title": "メディアター：パラメータ衝突が少なく、確信度に基づくメモリ効率的なLLMの統合",
      "summary": "モデルの統合は、異なるタスクで微調練習された大規模言語モデル（LLMs）を強化したものにまとめることである。しかし、モデル間のパラメータの衝突が平均値の性能低下につながる。モデルルーティングは、推論時に個々のモデルを選択してこの問題を解決するが、過剰なストレージコストと計算コストを課し、異なるモデルからの共有知識を活用しない。本研究では、異なるレイヤーがパラメータの衝突の程度が異なることを観察し、このヒントに基づいて、パラメータの衝突が最小限のレイヤーを平均化し、衝突が大きいレイヤーに対して新しいタスクレベルのエクスパートルーティングを使用することである。ストレージコストを更に減少するために、タスクの算術的なスパースさをヒントに、複数の微調練習されたエクスパートを徹密なエクスパートと数多くのスパースなエクスパートに分離する。分布外のサンプルを考慮し、入力データのタスク不確実性に基づいて適切なエクスパートを選択し、統合する。LLaMAとQwenの実際の推論タスクで様々なパラメータサイズを試し、実世界的な推理タスクで評価する。結果は、現在の方法と比較して、システムコストが少なくとも同様に性能向上を収得できることを示している。",
      "upvotes": 2,
      "discussionId": "67adad992883187d78409aa8"
    },
    "publishedAt": "2025-02-13T03:30:35.137Z",
    "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07985",
      "authors": [
        {
          "_id": "67ad9577b469222e0df18134",
          "user": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "isPro": false,
            "fullname": "Victor Gallego",
            "user": "vicgalle",
            "type": "user"
          },
          "name": "Víctor Gallego",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-13T06:47:20.731Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T22:06:25.000Z",
      "title": "MetaSC: テスト時の安全性スペクトラプティ最適化のための言語モデル",
      "summary": "私たちは、推論時にモデルの重みを変更しない限りで言語モデル（LM）の安全性論理を最適化する新しい動的な安全フレームワークを提案します。最近の自己批判法の進歩に基づき、私たちのアプローチは、自己批判と改修のプロセスを適応的に進めるために、安全性プロンプト（簡略化されて「規範」と呼ばれる）をイテレーティブに更新するメタ批判機構を活用しています。この推論時の最適化は、対抗的な「ジャイルブレイク」リクエストに対する性能を改善し、多様な一般的な安全関連タスクにおいても、モラルなハームを避けるや、正直な回答を求めることを通じて安全性を向上させます。数々の言語モデルに対する実験評価により、動的に最適化された安全性プロンプトは、固定システムプロンプトと静的な自己批判防御に比べて、显著に高い安全スコアを示します。コードは、https://github.com/vicgalle/meta-self-critique.git で公開されます。",
      "upvotes": 1,
      "discussionId": "67ad9578b469222e0df18162"
    },
    "publishedAt": "2025-02-13T01:47:30.377Z",
    "title": "MetaSC: Test-Time Safety Specification Optimization for Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07985.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fad8602b8423e1d80b8a965",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
      "fullname": "Victor Gallego",
      "name": "vicgalle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 117
    },
    "isAuthorParticipating": true
  }
]