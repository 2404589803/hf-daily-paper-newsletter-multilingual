[
  {
    "paper": {
      "id": "2507.03724",
      "authors": [
        {
          "_id": "686c7266364e2ad167eb5319",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531a",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531b",
          "name": "Chenyang Xi",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531c",
          "name": "Hanyu Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531d",
          "name": "Chen Tang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531e",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb531f",
          "name": "Ding Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5320",
          "name": "Jiawei Yang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5321",
          "name": "Chunyu Li",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5322",
          "name": "Qingchen Yu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5323",
          "name": "Jihao Zhao",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5324",
          "name": "Yezhaohui Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5325",
          "name": "Peng Liu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5326",
          "name": "Zehao Lin",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5327",
          "name": "Pengyuan Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5328",
          "name": "Jiahao Huo",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5329",
          "name": "Tianyi Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532a",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532b",
          "name": "Kehang Li",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532c",
          "name": "Zhen Tao",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532d",
          "name": "Junpeng Ren",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532e",
          "name": "Huayi Lai",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb532f",
          "name": "Hao Wu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5330",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5331",
          "name": "Zhenren Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5332",
          "name": "Zhaoxin Fan",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5333",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5334",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5335",
          "name": "Junchi Yan",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5336",
          "name": "Mingchuan Yang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5337",
          "name": "Tong Xu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5338",
          "name": "Wei Xu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb5339",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533a",
          "name": "Haofeng Wang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533b",
          "name": "Hongkang Yang",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533c",
          "user": {
            "_id": "686c965f418acea658859af4",
            "avatarUrl": "/avatars/4f453abeb7e82a3042cbec751b5cdb63.svg",
            "isPro": false,
            "fullname": "Wentao Zhang",
            "user": "Wentao-PKU",
            "type": "user"
          },
          "name": "Wentao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:32.391Z",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533d",
          "name": "Zhi-Qin John Xu",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533e",
          "name": "Siheng Chen",
          "hidden": false
        },
        {
          "_id": "686c7266364e2ad167eb533f",
          "name": "Feiyu Xiong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/twRhDA4ThpQMfInSFdYDA.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/9xHrSG0a8X2CYHxvfMFSH.gif",
        "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/aaUeE4u2RhCnFYNKCqvyn.gif"
      ],
      "publishedAt": "2025-07-04T17:21:46.000Z",
      "submittedOnDailyAt": "2025-07-08T01:46:43.501Z",
      "title": "MemOS: アイ・システム向けのメモリーオシャス",
      "submittedOnDailyBy": {
        "_id": "669e0b93c7cb0568dac6e92e",
        "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
        "isPro": false,
        "fullname": "hanyu Wang",
        "user": "UglyToilet",
        "type": "user"
      },
      "summary": "大型言語モデル（LLMs）は人工知能（AGI）の重要なインフラとなっており、それらのメモリ管理システムの定義不足は、長文脈論理、継続的な個人化、知識の一貫性の開発に負面影響を及ぼしています。現在のモデルは静的パラメータと短時間的なコンテキスト状態に依存し、ユーザーの好みを追跡したり、長期間で知識を更新する能力を限定しています。レビューアウガーデンション（RAG）は外部の知識をプレインテキストで追加していますが、それは状態なしの軽便な解決策であり、生命周期制御または持続可能な表現の統合には欠けています。最近の研究は、メモリ階層の視点からLLMsの訓練と推論コストをモデル化し、パラメータメモリと外部リビューの間に明記したメモリ層を追加することで、特定の知識の外観化によりこれらのコストを大幅に減少できることを示しています。計算効率よりも、LLMsは時間とコンテキストによる情報の分布から生じる広い課題を見つめています。これらの課題を解決するために、我々はMemOS（メモリオペレーティングシステム）を提案しています。MemOSはメモリを管理可能なシステムリソースとして扱い、プレインテキスト、活性化ベース、パラメータレベルのメモリの表現、スケジューリング、および進化を統一して、コスト効率的なストレージと検索を可能にします。基本的な単位として、MemCubeはメモリ内容と元データ、バージョン情報などのメタデータをまとめています。MemCubeは時間により組み立て、移動、融合することができ、メモリタイプの柔軟な転換を可能にし、パラメータベースの学習と検索を結びつけます。MemOSはメモリセンター的なシステムフレームワークを構築し、LLMsに制御可能性、可塑性、進化可能性を提供し、継続的な学習と個人化モデリングの基盤を築くことを目指しています。",
      "upvotes": 65,
      "discussionId": "686c7266364e2ad167eb5340",
      "projectPage": "https://memos.openmem.net/",
      "githubRepo": "https://github.com/MemTensor/MemOS",
      "ai_summary": "MemOS is proposed as a memory operating system for Large Language Models to enhance memory management, enabling efficient storage and retrieval, and facilitating continual learning and personalized modeling.",
      "ai_keywords": [
        "LLMs",
        "Artificial General Intelligence",
        "AGI",
        "Retrieval-Augmented Generation",
        "RAG",
        "MemOS",
        "memory operating system",
        "MemCube",
        "activation-based memory"
      ],
      "githubStars": 527
    },
    "publishedAt": "2025-07-04T13:21:46.000Z",
    "title": "MemOS: A Memory OS for AI System",
    "summary": "Large Language Models (LLMs) have become an essential infrastructure for\nArtificial General Intelligence (AGI), yet their lack of well-defined memory\nmanagement systems hinders the development of long-context reasoning, continual\npersonalization, and knowledge consistency.Existing models mainly rely on\nstatic parameters and short-lived contextual states, limiting their ability to\ntrack user preferences or update knowledge over extended periods.While\nRetrieval-Augmented Generation (RAG) introduces external knowledge in plain\ntext, it remains a stateless workaround without lifecycle control or\nintegration with persistent representations.Recent work has modeled the\ntraining and inference cost of LLMs from a memory hierarchy perspective,\nshowing that introducing an explicit memory layer between parameter memory and\nexternal retrieval can substantially reduce these costs by externalizing\nspecific knowledge. Beyond computational efficiency, LLMs face broader\nchallenges arising from how information is distributed over time and context,\nrequiring systems capable of managing heterogeneous knowledge spanning\ndifferent temporal scales and sources. To address this challenge, we propose\nMemOS, a memory operating system that treats memory as a manageable system\nresource. It unifies the representation, scheduling, and evolution of\nplaintext, activation-based, and parameter-level memories, enabling\ncost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates\nboth memory content and metadata such as provenance and versioning. MemCubes\ncan be composed, migrated, and fused over time, enabling flexible transitions\nbetween memory types and bridging retrieval with parameter-based learning.\nMemOS establishes a memory-centric system framework that brings\ncontrollability, plasticity, and evolvability to LLMs, laying the foundation\nfor continual learning and personalized modeling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/twRhDA4ThpQMfInSFdYDA.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/9xHrSG0a8X2CYHxvfMFSH.gif",
      "https://cdn-uploads.huggingface.co/production/uploads/669e0b93c7cb0568dac6e92e/aaUeE4u2RhCnFYNKCqvyn.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03724.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669e0b93c7cb0568dac6e92e",
      "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
      "fullname": "hanyu Wang",
      "name": "UglyToilet",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05163",
      "authors": [
        {
          "_id": "686c928b364e2ad167eb53f1",
          "name": "Yutian Chen",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f2",
          "name": "Shi Guo",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f3",
          "name": "Tianshuo Yang",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f4",
          "name": "Lihe Ding",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f5",
          "name": "Xiuyuan Yu",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f6",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "686c928b364e2ad167eb53f7",
          "name": "Tianfan Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T16:18:35.000Z",
      "submittedOnDailyAt": "2025-07-08T02:14:44.087Z",
      "title": "4DSloMo: 4Dコンストラクションに高速シーンを対応した非同期撮影",
      "submittedOnDailyBy": {
        "_id": "64970d3d9c3b29dca8633f87",
        "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
        "isPro": false,
        "fullname": "JunhaoZhuang",
        "user": "JunhaoZhuang",
        "type": "user"
      },
      "summary": "高速ディナミックシーンの4D再構築は高速移動分析とリアルタイプの4D再構築に重要です。しかし、多くの4D撮影システムは30FPS（フレームごとの秒数）以下のレートに限られており、低FPS入力から高速移動の直接な4D再構築は望ましくない結果を生じる可能性があります。本論文では、新しい撮影および処理モジュールを用いた低FPSカメラのみを用いた高速4D撮影システムを提案します。撮影側では、カメラの開始時間をスタッグにして効果的なフレームレートを上げる非同期撮影スキームを提案し、25FPSの基盤フレームレートを拡張して100-200FPSの等価フレームレートを達成します。処理側では、4D稀少視点再構築によるアーティファクトを修正するために新しい生成モデルを提案し、特に、稀少4D再構築のためのビデオディフュージョンベースのアーティファクト修正モデルを学習し、欠損の詳細を修正し、時間的な一貫性を維持し、全体的な再構築品質を向上させることを目指します。実験結果は、本方法が同期撮影に比べて高速4D再構築を大幅に向上させることを示しています。",
      "upvotes": 29,
      "discussionId": "686c928b364e2ad167eb53f8",
      "ai_summary": "A high-speed 4D capturing system using low FPS cameras with asynchronous capture and video-diffusion-based artifact correction enhances reconstruction quality.",
      "ai_keywords": [
        "asynchronous capture",
        "video-diffusion-based artifact-fix model",
        "sparse 4D reconstruction",
        "temporal consistency"
      ]
    },
    "publishedAt": "2025-07-07T12:18:35.000Z",
    "title": "4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous\n  Capture",
    "summary": "Reconstructing fast-dynamic scenes from multi-view videos is crucial for\nhigh-speed motion analysis and realistic 4D reconstruction. However, the\nmajority of 4D capture systems are limited to frame rates below 30 FPS (frames\nper second), and a direct 4D reconstruction of high-speed motion from low FPS\ninput may lead to undesirable results. In this work, we propose a high-speed 4D\ncapturing system only using low FPS cameras, through novel capturing and\nprocessing modules. On the capturing side, we propose an asynchronous capture\nscheme that increases the effective frame rate by staggering the start times of\ncameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our\nmethod achieves an equivalent frame rate of 100-200 FPS without requiring\nspecialized high-speed cameras. On processing side, we also propose a novel\ngenerative model to fix artifacts caused by 4D sparse-view reconstruction, as\nasynchrony reduces the number of viewpoints at each timestamp. Specifically, we\npropose to train a video-diffusion-based artifact-fix model for sparse 4D\nreconstruction, which refines missing details, maintains temporal consistency,\nand improves overall reconstruction quality. Experimental results demonstrate\nthat our method significantly enhances high-speed 4D reconstruction compared to\nsynchronous capture.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05163.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64970d3d9c3b29dca8633f87",
      "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg",
      "fullname": "JunhaoZhuang",
      "name": "JunhaoZhuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04447",
      "authors": [
        {
          "_id": "686cab67364e2ad167eb5464",
          "name": "Wenyao Zhang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5465",
          "name": "Hongsi Liu",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5466",
          "user": {
            "_id": "63c3e8abc7d7f4c63a515a02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
            "isPro": false,
            "fullname": "Zekun Qi",
            "user": "qizekun",
            "type": "user"
          },
          "name": "Zekun Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:05:57.749Z",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5467",
          "name": "Yunnan Wang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5468",
          "name": "XinQiang Yu",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5469",
          "name": "Jiazhao Zhang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546a",
          "user": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "isPro": false,
            "fullname": "Runpei Dong",
            "user": "RunpeiDong",
            "type": "user"
          },
          "name": "Runpei Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:00.401Z",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546b",
          "name": "Jiawei He",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546c",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546d",
          "name": "Zhizheng Zhang",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546e",
          "name": "Li Yi",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb546f",
          "name": "Wenjun Zeng",
          "hidden": false
        },
        {
          "_id": "686cab67364e2ad167eb5470",
          "name": "Xin Jin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/Oe8hIH4_I_Pql9N72iHbn.mp4"
      ],
      "publishedAt": "2025-07-06T16:14:29.000Z",
      "submittedOnDailyAt": "2025-07-08T03:55:36.991Z",
      "title": "ドリームVLA: 世界知識を網羅的に理解した視覚・言語・行動モデル",
      "submittedOnDailyBy": {
        "_id": "6201fc5d91d53938a6432fbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
        "isPro": false,
        "fullname": "Runpei Dong",
        "user": "RunpeiDong",
        "type": "user"
      },
      "summary": "最近の視覚言語アクション（VLA）モデルの進展は、画像生成とアクション予測の統合により、機械人操作の一般化と推理に望ましい成果を示している。しかし、現在の方法は、冗長な情報を含む難しい画像ベースの予測に限られており、動的、空間的、そして語義的な情報を含む構成的な世界知識を欠く。これらの制限を解決するために、我々は、逆動力学モデリングを可能にするために、詳細な世界知識予測を統合する新しいVLAフレームワークを提案します。特に、DreamVLAは、動的な領域をガイドする世界知識予測と空間的、語義的カップルを含むものを導入し、アクションプランニングにおいてコンパクトで詳細な表現を提供する。この設計は、人間が世界と相互作用するように、抽象的多タイプの理由の連鎖を形成して行動するように合致している。トレーニング中に動的的、空間的、語義的な情報の干渉を軽減するために、ブロックワイズ構造付きのアテンション機構を採用し、その相互のアテンションをマスクし、情報漏れを防き、各表現をクリーンで分離させる。また、未来のアクションの条件分布をモデル化するために、拡散ベースのトランスフォーマーを使用し、アクション表現を共有的な潜在的特徴から分離する。リアルウォールとシミュレーション環境での拡散的な実験は、DreamVLAはリアルロボットタスクで76.7%の成功率、CALVIN ABC-Dベンチマークで4.44の平均長さを達成したことを示している。",
      "upvotes": 26,
      "discussionId": "686cab67364e2ad167eb5471",
      "projectPage": "https://zhangwenyao1.github.io/DreamVLA/",
      "githubRepo": "https://github.com/Zhangwenyao1/DreamVLA",
      "ai_summary": "DreamVLA improves robot manipulation through a VLA framework that incorporates world knowledge, dynamic-region guidance, and a diffusion-based transformer to ensure clear, disentangled representations for action planning.",
      "ai_keywords": [
        "vision-language-action",
        "dynamic-region-guided",
        "world knowledge prediction",
        "spatial and semantic cues",
        "block-wise structured attention",
        "diffusion-based transformer"
      ],
      "githubStars": 24
    },
    "publishedAt": "2025-07-06T12:14:29.000Z",
    "title": "DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive\n  World Knowledge",
    "summary": "Recent advances in vision-language-action (VLA) models have shown promise in\nintegrating image generation with action prediction to improve generalization\nand reasoning in robot manipulation. However, existing methods are limited to\nchallenging image-based forecasting, which suffers from redundant information\nand lacks comprehensive and critical world knowledge, including dynamic,\nspatial and semantic information. To address these limitations, we propose\nDreamVLA, a novel VLA framework that integrates comprehensive world knowledge\nforecasting to enable inverse dynamics modeling, thereby establishing a\nperception-prediction-action loop for manipulation tasks. Specifically,\nDreamVLA introduces a dynamic-region-guided world knowledge prediction,\nintegrated with the spatial and semantic cues, which provide compact yet\ncomprehensive representations for action planning. This design aligns with how\nhumans interact with the world by first forming abstract multimodal reasoning\nchains before acting. To mitigate interference among the dynamic, spatial and\nsemantic information during training, we adopt a block-wise structured\nattention mechanism that masks their mutual attention, preventing information\nleakage and keeping each representation clean and disentangled. Moreover, to\nmodel the conditional distribution over future actions, we employ a\ndiffusion-based transformer that disentangles action representations from\nshared latent features. Extensive experiments on both real-world and simulation\nenvironments demonstrate that DreamVLA achieves 76.7% success rate on real\nrobot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/Oe8hIH4_I_Pql9N72iHbn.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04447.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6201fc5d91d53938a6432fbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
      "fullname": "Runpei Dong",
      "name": "RunpeiDong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05197",
      "authors": [
        {
          "_id": "686c7f78364e2ad167eb5354",
          "name": "Shihan Dou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5355",
          "name": "Shichun Liu",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5356",
          "user": {
            "_id": "655c6b1abfb531437a54c0e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/V8Md2mMX83hrSowKk6qMS.jpeg",
            "isPro": false,
            "fullname": "Yuming Yang",
            "user": "Umean",
            "type": "user"
          },
          "name": "Yuming Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:28.979Z",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5357",
          "name": "Yicheng Zou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5358",
          "name": "Yunhua Zhou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5359",
          "name": "Shuhao Xing",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535a",
          "name": "Chenhao Huang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535b",
          "name": "Qiming Ge",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535c",
          "name": "Demin Song",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535d",
          "name": "Haijun Lv",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535e",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb535f",
          "name": "Chengqi Lv",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5360",
          "name": "Enyu Zhou",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5361",
          "name": "Honglin Guo",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5362",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5363",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5364",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5365",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5366",
          "name": "Xipeng Qiu",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5367",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5368",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "686c7f78364e2ad167eb5369",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T16:56:31.000Z",
      "submittedOnDailyAt": "2025-07-08T02:18:45.573Z",
      "title": "予っちめられた政策識別器は一般的な報酬モデルです。",
      "submittedOnDailyBy": {
        "_id": "6234238485575ce6ff1f169a",
        "avatarUrl": "/avatars/e5fff05f21cdea4e5aebc8ba426fac29.svg",
        "isPro": false,
        "fullname": "Yicheng Zou",
        "user": "RowitZou",
        "type": "user"
      },
      "summary": "We offer a novel perspective on reward modeling by formulating it as a policy discriminator, which quantifies the difference between two policies to generate a reward signal, guiding the training policy towards a target policy with desired behaviors. Based on this conceptual insight, we propose a scalable pre-training method named Policy Discriminative Learning (POLAR), which trains a reward model (RM) to discern identical policies and discriminate different ones. Unlike traditional reward modeling methods relying on absolute preferences, POLAR captures the relative difference between one policy and an arbitrary target policy, which is a scalable, high-level optimization objective suitable for modeling generic ranking relationships. Leveraging the POLAR pre-training paradigm, we present a series of RMs with parameter scales from 1.8B to 7B. Empirical results show that POLAR substantially outperforms traditional non-pre-trained methods, significantly enhancing RM performance. For instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on STEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA baselines. POLAR also shows robust generalization capabilities in RLHF using Reinforcement Fine-tuning (RFT), providing reliable reward signals and markedly enhancing policy performance--improving LLaMa3.1-8B from an average of 47.36% to 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover, scaling experiments reveal a clear power-law relationship between computation and performance, supported by linear correlation coefficients approaching 0.99. The impressive performance, strong generalization, and scaling properties suggest that POLAR is a promising direction for developing general and strong reward models.",
      "upvotes": 24,
      "discussionId": "686c7f78364e2ad167eb536a",
      "githubRepo": "https://github.com/InternLM/POLAR",
      "ai_summary": "A scalable reward modeling method, Policy Discriminative Learning (POLAR), enhances reward model performance and generalizes robustly in reinforcement learning through policy comparison.",
      "ai_keywords": [
        "policy discriminator",
        "reward model",
        "Policy Discriminative Learning",
        "POLAR",
        "Reinforcement Fine-tuning"
      ],
      "githubStars": 35
    },
    "publishedAt": "2025-07-07T12:56:31.000Z",
    "title": "Pre-Trained Policy Discriminators are General Reward Models",
    "summary": "We offer a novel perspective on reward modeling by formulating it as a policy\ndiscriminator, which quantifies the difference between two policies to generate\na reward signal, guiding the training policy towards a target policy with\ndesired behaviors. Based on this conceptual insight, we propose a scalable\npre-training method named Policy Discriminative Learning (POLAR), which trains\na reward model (RM) to discern identical policies and discriminate different\nones. Unlike traditional reward modeling methods relying on absolute\npreferences, POLAR captures the relative difference between one policy and an\narbitrary target policy, which is a scalable, high-level optimization objective\nsuitable for modeling generic ranking relationships. Leveraging the POLAR\npre-training paradigm, we present a series of RMs with parameter scales from\n1.8B to 7B. Empirical results show that POLAR substantially outperforms\ntraditional non-pre-trained methods, significantly enhancing RM performance.\nFor instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on\nSTEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA\nbaselines. POLAR also shows robust generalization capabilities in RLHF using\nReinforcement Fine-tuning (RFT), providing reliable reward signals and markedly\nenhancing policy performance--improving LLaMa3.1-8B from an average of 47.36%\nto 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover,\nscaling experiments reveal a clear power-law relationship between computation\nand performance, supported by linear correlation coefficients approaching 0.99.\nThe impressive performance, strong generalization, and scaling properties\nsuggest that POLAR is a promising direction for developing general and strong\nreward models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05197.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6234238485575ce6ff1f169a",
      "avatarUrl": "/avatars/e5fff05f21cdea4e5aebc8ba426fac29.svg",
      "fullname": "Yicheng Zou",
      "name": "RowitZou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.00994",
      "authors": [
        {
          "_id": "6864e267d59a9eda59024bab",
          "user": {
            "_id": "65fa95405355a52c784633fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fa95405355a52c784633fc/rSfBUHPa7eSAsLd8DuOq4.png",
            "isPro": false,
            "fullname": "Hippolyte Gisserot-Boukhlef",
            "user": "hgissbkh",
            "type": "user"
          },
          "name": "Hippolyte Gisserot-Boukhlef",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:53:51.109Z",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bac",
          "user": {
            "_id": "62be186a5f59ff2320e6e32b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
            "isPro": false,
            "fullname": "Nicolas-BZRD",
            "user": "Nicolas-BZRD",
            "type": "user"
          },
          "name": "Nicolas Boizard",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-02T12:22:12.240Z",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bad",
          "name": "Manuel Faysse",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bae",
          "name": "Duarte M. Alves",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024baf",
          "name": "Emmanuel Malherbe",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bb0",
          "name": "André F. T. Martins",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bb1",
          "name": "Céline Hudelot",
          "hidden": false
        },
        {
          "_id": "6864e267d59a9eda59024bb2",
          "name": "Pierre Colombo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-01T17:45:48.000Z",
      "submittedOnDailyAt": "2025-07-08T07:51:12.578Z",
      "title": "そのままのマスク言語モデリングでエンコーダーを予ちゅうしますか？",
      "submittedOnDailyBy": {
        "_id": "62be186a5f59ff2320e6e32b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
        "isPro": false,
        "fullname": "Nicolas-BZRD",
        "user": "Nicolas-BZRD",
        "type": "user"
      },
      "summary": "学習高品質のテキスト表現は、広範囲のNLPタスクに基盤的な重要性を持っています。マスクされた言語モデリング（MLM）を基盤としたエンコーダーの事前学習は、伝統的に用いられてきましたが、最近の証拠から、原因関係言語モデリング（CLM）で事前学習されたデコーダーモデルが、エンコーダーとして再利用でき、テキスト表現のベンチマークで伝統的なエンコーダーを超える性能を示すことが明らかになりました。しかし、これらの効果がCLMの目的の固有の優位性から来るか、モデルやデータサイズなどの混雑因子から来るかは、まだ明らかではありません。本論文では、この問題を解決するために、大規模な、調節された事前学習の消去試験を行い、210万から10億パラメータの範囲で30モデルを訓練し、15,000以上の微調節と評価を実行しました。私たちは、MLMで訓練することは、テキスト表現タスク全体で一般的により良い性能を示すことを見出しましたが、CLMで訓練されたモデルはデータエフェクティブであり、微調節の安定性が向上します。これらの発見に基づき、固定的な計算訓練バジュードで最適な性能を達成するための二段階訓練戦略を実験的に示しました。また、この戦略は、既存のLLMエコシステムから読み込み可能な事前学習デコーダーモデルを初期化し、最先端のエンコーダーモデルの訓練負担を減らすことにより、より有効になることを示しました。すべてのプロジェクトアーティファクトを公開して、進める研究を促進します。",
      "upvotes": 23,
      "discussionId": "6864e267d59a9eda59024bb3",
      "githubRepo": "https://github.com/Nicolas-BZRD/EuroBERT",
      "githubStars": 59
    },
    "publishedAt": "2025-07-01T13:45:48.000Z",
    "title": "Should We Still Pretrain Encoders with Masked Language Modeling?",
    "summary": "Learning high-quality text representations is fundamental to a wide range of\nNLP tasks. While encoder pretraining has traditionally relied on Masked\nLanguage Modeling (MLM), recent evidence suggests that decoder models\npretrained with Causal Language Modeling (CLM) can be effectively repurposed as\nencoders, often surpassing traditional encoders on text representation\nbenchmarks. However, it remains unclear whether these gains reflect an inherent\nadvantage of the CLM objective or arise from confounding factors such as model\nand data scale. In this paper, we address this question through a series of\nlarge-scale, carefully controlled pretraining ablations, training a total of 30\nmodels ranging from 210 million to 1 billion parameters, and conducting over\n15,000 fine-tuning and evaluation runs. We find that while training with MLM\ngenerally yields better performance across text representation tasks,\nCLM-trained models are more data-efficient and demonstrate improved fine-tuning\nstability. Building on these findings, we experimentally show that a biphasic\ntraining strategy that sequentially applies CLM and then MLM, achieves optimal\nperformance under a fixed computational training budget. Moreover, we\ndemonstrate that this strategy becomes more appealing when initializing from\nreadily available pretrained CLM models (from the existing LLM ecosystem),\nreducing the computational burden needed to train best-in-class encoder models.\nWe release all project artifacts at https://hf.co/MLMvsCLM to foster further\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00994.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "62be186a5f59ff2320e6e32b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
      "fullname": "Nicolas-BZRD",
      "name": "Nicolas-BZRD",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03483",
      "authors": [
        {
          "_id": "686c90d4364e2ad167eb53d8",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53d9",
          "name": "Guanyu Li",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53da",
          "name": "Yutao Fan",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53db",
          "name": "Honglin Guo",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53dc",
          "name": "Yufang Liu",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53dd",
          "name": "Xiaoran Fan",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53de",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53df",
          "name": "Jingchao Ding",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e0",
          "name": "Wangmeng Zuo",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e1",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e2",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e3",
          "name": "Tao Ji",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e4",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e5",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "686c90d4364e2ad167eb53e6",
          "name": "Xuanjing Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T11:20:09.000Z",
      "submittedOnDailyAt": "2025-07-08T02:00:41.875Z",
      "title": "BMMR: 大規模のバイリンガル・マルチモデル・マルチディスカリティの理由\nデータセット",
      "submittedOnDailyBy": {
        "_id": "638ef0b0c67af472d31674a6",
        "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg",
        "isPro": false,
        "fullname": "Honglin Guo",
        "user": "KYLN24",
        "type": "user"
      },
      "summary": "この論文では、BMMRという大規模なバイリンガル、マルチモデル、多学問類統合理論データセットを紹介します。このデータセットは、コミュニティが大規模なマルチモデル（LMMs）の開発と評価を行うために使用できます。BMMRは、110,000点の大学レベルの質問からなり、300件のUNESCO定義の主題を拡張し、多様な形式（選択肢、欠け込み、開放質問）からなり、本、試験、クイズなどの印刷やデジタルメディアからサンプリングされています。全データは、人間が参加したスケーラブルなフレームワークでカレーティングされ、各インスタンスに高品質の理由のパスが付いています。データセットは、BMMR-EvalとBMMR-Trainの2つの部門に分けられています。BMMR-Evalは、20,458点の高品質のインスタンスを含み、中国語と英語での多学問類統合理論の知識と理由を評価するために使用できます。BMMR-Trainは、88,991点のインスタンスを含み、現在の数学の理由に焦点を当てていることを拡張し、多様な学問類とディスカイドームに対する研究と開発を支援します。また、BMMR-Verifierというプロセスベースの多学問類経験評価ツールを提案し、理由のパスの正確なフィンエガイン評価を行います。24モデルの拡張実験により、(i) SOTAモデル（例：o3とGemini-2.5-Pro）はBMMR-Evalで大きな開発余地が残っています；(ii) 理由モデルは学問類のバイアスを示し、特定の主題でのみLMMsよりも優れています；(iii) オープンソースモデルはプロピエタリーモデルよりも追い抜かれています；(iv) BMMR-Trainでの微調節はこの間違いを狭めます。また、BMMR-Verifierと他の詳細な研究を用いて、現在のLMMsがマルチディスカイドーム統合理論で面臨する課題を明らかにします。データを公開し、この研究でコミュニティにおかわりを提供したいと考えています。",
      "upvotes": 19,
      "discussionId": "686c90d4364e2ad167eb53e7",
      "projectPage": "https://bmmr.pages.dev/",
      "githubRepo": "https://github.com/woooodyy/BMMR",
      "ai_summary": "A large-scale dataset and verification tool are introduced for assessing and improving cross-disciplinary reasoning capabilities in multimodal models.",
      "ai_keywords": [
        "bilingual",
        "multimodal",
        "multi-disciplinary",
        "large multimodal models",
        "LMMs",
        "UNESCO-defined subjects",
        "multiple-choice",
        "fill-in-the-blank",
        "open-ended QA",
        "human-in-the-loop",
        "scalable framework",
        "high-quality reasoning path",
        "multidisciplinary reasoning",
        "BMMR-Verifier",
        "reasoning models",
        "discipline bias",
        "reasoning-chain analysis"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-07-04T07:20:09.000Z",
    "title": "BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning\n  Dataset",
    "summary": "In this paper, we introduce BMMR, a large-scale bilingual, multimodal,\nmulti-disciplinary reasoning dataset for the community to develop and evaluate\nlarge multimodal models (LMMs). BMMR comprises 110k college-level questions\nspanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice,\nfill-in-the-blank, and open-ended QA-and sourced from both print and digital\nmedia such as books, exams, and quizzes. All data are curated and filtered via\na human-in-the-loop and scalable framework, and each instance is paired with a\nhigh-quality reasoning path. The dataset is organized into two parts: BMMR-Eval\nthat comprises 20,458 high-quality instances to comprehensively assess LMMs'\nknowledge and reasoning across multiple disciplines in both Chinese and\nEnglish; and BMMR-Train that contains 88,991 instances to support further\nresearch and development, extending the current focus on mathematical reasoning\nto diverse disciplines and domains. In addition, we propose the process-based\nmulti-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained\nevaluation of reasoning paths. Extensive experiments on 24 models reveal that\n(i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom\non BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs\nonly on specific subjects; (iii) open-source models still trail their\nproprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap.\nAdditionally, we conduct reasoning-chain analyses using BMMR-Verifier and other\nin-depth studies, uncovering the challenges LMMs currently face in\nmultidisciplinary reasoning. We will release the data, and we hope our work can\noffer insights and contributions to the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03483.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638ef0b0c67af472d31674a6",
      "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg",
      "fullname": "Honglin Guo",
      "name": "KYLN24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.02029",
      "authors": [
        {
          "_id": "68679569213f123a1f88b87c",
          "name": "BAAI RoboBrain Team",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b87d",
          "user": {
            "_id": "668fa476cbcaf7ab0e4c58b3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668fa476cbcaf7ab0e4c58b3/F5Jj-nPCjU6uxZyfkY3qw.jpeg",
            "isPro": false,
            "fullname": "Mingyu Cao",
            "user": "cmyopu",
            "type": "user"
          },
          "name": "Mingyu Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-07T11:24:50.715Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b87e",
          "name": "Huajie Tan",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b87f",
          "user": {
            "_id": "668f5478b3991ac0c3fc9c2f",
            "avatarUrl": "/avatars/a775853d3b88e7b1c8494ca837b5495c.svg",
            "isPro": false,
            "fullname": "yuhengji",
            "user": "yuheng2000",
            "type": "user"
          },
          "name": "Yuheng Ji",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:53:03.017Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b880",
          "user": {
            "_id": "67e406e64b80e9b39e2a85d6",
            "avatarUrl": "/avatars/a021be64341e5d7a079858916fa34c28.svg",
            "isPro": false,
            "fullname": "MinglanLin",
            "user": "MinglanLin",
            "type": "user"
          },
          "name": "Minglan Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:52:59.066Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b881",
          "name": "Zhiyu Li",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b882",
          "user": {
            "_id": "66aadfc1344279e0243d4569",
            "avatarUrl": "/avatars/d2afbceb2e6279e42ed9ad98dffa7f0a.svg",
            "isPro": false,
            "fullname": "Caozhou",
            "user": "Caozhou1995",
            "type": "user"
          },
          "name": "Zhou Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:48.313Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b883",
          "name": "Pengwei Wang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b884",
          "user": {
            "_id": "63f08dc79cf89c9ed1bb89cd",
            "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg",
            "isPro": false,
            "fullname": "Zhoues",
            "user": "Zhoues",
            "type": "user"
          },
          "name": "Enshen Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:52:57.069Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b885",
          "name": "Yi Han",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b886",
          "name": "Yingbo Tang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b887",
          "name": "Xiangqi Xu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b888",
          "name": "Wei Guo",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b889",
          "name": "Yaoxu Lyu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88a",
          "name": "Yijie Xu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88b",
          "name": "Jiayu Shi",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88c",
          "user": {
            "_id": "650bf938677f9e45963d672e",
            "avatarUrl": "/avatars/7d4159067b5005a3a635e36b26b7b239.svg",
            "isPro": false,
            "fullname": "Cheng Chi",
            "user": "ChuckChi",
            "type": "user"
          },
          "name": "Cheng Chi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:53:01.035Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88d",
          "name": "Mengdi Zhao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88e",
          "name": "Xiaoshuai Hao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b88f",
          "name": "Shanyu Rong",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b890",
          "name": "Zhengliang Cai",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b891",
          "name": "Bolun Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b892",
          "name": "Shuyi Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b893",
          "name": "Huaihai Lyu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b894",
          "name": "Mengfei Du",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b895",
          "name": "Lingfeng Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b896",
          "name": "Xi Feng",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b897",
          "name": "Xiaodan Liu",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b898",
          "name": "Yance Jiao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b899",
          "name": "Chenrui He",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89a",
          "user": {
            "_id": "63157214362e3e95ea553db5",
            "avatarUrl": "/avatars/a421c25128c71be6d0c92490cebbbccc.svg",
            "isPro": false,
            "fullname": "lyu",
            "user": "ceci3",
            "type": "user"
          },
          "name": "Mengsi Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:50.528Z",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89b",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89c",
          "name": "Yulong Ao",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89d",
          "name": "Xue Sun",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89e",
          "name": "Zheqi He",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b89f",
          "name": "Jingshu Zheng",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a0",
          "name": "Xi Yang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a1",
          "name": "Donghai Shi",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a2",
          "name": "Kunchang Xie",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a3",
          "name": "Bochao Zhang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a4",
          "name": "Shaokai Nie",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a5",
          "name": "Chunlei Men",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a6",
          "name": "Yonghua Lin",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a7",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a8",
          "name": "Tiejun Huang",
          "hidden": false
        },
        {
          "_id": "68679569213f123a1f88b8a9",
          "name": "Shanghang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:05:33.000Z",
      "submittedOnDailyAt": "2025-07-08T06:32:16.956Z",
      "title": "RoboBrain 2.0 技術報告",
      "submittedOnDailyBy": {
        "_id": "63a369d98c0c89dcae3b8329",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
        "isPro": true,
        "fullname": "Adina Yakefu",
        "user": "AdinaY",
        "type": "user"
      },
      "summary": "ロボブレイン2.0、我々の最新世代の具象化ビジョン・言語基礎モデルを紹介します。これは、物理環境での複雑な具象化タスクの認識、理由論、計画を統合するモデルです。これは2つのバージョンを備えています：7Bモデルと32Bモデル。ビジョンエンコーダと言語モデルを構成するヒューロジカルアーキテクチャを特徴としています。その小さなサイズにもかかわらず、ロボブレイン2.0は幅広い範囲の具象化理由論タスクで強力な性能を収めています。空間ベンチマークと時間ベンチマークでも、32Bバージョンは先駆者の結果を達成し、開放ソースモデルや専有モデルを超えています。特に、空間理解（例：機能予測、空間指摘、軌跡予測）と時間的決策論（例：閉路インタラクション、多アガント長期計画、場面グラフ更新）を支援しています。この報告書では、モデルアーキテクチャ、データ構築、多段階トレーニング戦略、インフラと実用的なアプリケーションを詳細に説明しています。ロボブレイン2.0は具象化AIの研究を進め、一般的な具象化アガントの構築に向けた実用的なステップとして役立つことを望みます。コード、チェックポイントとベンチマークは、https://superrobobrain.github.io から利用できます。",
      "upvotes": 14,
      "discussionId": "68679569213f123a1f88b8aa",
      "ai_summary": "RoboBrain 2.0, a vision-language foundation model, achieves top performance in embodied tasks through its heterogeneous architecture and multi-stage training strategies.",
      "ai_keywords": [
        "embodied vision-language",
        "embodied reason",
        "vision encoder",
        "spatial understanding",
        "affordance prediction",
        "spatial referring",
        "trajectory forecasting",
        "temporal decision-making",
        "closed-loop interaction",
        "multi-agent long-horizon planning",
        "scene graph updating"
      ]
    },
    "publishedAt": "2025-07-02T13:05:33.000Z",
    "title": "RoboBrain 2.0 Technical Report",
    "summary": "We introduce RoboBrain 2.0, our latest generation of embodied vision-language\nfoundation models, designed to unify perception, reasoning, and planning for\ncomplex embodied tasks in physical environments. It comes in two variants: a\nlightweight 7B model and a full-scale 32B model, featuring a heterogeneous\narchitecture with a vision encoder and a language model. Despite its compact\nsize, RoboBrain 2.0 achieves strong performance across a wide spectrum of\nembodied reasoning tasks. On both spatial and temporal benchmarks, the 32B\nvariant achieves leading results, surpassing prior open-source and proprietary\nmodels. In particular, it supports key real-world embodied AI capabilities,\nincluding spatial understanding (e.g., affordance prediction, spatial\nreferring, trajectory forecasting) and temporal decision-making (e.g.,\nclosed-loop interaction, multi-agent long-horizon planning, and scene graph\nupdating). This report details the model architecture, data construction,\nmulti-stage training strategies, infrastructure and practical applications. We\nhope RoboBrain 2.0 advances embodied AI research and serves as a practical step\ntoward building generalist embodied agents. The code, checkpoint and benchmark\nare available at https://superrobobrain.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02029.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 857
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04009",
      "authors": [
        {
          "_id": "686cd234cc230c60b4100aec",
          "name": "Ziyang Miao",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100aed",
          "name": "Qiyu Sun",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100aee",
          "name": "Jingyuan Wang",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100aef",
          "user": {
            "_id": "66a48a77f9565635ebc33a87",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a48a77f9565635ebc33a87/WW8BFd1D9xZbGIPZPfdHk.png",
            "isPro": false,
            "fullname": "GYC",
            "user": "oGYCo",
            "type": "user"
          },
          "name": "Yuchen Gong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:42.785Z",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100af0",
          "user": {
            "_id": "642fef28a043f0ac7defa8a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
            "isPro": false,
            "fullname": "Yaowei Zheng",
            "user": "hiyouga",
            "type": "user"
          },
          "name": "Yaowei Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:39:35.178Z",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100af1",
          "name": "Shiqi Li",
          "hidden": false
        },
        {
          "_id": "686cd234cc230c60b4100af2",
          "name": "Richong Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/Ztx5877vKyqqZINgge_1B.mp4"
      ],
      "publishedAt": "2025-07-05T11:38:59.000Z",
      "submittedOnDailyAt": "2025-07-08T06:40:18.914Z",
      "title": "Easy Dataset: ユニットフレームワークと拡張可能なフレームワークで無構造ドキュメントからのLLMの微調節データの合成",
      "submittedOnDailyBy": {
        "_id": "642fef28a043f0ac7defa8a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
        "isPro": false,
        "fullname": "Yaowei Zheng",
        "user": "hiyouga",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、一般的なタスクでは驚異的な性能を示していますが、特定の領域に適用するには、高品質の領域データの不足が難題となります。現在のデータ合成ツールは、異なるドキュメントから信頼性のある微調節データを有効に抽出するのに難しく、この制限を解決するために、Easy Datasetというユニットフレームワークを提案します。このフレームワークは、直感的なグラフィカルユーザーインターフェース（GUI）を通じて、無構造化ドキュメントからの微調節データを合成することを可能にします。特に、Easy Datasetは、ユーザーが簡単にテキスト抽出モデルとクラッキングスタテジーを設定でき、純粋なドキュメントをコラム化したテキストチャンクに変換することを可能にします。その後、公開的なLLMsを使用して、ポートフォール駆動のプロンプティングアプローチを活用して、多様な質問・答えペアを生成します。このパイプライン全体では、人間がループ内の可視化インターフェースを利用して、中間出力の検討と改善を促進し、データの品質を確保します。金融タスクの質問・答えタスクにおいての実験は、合成データによるLLMsの微調節は、領域特有の性能を大幅に向上させ、一般的な知識を保持することを示しています。ソースコードとインストール可能なパッケージは、https://github.com/ConardLi/easy-dataset にアクセス可能で、これによりGitHubのスター数は9,000点を超えています。",
      "upvotes": 12,
      "discussionId": "686cd234cc230c60b4100af3",
      "githubRepo": "https://github.com/ConardLi/easy-dataset",
      "ai_summary": "A unified framework called Easy Dataset synthesizes fine-tuning data from unstructured documents using a GUI and LLMs, improving domain-specific performance of LLMs while maintaining general knowledge.",
      "ai_keywords": [
        "Large language models",
        "fine-tuning",
        "unstructured documents",
        "graphical user interface",
        "text extraction models",
        "chunking strategies",
        "persona-driven prompting",
        "human-in-the-loop",
        "financial question-answering task"
      ],
      "githubStars": 9180
    },
    "publishedAt": "2025-07-05T07:38:59.000Z",
    "title": "Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM\n  Fine-Tuning Data from Unstructured Documents",
    "summary": "Large language models (LLMs) have shown impressive performance on\ngeneral-purpose tasks, yet adapting them to specific domains remains\nchallenging due to the scarcity of high-quality domain data. Existing data\nsynthesis tools often struggle to extract reliable fine-tuning data from\nheterogeneous documents effectively. To address this limitation, we propose\nEasy Dataset, a unified framework for synthesizing fine-tuning data from\nunstructured documents via an intuitive graphical user interface (GUI).\nSpecifically, Easy Dataset allows users to easily configure text extraction\nmodels and chunking strategies to transform raw documents into coherent text\nchunks. It then leverages a persona-driven prompting approach to generate\ndiverse question-answer pairs using public-available LLMs. Throughout the\npipeline, a human-in-the-loop visual interface facilitates the review and\nrefinement of intermediate outputs to ensure data quality. Experiments on a\nfinancial question-answering task show that fine-tuning LLMs on the synthesized\ndataset significantly improves domain-specific performance while preserving\ngeneral knowledge. The source code and installable package are available at\nhttps://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub\nstars.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/Ztx5877vKyqqZINgge_1B.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04009.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642fef28a043f0ac7defa8a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
      "fullname": "Yaowei Zheng",
      "name": "hiyouga",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2185
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03253",
      "authors": [
        {
          "_id": "686c86ff364e2ad167eb53a8",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53a9",
          "name": "Shenghua Liu",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53aa",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ab",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ac",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ad",
          "name": "Yiwei Wang",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53ae",
          "user": {
            "_id": "63120517ae8896941da4c5da",
            "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
            "isPro": false,
            "fullname": "Lingrui Mei",
            "user": "Chevalier",
            "type": "user"
          },
          "name": "Lingrui Mei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T09:05:44.909Z",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53af",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53b0",
          "name": "Jiafeng Guo",
          "hidden": false
        },
        {
          "_id": "686c86ff364e2ad167eb53b1",
          "name": "Xueqi Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T02:19:58.000Z",
      "submittedOnDailyAt": "2025-07-08T01:53:47.804Z",
      "title": "RefineX: スケール付きで専門家ガイドされたプログラムからの予習データの精進学習",
      "submittedOnDailyBy": {
        "_id": "642577e06d0f0f5f1dc68904",
        "avatarUrl": "/avatars/3df7cfcf9ca6cd9c9e9b10a73f4efc35.svg",
        "isPro": false,
        "fullname": "Bibaolong",
        "user": "Bibaolong",
        "type": "user"
      },
      "summary": "大規模言語モデル（LLMs）の基盤的な能力は、予習データの質に深く影響されています。しかし、スケールでデータの質を向上させるのは大問題で、主に精細化の効果と処理の効率性のトレードオフによって課題です。ルールベースのフィルタリングが主導的なパラダイムであるが、通常はドキュメントレベルで動作し、特定の内容の精細化に必要な粒度性を欠くことが多いです。ProXなどの新興の研究をヒントに、RefineXという新しいフレームワークを提案します。RefineXは、プログラミングエディタータスクを通じて、大規模で外科的な予習データの精細化を実現します。RefineXは、効率的なデータ精細化を可能にしながら、元のテキストの多様性と自然性を信頼的に保持します。RefineXの核心的な強みは、高品質のデータ精細化結果を最小限の編集プログラムに収納することです。この高精度の結果の統合パイプラインは、効率的で信頼性のある精細化モデルを訓練することで、コーパス内のすべてのインスタンスをスケールでシステマティックに改善することができます。RefineXは、スクラッチからの予習を評価し、多くのモデルサイズで、多様な下流タスクでも統一的に上位の性能を示します。750Mモデルでは、lightevalタスクでは2.6%-7.2%の平均増加率を収め、大幅に訓練トークンを減らしながら相当の性能を達成します。進めた分析は、RefineXは高い効率性と精度で信頼的にテキストの質を向上させ、先行の手法よりもより効果的で、終端デジタル生成やProx-Cを超えることを示します。これらの結果から、RefineXは現代のLLMプロキーで予習データの最適化のスケーラブルで効果的で信頼性のある解決策として位置づけられています。",
      "upvotes": 12,
      "discussionId": "686c86ff364e2ad167eb53b2",
      "ai_summary": "RefineX is a scalable framework for improving the quality of large language model pre-training data through programmatic editing, yielding better performance than alternative methods across various downstream tasks.",
      "ai_keywords": [
        "large language models",
        "pre-training corpora",
        "rule-based filtering",
        "document level",
        "granular refinement",
        "programmatic editing",
        "refine model",
        "data refinement",
        "text quality",
        "end-to-end generation",
        "lighteval tasks"
      ]
    },
    "publishedAt": "2025-07-03T22:19:58.000Z",
    "title": "RefineX: Learning to Refine Pre-training Data at Scale from\n  Expert-Guided Programs",
    "summary": "The foundational capabilities of large language models (LLMs) are deeply\ninfluenced by the quality of their pre-training corpora. However, enhancing\ndata quality at scale remains a significant challenge, primarily due to the\ntrade-off between refinement effectiveness and processing efficiency. While\nrule-based filtering remains the dominant paradigm, it typically operates at\nthe document level and lacks the granularity needed to refine specific content\nwithin documents. Inspired by emerging work such as ProX, we propose\nRefineX, a novel framework for large-scale, surgical refinement of\npre-training data through programmatic editing tasks. RefineX enables efficient\nand fine-grained data refinement while reliably preserving the diversity and\nnaturalness of raw text. The core strength of RefineX lies in distilling\nhigh-quality, expert-guided end-to-end refinement results into minimal\nedit-based deletion programs. This high-precision distillation pipeline is used\nto train an efficient and reliable refine model that can systematically improve\nevery instance in the corpus at scale. We evaluate RefineX across from-scratch\npre-training at multiple model scales and find that it consistently outperforms\nmodels trained on raw, filtered, or alternatively refined data across diverse\ndownstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on\nlighteval tasks, and achieves comparable performance using significantly fewer\ntraining tokens. Further analysis shows that RefineX reliably enhances text\nquality with both high efficiency and precision, outperforming prior approaches\nsuch as end-to-end generation and Prox-C. These results position RefineX as a\nscalable, effective, and reliable solution for optimizing pre-training data in\nmodern LLM pipelines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642577e06d0f0f5f1dc68904",
      "avatarUrl": "/avatars/3df7cfcf9ca6cd9c9e9b10a73f4efc35.svg",
      "fullname": "Bibaolong",
      "name": "Bibaolong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05108",
      "authors": [
        {
          "_id": "686cc8bc364e2ad167eb54e3",
          "name": "Yuyi Zhang",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e4",
          "name": "Peirong Zhang",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e5",
          "name": "Zhenhua Yang",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e6",
          "name": "Pengyu Yan",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e7",
          "name": "Yongxin Shi",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e8",
          "name": "Pengwei Liu",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54e9",
          "name": "Fengjun Guo",
          "hidden": false
        },
        {
          "_id": "686cc8bc364e2ad167eb54ea",
          "name": "Lianwen Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T15:26:17.000Z",
      "submittedOnDailyAt": "2025-07-08T06:01:26.616Z",
      "title": "文化遺産の再生：歴史文書の全面的なリフォームの新しいアプローチ",
      "submittedOnDailyBy": {
        "_id": "65fba5700b78c48c9e393a3e",
        "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
        "isPro": false,
        "fullname": "Yuyi Zhang",
        "user": "ZZXF",
        "type": "user"
      },
      "summary": "歴史文書は無價な文化遺産であり、時間の流れにより損傷、水の侵食、酸化などにより重大な退化を経験しています。現在の歴史文書の再現（HDR）手法は主に単一モデルや限定サイズの再現に焦点を当てているため、実用的な需要に適合していません。この空間を埋めるために、FPHDR（Full-Page HDRデータセット）とAutoHDR（自動HDRソリューション）を提出します。FPHDRは1,633枚の実物画像と6,543枚の合成画像を含み、文字レベルと行レベルの位置情報、および損傷程度に応じた文字注釈を持っています。AutoHDRは、歴史学者の再現ワークフローをモゥモゥしみ、OCR助けされた損傷位置検出、視覚言語コンテキストテキスト予測、パッチ自動回帰的な外観再現の3段階アプローチを用いています。AutoHDRのモジュール化アーキテクチャにより、無際間の人間と機械の協力が可能で、再現ステージごとに柔軟な干渉と最適化が可能です。実験はAutoHDRの驚異的な性能を示し、損傷が厳しい文書を処理する場合、OCR精度が46.83%から84.05%に上がり、人間と機械の協力によりさらに94.25%に達しました。私たちは、この研究は自動化された歴史文書再現における重要な進展であり、文化遺産の保存に大幅に貢献します。モデルとデータセットはhttps://github.com/SCUT-DLVCLab/AutoHDRから利用可能です。",
      "upvotes": 7,
      "discussionId": "686cc8bd364e2ad167eb54eb",
      "githubRepo": "https://github.com/SCUT-DLVCLab/AutoHDR",
      "githubStars": 22
    },
    "publishedAt": "2025-07-07T11:26:17.000Z",
    "title": "Reviving Cultural Heritage: A Novel Approach for Comprehensive\n  Historical Document Restoration",
    "summary": "Historical documents represent an invaluable cultural heritage, yet have\nundergone significant degradation over time through tears, water erosion, and\noxidation. Existing Historical Document Restoration (HDR) methods primarily\nfocus on single modality or limited-size restoration, failing to meet practical\nneeds. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel\nautomated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and\n6,543 synthetic images with character-level and line-level locations, as well\nas character annotations in different damage grades. AutoHDR mimics historians'\nrestoration workflows through a three-stage approach: OCR-assisted damage\nlocalization, vision-language context text prediction, and patch autoregressive\nappearance restoration. The modular architecture of AutoHDR enables seamless\nhuman-machine collaboration, allowing for flexible intervention and\noptimization at each restoration stage. Experiments demonstrate AutoHDR's\nremarkable performance in HDR. When processing severely damaged documents, our\nmethod improves OCR accuracy from 46.83\\% to 84.05\\%, with further enhancement\nto 94.25\\% through human-machine collaboration. We believe this work represents\na significant advancement in automated historical document restoration and\ncontributes substantially to cultural heritage preservation. The model and\ndataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05108.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fba5700b78c48c9e393a3e",
      "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
      "fullname": "Yuyi Zhang",
      "name": "ZZXF",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.03745",
      "authors": [
        {
          "_id": "686ca04e364e2ad167eb543c",
          "user": {
            "_id": "63fedca388b9695964c33ad8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fedca388b9695964c33ad8/XFE8Tm9yRRbo6XdDw7c2O.png",
            "isPro": false,
            "fullname": "Aki",
            "user": "AkiCumulo",
            "type": "user"
          },
          "name": "Akio Kodaira",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:07.401Z",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb543d",
          "name": "Tingbo Hou",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb543e",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb543f",
          "name": "Masayoshi Tomizuka",
          "hidden": false
        },
        {
          "_id": "686ca04e364e2ad167eb5440",
          "name": "Yue Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/eFed7d-PiKdUBGIPDkIoo.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/50KoaI5mgYLe_IvIOaVJh.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/zWzNfOJX8C-CNICabQyHV.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/m072g2U7H133AyOGShNZ0.mp4"
      ],
      "publishedAt": "2025-07-04T18:00:01.000Z",
      "submittedOnDailyAt": "2025-07-08T04:21:42.132Z",
      "title": "StreamDiT: ライナーテキストからの実時間ビデオ生成",
      "submittedOnDailyBy": {
        "_id": "63fedca388b9695964c33ad8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fedca388b9695964c33ad8/XFE8Tm9yRRbo6XdDw7c2O.png",
        "isPro": false,
        "fullname": "Aki",
        "user": "AkiCumulo",
        "type": "user"
      },
      "summary": "最近、transformerベースのdiffusionモデルを数ブィリオンパラメータに拡大して、高品質のビデオを生成することができることで、text-to-video (T2V) 生成に大きな進歩が達成されました。しかし、現在のモデルは通常、オンラインで短いクリップだけを生成し、インタラクティブやリアルタイムアプリケーションの使用場合が限定されています。本論文では、これらの課題を解決するために、StreamDiT、ストリーミングビデオ生成モデルを提案します。StreamDiTの訓練は、移動バッファを追加したflow matchingに基づきます。バッファーフレームの異なる分割シナプスでの混合訓練を設計し、内容の一貫性と視覚品質を両方向上げます。StreamDiTのモデリングは、時間埋め込みとウィンドウアテンションを用いたadaLN DiTに基づきます。提案した方法を実践するために、4BパラメータのStreamDiTモデルを訓練します。また、StreamDiTに適したmultistep distillationメソッドを提案します。選択された分割シナプスの各セグメントで、サンプリングディスタンスを実行します。ディスタンス後、関数評価の総数（NFEs）はバッファーのチャンク数に減少します。最終的に、我々のディスタンスモデルは1ファイトプシーで16FPSでのリアルタイム動作を実現し、512pレンダリングでのビデオストリーム生成が可能です。我々の方法を定量的メトリックと人間評価を通じて評価します。我々のモデルは、流れ生成、インタラクティブ生成、ビデオからビデオのようなリアルタイムアプリケーションを可能にします。プロジェクトウェブサイトで、ビデオ結果や例を提供します：<a href=\"https://cumulo-autumn.github.io/StreamDiT/\">this https URL.</a>",
      "upvotes": 7,
      "discussionId": "686ca04f364e2ad167eb5441",
      "projectPage": "https://cumulo-autumn.github.io/StreamDiT/",
      "ai_summary": "A streaming video generation model named StreamDiT, based on transformer-based diffusion models, enables real-time video generation with high content consistency and visual quality.",
      "ai_keywords": [
        "transformer-based diffusion models",
        "StreamDiT",
        "flow matching",
        "moving buffer",
        "mixed training",
        "adaLN DiT",
        "varying time embedding",
        "window attention",
        "multistep distillation",
        "sampling distillation",
        "function evaluations",
        "real-time performance",
        "streaming generation",
        "interactive generation",
        "video-to-video"
      ]
    },
    "publishedAt": "2025-07-04T14:00:01.000Z",
    "title": "StreamDiT: Real-Time Streaming Text-to-Video Generation",
    "summary": "Recently, great progress has been achieved in text-to-video (T2V) generation\nby scaling transformer-based diffusion models to billions of parameters, which\ncan generate high-quality videos. However, existing models typically produce\nonly short clips offline, restricting their use cases in interactive and\nreal-time applications. This paper addresses these challenges by proposing\nStreamDiT, a streaming video generation model. StreamDiT training is based on\nflow matching by adding a moving buffer. We design mixed training with\ndifferent partitioning schemes of buffered frames to boost both content\nconsistency and visual quality. StreamDiT modeling is based on adaLN DiT with\nvarying time embedding and window attention. To practice the proposed method,\nwe train a StreamDiT model with 4B parameters. In addition, we propose a\nmultistep distillation method tailored for StreamDiT. Sampling distillation is\nperformed in each segment of a chosen partitioning scheme. After distillation,\nthe total number of function evaluations (NFEs) is reduced to the number of\nchunks in a buffer. Finally, our distilled model reaches real-time performance\nat 16 FPS on one GPU, which can generate video streams at 512p resolution. We\nevaluate our method through both quantitative metrics and human evaluation. Our\nmodel enables real-time applications, e.g. streaming generation, interactive\ngeneration, and video-to-video. We provide video results and more examples in\nour project website: <a href=\"https://cumulo-autumn.github.io/StreamDiT/\">this\nhttps URL.</a>",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/eFed7d-PiKdUBGIPDkIoo.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/50KoaI5mgYLe_IvIOaVJh.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/zWzNfOJX8C-CNICabQyHV.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/63fedca388b9695964c33ad8/m072g2U7H133AyOGShNZ0.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fedca388b9695964c33ad8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fedca388b9695964c33ad8/XFE8Tm9yRRbo6XdDw7c2O.png",
      "fullname": "Aki",
      "name": "AkiCumulo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.04952",
      "authors": [
        {
          "_id": "686c8487364e2ad167eb5386",
          "name": "Chenchen Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5387",
          "name": "Yuhang Li",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5388",
          "name": "Can Xu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5389",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538a",
          "name": "Ao Liu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538b",
          "name": "Shihui Hu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538c",
          "name": "Dengpeng Wu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538d",
          "name": "Guanhua Huang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538e",
          "name": "Kejiao Li",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb538f",
          "name": "Qi Yi",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5390",
          "name": "Ruibin Xiong",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5391",
          "name": "Haotian Zhu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5392",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5393",
          "name": "Yuhao Jiang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5394",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5395",
          "name": "Zenan Xu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5396",
          "name": "Bohui Zhai",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5397",
          "name": "Guoxiang He",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5398",
          "name": "Hebin Li",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb5399",
          "name": "Jie Zhao",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539a",
          "name": "Le Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539b",
          "name": "Lingyun Tan",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539c",
          "name": "Pengyu Guo",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539d",
          "name": "Xianshu Pang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539e",
          "name": "Yang Ruan",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb539f",
          "name": "Zhifeng Zhang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a0",
          "name": "Zhonghu Wang",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a1",
          "name": "Ziyan Xu",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a2",
          "name": "Zuopu Yin",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a3",
          "name": "Wiggin Zhou",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a4",
          "name": "Chayse Zhou",
          "hidden": false
        },
        {
          "_id": "686c8487364e2ad167eb53a5",
          "name": "Fengzong Lian",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/Ib4IFSlNDtkWKjjyVWG-S.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/4OtfeyvZ2jMLaebiU0NKA.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/k-024E68E9-6iaF0QtZad.png",
        "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/VBS9HcDQ2oHj3bmxgyZVB.png"
      ],
      "publishedAt": "2025-07-07T12:53:00.000Z",
      "submittedOnDailyAt": "2025-07-08T02:53:13.802Z",
      "title": "ArtifactsBench: ビジュアル・インタラクティブ間のギャップを経由したLLMコード生成評価",
      "submittedOnDailyBy": {
        "_id": "64b74b906ab5d14ca7f289cd",
        "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
        "isPro": false,
        "fullname": "xxzcc",
        "user": "xxzcc",
        "type": "user"
      },
      "summary": "LLMの生成能力は、静的コードから動的、インタラクティブな視覚的なアドキャレートへと急速に拡大しています。この進歩は、既存のベンチマークがアルゴリズム的正確性を焦点にし、現代のユーザー体験を定義する視覚的なフィデリティとインタラクティブな整合性に関心を持たないという重要な評価の隙間によって制限されています。この隙間をカットするために、我々は、可視化コード生成の自動的、多タイプ評価の新しいベンチマークとパラダイムを介して、ArtifactsBenchを紹介します。我々のフレームワークは、生成されたアドキャレートの各コンテンツをプログラミング的に描画し、時系列的なスクリーンショットでその動的なビジョンを捉えます。この視覚的なエビデンスとソースコードは、Multimodal LLM（MLLM）-as-Judgeによって評価され、細かいチェックリストのガイドによって厳密にハードルを設定され、全体的で再現可能なスコアを確保します。我々は、1,825種類の多様なタスクのベンチマークを構築し、30つ以上の先進的なLLMを評価します。我々の自動的な評価は、WebDev Arenaとの94.4%のスコア一致率を達成し、ユーザーエキスパートとの90%以上のペアワイズ同意を超えます。これにより、ArtifactsBenchは、人間の好みによる質の評価をスケール的に信頼的に自動化する最初のフレームワークとして立ち上がります。我々の分析は、現在のSOTAの高解像度マップを提供し、一般的なモデルが領域専門的なモデルを上回ることを示します。我々は、ArtifactsBenchのベンチマーク、評価ハーネス、ベースライン結果を公開し、https://artifactsbenchmark.github.io/にアクセスできるようにし、コミュニティにスケーラブルで正確なツールを提供し、ユーザーエキューショナルな生成モデルの開発を加速します。",
      "upvotes": 6,
      "discussionId": "686c8487364e2ad167eb53a6",
      "ai_summary": "ArtifactsBench, a novel benchmark and evaluation framework, automates the assessment of visual code generation quality using temporal screenshots and a multimodal language model judge.",
      "ai_keywords": [
        "Large Language Models",
        "LLMS",
        "dynamic",
        "interactive visual artifacts",
        "visual fidelity",
        "interactive integrity",
        "ArtifactsBench",
        "benchmark",
        "Multimodal LLM",
        "MLLM-as-Judge",
        "fine-grained",
        "per-task checklist",
        "ranking consistency",
        "WebDev Arena",
        "pairwise agreement",
        "human-perceived quality",
        "generalist models",
        "domain-specific ones"
      ]
    },
    "publishedAt": "2025-07-07T08:53:00.000Z",
    "title": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation",
    "summary": "The generative capabilities of Large Language Models (LLMs) are rapidly\nexpanding from static code to dynamic, interactive visual artifacts. This\nprogress is bottlenecked by a critical evaluation gap: established benchmarks\nfocus on algorithmic correctness and are blind to the visual fidelity and\ninteractive integrity that define modern user experiences. To bridge this gap,\nwe introduce ArtifactsBench, a new benchmark and paradigm for the automated,\nmultimodal evaluation of visual code generation. Our framework programmatically\nrenders each generated artifact and captures its dynamic behavior through\ntemporal screenshots. This visual evidence, alongside the source code, is then\nassessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a\nfine-grained, per-task checklist to ensure holistic and reproducible scoring.\nWe construct a new benchmark of 1,825 diverse tasks and evaluate over 30\nleading LLMs. Our automated evaluation achieves a striking 94.4% ranking\nconsistency with WebDev Arena, the gold-standard for human preference in web\ndevelopment, and over 90% pairwise agreement with human experts. This\nestablishes ArtifactsBench as the first framework to reliably automate the\nassessment of human-perceived quality at scale. Our analysis provides a\nhigh-resolution map of the current SOTA, revealing that generalist models often\noutperform domain-specific ones. We open-source ArtifactsBench, including the\nbenchmark, evaluation harness, and baseline results at\nhttps://artifactsbenchmark.github.io/, to provide the community with a scalable\nand accurate tool to accelerate the development of user-centric generative\nmodels.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/Ib4IFSlNDtkWKjjyVWG-S.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/4OtfeyvZ2jMLaebiU0NKA.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/k-024E68E9-6iaF0QtZad.png",
      "https://cdn-uploads.huggingface.co/production/uploads/64b74b906ab5d14ca7f289cd/VBS9HcDQ2oHj3bmxgyZVB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b74b906ab5d14ca7f289cd",
      "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
      "fullname": "xxzcc",
      "name": "xxzcc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04590",
      "authors": [
        {
          "_id": "686c96a0364e2ad167eb540c",
          "name": "Rui Meng",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb540d",
          "user": {
            "_id": "64778fb8168cb428e00f69b0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64778fb8168cb428e00f69b0/D_XYg74zHG9K3HUJj_gD4.jpeg",
            "isPro": true,
            "fullname": "Ziyan Jiang",
            "user": "ziyjiang",
            "type": "user"
          },
          "name": "Ziyan Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:15.117Z",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb540e",
          "name": "Ye Liu",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb540f",
          "name": "Mingyi Su",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5410",
          "name": "Xinyi Yang",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5411",
          "name": "Yuepeng Fu",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5412",
          "name": "Can Qin",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5413",
          "name": "Zeyuan Chen",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5414",
          "name": "Ran Xu",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5415",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5416",
          "name": "Yingbo Zhou",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5417",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "686c96a0364e2ad167eb5418",
          "name": "Semih Yavuz",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T00:51:57.000Z",
      "submittedOnDailyAt": "2025-07-08T02:26:58.880Z",
      "title": "VLM2Vec-V2: 映画、画像、ビジュアルドキュメントのマルチモーダルエンベッディングの進歩",
      "submittedOnDailyBy": {
        "_id": "64778fb8168cb428e00f69b0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64778fb8168cb428e00f69b0/D_XYg74zHG9K3HUJj_gD4.jpeg",
        "isPro": true,
        "fullname": "Ziyan Jiang",
        "user": "ziyjiang",
        "type": "user"
      },
      "summary": "多モデル埋め込みモデルは、様々な下流タスク（セマンティック類似性、情報検索、クラスタリング）を可能にするために重要な役割を果たしています。しかし、現在の多モデル埋め込み（VLM2Vec、E5-V、GME）は主に自然画像に焦点を当てています。他の可視形式（ビデオ、可視文書）に対する支援は限られており、実世界的な場合においての適用範囲が狭まります。AIアグエント、多モデル検索と推薦、検索アウゲージ生成（RAG）などの場合において、この制約があるため、この隙を塞ぐために、VLM2Vec-V2、多様な可視形式での埋め込みを学習する統一フレームワークを提案します。まず、MMEB-V2、MMEBに新しい5タスクタイプを追加した詳細ベンチマークを紹介します。そして、VLM2Vec-V2、構文、画像、ビデオ、可視文書の入力をサポートする一般的な埋め込みモデルを訓練します。拡大の実験により、VLM2Vec-V2は新しいビデオと文書検索タスクで強力な性能を示し、元の画像ベンチマークで先行の基準を超えます。拡大の評価を通じて、本研究は、多モデル埋め込みモデルの一般化能力についての見解を提供し、統一的な埋め込み学習の効果的な戦略を明らかにし、研究および実世界的な設定でのよりスケーラブルなおよび適応可能な表現学習の基盤を筑みます。",
      "upvotes": 4,
      "discussionId": "686c96a0364e2ad167eb5419",
      "projectPage": "https://tiger-ai-lab.github.io/VLM2Vec/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VLM2Vec",
      "ai_summary": "A unified framework VLM2Vec-V2 is proposed for learning embeddings across diverse visual forms such as videos and documents, demonstrating strong performance on new tasks and improving upon existing benchmarks for images.",
      "ai_keywords": [
        "multimodal embedding models",
        "VLM2Vec",
        "E5-V",
        "GME",
        "MMEB-V2",
        "visual document retrieval",
        "video retrieval",
        "temporal grounding",
        "video classification",
        "video question answering",
        "general-purpose embedding model",
        "unified embedding learning",
        "representation learning"
      ],
      "githubStars": 290
    },
    "publishedAt": "2025-07-06T20:51:57.000Z",
    "title": "VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and\n  Visual Documents",
    "summary": "Multimodal embedding models have been crucial in enabling various downstream\ntasks such as semantic similarity, information retrieval, and clustering over\ndifferent modalities. However, existing multimodal embeddings like VLM2Vec,\nE5-V, GME are predominantly focused on natural images, with limited support for\nother visual forms such as videos and visual documents. This restricts their\napplicability in real-world scenarios, including AI agents, multi-modal search\nand recommendation, and retrieval-augmented generation (RAG). To close this\ngap, we propose VLM2Vec-V2, a unified framework for learning embeddings across\ndiverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark\nthat extends MMEB with five new task types: visual document retrieval, video\nretrieval, temporal grounding, video classification and video question\nanswering - spanning text, image, video, and visual document inputs. Next, we\ntrain VLM2Vec-V2, a general-purpose embedding model that supports text, image,\nvideo, and visual document inputs. Extensive experiments show that VLM2Vec-V2\nachieves strong performance not only on the newly introduced video and document\nretrieval tasks, but also improves over prior baselines on the original image\nbenchmarks. Through extensive evaluation, our study offers insights into the\ngeneralizability of various multimodal embedding models and highlights\neffective strategies for unified embedding learning, laying the groundwork for\nmore scalable and adaptable representation learning in both research and\nreal-world settings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04590.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64778fb8168cb428e00f69b0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64778fb8168cb428e00f69b0/D_XYg74zHG9K3HUJj_gD4.jpeg",
      "fullname": "Ziyan Jiang",
      "name": "ziyjiang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03607",
      "authors": [
        {
          "_id": "686cb574364e2ad167eb54c3",
          "user": {
            "_id": "65e5bc754230174d547fa1dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xQ1NGVasxexuR_Hs0dvNl.jpeg",
            "isPro": false,
            "fullname": "Cédric",
            "user": "cedricbonhomme",
            "type": "user"
          },
          "name": "Cédric Bonhomme",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:05:41.165Z",
          "hidden": false
        },
        {
          "_id": "686cb574364e2ad167eb54c4",
          "user": {
            "_id": "677d08a57038d6b09078649a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jEUTW_G_VCgbvqfCxlNTj.png",
            "isPro": false,
            "fullname": "Dulaunoy",
            "user": "adulau",
            "type": "user"
          },
          "name": "Alexandre Dulaunoy",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:39:37.700Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65e5bc754230174d547fa1dc/S4_uGwFUDxVHZqJMljzP8.jpeg"
      ],
      "publishedAt": "2025-07-04T14:28:14.000Z",
      "submittedOnDailyAt": "2025-07-08T06:42:20.895Z",
      "title": "VLAI: ROBERTaベースモデルによる自動化バグの厳重度分類",
      "submittedOnDailyBy": {
        "_id": "65e5bc754230174d547fa1dc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xQ1NGVasxexuR_Hs0dvNl.jpeg",
        "isPro": false,
        "fullname": "Cédric",
        "user": "cedricbonhomme",
        "type": "user"
      },
      "summary": "この論文では、テキスト記述から直接ソフトウェアの脆弱性の効果度レベルを予測するためのTransformerベースモデルVLAIを紹介します。RoBERTaにビルドされたVLAIは、600,000以上の実世界的な脆弱性データにファインチューニングされ、効果度カテゴリーの予測に82%以上の精度を達成し、手動のCVSSスコアリングよりも速く、より一貫したトライジングを可能にします。モデルとデータセットはオープンソースで、Vulnerability-Lookupサービスに統合されています。",
      "upvotes": 4,
      "discussionId": "686cb574364e2ad167eb54c5",
      "projectPage": "https://www.vulnerability-lookup.org",
      "githubRepo": "https://github.com/vulnerability-lookup/VulnTrain",
      "ai_summary": "A transformer-based model predicts software vulnerability severity levels directly from text, enhancing triage efficiency and consistency.",
      "ai_keywords": [
        "transformer",
        "RoBERTa",
        "parameter-efficient fine-tuning",
        "predicting severity categories",
        "CVSS scoring",
        "open-source",
        "Vulnerability-Lookup service"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-07-04T10:28:14.000Z",
    "title": "VLAI: A RoBERTa-Based Model for Automated Vulnerability Severity\n  Classification",
    "summary": "This paper presents VLAI, a transformer-based model that predicts software\nvulnerability severity levels directly from text descriptions. Built on\nRoBERTa, VLAI is fine-tuned on over 600,000 real-world vulnerabilities and\nachieves over 82% accuracy in predicting severity categories, enabling faster\nand more consistent triage ahead of manual CVSS scoring. The model and dataset\nare open-source and integrated into the Vulnerability-Lookup service.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e5bc754230174d547fa1dc/S4_uGwFUDxVHZqJMljzP8.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03607.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e5bc754230174d547fa1dc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xQ1NGVasxexuR_Hs0dvNl.jpeg",
      "fullname": "Cédric",
      "name": "cedricbonhomme",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.04036",
      "authors": [
        {
          "_id": "686c92c5364e2ad167eb53fa",
          "name": "Jingwei Shi",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fb",
          "user": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "name": "Zeyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:20.306Z",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fc",
          "name": "Biao Wu",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fd",
          "name": "Yanjie Liang",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53fe",
          "name": "Meng Fang",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb53ff",
          "name": "Ling Chen",
          "hidden": false
        },
        {
          "_id": "686c92c5364e2ad167eb5400",
          "name": "Yang Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/Kwa3r-mdnTK5Itx86d1f6.mp4"
      ],
      "publishedAt": "2025-07-05T13:24:15.000Z",
      "submittedOnDailyAt": "2025-07-08T02:14:06.764Z",
      "title": "PresentAgent: プレゼンテーションビデオ生成用マルチモーダルアグェント",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "プレゼンタジェントは、長文書をナレードプレゼンテーションビデオに変換する多モダルアグェントです。現在の手法は、静的スライドまたはテキスト要約に限定されているが、我々の方法はこれらの制限を超え、完全にシンクロニズムを取る可視的および口頭内容を生成し、人間のプレゼンテーションの風格に近似したものを作成します。この統合を実現するために、プレゼンタジェントは入力文書をシステマティックに分割し、スライドモードの可視的フレームを計画し渲染し、大語言モデルと文脈テキストからサイドプロデュースモデルを使用してコンテキスト付きの口頭ナレーションを生成し、最終的なビデオを精密な音声可視認識によって無隙合わせます。このような多モダル出力の評価の複雑性を考慮して、我々は、ビジョン言語モデルをもちろんして、内容忠実性、可視性、プロンプトベース評価によるオーディエンス理解の3つの重要な次元での評価を行う統一評価フレームワークを導入します。我々の実験的検証は、30ページのドキュメントプレゼンテーションペアのカレーレッドデータセットで行われ、プレゼンタジェントはすべての評価指標で人間レベルの品質を接近していることを示します。これらの結果は、静的なテキストマテリアルを動的、有效率、アクセス可能なプレゼンテーションフォーマットに変換する制御可能な多モダルアグェントの大きなポテンシャルを明らかにしています。コードは、https://github.com/AIGeeksGroup/PresentAgent から利用可能です。",
      "upvotes": 3,
      "discussionId": "686c92c6364e2ad167eb5401",
      "githubRepo": "https://github.com/AIGeeksGroup/PresentAgent",
      "ai_summary": "A multimodal agent transforms documents into detailed presentation videos with audio, evaluated using a comprehensive framework involving vision-language models.",
      "ai_keywords": [
        "multimodal agent",
        "narrated presentation videos",
        "static slides",
        "text summaries",
        "large language models",
        "Text-to-Speech models",
        "audio-visual alignment",
        "Vision-Language Models",
        "content fidelity",
        "visual clarity",
        "audience comprehension",
        "prompt-based evaluation"
      ],
      "githubStars": 11
    },
    "publishedAt": "2025-07-05T09:24:15.000Z",
    "title": "PresentAgent: Multimodal Agent for Presentation Video Generation",
    "summary": "We present PresentAgent, a multimodal agent that transforms long-form\ndocuments into narrated presentation videos. While existing approaches are\nlimited to generating static slides or text summaries, our method advances\nbeyond these limitations by producing fully synchronized visual and spoken\ncontent that closely mimics human-style presentations. To achieve this\nintegration, PresentAgent employs a modular pipeline that systematically\nsegments the input document, plans and renders slide-style visual frames,\ngenerates contextual spoken narration with large language models and\nText-to-Speech models, and seamlessly composes the final video with precise\naudio-visual alignment. Given the complexity of evaluating such multimodal\noutputs, we introduce PresentEval, a unified assessment framework powered by\nVision-Language Models that comprehensively scores videos across three critical\ndimensions: content fidelity, visual clarity, and audience comprehension\nthrough prompt-based evaluation. Our experimental validation on a curated\ndataset of 30 document-presentation pairs demonstrates that PresentAgent\napproaches human-level quality across all evaluation metrics. These results\nhighlight the significant potential of controllable multimodal agents in\ntransforming static textual materials into dynamic, effective, and accessible\npresentation formats. Code will be available at\nhttps://github.com/AIGeeksGroup/PresentAgent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ec877bb93654d4ca5c92e9/Kwa3r-mdnTK5Itx86d1f6.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04036.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05259",
      "authors": [
        {
          "_id": "686ca7ef364e2ad167eb544d",
          "user": {
            "_id": "6499eca0685215f7247bd5ce",
            "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
            "isPro": false,
            "fullname": "Chun-Hsiao Yeh",
            "user": "danielchyeh",
            "type": "user"
          },
          "name": "Chun-Hsiao Yeh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:03.462Z",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb544e",
          "name": "Yilin Wang",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb544f",
          "name": "Nanxuan Zhao",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5450",
          "name": "Richard Zhang",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5451",
          "name": "Yuheng Li",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5452",
          "name": "Yi Ma",
          "hidden": false
        },
        {
          "_id": "686ca7ef364e2ad167eb5453",
          "name": "Krishna Kumar Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T17:59:56.000Z",
      "submittedOnDailyAt": "2025-07-08T03:46:34.916Z",
      "title": "X-Plannerは、複雑なインストラクションに基づく画像編集を可能にし、簡単な編集だけでなく、複雑なコンテキストや条件に応じた画像編集を行うことができるツールです。",
      "submittedOnDailyBy": {
        "_id": "6499eca0685215f7247bd5ce",
        "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
        "isPro": false,
        "fullname": "Chun-Hsiao Yeh",
        "user": "danielchyeh",
        "type": "user"
      },
      "summary": "最近のディフォーションベースの画像編集手法は、テキストガイドされたタスクにおいて显著な進歩を遂げましたが、複雑な、間接的な指示を理解することが難しく、また、現在のモデルは、アウトレットの保持、非意図的な編集、または手動マスクの重視により困難を見出しています。これらの課題を解決するために、我々はX-PlannerというMultimodal Large Language Model（MLLM）に基づく計画システムを介して、ユーザーの意図と編集モデルの能力を効果的に結びつけることを目指します。X-Plannerは、chain-of-thought reasoningを用いて、複雑な指示を簡単で明確な次第的な指示に分解します。各次第的な指示に対して、X-Plannerは精度の高い編集タイプと分割マスクを自動的に生成し、手動の介入を除去し、局所的でアウトレットを保持した編集を確保します。また、X-Plannerの訓練に向けて大規模なデータを生成する新しい自動プロセスを提案し、現在のベンチマークと我々に新たに追加した複雑な編集ベンチマークにおいて最先端の結果を収めます。",
      "upvotes": 2,
      "discussionId": "686ca7f0364e2ad167eb5454",
      "projectPage": "https://danielchyeh.github.io/x-planner/",
      "ai_summary": "X-Planner, a planning system utilizing a multimodal large language model, decomposes complex text-guided image editing instructions into precise sub-instructions, ensuring localized, identity-preserving edits and achieving top performance on established benchmarks.",
      "ai_keywords": [
        "Multimodal Large Language Model",
        "MLLM",
        "chain-of-thought reasoning",
        "segmentation masks",
        "image editing",
        "identity preservation",
        "automated pipeline"
      ]
    },
    "publishedAt": "2025-07-07T13:59:56.000Z",
    "title": "Beyond Simple Edits: X-Planner for Complex Instruction-Based Image\n  Editing",
    "summary": "Recent diffusion-based image editing methods have significantly advanced\ntext-guided tasks but often struggle to interpret complex, indirect\ninstructions. Moreover, current models frequently suffer from poor identity\npreservation, unintended edits, or rely heavily on manual masks. To address\nthese challenges, we introduce X-Planner, a Multimodal Large Language Model\n(MLLM)-based planning system that effectively bridges user intent with editing\nmodel capabilities. X-Planner employs chain-of-thought reasoning to\nsystematically decompose complex instructions into simpler, clear\nsub-instructions. For each sub-instruction, X-Planner automatically generates\nprecise edit types and segmentation masks, eliminating manual intervention and\nensuring localized, identity-preserving edits. Additionally, we propose a novel\nautomated pipeline for generating large-scale data to train X-Planner which\nachieves state-of-the-art results on both existing benchmarks and our newly\nintroduced complex editing benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05259.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6499eca0685215f7247bd5ce",
      "avatarUrl": "/avatars/b6fea0c33c3c930c7314b99b414072a9.svg",
      "fullname": "Chun-Hsiao Yeh",
      "name": "danielchyeh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.04562",
      "authors": [
        {
          "_id": "686ca9e2364e2ad167eb5461",
          "name": "Janna Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-06T22:26:59.000Z",
      "submittedOnDailyAt": "2025-07-08T03:51:31.373Z",
      "title": "「実世界予測に対するLLMsと人間の超予測者の比較評価」",
      "submittedOnDailyBy": {
        "_id": "66b3d98e040c500914ef558f",
        "avatarUrl": "/avatars/a90f8306dbd7747520ce5b941ee3bbcb.svg",
        "isPro": false,
        "fullname": "Janna",
        "user": "jannalu",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、多様なタスクで驚異的な能力を示していますが、未来のイベントを予測する能力は研究不足です。1年前、大語言モデルは、人間の集団の精度に近づくことが難しかったでした。Metaculusからの464件の予測問題で最先端のLLMsを評価し、その性能を人間の超予測者と比較しました。Frontierモデルは、Brierスコアを超えるように見えるが、超予測者のグループと比較しても显著に劣ります。",
      "upvotes": 1,
      "discussionId": "686ca9e2364e2ad167eb5462",
      "ai_summary": "State-of-the-art large language models are evaluated on forecasting questions and show lower accuracy compared to human superforecasters.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "Brier scores",
        "human superforecasters"
      ]
    },
    "publishedAt": "2025-07-06T18:26:59.000Z",
    "title": "Evaluating LLMs on Real-World Forecasting Against Human Superforecasters",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their ability to forecast future events remains\nunderstudied. A year ago, large language models struggle to come close to the\naccuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting\nquestions from Metaculus, comparing their performance against human\nsuperforecasters. Frontier models achieve Brier scores that ostensibly surpass\nthe human crowd but still significantly underperform a group of\nsuperforecasters.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04562.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b3d98e040c500914ef558f",
      "avatarUrl": "/avatars/a90f8306dbd7747520ce5b941ee3bbcb.svg",
      "fullname": "Janna",
      "name": "jannalu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04376",
      "authors": [
        {
          "_id": "686cb0ab364e2ad167eb54a1",
          "name": "Georgios Ioannides",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a2",
          "name": "Christos Constantinou",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a3",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a4",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:05:49.355Z",
          "hidden": false
        },
        {
          "_id": "686cb0ab364e2ad167eb54a5",
          "name": "Aaron Elkins",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-06T12:46:57.000Z",
      "submittedOnDailyAt": "2025-07-08T04:20:01.196Z",
      "title": "MOD-X: モジュール化オープン分散型交換フレームワークの提案\n  異なるオーバーライド可能な人工エージェントの相互換能性",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "AIシステムがモノリットモデルから特殊化されたアガントのエコシステムに移行していく中、標準化された通信プロトコルの必要性は日々重要になっており、この論文では、現在のプロトコルの主要な制限を解決するための新しいアーキテクチャフレームプロジェクトMOD-X（Modular Open Decentralized eXchange）を紹介します。現在のアプローチと異なり、MOD-XはUniversal Message Bus、詳細な状態管理、翻訳機能、ブロックチェーンベースのセキュリティ機構を採用したレイヤ構造を提案しています。MOD-Xのアーキテクチャを紹介し、現在のプロトコルと比較し、異なるアーキテクチャ、ビジネス、能力、知識表現のエリジェントアガント（ルールベースシステム、ニューラルネットワーク、記号的論理エンジン、遺伝ソフトウェア）の統合を可能にするワークエグゼンチャリングを通じて、このアーキテクチャの実用的な実装における機能を示します。MOD-Xの主な革新点として、プロダクトサブスクライブコミュニケーションモデル、語義的な能力発見、動的なワークフローアーチテクチャを含むものがあり、理論的な形式論と実用的な実装をつなぐフレームワークを提供します。このアーキテクチャは、中央調整が不要な限り効果的なスケーリングを可能にする真正にデカントライゼッド、互換性のあるアガントエコシステムの拡大する必要性を解決しています。",
      "upvotes": 1,
      "discussionId": "686cb0ab364e2ad167eb54a6"
    },
    "publishedAt": "2025-07-06T08:46:57.000Z",
    "title": "MOD-X: A Modular Open Decentralized eXchange Framework proposal for\n  Heterogeneous Interoperable Artificial Agents",
    "summary": "As Artificial Intelligence systems evolve from monolithic models to\necosystems of specialized agents, the need for standardized communication\nprotocols becomes increasingly critical. This paper introduces MOD-X (Modular\nOpen Decentralized eXchange), a novel architectural framework proposal for\nagent interoperability that addresses key limitations of existing protocols.\nUnlike current approaches, MOD-X proposes a layered architecture with a\nUniversal Message Bus, thorough state management, translation capabilities, and\nblockchain-based security mechanisms. We present MOD-X's architecture, compare\nit with existing protocols, and demonstrate its application through a worked\nexample how it enables integration between heterogeneous specialist agents\n(agents with different architectures, vendors, capabilities, and knowledge\nrepresentations--including rule-based systems, neural networks, symbolic\nreasoning engines, and legacy software with agent wrappers). MOD-X's key\ninnovations include a publish-subscribe communication model, semantic\ncapability discovery, and dynamic workflow orchestration--providing a framework\nthat bridges theoretical formalism with practical implementation. This\narchitecture addresses the growing need for truly decentralized, interoperable\nagent ecosystems that can scale effectively without the need for central\ncoordination.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.03336",
      "authors": [
        {
          "_id": "686c67e2364e2ad167eb5314",
          "user": {
            "_id": "637859f98f288aba3d01f588",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637859f98f288aba3d01f588/eP8YNMOtxTvxH-rYEYn4f.png",
            "isPro": false,
            "fullname": "Ashutosh Hathidara",
            "user": "ashutosh1919",
            "type": "user"
          },
          "name": "Ashutosh Hathidara",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-08T08:06:34.896Z",
          "hidden": false
        },
        {
          "_id": "686c67e2364e2ad167eb5315",
          "name": "Julien Yu",
          "hidden": false
        },
        {
          "_id": "686c67e2364e2ad167eb5316",
          "name": "Sebastian Schreiber",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T06:49:02.000Z",
      "submittedOnDailyAt": "2025-07-08T02:47:33.416Z",
      "title": "ディスアンバジェーションセンター付きの微調整が企業ツール呼び出しのLLMに実際に近づき、リスクを少なくすることができます。",
      "submittedOnDailyBy": {
        "_id": "637859f98f288aba3d01f588",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637859f98f288aba3d01f588/eP8YNMOtxTvxH-rYEYn4f.png",
        "isPro": false,
        "fullname": "Ashutosh Hathidara",
        "user": "ashutosh1919",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、企業のAPIを呼び出すタスクにより増えていますが、近似のツールが同じユーザーのインテントと競う場合や、必要な引数が欠定された場合には、例えば常に失敗します。ここでは、DiaFORGE（ダイアフォージュ、オーガニックレスポンス生成と評価のダイアローグフレームワーク）を紹介します。DiaFORGEは、3つのステップのパイプラインで、ダイバージェンスシンテジスティックです。また、これは、(i) アシスタントが高度に類似したツールを区別するためのプロフェッショナル駆動の多ターンダイアローグを合成し、(ii) 3B - 70Bパラメータの開放ソースモデルに理由記録を含むサブジェクト調整を行い、(iii) 実世界的な準備度を評価するための動的なセットです。DiaBENCHという動的なベンチマークでは、DiaFORGEで訓練されたモデルは、最適化されたプロンプティングのもと、GPT-4oを27pp、Claude-3.5-Sonnetを49pp上げます。また、DiaFORGEでは、ツールの呼び出し成功率を向上させます。また、DiaFORGEでは、5000件の企業APIの製品エンジン規格と、厳密に検証されたダイバージェンスフォーカスダイアローグをペアにして開放コーパスをリリースし、信頼性のある、企業準備されたツール呼び出しアガントの構築の実用的なプランを提供します。",
      "upvotes": 1,
      "discussionId": "686c67e2364e2ad167eb5317",
      "ai_summary": "DiaFORGE is a disambiguation framework that enhances large language models' ability to invoke enterprise APIs accurately through dialogue synthesis, supervised fine-tuning, and real-world evaluation.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "enterprise APIs",
        "persona-driven",
        "multi-turn dialogues",
        "supervised fine-tuning",
        "reasoning traces",
        "end-to-end goal completion",
        "dynamic benchmark",
        "tool-invocation success",
        "disambiguation-focused dialogues"
      ]
    },
    "publishedAt": "2025-07-04T02:49:02.000Z",
    "title": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky",
    "summary": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03336.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637859f98f288aba3d01f588",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637859f98f288aba3d01f588/eP8YNMOtxTvxH-rYEYn4f.png",
      "fullname": "Ashutosh Hathidara",
      "name": "ashutosh1919",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.02659",
      "authors": [
        {
          "_id": "686736ed9db35afc9c304cea",
          "name": "Ramchalam Kinattinkara Ramakrishnan",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304ceb",
          "user": {
            "_id": "65d989790733541e06823258",
            "avatarUrl": "/avatars/cf658c5f586c02e9b1d610b5e49b2826.svg",
            "isPro": false,
            "fullname": "Zhaocong Yuan",
            "user": "justinyyy",
            "type": "user"
          },
          "name": "Zhaocong Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-07T15:46:47.568Z",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cec",
          "name": "Shaojie Zhuo",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304ced",
          "name": "Chen Feng",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cee",
          "name": "Yicheng Lin",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cef",
          "name": "Chenzheng Su",
          "hidden": false
        },
        {
          "_id": "686736ed9db35afc9c304cf0",
          "name": "Xiaopeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T14:20:41.000Z",
      "submittedOnDailyAt": "2025-07-08T02:34:16.093Z",
      "title": "オムニドラフト：オンデモ用のスペシャルデコーディング向けのクロスボキャベラリー・オンラインアダプティブドラフト",
      "submittedOnDailyBy": {
        "_id": "65d989790733541e06823258",
        "avatarUrl": "/avatars/cf658c5f586c02e9b1d610b5e49b2826.svg",
        "isPro": false,
        "fullname": "Zhaocong Yuan",
        "user": "justinyyy",
        "type": "user"
      },
      "summary": "スペシュラティブデコーディングは、小さな効率的なドラフトモデルを持つことを一般的に指示しています。このモデルは、特定のターゲットモデルシリーズ（例えばLlamaやQwenモデル）にオフラインで予習されたか、ドリスティングされたものです。しかし、オンライン部署の設定では、2つの大きな課題があります：1）ドラフトモデルと不適合したターゲットモデルの使用；2）使用と時間によるラジエンシーの向上の期待。本稿では、OmniDraftという一ノイドフレームワークを提案し、ドラフトモデルがどのターゲットモデルとも動的に対応できるようにすることを可能にします。また、ドラフトモデルとターゲットモデルのクロスボキャブラリーミスマッチを解決するために、組み合わせドリスティングファイナルチューニングを導入し、適応ドラフティングテクニックを活用して解码速度を進めます。OmniDraftは、モデルコスト、エフィシェンス、ユーザーカスタマイズが主な議論点の場合に、デバイス上のLLMアプリケーションに特に適しています。これは、上記の課題を解決する必要をさらに明らかにし、「一つのドラフターです」パラダイムをモットーにします。数学計算、コーディング、テキスト生成のタスクでのオンライン学習を通じて、OmniDraftフレームワークの効果を示します。特に、OmniDraftは、Llama-68Mモデルを1つによって、Vicuna-7B、Qwen2-7B、Llama3-8Bモデルなどの多様なターゲットモデルとスペシュラティブデコーディングを行うことができ、また1.5-2倍のスピードアップを提供します。",
      "upvotes": 0,
      "discussionId": "686736ed9db35afc9c304cf1",
      "ai_summary": "OmniDraft, a unified framework, addresses cross-vocabulary mismatch and improves decoding speed by allowing a single draft model to interact dynamically with diverse target models in online settings.",
      "ai_keywords": [
        "n-gram cache",
        "hybrid distillation fine-tuning",
        "adaptive drafting",
        "on-device LLM applications"
      ]
    },
    "publishedAt": "2025-07-03T10:20:41.000Z",
    "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
    "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the ``one drafter for all'' paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02659.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d989790733541e06823258",
      "avatarUrl": "/avatars/cf658c5f586c02e9b1d610b5e49b2826.svg",
      "fullname": "Zhaocong Yuan",
      "name": "justinyyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]