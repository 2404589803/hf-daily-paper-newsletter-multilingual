[
  {
    "paper": {
      "id": "2504.08685",
      "authors": [
        {
          "_id": "67fc6ffc59b22e7c34d64c2e",
          "name": "Team Seawead",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c2f",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c30",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c31",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c32",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c33",
          "name": "Zhibei Ma",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c34",
          "name": "Haoyuan Guo",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c35",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c36",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c37",
          "name": "Sen Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c38",
          "name": "Feng Cheng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c39",
          "name": "Feilong Zuo Xuejiao Zeng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3a",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3b",
          "name": "Fangyuan Kong",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3c",
          "name": "Zhiwu Qing",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3d",
          "name": "Fei Xiao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3e",
          "name": "Meng Wei",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c3f",
          "name": "Tuyen Hoang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c40",
          "name": "Siyu Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c41",
          "name": "Peihao Zhu",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c42",
          "name": "Qi Zhao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c43",
          "name": "Jiangqiao Yan",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c44",
          "name": "Liangke Gui",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c45",
          "name": "Sheng Bi",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c46",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c47",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c48",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c49",
          "name": "Huixia Li",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4a",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4b",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4c",
          "user": {
            "_id": "636a4e4fa55bbbdf8c877667",
            "avatarUrl": "/avatars/efdb68c56a4a44fdac52750c07a6cc35.svg",
            "isPro": false,
            "fullname": "Ling Feng",
            "user": "lingff",
            "type": "user"
          },
          "name": "Feng Ling",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:31.964Z",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4d",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4e",
          "name": "Houmin Wei",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c4f",
          "name": "Huafeng Kuang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c50",
          "name": "Jerry Duncan",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c51",
          "name": "Junda Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c52",
          "name": "Junru Zheng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c53",
          "name": "Li Sun",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c54",
          "name": "Manlin Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c55",
          "name": "Renfei Sun",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c56",
          "name": "Xiaobin Zhuang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c57",
          "name": "Xiaojie Li",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c58",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c59",
          "name": "Xuyan Chi",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5a",
          "name": "Yanghua Peng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5b",
          "name": "Yuping Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5c",
          "name": "Yuxuan Wang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5d",
          "name": "Zhongkai Zhao",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5e",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c5f",
          "name": "Zuquan Song",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c60",
          "user": {
            "_id": "6421183b69a2c2933882d652",
            "avatarUrl": "/avatars/66813a8fa22915087cccd4dbfb945ca7.svg",
            "isPro": false,
            "fullname": "Zhenheng Yang",
            "user": "zhenheny",
            "type": "user"
          },
          "name": "Zhenheng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:34.053Z",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c61",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c62",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "67fc6ffc59b22e7c34d64c63",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a5cba3bea0116f8f7187a7/Zl6Xq55_4dviWTCMwOptb.jpeg"
      ],
      "publishedAt": "2025-04-11T16:46:20.000Z",
      "submittedOnDailyAt": "2025-04-14T00:55:27.428Z",
      "title": "海藻-7B: ビデオ生成ベースモデルの効率的な訓練",
      "submittedOnDailyBy": {
        "_id": "64a5cba3bea0116f8f7187a7",
        "avatarUrl": "/avatars/97b3bad82e359c57d1ab7d6e3aa7748b.svg",
        "isPro": false,
        "fullname": "Lu Jiang",
        "user": "roadjiang",
        "type": "user"
      },
      "summary": "この技術報告書では、ビデオ生成ベースモデルの訓練における費用効率的な戦略を提案します。ここでは、約7億パラメータ（7B）を持つ中身サイズの研究モデル「Seaweed-7B」を、665,000 H100 GPU時間でシャートカットで訓練します。計算資源が中度でも、Seaweed-7Bは大きなビデオ生成モデルと比較して高度に競争的な性能を示します。資源制限のある環境では、設計選択が特に重要です。この技術報告書では、中身サイズのディフュージョンモデルの性能向上に寄与するキーな設計決定を重点的に挙げます。実験的には、2つの観察を行います：1) Seaweed-7Bは、大幅に大きなGPU資源で訓練された大きなモデルと比較して、性能を比べることができ、そしてそれを超えることもできます。2) 強い一般化能力を持つモデルは、軽量ファイナルチューニングまたは継続訓練を通じて、幅広い下流アプリケーションに効果的に適応できます。プロジェクトページは、https://seaweed.video/ から参照してください。",
      "upvotes": 57,
      "discussionId": "67fc700159b22e7c34d64d78",
      "projectPage": "https://seaweed.video/"
    },
    "publishedAt": "2025-04-11T12:46:20.000Z",
    "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
    "summary": "This technical report presents a cost-efficient strategy for training a video\ngeneration foundation model. We present a mid-sized research model with\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\nresources, Seaweed-7B demonstrates highly competitive performance compared to\ncontemporary video generation models of much larger size. Design choices are\nespecially crucial in a resource-constrained setting. This technical report\nhighlights the key design decisions that enhance the performance of the\nmedium-sized diffusion model. Empirically, we make two observations: (1)\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\ntrained on substantially greater GPU resources, and (2) our model, which\nexhibits strong generalization ability, can be effectively adapted across a\nwide range of downstream applications either by lightweight fine-tuning or\ncontinue training. See the project page at https://seaweed.video/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a5cba3bea0116f8f7187a7/Zl6Xq55_4dviWTCMwOptb.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08685.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "64a5cba3bea0116f8f7187a7",
      "avatarUrl": "/avatars/97b3bad82e359c57d1ab7d6e3aa7748b.svg",
      "fullname": "Lu Jiang",
      "name": "roadjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08736",
      "authors": [
        {
          "_id": "67fc8e37864dfcbd93d3b802",
          "user": {
            "_id": "668125557b50b433cda2a211",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
            "isPro": false,
            "fullname": "Tianwei Xiong",
            "user": "YuuTennYi",
            "type": "user"
          },
          "name": "Tianwei Xiong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:29.330Z",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b803",
          "name": "Jun Hao Liew",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b804",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b805",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67fc8e37864dfcbd93d3b806",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:26.140Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T17:59:58.000Z",
      "submittedOnDailyAt": "2025-04-14T02:57:04.488Z",
      "title": "GigaTok: 30億パラメータの可視トークナイザーを拡大して自動復元画像生成に適用",
      "submittedOnDailyBy": {
        "_id": "668125557b50b433cda2a211",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
        "isPro": false,
        "fullname": "Tianwei Xiong",
        "user": "YuuTennYi",
        "type": "user"
      },
      "summary": "自動回帰（AR）画像生成では、可視トーキナイザーは画像を総結的な離散潜在トークンに圧縮し、次のトークン予測による可視生成の下流の自動回帰モデルの効率的な訓練を可能にします。ビジュアルトーキナイザーの拡大による画像再構成質変が改善される一方、下流の生成質変が悪化することが多いことは、現在の文献で十分に解決されていない問題です。これに対して、我々は、GigaTokという最初のアプローチを提案し、可視トーキナイザーの拡大により画像再構成、生成、表現学習を同時に改善することを目指します。潜在空間の拡大による複雑性の増加が再構成と生成の対立の主な原因となることを識別しました。これを軽減するために、我々は、前処理された可視エンコーダーからの語意的に一致する特徴をトーキナイザーの特徴と一致させる語意的正規化を提案します。この制約は、拡大による潜在空間の過度の複雑性を防ぎ、再構成と下流の自動回帰生成の両方で一致的な改善を収めます。語意的正規化に基づき、我々はトーキナイザーの拡大の3つのキープラクティスを調査します：（1）1Dトーキナイザーを使用してより良いスケーラビリティ、（2）エンコーダーとデコーダーの両方を拡大する際にデコーダーのスケーリングを優先順位として、（3）ベリースケールのトーキナイザーのトレーニングの安定化を目的としてエントロピー損失を使用します。3万億パラメーターに拡大して、GigaTokは再構成、下流のAR生成、下流のAR表現質変において最先端の性能を達成します。",
      "upvotes": 23,
      "discussionId": "67fc8e38864dfcbd93d3b836",
      "projectPage": "https://silentview.github.io/GigaTok/",
      "githubRepo": "https://github.com/SilentView/GigaTok"
    },
    "publishedAt": "2025-04-11T13:59:58.000Z",
    "title": "GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for\n  Autoregressive Image Generation",
    "summary": "In autoregressive (AR) image generation, visual tokenizers compress images\ninto compact discrete latent tokens, enabling efficient training of downstream\nautoregressive models for visual generation via next-token prediction. While\nscaling visual tokenizers improves image reconstruction quality, it often\ndegrades downstream generation quality -- a challenge not adequately addressed\nin existing literature. To address this, we introduce GigaTok, the first\napproach to simultaneously improve image reconstruction, generation, and\nrepresentation learning when scaling visual tokenizers. We identify the growing\ncomplexity of latent space as the key factor behind the reconstruction vs.\ngeneration dilemma. To mitigate this, we propose semantic regularization, which\naligns tokenizer features with semantically consistent features from a\npre-trained visual encoder. This constraint prevents excessive latent space\ncomplexity during scaling, yielding consistent improvements in both\nreconstruction and downstream autoregressive generation. Building on semantic\nregularization, we explore three key practices for scaling tokenizers:(1) using\n1D tokenizers for better scalability, (2) prioritizing decoder scaling when\nexpanding both encoder and decoder, and (3) employing entropy loss to stabilize\ntraining for billion-scale tokenizers. By scaling to 3 space billion\nparameters, GigaTok achieves state-of-the-art performance in reconstruction,\ndownstream AR generation, and downstream AR representation quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08736.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668125557b50b433cda2a211",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668125557b50b433cda2a211/j3z3wT5Rv9IyUKtbzQpnc.png",
      "fullname": "Tianwei Xiong",
      "name": "YuuTennYi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08388",
      "authors": [
        {
          "_id": "67fc7367df5f5d1e87c14c6a",
          "name": "Junliang Guo",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6b",
          "name": "Yang Ye",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6c",
          "name": "Tianyu He",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6d",
          "name": "Haoyu Wu",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6e",
          "name": "Yushu Jiang",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c6f",
          "name": "Tim Pearce",
          "hidden": false
        },
        {
          "_id": "67fc7367df5f5d1e87c14c70",
          "name": "Jiang Bian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T09:41:04.000Z",
      "submittedOnDailyAt": "2025-04-14T02:07:06.500Z",
      "title": "MineWorld: マイクラストーン上の時間実時間で開放ソースの相互作用世界モデル",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "ワールドモデリングは、計算機エージェントが人間と効果的に相互作用し、動的な環境で操作できるようにするための重要な任務です。本稿では、ワールドモデリングのテストボックスとして利用されている開放エンドプローチのサンドボックスゲーム「Minecraft」に対して、時系列的に相互作用可能なワールドモデル「MineWorld」を提案します。「MineWorld」は、ビューセンスと行動を入力とし、それに対応する行動によって生成される次のシーンを生成するための視覚行動自動回帰ツレンダーによって駆動されています。特に、画像トーキナナと行動トーキナナを用いて、ビューセンスと行動を離散トーキングIDに変換し、これらの2種類のIDを交差した結合でモデルの入力を構成します。その後、次のトーキング予測を用いて、ゲーム状態の豊富な表現と状態と行動の間の関係を同時に学習します。推論時には、同時に各フレームの空間的冗餘トーキングを予測する新しい並列デコーディングアルゴリズムを開発し、異なるスケールのモデルが4から7フレームごとに生成し、ゲームプレイヤーとの時系列的な相互作用を可能にします。評価において、新しいメトリックを提案し、新しいシーンの生成時にビジュアル品質と行動の適応能力を評価することで、ワールドモデルの重要性を示します。詳細な評価により、「MineWorld」の効果性が、ソートドライブバージョンのディフュージョンベースのワールドモデルよりも显著に上回りました。コードとモデルはリリースされています。",
      "upvotes": 12,
      "discussionId": "67fc7367df5f5d1e87c14ca6",
      "githubRepo": "https://github.com/microsoft/MineWorld"
    },
    "publishedAt": "2025-04-11T05:41:04.000Z",
    "title": "MineWorld: a Real-Time and Open-Source Interactive World Model on\n  Minecraft",
    "summary": "World modeling is a crucial task for enabling intelligent agents to\neffectively interact with humans and operate in dynamic environments. In this\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\nan open-ended sandbox game which has been utilized as a common testbed for\nworld modeling. MineWorld is driven by a visual-action autoregressive\nTransformer, which takes paired game scenes and corresponding actions as input,\nand generates consequent new scenes following the actions. Specifically, by\ntransforming visual game scenes and actions into discrete token ids with an\nimage tokenizer and an action tokenizer correspondingly, we consist the model\ninput with the concatenation of the two kinds of ids interleaved. The model is\nthen trained with next token prediction to learn rich representations of game\nstates as well as the conditions between states and actions simultaneously. In\ninference, we develop a novel parallel decoding algorithm that predicts the\nspatial redundant tokens in each frame at the same time, letting models in\ndifferent scales generate 4 to 7 frames per second and enabling real-time\ninteractions with game players. In evaluation, we propose new metrics to assess\nnot only visual quality but also the action following capacity when generating\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\nbased world models significantly. The code and model have been released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08388.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.07963",
      "authors": [
        {
          "_id": "67f86da6ac109135e18e150f",
          "name": "Shoufa Chen",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1510",
          "name": "Chongjian Ge",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1511",
          "name": "Shilong Zhang",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1512",
          "name": "Peize Sun",
          "hidden": false
        },
        {
          "_id": "67f86da6ac109135e18e1513",
          "name": "Ping Luo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6412a33900634c4fe9873652/tQXTl59CIq5IxKj_5dk7f.jpeg"
      ],
      "publishedAt": "2025-04-10T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-14T04:05:17.975Z",
      "title": "PixelFlow: フローを用いたピクセル空間での生成モデル",
      "submittedOnDailyBy": {
        "_id": "6412a33900634c4fe9873652",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
        "isPro": false,
        "fullname": "Shoufa Chen",
        "user": "ShoufaChen",
        "type": "user"
      },
      "summary": "PixelFlowは、主流の潜在空間モデルと異なり、実ピクセル空間で直接動作する画像生成モデルの家族です。このアプローチは、事前学習されたVariational Autoencoder (VAE)の必要性を削減し、全体のモデルが端末から端末まで学習可能にします。効率的なカスケードフローモデリングを通じて、PixelFlowは実ピクセル空間で計算コストを抑えることができます。256x256のImageNetクラス条件付き画像生成ベンチマークでは、FID値が1.98となります。定性的な文字から画像の結果は、画像の質、アーティスト性、セマンティック制御においてPixelFlowが優れていることを示します。私たちは、この新しいパラダイムが次世代の可視化生成モデルに新しい機会を開き、インスピレーションを与えることを望むと思います。コードとモデルは、https://github.com/ShoufaChen/PixelFlowにアクセスできます。",
      "upvotes": 8,
      "discussionId": "67f86da7ac109135e18e154b",
      "githubRepo": "https://github.com/ShoufaChen/PixelFlow"
    },
    "publishedAt": "2025-04-10T13:59:56.000Z",
    "title": "PixelFlow: Pixel-Space Generative Models with Flow",
    "summary": "We present PixelFlow, a family of image generation models that operate\ndirectly in the raw pixel space, in contrast to the predominant latent-space\nmodels. This approach simplifies the image generation process by eliminating\nthe need for a pre-trained Variational Autoencoder (VAE) and enabling the whole\nmodel end-to-end trainable. Through efficient cascade flow modeling, PixelFlow\nachieves affordable computation cost in pixel space. It achieves an FID of 1.98\non 256times256 ImageNet class-conditional image generation benchmark. The\nqualitative text-to-image results demonstrate that PixelFlow excels in image\nquality, artistry, and semantic control. We hope this new paradigm will inspire\nand open up new opportunities for next-generation visual generation models.\nCode and models are available at https://github.com/ShoufaChen/PixelFlow.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6412a33900634c4fe9873652/tQXTl59CIq5IxKj_5dk7f.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07963.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6412a33900634c4fe9873652",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg",
      "fullname": "Shoufa Chen",
      "name": "ShoufaChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08600",
      "authors": [
        {
          "_id": "67fc9e72b2383c63dc413dcb",
          "name": "Peixian Ma",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dcc",
          "user": {
            "_id": "6575a625b951d40e7a4d8685",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
            "isPro": false,
            "fullname": "zhuangxialie",
            "user": "ZhuangXialie",
            "type": "user"
          },
          "name": "Xialie Zhuang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:23.810Z",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dcd",
          "name": "Chengjin Xu",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dce",
          "name": "Xuhui Jiang",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dcf",
          "name": "Ran Chen",
          "hidden": false
        },
        {
          "_id": "67fc9e72b2383c63dc413dd0",
          "name": "Jian Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T15:01:30.000Z",
      "submittedOnDailyAt": "2025-04-14T04:07:17.501Z",
      "title": "SQL-R1: 強化学習による自然言語からSQLの推理モデルの訓練",
      "submittedOnDailyBy": {
        "_id": "6575a625b951d40e7a4d8685",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
        "isPro": false,
        "fullname": "zhuangxialie",
        "user": "ZhuangXialie",
        "type": "user"
      },
      "summary": "自然言語からSQL（NL2SQL）は、自然言語クエリを構造化されたSQL文に変換し、データベースと直感的なインタラクションを可能にします。最近、データベースアプリケーション内での人間・コンピュータインタラクションの向上についての進展がありますが、複雑なシナリオにおいて、多テーブルジョインと隠れたクエリに関する推論性能においては、大きな課題が残っています。現在の方法は、主に観学微調練習（SFT）を用いてNL2SQLモデルを訓練していますが、新しい環境（例：金融、健康関連）での適応性と解釈性には限界があります。複雑な状況でNL2SQLモデルの推論性能を向上させるために、SQL-R1という新しいNL2SQL推論モデルを導入します。これは強化学習（RL）アルゴリズムで訓練されています。NL2SQLタスクに特化されたRLベースの報酬関数を設計し、強化学習の効果性における初期効果の影響を議論しました。また、合成されたNL2SQLデータの少ない量で強化学習による競合的な精度を達成し、さらにデータエンジニアリングを探求しました。既存の実験では、SQL-R1はベースモデルのみを使用してビザードのスピダーとBIRDでの実行精度はそれぞれ88.6%と66.6%を達成しました。",
      "upvotes": 6,
      "discussionId": "67fc9e73b2383c63dc413e19"
    },
    "publishedAt": "2025-04-11T11:01:30.000Z",
    "title": "SQL-R1: Training Natural Language to SQL Reasoning Model By\n  Reinforcement Learning",
    "summary": "Natural Language to SQL (NL2SQL) enables intuitive interactions with\ndatabases by transforming natural language queries into structured SQL\nstatements. Despite recent advancements in enhancing human-computer interaction\nwithin database applications, significant challenges persist, particularly\nregarding the inference performance in complex scenarios involving multi-table\njoins and nested queries. Current methodologies primarily utilize supervised\nfine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and\ninterpretability in new environments (e.g., finance and healthcare). In order\nto enhance the reasoning performance of the NL2SQL model in the above complex\nsituations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the\nreinforcement learning (RL) algorithms. We design a specialized RL-based reward\nfunction tailored for NL2SQL tasks and discussed the impact of cold start on\nthe effectiveness of intensive training. In addition, we achieve competitive\naccuracy using only a tiny amount of synthetic NL2SQL data for augmented\ntraining and further explore data engineering for RL. In existing experiments,\nSQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider\nand BIRD, respectively, only using the 7B base model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6575a625b951d40e7a4d8685",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
      "fullname": "zhuangxialie",
      "name": "ZhuangXialie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.07405",
      "authors": [
        {
          "_id": "67fa383909d06d0501a5e34e",
          "user": {
            "_id": "660d844462d63ad0009a9859",
            "avatarUrl": "/avatars/6822fc1a82c64dbfd40b88080a5fb1ae.svg",
            "isPro": false,
            "fullname": "Linyan Huang",
            "user": "DevLinyan",
            "type": "user"
          },
          "name": "Linyan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-13T19:24:47.680Z",
          "hidden": false
        },
        {
          "_id": "67fa383909d06d0501a5e34f",
          "name": "Haonan Lin",
          "hidden": false
        },
        {
          "_id": "67fa383909d06d0501a5e350",
          "name": "Yanning Zhou",
          "hidden": false
        },
        {
          "_id": "67fa383909d06d0501a5e351",
          "name": "Kaiwen Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-10T02:58:22.000Z",
      "submittedOnDailyAt": "2025-04-14T00:54:04.513Z",
      "title": "FlexIP: プレシャスとパーソナリティの動的な制御によるカスタマイズされた画像生成",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "2D生成モデルの急速な進歩に伴い、主題の識別を保つながり、多様な編集を可能にすることが重要な研究焦点として議論されています。現在の方法は通常、識別の保護とパーソナリゼーションの操作の間に固有のトレードオフを課しています。私たちは、Style-wise操作のためのPersonalization Adapterと、識別の保持のためのPreservation Adapterを挟む2つの専門化コンポーネントを持つ新しいフレームワークFlexIPを紹介します。このフレームワークは、生成モデルに直接コントロール機構を注入し、推論時に重みアダプタの動的調整を通じて柔軟なパラメータ化コントロールを可能にします。実験結果は、私たちのアプローチが価格観念的な方法の性能の制限を突破し、上位の識別の保護を実現し、より多様なパーソナリゼーションへの生成能力を支援できることを示しています（プロジェクトページ：https://flexip-tech.github.io/flexip/）。",
      "upvotes": 6,
      "discussionId": "67fa383c09d06d0501a5e3ef",
      "projectPage": "https://flexip-tech.github.io/flexip"
    },
    "publishedAt": "2025-04-09T22:58:22.000Z",
    "title": "FlexIP: Dynamic Control of Preservation and Personality for Customized\n  Image Generation",
    "summary": "With the rapid advancement of 2D generative models, preserving subject\nidentity while enabling diverse editing has emerged as a critical research\nfocus. Existing methods typically face inherent trade-offs between identity\npreservation and personalized manipulation. We introduce FlexIP, a novel\nframework that decouples these objectives through two dedicated components: a\nPersonalization Adapter for stylistic manipulation and a Preservation Adapter\nfor identity maintenance. By explicitly injecting both control mechanisms into\nthe generative model, our framework enables flexible parameterized control\nduring inference through dynamic tuning of the weight adapter. Experimental\nresults demonstrate that our approach breaks through the performance\nlimitations of conventional methods, achieving superior identity preservation\nwhile supporting more diverse personalized generation capabilities (Project\nPage: https://flexip-tech.github.io/flexip/).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.07405.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08366",
      "authors": [
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3b",
          "name": "Sauradip Nag",
          "hidden": false
        },
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3c",
          "name": "Daniel Cohen-Or",
          "hidden": false
        },
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3d",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "67fc6d8d74c3c0f0d6f24c3e",
          "name": "Ali Mahdavi-Amiri",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T09:01:09.000Z",
      "submittedOnDailyAt": "2025-04-14T00:36:54.457Z",
      "title": "In-2-4D: 2枚の単一視点画像から4次元生成の間接化",
      "submittedOnDailyBy": {
        "_id": "6399ab3296ce14c5dcf4ccbf",
        "avatarUrl": "/avatars/89aeedc96f73d76f7a6da96454f40fd2.svg",
        "isPro": false,
        "fullname": "Sauradip Nag",
        "user": "sauradip",
        "type": "user"
      },
      "summary": "私たちは、最小限の入力設定での生成的4D（すなわち、3D + 動き）間の間接化の新しい問題「In-2-4D」を提案します：2つの単一ビューの画像で、対象物の2つの異なる動き状態を撮影しています。2つの画像が、動きの開始と終了状態を表している場合、私たちの目標は、4Dでの動きを生成し、再構築することです。ビデオインタープレートモデルを使用して、動きを予測することにより、大きなフレーム間の動きが不明確な解釈につながることを防ぎます。これを克服するために、私たちは、視覚的に入力状態に近いキーフレームを特定し、顕著な動きを示すものを見つけ、それらの間にスムーズなフレームを生成するための階層的アプローチを使用します。各フレームにおいて、キーフレームの3D表現をガウシアンスプラッティングを使用して構築します。フレームの時間的なフレームは、動きをガウシアンを動的に変形させることを可能にします。時間的な一貫性と3D動きの精確化を改善するために、多ビューディフュージョンの自己アテンションを時間ステップごとに拡大し、剛性変形正規化を適用します。最後に、独立に生成された3D動きセグメントを、境界変形フィールドをインタープレートし、ガイドビデオと一致させることで、スムーズなおこりなくなるコンテキストを確保することで、それらを統合します。詳細な質的的と量性的な実験およびユーザーステージで、私たちは、私たちの方法とその構成要素の効果を示します。プロジェクトページは、https://in-2-4d.github.io/ にあります。",
      "upvotes": 4,
      "discussionId": "67fc6d9374c3c0f0d6f24e12",
      "projectPage": "https://in-2-4d.github.io/",
      "githubRepo": "https://github.com/sauradip/In-2-4D"
    },
    "publishedAt": "2025-04-11T05:01:09.000Z",
    "title": "In-2-4D: Inbetweening from Two Single-View Images to 4D Generation",
    "summary": "We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion)\ninbetweening from a minimalistic input setting: two single-view images\ncapturing an object in two distinct motion states. Given two images\nrepresenting the start and end states of an object in motion, our goal is to\ngenerate and reconstruct the motion in 4D. We utilize a video interpolation\nmodel to predict the motion, but large frame-to-frame motions can lead to\nambiguous interpretations. To overcome this, we employ a hierarchical approach\nto identify keyframes that are visually close to the input states and show\nsignificant motion, then generate smooth fragments between them. For each\nfragment, we construct the 3D representation of the keyframe using Gaussian\nSplatting. The temporal frames within the fragment guide the motion, enabling\ntheir transformation into dynamic Gaussians through a deformation field. To\nimprove temporal consistency and refine 3D motion, we expand the self-attention\nof multi-view diffusion across timesteps and apply rigid transformation\nregularization. Finally, we merge the independently generated 3D motion\nsegments by interpolating boundary deformation fields and optimizing them to\nalign with the guiding video, ensuring smooth and flicker-free transitions.\nThrough extensive qualitative and quantitiave experiments as well as a user\nstudy, we show the effectiveness of our method and its components. The project\npage is available at https://in-2-4d.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08366.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6399ab3296ce14c5dcf4ccbf",
      "avatarUrl": "/avatars/89aeedc96f73d76f7a6da96454f40fd2.svg",
      "fullname": "Sauradip Nag",
      "name": "sauradip",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08716",
      "authors": [
        {
          "_id": "67fca0ca05cd5b5035123b7e",
          "name": "Wissam Antoun",
          "hidden": false
        },
        {
          "_id": "67fca0ca05cd5b5035123b7f",
          "name": "Benoît Sagot",
          "hidden": false
        },
        {
          "_id": "67fca0ca05cd5b5035123b80",
          "name": "Djamé Seddah",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T17:29:35.000Z",
      "submittedOnDailyAt": "2025-04-14T04:20:01.034Z",
      "title": "ModernBERT または DeBERTaV3? 構造とデータの影響について Transformer Encoder モデルの性能を検討",
      "submittedOnDailyBy": {
        "_id": "5e6a3d4ea9afd5125d9ec064",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
        "isPro": true,
        "fullname": "Stefan Schweter",
        "user": "stefan-it",
        "type": "user"
      },
      "summary": "予測モデルより効率と性能向上を目指した構造的な進歩を導入したデベルタV3とモダンBERTなどの予学習テンソルライブラーエンコーダモデルがあります。モダンBERTの著者は、DeBERTaV3よりも複数のベンチマークで改善された性能を報告していますが、公開されていないトレーニングデータと共有データセットでの比較のないことで、これらの効果が構造的な改善かトレーニングデータの違いによるものかを判定することが難しいです。本研究では、カメンベルタV2と同じデータセットでモダンBERTを予学習し、モデル設計の影響を孤立して制御的な研究を行いました。結果として、前のモデルの生成はサンプルエフフィシェンスと全体のベンチマーク性能において優れていますが、モダンBERTの主な優れ点は速いトレーニングと推論速度です。しかし、新しい提案モデルはBERTやRoBERTaと比べて意味のある構造的な改善を提供しています。また、高品質の予学習データは収束を加速しますが、最終的な性能には显著な改善はありません、これはベンチマークのサタションの可能性を示しています。これらの発見は、トレンジャーモデルの評価において予学習データと構造的な革新を区別する重要性を示しています。",
      "upvotes": 3,
      "discussionId": "67fca0ca05cd5b5035123ba6"
    },
    "publishedAt": "2025-04-11T13:29:35.000Z",
    "title": "ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\n  Transformer Encoder Models Performance",
    "summary": "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce\narchitectural advancements aimed at improving efficiency and performance.\nAlthough the authors of ModernBERT report improved performance over DeBERTaV3\non several benchmarks, the lack of disclosed training data and the absence of\ncomparisons using a shared dataset make it difficult to determine whether these\ngains are due to architectural improvements or differences in training data. In\nthis work, we conduct a controlled study by pretraining ModernBERT on the same\ndataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of\nmodel design. Our results show that the previous model generation remains\nsuperior in sample efficiency and overall benchmark performance, with\nModernBERT's primary advantage being faster training and inference speed.\nHowever, the new proposed model still provides meaningful architectural\nimprovements compared to earlier models such as BERT and RoBERTa. Additionally,\nwe observe that high-quality pre-training data accelerates convergence but does\nnot significantly improve final performance, suggesting potential benchmark\nsaturation. These findings show the importance of disentangling pretraining\ndata from architectural innovations when evaluating transformer models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08716.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e6a3d4ea9afd5125d9ec064",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
      "fullname": "Stefan Schweter",
      "name": "stefan-it",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2491
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.08192",
      "authors": [
        {
          "_id": "67fcb3584a92187863e732d5",
          "name": "Aashiq Muhamed",
          "hidden": false
        },
        {
          "_id": "67fcb3584a92187863e732d6",
          "name": "Jacopo Bonato",
          "hidden": false
        },
        {
          "_id": "67fcb3584a92187863e732d7",
          "name": "Mona Diab",
          "hidden": false
        },
        {
          "_id": "67fcb3584a92187863e732d8",
          "name": "Virginia Smith",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T01:24:03.000Z",
      "submittedOnDailyAt": "2025-04-14T05:34:15.388Z",
      "title": "SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder\n  LLMの精度学習のガードライン",
      "submittedOnDailyBy": {
        "_id": "64755a83e0b188d3cb2579d8",
        "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
        "isPro": false,
        "fullname": "Aashiq Muhamed",
        "user": "aashiqmuhamed",
        "type": "user"
      },
      "summary": "マシンユニーラーニングは、モデルから不満足な知識を削除してLLMの安全性を向上させる有望なアプローチです。しかし、広く使用されている勾配ベースのユニーラーニング方法は、高い計算コスト、超パラメータ不穩定、順序的なユニーラーニング能力の低いほか、再学習攻撃の脆弱性、低いデータエフィシェンス、解釈性のないなどの問題に悩まされています。スパース・アウトオーダイザーは、特定の活性化ベースのユニーラーニングを可能にしてこれらの面に対して改善するのに適していますが、先行のアプローチは勾配ベースの方法に劣ります。本稿では、先行の結果と反対して、SAEは動的に使用されるとユニーラーニングを大幅に改善することを示します。Dynamic DAE Guardrails（DSG）を紹介します。DSGは、原則的な特徴選択と動的なクラスフィカイザーを活用した精度ユニーラーニングの新しい方法です。実験により、DSGは主なユニーラーニング方法を大幅に超え、忘れと役立ちの補いのバランスを上げます。DSGは、勾配ベースのアプローチの主要な欠点を解決し、効率的な計算コストと安定性、順序的なユニーラーニングの強固な性能、再学習攻撃に強い抵抗、データエフィシェンスの改善（ゼロショットセット含む）、ユニーラーニングの解釈性を提供します。",
      "upvotes": 2,
      "discussionId": "67fcb3594a92187863e732fa"
    },
    "publishedAt": "2025-04-10T21:24:03.000Z",
    "title": "SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder\n  Guardrails for Precision Unlearning in LLMs",
    "summary": "Machine unlearning is a promising approach to improve LLM safety by removing\nunwanted knowledge from the model. However, prevailing gradient-based\nunlearning methods suffer from issues such as high computational costs,\nhyperparameter instability, poor sequential unlearning capability,\nvulnerability to relearning attacks, low data efficiency, and lack of\ninterpretability. While Sparse Autoencoders are well-suited to improve these\naspects by enabling targeted activation-based unlearning, prior approaches\nunderperform gradient-based methods. This work demonstrates that, contrary to\nthese earlier findings, SAEs can significantly improve unlearning when employed\ndynamically. We introduce Dynamic DAE Guardrails (DSG), a novel\nmethod for precision unlearning that leverages principled feature selection and\na dynamic classifier. Our experiments show DSG substantially outperforms\nleading unlearning methods, achieving superior forget-utility trade-offs. DSG\naddresses key drawbacks of gradient-based approaches for unlearning -- offering\nenhanced computational efficiency and stability, robust performance in\nsequential unlearning, stronger resistance to relearning attacks, better data\nefficiency including zero-shot settings, and more interpretable unlearning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08192.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64755a83e0b188d3cb2579d8",
      "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
      "fullname": "Aashiq Muhamed",
      "name": "aashiqmuhamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01883",
      "authors": [
        {
          "_id": "67fcb50ea69150c25fb4b645",
          "name": "Aashiq Muhamed",
          "hidden": false
        },
        {
          "_id": "67fcb50ea69150c25fb4b646",
          "name": "Mona Diab",
          "hidden": false
        },
        {
          "_id": "67fcb50ea69150c25fb4b647",
          "name": "Virginia Smith",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T16:40:43.000Z",
      "submittedOnDailyAt": "2025-04-14T05:41:34.796Z",
      "title": "CoRAG: コラボレーションサインデッドアップデータ生成",
      "submittedOnDailyBy": {
        "_id": "64755a83e0b188d3cb2579d8",
        "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
        "isPro": false,
        "fullname": "Aashiq Muhamed",
        "user": "aashiqmuhamed",
        "type": "user"
      },
      "summary": "レビュアル・アウゲーション（RAG）モデルは、知識密集型タスクに特化し、特にフィーチャーショット学習の制約下でも優れています。私たちは、CoRAGを紹介します。CoRAGはRAGを拡張したフレームワークで、ユーザーは共有パスジョンストアを共有して共有モデルを共同学習します。CoRAGの評価については、CRABを紹介します。CRABは、共同学習の同質性開放ドメイン質問回答ベンチマークです。私たちの実験は、CoRAGは低リソーススケーラーでパラメトリックな共同学習方法や地域的に学習されたRAGモデルを一致して優れていることを示します。進めた分析は、共有ストア内の関連性のあるパスジョンの重要性、関連性のないパスジョンの驚きのベーナス、ハードネガティブの性能に負面影響する可能性の重要性を明らかにします。これは、共同RAGでの新しい考慮を導入します：集団的に豊富化された知識ベースの活用と他のユーザーからの有害なパスジョンの挙動のリスクのトレードオフです。私たちの見つけは、CoRAGの可能性を強調し、キーのデザインの課題と未来の研究の有望な道筋を明らかにします。",
      "upvotes": 2,
      "discussionId": "67fcb510a69150c25fb4b6b1"
    },
    "publishedAt": "2025-04-02T12:40:43.000Z",
    "title": "CoRAG: Collaborative Retrieval-Augmented Generation",
    "summary": "Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive\ntasks, especially under few-shot learning constraints. We introduce CoRAG, a\nframework extending RAG to collaborative settings, where clients jointly train\na shared model using a collaborative passage store. To evaluate CoRAG, we\nintroduce CRAB, a benchmark for collaborative homogeneous open-domain question\nanswering. Our experiments demonstrate that CoRAG consistently outperforms both\nparametric collaborative learning methods and locally trained RAG models in\nlow-resource scenarios. Further analysis reveals the critical importance of\nrelevant passages within the shared store, the surprising benefits of\nincorporating irrelevant passages, and the potential for hard negatives to\nnegatively impact performance. This introduces a novel consideration in\ncollaborative RAG: the trade-off between leveraging a collectively enriched\nknowledge base and the potential risk of incorporating detrimental passages\nfrom other clients. Our findings underscore the viability of CoRAG, while also\nhighlighting key design challenges and promising avenues for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01883.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64755a83e0b188d3cb2579d8",
      "avatarUrl": "/avatars/2c50590905f4bd398a4c9991e1b4b5bb.svg",
      "fullname": "Aashiq Muhamed",
      "name": "aashiqmuhamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.05303",
      "authors": [
        {
          "_id": "67fcbbe8daf0cf6803943949",
          "user": {
            "_id": "6492bf9681d93008eb33f167",
            "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
            "isPro": false,
            "fullname": "Sai Kumar Dwivedi",
            "user": "saidwivedi",
            "type": "user"
          },
          "name": "Sai Kumar Dwivedi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:12.532Z",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394a",
          "name": "Dimitrije Antić",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394b",
          "name": "Shashank Tripathi",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394c",
          "name": "Omid Taheri",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394d",
          "name": "Cordelia Schmid",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394e",
          "name": "Michael J. Black",
          "hidden": false
        },
        {
          "_id": "67fcbbe8daf0cf680394394f",
          "name": "Dimitrios Tzionas",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T17:59:33.000Z",
      "submittedOnDailyAt": "2025-04-14T06:14:23.936Z",
      "title": "InteractVLM: 3D Interaction Reasoning from 2D Foundational Models",
      "submittedOnDailyBy": {
        "_id": "6492bf9681d93008eb33f167",
        "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
        "isPro": false,
        "fullname": "Sai Kumar Dwivedi",
        "user": "saidwivedi",
        "type": "user"
      },
      "summary": "InteractVLMは、人間や物体の3D接触点を単一の自然環境画像から推定する新しい手法です。これにより、人間と物体の3D的な共通構造の再構築が正確に行えます。これは、遮蔽、深さの不明確性、物体の形状の極端な変化による複雑さにより難しいです。現在の方法は、高価な動作捕捉システムからの3D接触アノテーションや、冗長な手動ラベリングを通じて得られています。これにより、スケーラビリティと一般化能力が限られています。この問題を解決するために、InteractVLMは、大規模な視覚言語モデル（VLMs）の広い視覚知識を活用し、限られた3D接触データでの微調節を通じて実現します。しかし、これらのモデルを直接適用するのは、2Dでの論理を行うことにより非単純です。そこで、InteractVLMは、新型のRender-Localize-Liftモジュールを導入しています。これは、(1) 3Dの体と物体の表面を多点撮影により2D空間に埋め込み、(2) 新しい多点撮影位置推定モデル（MV-Loc）を学習して2Dでの接触を推定、(3) これらを3Dにアップロードすることで実現します。また、InteractVLMは、Semantic Human Contact estimationという新しいタスクを提案しています。これでは、人間の接触予測は物体のセマンティクスに基づいて行われ、豊富な相互作用モデリングが可能になります。InteractVLMは、接触推定と3D再構築において現在の方法を超え、自然環境画像からの3D再構築を支援します。コードとモデルは、https://interactvlm.is.tue.mpg.de から利用できます。",
      "upvotes": 0,
      "discussionId": "67fcbbeadaf0cf68039439b9",
      "projectPage": "https://interactvlm.is.tue.mpg.de/",
      "githubRepo": "https://github.com/saidwivedi/InteractVLM"
    },
    "publishedAt": "2025-04-07T13:59:33.000Z",
    "title": "InteractVLM: 3D Interaction Reasoning from 2D Foundational Models",
    "summary": "We introduce InteractVLM, a novel method to estimate 3D contact points on\nhuman bodies and objects from single in-the-wild images, enabling accurate\nhuman-object joint reconstruction in 3D. This is challenging due to occlusions,\ndepth ambiguities, and widely varying object shapes. Existing methods rely on\n3D contact annotations collected via expensive motion-capture systems or\ntedious manual labeling, limiting scalability and generalization. To overcome\nthis, InteractVLM harnesses the broad visual knowledge of large Vision-Language\nModels (VLMs), fine-tuned with limited 3D contact data. However, directly\napplying these models is non-trivial, as they reason only in 2D, while\nhuman-object contact is inherently 3D. Thus we introduce a novel\nRender-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D\nspace via multi-view rendering, (2) trains a novel multi-view localization\nmodel (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D.\nAdditionally, we propose a new task called Semantic Human Contact estimation,\nwhere human contact predictions are conditioned explicitly on object semantics,\nenabling richer interaction modeling. InteractVLM outperforms existing work on\ncontact estimation and also facilitates 3D reconstruction from an in-the wild\nimage. Code and models are available at https://interactvlm.is.tue.mpg.de.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05303.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6492bf9681d93008eb33f167",
      "avatarUrl": "/avatars/3efa34de104ae400ba5a75b54f0f5b6e.svg",
      "fullname": "Sai Kumar Dwivedi",
      "name": "saidwivedi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.05262",
      "authors": [
        {
          "_id": "67fcc9a980568c7ef6180dcb",
          "name": "Yang Yan",
          "hidden": false
        },
        {
          "_id": "67fcc9a980568c7ef6180dcc",
          "name": "Yu Lu",
          "hidden": false
        },
        {
          "_id": "67fcc9a980568c7ef6180dcd",
          "name": "Renjun Xu",
          "hidden": false
        },
        {
          "_id": "67fcc9a980568c7ef6180dce",
          "name": "Zhenzhong Lan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-07T16:57:10.000Z",
      "submittedOnDailyAt": "2025-04-14T07:11:56.706Z",
      "title": "ディファイデンスレベルのLLMsは本当に基本的な加算を理解しますか？ルール学習とメモリー化の比較",
      "submittedOnDailyBy": {
        "_id": "62ce6dd785cfd21c04c7e6f5",
        "avatarUrl": "/avatars/89837a5dea6e2d753d59caad142bed4a.svg",
        "isPro": false,
        "fullname": "ZhenzhongLan",
        "user": "DannyLan",
        "type": "user"
      },
      "summary": "ベンチマークスコアが高いにも関わらず、大規模言語モデル（LLMs）は簡単な問題に失敗し、重要な質問が浮かび上がってきます：LLMsは数学の原理を学ぶのか、またはパターンの記憶によるだけですか？最近の研究よりも複雑なベンチマークを設計することではなく、我々は基本的な2つの整数の加算（0から2^64）を使用して、2つの核心的な特性を調査しています：可換性（A+B=B+A）と構成的な一般化（同型符号的マッピングをよび、例えば7→y）。状態の最先端のLLMsは数値加算において73.8-99.8%の精度を達成しますが、符号的マッピングの下では性能が7.5%以下に落ちることにより、学習したルールの一般化に失敗しています。数字の数に対する非単調的な性能スケーリングと、頻繁な可換性の破続（1,700以上のA+B≠B+Aのケース）はこれをさらに補足しています。加算ルールの明記した提供は平均81.2%の性能低下を引き起こし、自動説明は基線精度を維持し、LLMの演算処理が人間に定められた原理に合わないことを示しています。我々の見つけたことは、現在のLLMsは記憶パターンをもとにして真のルール学習に依存していることを示し、構造的な制限と新しいアプローチの必要性を明らかにしています。",
      "upvotes": 0,
      "discussionId": "67fcc9aa80568c7ef6180e24"
    },
    "publishedAt": "2025-04-07T12:57:10.000Z",
    "title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\n  vs. Memorization in Large Language Models",
    "summary": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n(0 to 2^{64}), probing two core properties: commutativity (A+B=B+A) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., 7\nrightarrow y). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to leq7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of A+B neq B+A) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.05262.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ce6dd785cfd21c04c7e6f5",
      "avatarUrl": "/avatars/89837a5dea6e2d753d59caad142bed4a.svg",
      "fullname": "ZhenzhongLan",
      "name": "DannyLan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]