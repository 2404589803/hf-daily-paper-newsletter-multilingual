[
  {
    "paper": {
      "id": "2505.21115",
      "authors": [
        {
          "_id": "68372d97e4af3c39dcec8e65",
          "user": {
            "_id": "5dfa8e07da6d0311fd3d5430",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651090418656-5dfa8e07da6d0311fd3d5430.png",
            "isPro": false,
            "fullname": "Sergey Pletenev",
            "user": "memyprokotow",
            "type": "user"
          },
          "name": "Sergey Pletenev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:55.604Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e66",
          "user": {
            "_id": "660ee18e2dcd816ad14b3739",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
            "isPro": false,
            "fullname": "Maria Marina",
            "user": "zlatamaria",
            "type": "user"
          },
          "name": "Maria Marina",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:59.278Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e67",
          "user": {
            "_id": "6682607ece294ddc5e72f4fb",
            "avatarUrl": "/avatars/2a304bc3eb56ec7d13297d28fbb062ae.svg",
            "isPro": false,
            "fullname": "Ivanov",
            "user": "VirVen",
            "type": "user"
          },
          "name": "Nikolay Ivanov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-02T07:48:42.811Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e68",
          "name": "Daria Galimzianova",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e69",
          "user": {
            "_id": "643010b2ff56d6c2004699a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/OYsf1Hp-KAievw_M8XBG9.jpeg",
            "isPro": false,
            "fullname": "Krayko Nikita",
            "user": "nakrayko",
            "type": "user"
          },
          "name": "Nikita Krayko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:53.523Z",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6a",
          "name": "Mikhail Salnikov",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6b",
          "name": "Vasily Konovalov",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6c",
          "name": "Alexander Panchenko",
          "hidden": false
        },
        {
          "_id": "68372d97e4af3c39dcec8e6d",
          "user": {
            "_id": "63bbfd74141c7d395c471768",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
            "isPro": false,
            "fullname": "Viktor Moskvoretskii",
            "user": "VityaVitalich",
            "type": "user"
          },
          "name": "Viktor Moskvoretskii",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:57.325Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T12:35:13.000Z",
      "submittedOnDailyAt": "2025-06-09T07:27:12.232Z",
      "title": "明日も同じでしょうか？多言語永極問題\n信頼性のあるQAの改善に向けたクラス分け",
      "submittedOnDailyBy": {
        "_id": "660ee18e2dcd816ad14b3739",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
        "isPro": false,
        "fullname": "Maria Marina",
        "user": "zlatamaria",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、問答（QA）タスクで頻繁にハロケーシャティング（ハロケーシャティングは、事実や情報を違うものにして出力すること）を行います。これに寄与する重要なだけではなく、まだ調査が不足している要因の一つが、問題の時間的性質です。それは、どのようにも答えが時間に沿って変化しない「永暦型」（evergreen）か、答えが時間に沿って変化する「変動型」（mutable）です。本研究では、EverGreenQAという最初の多言語QAデータセットを紹介し、永暦型のラベルをサポートして評価と訓練を行うことができます。EverGreenQAを使用して、12つの現代のLLMsをベンチマークし、それらが問題の時間的性質を明記的に（語義化された判断によって）または隠れて（不確かさシグナルによって）表現しているかを評価します。また、EG-E5という軽量マルチラング語分類器を訓練し、このタスクで最先端の性能を達成します。最後に、永暦型分類の実用的な効用を示します：自知覚の推定を改善し、QAデータセットをフィルタリング、GPT-4oの検索行為を説明する3つのアプリケーションです。",
      "upvotes": 41,
      "discussionId": "68372d98e4af3c39dcec8e88",
      "githubRepo": "https://github.com/s-nlp/Evergreen-classification",
      "ai_summary": "EverGreenQA, a multilingual QA dataset with evergreen labels, is introduced to benchmark LLMs on temporality encoding and assess their performance through verbalized judgments and uncertainty signals.",
      "ai_keywords": [
        "Large Language Models",
        "QA",
        "evergreen",
        "mutable",
        "temporality",
        "Multilingual QA dataset",
        "EG-E5",
        "lightweight multilingual classifier",
        "SoTA performance",
        "self-knowledge estimation",
        "filtering QA datasets",
        "GPT-4o retrieval behavior"
      ]
    },
    "publishedAt": "2025-05-27T08:35:13.000Z",
    "title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question\n  Classification to Improve Trustworthy QA",
    "summary": "Large Language Models (LLMs) often hallucinate in question answering (QA)\ntasks. A key yet underexplored factor contributing to this is the temporality\nof questions -- whether they are evergreen (answers remain stable over time) or\nmutable (answers change). In this work, we introduce EverGreenQA, the first\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\nencode question temporality explicitly (via verbalized judgments) or implicitly\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\nthe practical utility of evergreen classification across three applications:\nimproving self-knowledge estimation, filtering QA datasets, and explaining\nGPT-4o retrieval behavior.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21115.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "660ee18e2dcd816ad14b3739",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
      "fullname": "Maria Marina",
      "name": "zlatamaria",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01111",
      "authors": [
        {
          "_id": "6845b6a33ec10bdd8ab4da1b",
          "name": "Shunian Chen",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1c",
          "user": {
            "_id": "66440e86bfe15e84d369cb03",
            "avatarUrl": "/avatars/d15b3b3831bc74138206071612169f64.svg",
            "isPro": false,
            "fullname": "Xinyuan Xie",
            "user": "SatsukiVie",
            "type": "user"
          },
          "name": "Xinyuan Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:42.759Z",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1d",
          "name": "Zheshu Chen",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1e",
          "name": "Liyan Zhao",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da1f",
          "name": "Owen Lee",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da20",
          "name": "Zhan Su",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da21",
          "name": "Qilin Sun",
          "hidden": false
        },
        {
          "_id": "6845b6a33ec10bdd8ab4da22",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T18:29:17.000Z",
      "submittedOnDailyAt": "2025-06-09T01:59:54.914Z",
      "title": "FusionAudio-1.2M: マルチモーダルコンテキスト融合による細かい音声キャプチャーのための向こうへ",
      "submittedOnDailyBy": {
        "_id": "623be9e1d1eb227788764959",
        "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
        "isPro": false,
        "fullname": "Shunian Chen",
        "user": "Shunian",
        "type": "user"
      },
      "summary": "高品質な大規模な音声キャプチャは、音声理解の進歩に重要ですが、現在の自動化方法は、細かい詳細やコンテキストの正確性を欠くキャプチャを多く生成します。これは、限られた単模態や表面的な多模態情報の依存関係によるからです。人間の聴覚認識からヒントを得、多様なコースを適切に統合し、複雑な音声スケーンを分析する高級な聴覚認識を実現することを目指し、新しい2段階の自動化パイプラインを導入します。このパイプラインは、特別な学習済みモデルを使用して、多様なコンテキストコース（例：話し声、音楽、一般の音、関連の映像からの可視情報）を抽出します。その後、大規模な言語モデル（LLM）は、これらの豊富な多模態入力を組み合わせて、詳細なコンテキストに関連付けられた音声キャプチャを生成します。本研究の主な貢献は、(1) 提案されたスケーラブルな細かい音声キャプチャ生成方法、(2) フュージョンアドイオン、120万つのこのような詳細なキャプチャを含む新しい大規模データセット、600万ペアのQAペアを組み合わせたもの、(3) フュージョンアドイオンを用いた音声モデルの向上、特にCLAPベースの音声エンコーダーの音声-本文の対応と指示従いの向上によるものです。この論文は、複雑な音声環境の詳細で正確な自動化理解に向けてのプレイヤーを引き出します。コードとデータは、https://github.com/satsuki2486441738/FusionAudio に見つかります。",
      "upvotes": 21,
      "discussionId": "6845b6a43ec10bdd8ab4da23",
      "githubRepo": "https://github.com/FreedomIntelligence/FusionAudio",
      "ai_summary": "A novel two-stage pipeline using specialized pretrained models and a large language model enhances audio caption quality by integrating diverse multimodal cues and contextual information.",
      "ai_keywords": [
        "audio captioning",
        "auditory perception",
        "auditory scene analysis",
        "pretrained models",
        "large language model",
        "FusionAudio",
        "CLAP-based audio encoder",
        "audio-text alignment",
        "instruction following"
      ]
    },
    "publishedAt": "2025-06-01T14:29:17.000Z",
    "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion",
    "summary": "High-quality, large-scale audio captioning is crucial for advancing audio\nunderstanding, yet current automated methods often generate captions that lack\nfine-grained detail and contextual accuracy, primarily due to their reliance on\nlimited unimodal or superficial multimodal information. Drawing inspiration\nfrom human auditory perception, which adeptly integrates cross-modal cues and\nperforms sophisticated auditory scene analysis, we introduce a novel two-stage\nautomated pipeline. This pipeline first employs specialized pretrained models\nto extract diverse contextual cues (e.g., speech, music, general sounds, and\nvisual information from associated video). A large language model (LLM) then\nsynthesizes these rich, multimodal inputs to generate detailed and\ncontext-aware audio captions. Key contributions of this work include: (1) the\nproposed scalable method for fine-grained audio caption generation; (2)\nFusionAudio, a new large-scale dataset comprising 1.2 million such detailed\ncaptions, combined with 6 million QA pairs; and (3) enhanced audio models\ndeveloped using FusionAudio, specifically a CLAP-based audio encoder with\nsuperior audio-text alignment and instruction following. This paper paves the\nway for more nuanced and accurate automated understanding of complex audio\nenvironments. Code and data can be found in\nhttps://github.com/satsuki2486441738/FusionAudio.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01111.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623be9e1d1eb227788764959",
      "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
      "fullname": "Shunian Chen",
      "name": "Shunian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05984",
      "authors": [
        {
          "_id": "68463ee43ec10bdd8ab4da6f",
          "user": {
            "_id": "622326ae0129f2097d69a3e2",
            "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
            "isPro": false,
            "fullname": "Cheng-Han Chiang",
            "user": "dcml0714",
            "type": "user"
          },
          "name": "Cheng-Han Chiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:26.041Z",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da70",
          "user": {
            "_id": "64dc191bc307ee5369fbcb04",
            "avatarUrl": "/avatars/5a8a0db63a187e85d4ae2fff93a838f0.svg",
            "isPro": false,
            "fullname": "Xiaofei Wang",
            "user": "xiaofei-wang",
            "type": "user"
          },
          "name": "Xiaofei Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T01:54:46.319Z",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da71",
          "name": "Chung-Ching Lin",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da72",
          "name": "Kevin Lin",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da73",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da74",
          "name": "Radu Kopetz",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da75",
          "name": "Yao Qian",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da76",
          "name": "Zhendong Wang",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da77",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da78",
          "name": "Hung-yi Lee",
          "hidden": false
        },
        {
          "_id": "68463ee43ec10bdd8ab4da79",
          "name": "Lijuan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T11:05:48.000Z",
      "submittedOnDailyAt": "2025-06-09T00:28:11.753Z",
      "title": "音声に関心を持つ大規模言語モデルが、話し方の裁判としての役割を果たす",
      "submittedOnDailyBy": {
        "_id": "622326ae0129f2097d69a3e2",
        "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
        "isPro": false,
        "fullname": "Cheng-Han Chiang",
        "user": "dcml0714",
        "type": "user"
      },
      "summary": "音声に関連した大規模言語モデル（ALLMs）は、音声入力の文字的および非文字的な情報を理解できます。本論文では、ALLMsを自動的な判断者として、演説の話し方を評価することを調査します。SLMsで生成された演説を評価するために、ALLMの判断者を使用します。この評価には、声のスタイルの実行と役職演技の2つのタスクを使用します。評価する話し方は、感情、音量、話し速さ、単語の重み、ピッチの制御、非言語的要素などを含みます。4つの語言模型（SLMs）を使用してこれらの2つのタスクを完了し、人間とALLMsがSLMsの回答を判断します。GPT-4o-audioとGemini-2.5-proの2つのALLMの判断者を人間の評価結果と比較し、Geminiと人間の判断者の一致率が人間の評価者間の一致率と比較的に高いことを示します。これらの有望な結果から、ALLMsはSLMsの評価に用いる判断者として使用できることが示されます。また、現在のSLMs、特にGPT-4o-audioでも、話し方の制御と自然なディアロギーの生成において改良の余地があることが明らかになります。",
      "upvotes": 9,
      "discussionId": "68463ee43ec10bdd8ab4da7a",
      "ai_summary": "Audio-aware large language models can assess speaking styles in audio inputs, demonstrating performance comparable to human judges in evaluating synthesized speech along dimensions like emotion, volume, and pitch.",
      "ai_keywords": [
        "audio-aware large language models",
        "ALLMs",
        "speaking styles",
        "SLMs",
        "voice style instruction",
        "role-playing",
        "emotion",
        "volume",
        "speaking pace",
        "word emphasis",
        "pitch control",
        "non-verbal elements",
        "GPT-4o-audio",
        "Gemini-2.5-pro",
        "human evaluation",
        "agreement",
        "speaking style control",
        "natural dialogues"
      ]
    },
    "publishedAt": "2025-06-06T07:05:48.000Z",
    "title": "Audio-Aware Large Language Models as Judges for Speaking Styles",
    "summary": "Audio-aware large language models (ALLMs) can understand the textual and\nnon-textual information in the audio input. In this paper, we explore using\nALLMs as an automatic judge to assess the speaking styles of speeches. We use\nALLM judges to evaluate the speeches generated by SLMs on two tasks: voice\nstyle instruction following and role-playing. The speaking style we consider\nincludes emotion, volume, speaking pace, word emphasis, pitch control, and\nnon-verbal elements. We use four spoken language models (SLMs) to complete the\ntwo tasks and use humans and ALLMs to judge the SLMs' responses. We compare two\nALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and\nshow that the agreement between Gemini and human judges is comparable to the\nagreement between human evaluators. These promising results show that ALLMs can\nbe used as a judge to evaluate SLMs. Our results also reveal that current SLMs,\neven GPT-4o-audio, still have room for improvement in controlling the speaking\nstyle and generating natural dialogues.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622326ae0129f2097d69a3e2",
      "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
      "fullname": "Cheng-Han Chiang",
      "name": "dcml0714",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05629",
      "authors": [
        {
          "_id": "68464b273ec10bdd8ab4da86",
          "user": {
            "_id": "64a6518132cf858d6386ac52",
            "avatarUrl": "/avatars/4cabf3dab8b1ba06245ad8024f334181.svg",
            "isPro": false,
            "fullname": "Ananth Muppidi",
            "user": "ananthmuppidi",
            "type": "user"
          },
          "name": "Ananth Muppidi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T02:47:03.971Z",
          "hidden": false
        },
        {
          "_id": "68464b273ec10bdd8ab4da87",
          "user": {
            "_id": "5f89da6c5d083370c711f37c",
            "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
            "isPro": false,
            "fullname": "Abhilash Nandy",
            "user": "abhi1nandy2",
            "type": "user"
          },
          "name": "Abhilash Nandy",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-09T03:49:29.446Z",
          "hidden": false
        },
        {
          "_id": "68464b273ec10bdd8ab4da88",
          "user": {
            "_id": "65238ea295df08170c93933d",
            "avatarUrl": "/avatars/8364301e324274a550d12f2b184ea10e.svg",
            "isPro": false,
            "fullname": "Sambaran Bandyopadhyay",
            "user": "sambaran",
            "type": "user"
          },
          "name": "Sambaran Bandyopadhyay",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-09T02:47:03.971Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T23:13:22.000Z",
      "submittedOnDailyAt": "2025-06-09T01:27:41.831Z",
      "title": "レビューエコロジーフォーマティングプロモーションプログラム",
      "submittedOnDailyBy": {
        "_id": "5f89da6c5d083370c711f37c",
        "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
        "isPro": false,
        "fullname": "Abhilash Nandy",
        "user": "abhi1nandy2",
        "type": "user"
      },
      "summary": "特定領域のタスクでの大規模言語モデルの性能に必要な微調は、計算的に費用の高いであり技術的に難しい。本論文は、小さなパラメーターセットを学習してプレトレーンドモデルをダウンストリームタスクに適応するための可能性のあるソフトプロンプティングを用いたパラメーター効率的な微調に焦点を当てています。私たちは、入力トークンに基づくソフトプロンプトを生成し、重要度の異なるトークンに対して別の処理を行うための新しい入力依存ソフトプロンプティング手法（ID-SPAM）を提案します。私たちの方法は、訓練パラメーターの数を小さく保つことで簡単かつ効率的です。私たちは、提案された手法と比較して状況の最先端の技術に対してそのメリットを示し、改善されたゼロショット領域転移能力を示します。",
      "upvotes": 9,
      "discussionId": "68464b273ec10bdd8ab4da89",
      "ai_summary": "A new method using input-dependent soft prompting with a self-attention mechanism improves parameter-efficient fine-tuning for large language models, enhancing zero-shot domain transfer.",
      "ai_keywords": [
        "soft prompting",
        "parameter-efficient fine-tuning",
        "pre-trained models",
        "downstream tasks",
        "Input Dependent Soft Prompting technique",
        "self-Attention Mechanism",
        "zero shot domain transfer"
      ]
    },
    "publishedAt": "2025-06-05T19:13:22.000Z",
    "title": "Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs",
    "summary": "The performance of large language models in domain-specific tasks\nnecessitates fine-tuning, which is computationally expensive and technically\nchallenging. This paper focuses on parameter-efficient fine-tuning using soft\nprompting, a promising approach that adapts pre-trained models to downstream\ntasks by learning a small set of parameters. We propose a novel Input Dependent\nSoft Prompting technique with a self-Attention Mechanism (ID-SPAM) that\ngenerates soft prompts based on the input tokens and attends different tokens\nwith varying importance. Our method is simple and efficient, keeping the number\nof trainable parameters small. We show the merits of the proposed approach\ncompared to state-of-the-art techniques on various tasks and show the improved\nzero shot domain transfer capability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05629.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f89da6c5d083370c711f37c",
      "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
      "fullname": "Abhilash Nandy",
      "name": "abhi1nandy2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.01872",
      "authors": [
        {
          "_id": "683e77d41417d107337abf6e",
          "user": {
            "_id": "643f9e2288d9d4488fd81c52",
            "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
            "isPro": false,
            "fullname": "Tinghui Zhu",
            "user": "DarthZhu",
            "type": "user"
          },
          "name": "Tinghui Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:12:47.572Z",
          "hidden": false
        },
        {
          "_id": "683e77d41417d107337abf6f",
          "name": "Kai Zhang",
          "hidden": false
        },
        {
          "_id": "683e77d41417d107337abf70",
          "name": "Muhao Chen",
          "hidden": false
        },
        {
          "_id": "683e77d41417d107337abf71",
          "name": "Yu Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-02T17:01:40.000Z",
      "submittedOnDailyAt": "2025-06-09T07:51:42.399Z",
      "title": "拡張モデルディーションは、全モデルディーションへの道筋として正しい道ですか？",
      "submittedOnDailyBy": {
        "_id": "643f9e2288d9d4488fd81c52",
        "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
        "isPro": false,
        "fullname": "Tinghui Zhu",
        "user": "DarthZhu",
        "type": "user"
      },
      "summary": "多モーダル言語モデル（OLMs）は、テキスト、画像、映像、音声などの多様な入力モーダルティーに対して統合して理由を与えることを目指し、強い言語能力を維持することです。最近の進歩にもかかわらず、現在のモデル、特に開放ソースのものは、真の多モーダル性に達するまで遠く離れています。特に、特定のモーダルペアでの訓練によって学ばれたモーダルペアを超えて一般化することや、多モーダル入力を処理して強い性能を達成することが難しいことがあります。ここでは、トレーニングモデルの主な手法としてのモーダル拡張の効果を調査します。具体的には、モーダル拡張が核心的な言語能力を補足するかどうか、独立に訓練されたモーダル専門的なモデルを有効に統合することができるか、そして、これらの効果が順番的な拡張に比べてより良い知識共有と一般化を実現するかを調査します。詳細な実験を通じて、これらのトレードオフを分析し、現在のアプローチで真の多モーダル性を達成する可能性についてのフィードバックを提供します。",
      "upvotes": 9,
      "discussionId": "683e77d41417d107337abf8f",
      "projectPage": "https://darthzhu.github.io/lm-extend-page/",
      "githubRepo": "https://github.com/DarthZhu/lm-extend",
      "ai_summary": "Research investigates the impact of extending modality and model merging on maintaining language abilities and generalization in omni-modal language models.",
      "ai_keywords": [
        "omni-modal language models",
        "modality extension",
        "fine-tuning",
        "language abilities",
        "model merging",
        "generalization",
        "true omni-modality"
      ]
    },
    "publishedAt": "2025-06-02T13:01:40.000Z",
    "title": "Is Extending Modality The Right Path Towards Omni-Modality?",
    "summary": "Omni-modal language models (OLMs) aim to integrate and reason over diverse\ninput modalities--such as text, images, video, and audio--while maintaining\nstrong language capabilities. Despite recent advancements, existing models,\nespecially open-source ones, remain far from true omni-modality, struggling to\ngeneralize beyond the specific modality pairs they are trained on or to achieve\nstrong performance when processing multi-modal inputs. We study the effect of\nextending modality, the dominant technique for training multimodal models,\nwhere an off-the-shelf language model is fine-tuned on target-domain and\nlanguage data. Specifically, we investigate three key questions: (1) Does\nmodality extension compromise core language abilities? (2) Can model merging\neffectively integrate independently fine-tuned modality-specific models to\nachieve omni-modality? (3) Does omni-modality extension lead to better\nknowledge sharing and generalization compared to sequential extension? Through\nextensive experiments, we analyze these trade-offs and provide insights into\nthe feasibility of achieving true omni-modality using current approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643f9e2288d9d4488fd81c52",
      "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
      "fullname": "Tinghui Zhu",
      "name": "DarthZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06276",
      "authors": [
        {
          "_id": "68466dfb3ec10bdd8ab4dae2",
          "name": "Jiatao Gu",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae3",
          "name": "Tianrong Chen",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae4",
          "name": "David Berthelot",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae5",
          "name": "Huangjie Zheng",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae6",
          "name": "Yuyang Wang",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae7",
          "name": "Ruixiang Zhang",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae8",
          "name": "Laurent Dinh",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4dae9",
          "name": "Miguel Angel Bautista",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4daea",
          "name": "Josh Susskind",
          "hidden": false
        },
        {
          "_id": "68466dfb3ec10bdd8ab4daeb",
          "name": "Shuangfei Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:58:39.000Z",
      "submittedOnDailyAt": "2025-06-09T03:58:52.022Z",
      "title": "STARFlow: 高解像度画像の合成に向けた潜在的正規化フローのスケーリング",
      "submittedOnDailyBy": {
        "_id": "6164e72d73996c363c52e66d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png",
        "isPro": false,
        "fullname": "Jiatao Gu",
        "user": "thomagram",
        "type": "user"
      },
      "summary": "STARFlowは、スケーラブルな生成モデルです。これは、正規化フローに基づいて構築されており、高解像度画像合成で強力な性能を示します。STARFlowの核心はTransformer Autoregressive Flow（TARFlow）です。これは、正規化フローの表現力と、自動協調監視チャンナーの構造化モデリング能力を組み合わせたものです。まず、TARFlowの理論的な普遍性を証明します。この基盤に基づいて、以下の3つのキー的なアーキテクチャとアルゴリズムの革新を導入し、スケーラブル性を大幅に向上させます。1）深いスローウィード設計で、深いTransformerブロックがモデルの表現力を最大限に捉え、計算的に効率的な浅いTransformerブロックがその代わりにもっともベニフィエントです。2）学習された自動エンコーダーの潜在空間でのモデリングを行い、直接のピクセルレベルモデリングよりもより効果的です。3）新しいガイドアルゴリズムを導入し、サンプルの品質を大幅に向上させます。重要なことに、モデルは正規化フローとして終始一貫し、連続空間での正確な最大尤度訓練を可能にします。STARFlowは、クラス条件付きと文脈条件付き画像生成タスクで優れた性能を示し、サンプルの品質において最先端のディフュージョンモデルと比較的な性能を達成します。私たちの知識によると、この研究は、このスケールと解像度で正規化フローが効果的に動作する最初の成功事例です。",
      "upvotes": 5,
      "discussionId": "68466dfb3ec10bdd8ab4daec",
      "ai_summary": "STARFlow, a generative model combining normalizing flows with autoregressive Transformers, achieves competitive image synthesis performance with innovations in architecture and latent space modeling.",
      "ai_keywords": [
        "normalizing flows",
        "Transformer Autoregressive Flow",
        "TARFlow",
        "theoretical universality",
        "deep-shallow design",
        "pretrained autoencoders",
        "latent space",
        "guidance algorithm",
        "end-to-end normalizing flow",
        "exact maximum likelihood training",
        "class-conditional",
        "text-conditional image generation",
        "state-of-the-art diffusion models"
      ]
    },
    "publishedAt": "2025-06-06T13:58:39.000Z",
    "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\n  Synthesis",
    "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06276.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6164e72d73996c363c52e66d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png",
      "fullname": "Jiatao Gu",
      "name": "thomagram",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06253",
      "authors": [
        {
          "_id": "68469b773ec10bdd8ab4db88",
          "name": "Yuping He",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db89",
          "name": "Yifei Huang",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8a",
          "user": {
            "_id": "6392c73390b8e99a6779a7b0",
            "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
            "isPro": false,
            "fullname": "Guo Chen",
            "user": "cg1177",
            "type": "user"
          },
          "name": "Guo Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:02.757Z",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8b",
          "name": "Lidong Lu",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8c",
          "name": "Baoqi Pei",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8d",
          "name": "Jilan Xu",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8e",
          "name": "Tong Lu",
          "hidden": false
        },
        {
          "_id": "68469b773ec10bdd8ab4db8f",
          "name": "Yoichi Sato",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:25:48.000Z",
      "submittedOnDailyAt": "2025-06-09T07:00:04.801Z",
      "title": "バリデーションポートレイド：エックソコントリービションとエクソコントリービションの視点を結ぶ: コロラボレーションシンテジェンシック知能の調査",
      "submittedOnDailyBy": {
        "_id": "6392c73390b8e99a6779a7b0",
        "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
        "isPro": false,
        "fullname": "Guo Chen",
        "user": "cg1177",
        "type": "user"
      },
      "summary": "世界をその仮想中心から（第一人称）とその仮想中心から（第三人称）の両方の視点から見ることは、人類の認知の基礎であり、動的な環境の豊富な補充的な理解を可能にします。近年、マシンがこの両方の視点の補間的な機能を活用することを誘導する研究方向が映像理解の分野で興味深い研究課題として登場しました。本調査では、映像理解を両方の視点から詳細に調査します。まず、その仮想中心とその仮想中心の手法を統合した実用的な応用例を特徴的にし、その潜在的なデータマイリングを見積もります。次に、これらの応用例を実現するための重要な研究課題を特定します。その後、最近の進展を3つの主な研究方向にグループ化し、それぞれの方向における多様なタスクと関連する研究を分析します。また、両方の視点の研究をサポートするベンチマークデータセットについて議論し、その範囲、多様性、適用可能性を評価します。最後に、現在の研究の限界を議論し、未来の研究方向を提案します。両方の視点からの洞察を統合して、映像理解と人工知能の進歩を励まし、マシンが人間のように世界を見ることを目指します。関連する研究を調べるためのGitHubリポジトリは、https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Visionにあります。",
      "upvotes": 5,
      "discussionId": "68469b773ec10bdd8ab4db90",
      "projectPage": "https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision",
      "githubRepo": "https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision",
      "ai_summary": "A survey on leveraging both egocentric and exocentric video understanding for enhancing complementary tasks with a focus on three research directions and benchmark datasets.",
      "ai_keywords": [
        "egocentric",
        "exocentric",
        "video understanding",
        "research tasks",
        "benchmark datasets"
      ]
    },
    "publishedAt": "2025-06-06T13:25:48.000Z",
    "title": "Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence\n  with Egocentric-Exocentric Vision",
    "summary": "Perceiving the world from both egocentric (first-person) and exocentric\n(third-person) perspectives is fundamental to human cognition, enabling rich\nand complementary understanding of dynamic environments. In recent years,\nallowing the machines to leverage the synergistic potential of these dual\nperspectives has emerged as a compelling research direction in video\nunderstanding. In this survey, we provide a comprehensive review of video\nunderstanding from both exocentric and egocentric viewpoints. We begin by\nhighlighting the practical applications of integrating egocentric and\nexocentric techniques, envisioning their potential collaboration across\ndomains. We then identify key research tasks to realize these applications.\nNext, we systematically organize and review recent advancements into three main\nresearch directions: (1) leveraging egocentric data to enhance exocentric\nunderstanding, (2) utilizing exocentric data to improve egocentric analysis,\nand (3) joint learning frameworks that unify both perspectives. For each\ndirection, we analyze a diverse set of tasks and relevant works. Additionally,\nwe discuss benchmark datasets that support research in both perspectives,\nevaluating their scope, diversity, and applicability. Finally, we discuss\nlimitations in current works and propose promising future research directions.\nBy synthesizing insights from both perspectives, our goal is to inspire\nadvancements in video understanding and artificial intelligence, bringing\nmachines closer to perceiving the world in a human-like manner. A GitHub repo\nof related works can be found at\nhttps://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6392c73390b8e99a6779a7b0",
      "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
      "fullname": "Guo Chen",
      "name": "cg1177",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05523",
      "authors": [
        {
          "_id": "6846657d3ec10bdd8ab4daca",
          "user": {
            "_id": "630bc5ae86b8b9904c33e94b",
            "avatarUrl": "/avatars/b176d9b1691c05cc941409dd6c2b2228.svg",
            "isPro": false,
            "fullname": "Zikui Cai",
            "user": "Zikui",
            "type": "user"
          },
          "name": "Zikui Cai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:17.551Z",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacb",
          "name": "Andrew Wang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacc",
          "name": "Anirudh Satheesh",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacd",
          "name": "Ankit Nakhawa",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dace",
          "name": "Hyunwoo Jae",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dacf",
          "name": "Keenan Powell",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad0",
          "name": "Minghui Liu",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad1",
          "name": "Neel Jay",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad2",
          "name": "Sungbin Oh",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad3",
          "name": "Xiyao Wang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad4",
          "name": "Yongyuan Liang",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad5",
          "name": "Tom Goldstein",
          "hidden": false
        },
        {
          "_id": "6846657d3ec10bdd8ab4dad6",
          "name": "Furong Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T19:12:45.000Z",
      "submittedOnDailyAt": "2025-06-09T03:10:23.755Z",
      "title": "MORSE-500: プログラミング的に制御可能なビデオベンチマークで、多模態論理をタイプテストする",
      "submittedOnDailyBy": {
        "_id": "655fed9fdef5905d38b84af3",
        "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
        "isPro": false,
        "fullname": "Xiyao Wang",
        "user": "russwang",
        "type": "user"
      },
      "summary": "電視ビデオモデル（VLMs）の急速な進歩にもかかわらず、現在の多タイプ論理のベンチマークは3つの重要な次元で不足しています。まず、静的画像をほとんどだけに依存し、実世界的な環境の時系列的複雑性を捉えずにいます。二つ目に、数学問題解決に焦点を当て、抽象的な、物理的な、計画的な、空間的な、時系列的な能力を含む広範囲の論理スキルを無視しています。三つ目に、許多なベンチマークは速やかにシャットアウトし、失敗モードの診断や進歩の評価に限られています。これらの問題を解決するために、MORSE-500（多タイプ論理のストレステスト環境）を紹介します。これは、6つの補間論理カテゴリーを拡がる500フルスクリプトカットからなるビデオベンチマークです。各インスタンスは、マニム、Matplotlib、MoviePy、生成ビデオモデル、および編集された実写フードを用いた確定的なPythonスクリプトで生成されます。このスクリプト駆動設計では、視覚的複雑性、デトラクター密度、時系列的動作を細かく制御でき、モデルの向上に伴い困難度をシステマチックにスケールすることができます。静的ベンチマークと異なり、MORSE-500は進化することを目指して設計されています：その制御可能な生成パイプラインは、任意の難易度の新しいインスタンスを作成することを可能にし、次世代モデルのストレステストに最適です。最先端のシステムとの初期実験は、その時点で最強のものであるゲミーニ2.5 Pro、OpenAI o3、および強力なオープンソースモデルを含むものによって行われ、すべてのカテゴリーで大幅な性能間違いが見出され、特に抽象的なおよび計画的なタスクでは特に大きな欠陥が見出されました。データセット、生成スクリプト、評価ハーネスを公開し、透明的で可再現的で進歩的な多タイプ論理研究を支援します。",
      "upvotes": 5,
      "discussionId": "6846657d3ec10bdd8ab4dad7",
      "projectPage": "https://morse-500.github.io/",
      "githubRepo": "https://github.com/morse-benchmark/morse-500",
      "ai_summary": "MORSE-500, a video benchmark with 500 scripted clips, evaluates multimodal reasoning across six categories, highlighting performance gaps in abstract and planning tasks.",
      "ai_keywords": [
        "Vision-language models",
        "MORSE-500",
        "multimodal reasoning",
        "video benchmark",
        "scripted clips",
        "reasoning categories",
        "Manim",
        "Matplotlib",
        "MoviePy",
        "generative video models",
        "controllable generation",
        "spatial capabilities",
        "temporal capabilities",
        "abstract reasoning",
        "planning tasks"
      ]
    },
    "publishedAt": "2025-06-05T15:12:45.000Z",
    "title": "MORSE-500: A Programmatically Controllable Video Benchmark to\n  Stress-Test Multimodal Reasoning",
    "summary": "Despite rapid advances in vision-language models (VLMs), current benchmarks\nfor multimodal reasoning fall short in three key dimensions. First, they\noverwhelmingly rely on static images, failing to capture the temporal\ncomplexity of real-world environments. Second, they narrowly focus on\nmathematical problem-solving, neglecting the broader spectrum of reasoning\nskills -- including abstract, physical, planning, spatial, and temporal\ncapabilities -- required for robust multimodal intelligence. Third, many\nbenchmarks quickly saturate, offering limited headroom for diagnosing failure\nmodes or measuring continued progress. We introduce MORSE-500 (Multimodal\nReasoning Stress-test Environment), a video benchmark composed of 500 fully\nscripted clips with embedded questions spanning six complementary reasoning\ncategories. Each instance is programmatically generated using deterministic\nPython scripts (via Manim, Matplotlib, MoviePy), generative video models, and\ncurated real footage. This script-driven design allows fine-grained control\nover visual complexity, distractor density, and temporal dynamics -- enabling\ndifficulty to be scaled systematically as models improve. Unlike static\nbenchmarks that become obsolete once saturated, MORSE-500 is built to evolve:\nits controllable generation pipeline supports the creation of arbitrarily\nchallenging new instances, making it ideally suited for stress-testing\nnext-generation models. Initial experiments with state-of-the-art systems --\nincluding various Gemini 2.5 Pro and OpenAI o3 which represent the strongest\navailable at the time, alongside strong open-source models -- reveal\nsubstantial performance gaps across all categories, with particularly large\ndeficits in abstract and planning tasks. We release the full dataset,\ngeneration scripts, and evaluation harness to support transparent,\nreproducible, and forward-looking multimodal reasoning research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05523.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655fed9fdef5905d38b84af3",
      "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
      "fullname": "Xiyao Wang",
      "name": "russwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05573",
      "authors": [
        {
          "_id": "6846902a3ec10bdd8ab4db61",
          "name": "Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db62",
          "user": {
            "_id": "62e18206926f4892a4c782bd",
            "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
            "isPro": false,
            "fullname": "Chenguo Lin",
            "user": "chenguolin",
            "type": "user"
          },
          "name": "Chenguo Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:09.640Z",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db63",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db64",
          "name": "Honglei Yan",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db65",
          "name": "Yiqiang Feng",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db66",
          "name": "Yadong Mu",
          "hidden": false
        },
        {
          "_id": "6846902a3ec10bdd8ab4db67",
          "name": "Katerina Fragkiadaki",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/iNc42ij-Kj0Z4V0jD5lCp.mp4"
      ],
      "publishedAt": "2025-06-05T20:30:28.000Z",
      "submittedOnDailyAt": "2025-06-09T06:14:01.450Z",
      "title": "PartCrafter: 構造化3Dメッシュ生成による組成的潜在ディフュージョントランスフォーマー",
      "submittedOnDailyBy": {
        "_id": "62e18206926f4892a4c782bd",
        "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
        "isPro": false,
        "fullname": "Chenguo Lin",
        "user": "chenguolin",
        "type": "user"
      },
      "summary": "PartCrafterは、単一のRGB画像から記号的に意味のあるおよび幾何学的に異なる3Dメッシュを共に合成する最初の構造化された3D生成モデルです。現存する方法と違って、単一の3D形状を生成したり、画像を分割して各分割を再構築する2ステッププロセスを選択するものではなく、PartCrafterは、事前に分割された入力に依存しない統一した、構成的な生成アーキテクチャを採用しています。単一の画像に基づいて、複数の3Dパートを同時にノイズを除去し、個々の物体や複雑な多物体スペースのパート関係識別的な生成を可能にします。PartCrafterは、全体の物体を学習した3Dメッシュディフュージョントランスフォーマー（DiT）の学習された重み、エンコーダー、デコーダーを継承し、2つの鍵の革新を導入しています：（1）構成的な潜在空間、そこで各3Dパートは、離れた潜在トークンの集合で表現されます；（2）構造化された情報流を可能にする階層的な注意機構、個々のパート内およびすべてのパート間で、生成時にもグローバル的な一致性を確保し、パートレベルの詳細を保持します。パートレベルのスーパービジョンを支援するために、大規模な3D物体データセットからパートレベルの注釈を採掘して新しいデータセットをカレーレードしました。実験は、PartCrafterが既存の手法を超えて、分解可能な3Dメッシュの生成を行い、入力画像で直接見えないパートも含むことを示し、3D理解と合成におけるパート関係識別的な生成先驅を示しています。コードと訓練データはリリースされます。",
      "upvotes": 4,
      "discussionId": "6846902a3ec10bdd8ab4db68",
      "projectPage": "https://wgsxm.github.io/projects/partcrafter",
      "githubRepo": "https://github.com/wgsxm/PartCrafter",
      "ai_summary": "PartCrafter is a unified 3D generative model that synthesizes multiple semantically meaningful 3D meshes from a single image using a compositional latent space and hierarchical attention mechanism.",
      "ai_keywords": [
        "3D generative model",
        "multiple 3D meshes",
        "RGB image",
        "unified compositional generation architecture",
        "denoising",
        "3D diffusion transformer (DiT)",
        "compositional latent space",
        "disentangled latent tokens",
        "hierarchical attention mechanism",
        "part-level supervision",
        "part-aware generative priors"
      ]
    },
    "publishedAt": "2025-06-05T16:30:28.000Z",
    "title": "PartCrafter: Structured 3D Mesh Generation via Compositional Latent\n  Diffusion Transformers",
    "summary": "We introduce PartCrafter, the first structured 3D generative model that\njointly synthesizes multiple semantically meaningful and geometrically distinct\n3D meshes from a single RGB image. Unlike existing methods that either produce\nmonolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an\nimage and then reconstructing each segment, PartCrafter adopts a unified,\ncompositional generation architecture that does not rely on pre-segmented\ninputs. Conditioned on a single image, it simultaneously denoises multiple 3D\nparts, enabling end-to-end part-aware generation of both individual objects and\ncomplex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh\ndiffusion transformer (DiT) trained on whole objects, inheriting the pretrained\nweights, encoder, and decoder, and introduces two key innovations: (1) A\ncompositional latent space, where each 3D part is represented by a set of\ndisentangled latent tokens; (2) A hierarchical attention mechanism that enables\nstructured information flow both within individual parts and across all parts,\nensuring global coherence while preserving part-level detail during generation.\nTo support part-level supervision, we curate a new dataset by mining part-level\nannotations from large-scale 3D object datasets. Experiments show that\nPartCrafter outperforms existing approaches in generating decomposable 3D\nmeshes, including parts that are not directly visible in input images,\ndemonstrating the strength of part-aware generative priors for 3D understanding\nand synthesis. Code and training data will be released.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/iNc42ij-Kj0Z4V0jD5lCp.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05573.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e18206926f4892a4c782bd",
      "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
      "fullname": "Chenguo Lin",
      "name": "chenguolin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06199",
      "authors": [
        {
          "_id": "684637733ec10bdd8ab4da66",
          "user": {
            "_id": "674b2406591d7232820252cd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
            "isPro": false,
            "fullname": "Hongyan Zhi",
            "user": "Hoyard",
            "type": "user"
          },
          "name": "Hongyan Zhi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:27.821Z",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da67",
          "name": "Peihao Chen",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da68",
          "name": "Siyuan Zhou",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da69",
          "name": "Yubo Dong",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6a",
          "name": "Quanxi Wu",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6b",
          "name": "Lei Han",
          "hidden": false
        },
        {
          "_id": "684637733ec10bdd8ab4da6c",
          "name": "Mingkui Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T16:00:31.000Z",
      "submittedOnDailyAt": "2025-06-09T01:42:52.611Z",
      "title": "3DFlowAction: 3Dフローワールドからのクロス・エモダメント操作学習モデル",
      "submittedOnDailyBy": {
        "_id": "674b2406591d7232820252cd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
        "isPro": false,
        "fullname": "Hongyan Zhi",
        "user": "Hoyard",
        "type": "user"
      },
      "summary": "操作は、ロボットにとって長期にわたり難しい課題でしたが、人間は複雑な物体との交互作用を容易に行うことができます。例えば、カップをミックスレーチにつけることができます。この難易度の原因は、ロボットに操作スキルを教えるための大規模なユニークなデータセットの欠如です。現在のロボットデータセットは、簡単なスケーン内で異なるアクションスペースでロボットのアクションを記録しています。これは、異なるスケーン内の異なるロボットでの一連の強固なアクション表現の学習を妨げています。人間が操作タスクを理解する方法を観察し、物体が3D空間でどのように動くかを理解することが操作を指導するための重要なコミドであることを見出しました。このコミドは、機体に依存しないものであり、人間と異なるロボットにも適しています。これに基づき、人間とロボットの操作データから3Dフローワールドモデルを学習することを目指しています。このモデルは、3D空間で相互作用する物体の未来の動きを予測し、操作のアクションプランニングを指導します。特に、移動物体の自動検出プイルプラインを通じて、大規模な3D光学フローデータセットを合成します。その後、ビデオディフュージョンベースのワールドモデルは、これらのデータから操作物理を学習し、言語指示に基づいた3D光学フロートライエットを生成します。生成された3D物体光学フローにより、流れガイドディングレンダリング機構を提案し、予測された最終状態をレンダリングし、GPT-4oを利用して予測された流れがタスク説明と一致しているかを評価します。これにより、ロボットはクローズドロープランニング能力を持つことになります。最終的に、予測された3D光学フローを最適化ポリシーの制約として、操作に必要なロボットアクションのチャンクを決定することを考慮します。拡張的な実験は、多様なロボット操作タスクでの強固な一般化と、特定のハードウェアによる訓練を不要な信頼性のあるクローズドフォームエモージオンアダプタションを示します。",
      "upvotes": 3,
      "discussionId": "684637733ec10bdd8ab4da6d",
      "githubRepo": "https://github.com/Hoyyyaard/3DFlowAction/",
      "ai_summary": "A 3D flow world model learned from human and robot manipulation data, using video diffusion and GPT-4o, enables robots to perform diverse manipulation tasks with strong generalization and cross-embodiment adaptation.",
      "ai_keywords": [
        "3D flow world model",
        "moving object auto-detect pipeline",
        "video diffusion-based world model",
        "3D optical flow dataset",
        "ManiFlow-110k",
        "3D optical flow trajectories",
        "flow-guided rendering mechanism",
        "GPT-4o",
        "closed-loop planning",
        "optimization policy",
        "cross-embodiment adaptation"
      ]
    },
    "publishedAt": "2025-06-06T12:00:31.000Z",
    "title": "3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\n  Model",
    "summary": "Manipulation has long been a challenging task for robots, while humans can\neffortlessly perform complex interactions with objects, such as hanging a cup\non the mug rack. A key reason is the lack of a large and uniform dataset for\nteaching robots manipulation skills. Current robot datasets often record robot\naction in different action spaces within a simple scene. This hinders the robot\nto learn a unified and robust action representation for different robots within\ndiverse scenes. Observing how humans understand a manipulation task, we find\nthat understanding how the objects should move in the 3D space is a critical\nclue for guiding actions. This clue is embodiment-agnostic and suitable for\nboth humans and different robots. Motivated by this, we aim to learn a 3D flow\nworld model from both human and robot manipulation data. This model predicts\nthe future movement of the interacting objects in 3D space, guiding action\nplanning for manipulation. Specifically, we synthesize a large-scale 3D optical\nflow dataset, named ManiFlow-110k, through a moving object auto-detect\npipeline. A video diffusion-based world model then learns manipulation physics\nfrom these data, generating 3D optical flow trajectories conditioned on\nlanguage instructions. With the generated 3D object optical flow, we propose a\nflow-guided rendering mechanism, which renders the predicted final state and\nleverages GPT-4o to assess whether the predicted flow aligns with the task\ndescription. This equips the robot with a closed-loop planning ability.\nFinally, we consider the predicted 3D optical flow as constraints for an\noptimization policy to determine a chunk of robot actions for manipulation.\nExtensive experiments demonstrate strong generalization across diverse robotic\nmanipulation tasks and reliable cross-embodiment adaptation without\nhardware-specific training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06199.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674b2406591d7232820252cd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
      "fullname": "Hongyan Zhi",
      "name": "Hoyard",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05433",
      "authors": [
        {
          "_id": "68469df13ec10bdd8ab4db92",
          "name": "Zikang Liu",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db93",
          "name": "Tongtian Yue",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db94",
          "name": "Yepeng Tang",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db95",
          "name": "Longteng Guo",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db96",
          "name": "Junxian Cai",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db97",
          "name": "Qingbin Liu",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db98",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "68469df13ec10bdd8ab4db99",
          "name": "Jing Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T09:13:37.000Z",
      "submittedOnDailyAt": "2025-06-09T07:17:09.252Z",
      "title": "Prefix Grouper: 共通プレフィックスを用いた効率的なGRPOトレーニング",
      "submittedOnDailyBy": {
        "_id": "6448dcf1b6ac93fe6512e342",
        "avatarUrl": "/avatars/a6441f89eabd156181bafc47c0b2f8c8.svg",
        "isPro": false,
        "fullname": "Zikang Liu",
        "user": "JohnCage",
        "type": "user"
      },
      "summary": "Group Relative Policy Optimization (GRPO)は、共通の入力前缀を共有する候補出力の相対的な比較から勾配を計算して、価値計算を効率的に改善します。GRPOは効果的ですが、長い共通の入力前缀を処理する際には、グループメンバーそれぞれに冗長的にエンコードする必要があり、計算量の負荷が大きくなります。この不適切性は、長コンテキスト学習の場合には、主要なスケーラビリティボトルネックとなります。我々は、Shared-Prefix Forward戦略を用いて冗長な前缀計算を除去するために、Prefix Grouperという効率的なGRPOトレーニングアルゴリズムを提案します。特に、自動注意を2つの部分に再構成し、我々の方法は、共通の前缀を1度だけエンコードできるようにし、全ての微分可能性と終端から始めるトレーニングの相容性を維持します。Prefix Grouperは、標準的なGRPOとトレーニング等価であることを理論的および実験的に証明します：同じ前向き出力と後向き勾配を獲得し、最終的な価値計算の動態と最終的な価値計算の性能が変わらないように確保します。実験的には、我々の実験は、Prefix Grouperは一致した結果を収め、特に長い前缀の場合にはトレーニングの計算コストを大幅に減少させることを確認しました。提案された方法は完全にプラグアンプです：現在のGRPOベースのアーキテクチャと揃っていて、現在のトレーニングパイプラインに無視できるように、構造的な変更が必要ないだけで、入力構成と注意計算に最小限の変更が必要なものです。Prefix Grouperは、同じ計算バジュー内でも大きなグループサイズを使用できるように、GRPOのスケーラビリティを複雑なタスクや大きなモデルに対して向上させます。コードは現在、https://github.com/johncaged/PrefixGrouperで利用可能です。",
      "upvotes": 2,
      "discussionId": "68469df13ec10bdd8ab4db9a",
      "githubRepo": "https://github.com/johncaged/PrefixGrouper",
      "ai_summary": "Prefix Grouper reduces computational overhead in GRPO by encoding shared prefixes only once, improving scalability in long-context scenarios without altering training dynamics or policy performance.",
      "ai_keywords": [
        "Group Relative Policy Optimization (GRPO)",
        "self-attention",
        "Shared-Prefix Forward strategy",
        "computational overhead",
        "long-context learning scenarios",
        "differentiability",
        "end-to-end training",
        "training-equivalent"
      ]
    },
    "publishedAt": "2025-06-05T05:13:37.000Z",
    "title": "Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward",
    "summary": "Group Relative Policy Optimization (GRPO) enhances policy learning by\ncomputing gradients from relative comparisons among candidate outputs that\nshare a common input prefix. Despite its effectiveness, GRPO introduces\nsubstantial computational overhead when processing long shared prefixes, which\nmust be redundantly encoded for each group member. This inefficiency becomes a\nmajor scalability bottleneck in long-context learning scenarios. We propose\nPrefix Grouper, an efficient GRPO training algorithm that eliminates redundant\nprefix computation via a Shared-Prefix Forward strategy. In particular, by\nrestructuring self-attention into two parts, our method enables the shared\nprefix to be encoded only once, while preserving full differentiability and\ncompatibility with end-to-end training. We provide both theoretical and\nempirical evidence that Prefix Grouper is training-equivalent to standard GRPO:\nit yields identical forward outputs and backward gradients, ensuring that the\noptimization dynamics and final policy performance remain unchanged.\nEmpirically, our experiments confirm that Prefix Grouper achieves consistent\nresults while significantly reducing the computational cost of training,\nparticularly in long-prefix scenarios. The proposed method is fully\nplug-and-play: it is compatible with existing GRPO-based architectures and can\nbe seamlessly integrated into current training pipelines as a drop-in\nreplacement, requiring no structural modifications and only minimal changes to\ninput construction and attention computation. Prefix Grouper enables the use of\nlarger group sizes under the same computational budget, thereby improving the\nscalability of GRPO to more complex tasks and larger models. Code is now\navailable at https://github.com/johncaged/PrefixGrouper",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05433.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6448dcf1b6ac93fe6512e342",
      "avatarUrl": "/avatars/a6441f89eabd156181bafc47c0b2f8c8.svg",
      "fullname": "Zikang Liu",
      "name": "JohnCage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.04255",
      "authors": [
        {
          "_id": "684312988f9ec8394c514883",
          "user": {
            "_id": "65c43d6d2b723dbc4ddc29d2",
            "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
            "isPro": false,
            "fullname": "Kunal Pai",
            "user": "guineapig",
            "type": "user"
          },
          "name": "Kunal Pai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-07T05:45:12.032Z",
          "hidden": false
        },
        {
          "_id": "684312988f9ec8394c514884",
          "user": {
            "_id": "62a0dbe7bff710e3fb05f9ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a0dbe7bff710e3fb05f9ae/uZK0Zkv7YG7jWbweh5tQb.png",
            "isPro": false,
            "fullname": "Parth Shah",
            "user": "helloparthshah",
            "type": "user"
          },
          "name": "Parth Shah",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-06T16:08:57.423Z",
          "hidden": false
        },
        {
          "_id": "684312988f9ec8394c514885",
          "name": "Harshil Patel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-01T17:33:16.000Z",
      "submittedOnDailyAt": "2025-06-09T03:49:09.306Z",
      "title": "HASHIRU: ハイラーキーエージェントシステム ハイブリッドインテリジェントリソースの利用",
      "submittedOnDailyBy": {
        "_id": "65c43d6d2b723dbc4ddc29d2",
        "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
        "isPro": false,
        "fullname": "Kunal Pai",
        "user": "guineapig",
        "type": "user"
      },
      "summary": "Rapid Large Language Model (LLM) の進歩は、自動転換マルチエージェントシステム (MAS) の開発を促進しています。しかし、現在のフレームワークは通常、柔軟性、リソース認識、モデル多様性、自動転換ツールの作成に欠けています。本文では、「HASHIRU」（ハイブリッドインテリジェンスリソース利用のためのヒューリスティックエージェントシステム）という新しいMASフレームワークを紹介します。これは柔軟性、リソースエフィシェンス、適応性を向上させることを目的としています。HASHIRUは、タスクの需要とリソースの制約（コスト、メモリ）に基づいて専門的な「エムポイ」エージェントを動的に管理する「セオ」エージェントを特徴としています。そのハイブリッドインテリジェンスは、Ollamaを通じて小さな、地域的なLLMを優先し、必要に応じて外部APIと大きなモデルを柔軟に使用することを特徴としています。雇用/解雇コストを挟む経済モデルは、チームの安定とリソースの効率的な配分を促進します。このシステムは、自動転換APIツールの作成とメモリ関数を含みます。学術論文の評価（58%の成功率）、安全性評価（JailbreakBenchの一部で100%の成功率）、複雑な理由の評価（GSM8Kでは96% vs. 61%；JEEBenchでは80% vs. 68.3%；SVAMPでは92% vs. 84%）などのタスクでの評価は、HASHIRUの能力を示しています。ケーススタディは、自動転換コストモデルの生成、ツールの統合、バジェット管理による自動改善を示しています。HASHIRUは、動的なヒューリスティック制御、リソース認識のハイブリッドインテリジェンス、自動転換機能拡張を通じて、より強固、効率的、適応性のあるMASを提供しています。ソースコードとベンチマークは、https://github.com/HASHIRU-AI/HASHIRU と https://github.com/HASHIRU-AI/HASHIRUBench に提供されています。ライブデモは、https://hashiruagentx-hashiruai.hf.space にお問い合わせで利用できます。",
      "upvotes": 2,
      "discussionId": "684312998f9ec8394c514886",
      "githubRepo": "https://github.com/HASHIRU-AI/HASHIRU",
      "ai_summary": "HASHIRU, a novel MAS framework, enhances flexibility, resource efficiency, and adaptability by dynamically managing specialized agents and using a hybrid intelligence approach with smaller, local LLMs and external APIs.",
      "ai_keywords": [
        "Hierarchical Agent System",
        "Hybrid Intelligent Resource Utilization",
        "HASHIRU",
        "CEO agent",
        "employee agents",
        "Ollama",
        "external APIs",
        "economic model",
        "hiring/firing costs",
        "autonomous API tool creation",
        "academic paper review",
        "safety assessments",
        "GSM8K",
        "JEEBench",
        "SVAMP",
        "Gemini 2.0 Flash",
        "self-improvement",
        "autonomous cost model generation",
        "tool integration",
        "budget management"
      ]
    },
    "publishedAt": "2025-06-01T13:33:16.000Z",
    "title": "HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\n  Utilization",
    "summary": "Rapid Large Language Model (LLM) advancements are fueling autonomous\nMulti-Agent System (MAS) development. However, current frameworks often lack\nflexibility, resource awareness, model diversity, and autonomous tool creation.\nThis paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent\nResource Utilization), a novel MAS framework enhancing flexibility, resource\nefficiency, and adaptability. HASHIRU features a \"CEO\" agent dynamically\nmanaging specialized \"employee\" agents, instantiated based on task needs and\nresource constraints (cost, memory). Its hybrid intelligence prioritizes\nsmaller, local LLMs (via Ollama) while flexibly using external APIs and larger\nmodels when necessary. An economic model with hiring/firing costs promotes team\nstability and efficient resource allocation. The system also includes\nautonomous API tool creation and a memory function. Evaluations on tasks like\nacademic paper review (58% success), safety assessments (100% on a\nJailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash\non GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate\nHASHIRU's capabilities. Case studies illustrate its self-improvement via\nautonomous cost model generation, tool integration, and budget management.\nHASHIRU offers a promising approach for more robust, efficient, and adaptable\nMAS through dynamic hierarchical control, resource-aware hybrid intelligence,\nand autonomous functional extension. Source code and benchmarks are available\nat https://github.com/HASHIRU-AI/HASHIRU and\nhttps://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is\navailable at https://hashiruagentx-hashiruai.hf.space upon request.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04255.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c43d6d2b723dbc4ddc29d2",
      "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
      "fullname": "Kunal Pai",
      "name": "guineapig",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]