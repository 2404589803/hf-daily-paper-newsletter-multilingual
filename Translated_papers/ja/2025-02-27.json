[
  {
    "paper": {
      "id": "2502.18934",
      "authors": [
        {
          "_id": "67bfe1bf4426925c82fe5953",
          "name": "Kanana LLM Team",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5954",
          "name": "Yunju Bak",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5955",
          "name": "Hojin Lee",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5956",
          "user": {
            "_id": "60436d159e905013ae8715d7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1623809612769-60436d159e905013ae8715d7.jpeg",
            "isPro": false,
            "fullname": "Minho Ryu",
            "user": "bzantium",
            "type": "user"
          },
          "name": "Minho Ryu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:17.979Z",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5957",
          "user": {
            "_id": "66ebb4fdc5b2c25450fd17de",
            "avatarUrl": "/avatars/e6b40dcbe2eba838ba21be9221758a3c.svg",
            "isPro": false,
            "fullname": "Jiyeon Ham",
            "user": "jiyeonham",
            "type": "user"
          },
          "name": "Jiyeon Ham",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:11.786Z",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5958",
          "name": "Seungjae Jung",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5959",
          "user": {
            "_id": "66c82a50c1b3c03c61aea140",
            "avatarUrl": "/avatars/3c508f96bdca2f2ce9746d3decd4718e.svg",
            "isPro": false,
            "fullname": "daniel nam",
            "user": "daniel-rl2",
            "type": "user"
          },
          "name": "Daniel Wontae Nam",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:09.613Z",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe595a",
          "name": "Taegyeong Eo",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe595b",
          "name": "Donghun Lee",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe595c",
          "user": {
            "_id": "6142e17fe9e656d4459121e4",
            "avatarUrl": "/avatars/6baebd4598a845ec7fdb735eb0d53139.svg",
            "isPro": false,
            "fullname": "Doohae Jung",
            "user": "Doohae",
            "type": "user"
          },
          "name": "Doohae Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:06.858Z",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe595d",
          "user": {
            "_id": "60f559be68ee3ef098e407cf",
            "avatarUrl": "/avatars/e1f00ff1c1c9fa7f591535d39c7d5e44.svg",
            "isPro": false,
            "fullname": "Boseop Kim",
            "user": "seopbo",
            "type": "user"
          },
          "name": "Boseop Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:01.989Z",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe595e",
          "user": {
            "_id": "6605028007a154c768e1c4c7",
            "avatarUrl": "/avatars/88678edb83fdb466067e38acd22d07de.svg",
            "isPro": false,
            "fullname": "Nayeon Kim",
            "user": "lana-ny",
            "type": "user"
          },
          "name": "Nayeon Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:13.867Z",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe595f",
          "user": {
            "_id": "6136f65440e43b8f748a0833",
            "avatarUrl": "/avatars/f72a5ae3d3e94485de8aed8df94abdad.svg",
            "isPro": false,
            "fullname": "Jaesun Park",
            "user": "jaesun",
            "type": "user"
          },
          "name": "Jaesun Park",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:15.898Z",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5960",
          "name": "Hyunho Kim",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5961",
          "name": "Hyunwoong Ko",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5962",
          "user": {
            "_id": "63d268bb57ab367124ea7b75",
            "avatarUrl": "/avatars/11312cde1e9f077aa9e5103b48be5de6.svg",
            "isPro": false,
            "fullname": "Changmin Lee",
            "user": "changminlee",
            "type": "user"
          },
          "name": "Changmin Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:04.506Z",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5963",
          "name": "Kyoung-Woon On",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5964",
          "name": "Seulye Baeg",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5965",
          "name": "Junrae Cho",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5966",
          "name": "Sunghee Jung",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5967",
          "name": "Jieun Kang",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5968",
          "name": "EungGyun Kim",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe5969",
          "name": "Eunhwa Kim",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe596a",
          "name": "Byeongil Ko",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe596b",
          "name": "Daniel Lee",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe596c",
          "name": "Minchul Lee",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe596d",
          "name": "Miok Lee",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe596e",
          "name": "Shinbok Lee",
          "hidden": false
        },
        {
          "_id": "67bfe1bf4426925c82fe596f",
          "name": "Gaeun Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T08:36:20.000Z",
      "title": "カナナ: 計算効率的なバイリンガル言語モデル",
      "summary": "カナナ、韓国語と英語で出色な性能を示すバイリンガル言語モデルシリーズを紹介します。カナナの計算コストは、同じサイズの最先端モデルよりも大幅に低いです。レポートでは、計算効率的なモデルを実現するために使用された手法を詳細に説明しています。その中には、高品質データフィルタリング、ステージごとの事前学習、深さアップスケーリング、プリニングとディスタイルし、そして、カナナモデルの事後学習期間に使用された手法も説明しています。その中には、ユーザーとの無際ファインタイニングと好み最適化を含むサブプロモーションです。最後に、言語モデルの特定のスケーナリオに適用するための可能性のあるアプローチも詳細に説明しています。カナナモデルシリーズは、210億から325億パラメータの範囲で拡張されており、210億パラメータのモデル（ベース、インストラクト、エンベディング）は公開されて、韓国語モデルの研究に貢献することを目的としています。",
      "upvotes": 39,
      "discussionId": "67bfe1c04426925c82fe59a1"
    },
    "publishedAt": "2025-02-26T23:05:13.440Z",
    "title": "Kanana: Compute-efficient Bilingual Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18934.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60436d159e905013ae8715d7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1623809612769-60436d159e905013ae8715d7.jpeg",
      "fullname": "Minho Ryu",
      "name": "bzantium",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.19400",
      "authors": [
        {
          "_id": "67bfd6f15db054ee3c5a766b",
          "user": {
            "_id": "631d760344503b7227837242",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631d760344503b7227837242/3b6JRusFX6GKJpsN9ZdeJ.png",
            "isPro": false,
            "fullname": "Max Ku",
            "user": "vinesmsuic",
            "type": "user"
          },
          "name": "Max Ku",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:55.238Z",
          "hidden": false
        },
        {
          "_id": "67bfd6f15db054ee3c5a766c",
          "user": {
            "_id": "6365d5baa7a1324ccd5ecdb9",
            "avatarUrl": "/avatars/636d3f410b878e451a878a6cf171dd53.svg",
            "isPro": false,
            "fullname": "Thomas Chong",
            "user": "chongcht",
            "type": "user"
          },
          "name": "Thomas Chong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:49.567Z",
          "hidden": false
        },
        {
          "_id": "67bfd6f15db054ee3c5a766d",
          "name": "Jonathan Leung",
          "hidden": false
        },
        {
          "_id": "67bfd6f15db054ee3c5a766e",
          "user": {
            "_id": "67bfdfdbf856fd8ddbb7e0f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rIR0QnVM3wxMCulG2R9SJ.png",
            "isPro": false,
            "fullname": "Krish Shah",
            "user": "KrishKrosh",
            "type": "user"
          },
          "name": "Krish Shah",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:47.269Z",
          "hidden": false
        },
        {
          "_id": "67bfd6f15db054ee3c5a766f",
          "user": {
            "_id": "6696061aa8dbb9a9997dfff6",
            "avatarUrl": "/avatars/d8f0bbff362fd630e6e60aab141076d3.svg",
            "isPro": false,
            "fullname": "Alvin Yu",
            "user": "AlvinYuVotee",
            "type": "user"
          },
          "name": "Alvin Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:52.146Z",
          "hidden": false
        },
        {
          "_id": "67bfd6f15db054ee3c5a7670",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T18:50:09.000Z",
      "title": "TheoremExplainAgent: 向LLM定理理解的多模态解释への向けて",
      "summary": "特定領域の定理の理解は、文脈ベースの理由によるだけではなく、構造化された可視化説明を通じた効果的なコミュニケーションが深い理解に必要となる。大規模な言語モデル（LLMs）は文脈ベースの定理の理由に強い性能を示すが、協調的で教育的に意味のある可視化説明を生成する能力は開放的な課題である。本研究では、Manimアニメーションを用いて長時間の定理の説明ビデオ（5分以上）を生成するためのTheoremExplainAgentを導入した。多タイプ評価のために、240個の定理を含む多様なSTEM分野におけるTheoremExplainBenchを提案し、5つの自動評価指標を採用した。結果から、効果的なビデオの生成にはアグエントの計画が重要であることが明らかになり、o3-miniアグエントは成功率93.8%と総合スコア0.77を達成した。しかし、定量的および質的な研究によると、生成された多数のビデオは可視化要素の並びに少しの問題があることがわかった。また、多タイプ説明は文脈ベースの説明により見逃される深い理由の欠陥を明らかにし、多タイプ説明の重要性を強調した。",
      "upvotes": 17,
      "discussionId": "67bfd6f25db054ee3c5a7699"
    },
    "publishedAt": "2025-02-26T22:07:49.438Z",
    "title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19400.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6232
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.19361",
      "authors": [
        {
          "_id": "67bfe435ca6e3c22b6e29442",
          "name": "Yancheng He",
          "hidden": false
        },
        {
          "_id": "67bfe435ca6e3c22b6e29443",
          "name": "Shilong Li",
          "hidden": false
        },
        {
          "_id": "67bfe435ca6e3c22b6e29444",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "67bfe435ca6e3c22b6e29445",
          "name": "Weixun Wang",
          "hidden": false
        },
        {
          "_id": "67bfe435ca6e3c22b6e29446",
          "name": "Xingyuan Bu",
          "hidden": false
        },
        {
          "_id": "67bfe435ca6e3c22b6e29447",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:13:58.959Z",
          "hidden": false
        },
        {
          "_id": "67bfe435ca6e3c22b6e29448",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "67bfe435ca6e3c22b6e29449",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "67bfe435ca6e3c22b6e2944a",
          "name": "Wenbo Su",
          "hidden": false
        },
        {
          "_id": "67bfe435ca6e3c22b6e2944b",
          "name": "Bo Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T17:59:27.000Z",
      "title": "大語言モデルは長いコンテキストの思考連鎖での誤りを検出できるか？",
      "summary": "最近、o1-likeモデルが注目を集めています。これらのモデルは、長いChain-of-Thought（CoT）の理由を生成し、現在のLarge Language Models（LLMs）の理由能力を向上させています。本研究では、これらの長いCoTの質を理解し、現在のLLMsがこれらの長いCoTに対する批判能力を測定するために、DeltaBenchを介しています。DeltaBenchは、異なるo1-likeモデル（例：QwQ、DeepSeek-R1）から生成された長いCoTを含み、数学、コード、一般的な理由などの異なる理由任務に対して使用されています。これを通じて、長いCoTの理由における誤り検出能力を測定することができます。DeltaBenchに基づき、まず、生成された長いCoTに対して細かい分析を行い、異なるo1-likeモデルの効果性と効率性を発見します。次に、現在のプロセス報酬モデル（PRMs）と批判モデルに対して拡張的な評価を行い、各注釈されたプロセスの誤りを検出することを目的として、現在のPRMsと批判モデルの境界と制限を調査します。最終的に、DeltaBenchは開発者によってモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの長いCoTの理由能力をより理解することを望むとともに、そのモデルの",
      "upvotes": 12,
      "discussionId": "67bfe438ca6e3c22b6e2948e"
    },
    "publishedAt": "2025-02-26T23:04:47.406Z",
    "title": "Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19361.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65377c30e48353201e6fdda0",
      "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
      "fullname": "Jiaheng Liu",
      "name": "CheeryLJH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.19328",
      "authors": [
        {
          "_id": "67bfcb774d22a9379b29334c",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "67bfcb774d22a9379b29334d",
          "name": "Yunjia Qi",
          "hidden": false
        },
        {
          "_id": "67bfcb774d22a9379b29334e",
          "name": "Xiaozhi Wang",
          "hidden": false
        },
        {
          "_id": "67bfcb774d22a9379b29334f",
          "name": "Zijun Yao",
          "hidden": false
        },
        {
          "_id": "67bfcb774d22a9379b293350",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "67bfcb774d22a9379b293351",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "67bfcb774d22a9379b293352",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T17:19:12.000Z",
      "title": "Agentic Reward Modeling: 人間の好みと可証明で正しい信号を統合した信頼性のある報酬システム",
      "summary": "報酬モデル（RMs）は、大規模な言語モデル（LLMs）の訓練と推論時のスケーリングに重要な役割を果たします。しかし、現在の報酬モデルは主に人間の好みに焦点を当て、確認可能な正確性信号についての注目を欠くことで、LLMsの訓練に強力な潜力を示していることを過ごしています。本論文では、確認可能な正確性信号を組み合わせた報酬モデルとしてのアガント報酬モデリングを提案します。この報酬システムは、確認可能な正確性信号と報酬モデルを組み合わせて、信頼性のある報酬を提供することを目的としています。実験的には、人間の好みの報酬と事実性と指示従いの2つの確認可能な信号を組み合わせた報酬アガント「RewardAgent」を実装し、信頼性のある報酬を提供することを目指しています。現在の報酬モデルベンチマークと実世界的な下流タスクの推論時のbest-of-nサーチにおいて、実験を実施し、RewardAgentは現在の報酬モデルよりも显著に優れています。さらに、RewardAgentを用いて訓練好みのペアを構築し、DPOの目標を持ってLLMを訓練することで、傳統的な報酬モデルと比較して様々なNLPベンチマークで上位の性能を達成しました。我々のコードは公開しており、進める研究のためのフレックスを提供しています（https://github.com/THU-KEG/Agentic-Reward-Modeling）。",
      "upvotes": 11,
      "discussionId": "67bfcb784d22a9379b29338f"
    },
    "publishedAt": "2025-02-26T22:05:16.150Z",
    "title": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19328.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625a5446f1063e7085d5178a",
      "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
      "fullname": "Hao Peng",
      "name": "Wesleythu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.17955",
      "authors": [
        {
          "_id": "67bff526ca6e3c22b6e89d71",
          "user": {
            "_id": "65d2f1e0fe21569868393411",
            "avatarUrl": "/avatars/1401020e76d958bef3f33e7449773694.svg",
            "isPro": false,
            "fullname": "Tushar Aggarwal",
            "user": "AggarwalTushar",
            "type": "user"
          },
          "name": "Tushar Aggarwal",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-27T05:16:24.257Z",
          "hidden": false
        },
        {
          "_id": "67bff526ca6e3c22b6e89d72",
          "name": "Kumar Tanmay",
          "hidden": false
        },
        {
          "_id": "67bff526ca6e3c22b6e89d73",
          "user": {
            "_id": "61a7cbb0fcbbebe775bf17fd",
            "avatarUrl": "/avatars/8b54907c6a1ea90a1242f26e03e117af.svg",
            "isPro": false,
            "fullname": "Ayush Agrawal",
            "user": "ayush1801",
            "type": "user"
          },
          "name": "Ayush Agrawal",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:13:56.625Z",
          "hidden": false
        },
        {
          "_id": "67bff526ca6e3c22b6e89d74",
          "name": "Kumar Ayush",
          "hidden": false
        },
        {
          "_id": "67bff526ca6e3c22b6e89d75",
          "name": "Hamid Palangi",
          "hidden": false
        },
        {
          "_id": "67bff526ca6e3c22b6e89d76",
          "name": "Paul Pu Liang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T08:27:18.000Z",
      "title": "言語モデルの事実性は、質問の言語に依存します。",
      "summary": "多言語言モデル（LMs）は、複数の言語で事実的知識を一貫して記憶することを期待していますが、それらは、それぞれの言語で正しい情報を持っている場合にも、知識の移行が失敗することがあります。例えば、アラビア語での問い合わせでは、Rashed Al Shashai がサードイアを出身とすることを正確に認識することができますが、英語やスワハリ語で問い合わせられると、これらの知識を正確に認識することが難しくなります。この制限をシステマチックに調査するために、13種類の言語にわたる10,000件の国際関係の事実に基づいたベンチマークを提案し、事実的記憶スコア、知識の移行性スコア、コースライングラウンド事実的知識の移行性スコアを提案しました。これらのスコアは、LMsが異なる言語での事実的記憶と知識の移行性を定量化するために使用されます。我々の結果は、現在の最先端のLMsの基本的な弱点を明らかにし、特に、異なる言語での知識の移行性が低いことを示し、これにより、言語によって強調される性能の不確定性を示します。我々の発見は、LMsが言語特有の事実的信頼性を認識し、異なる言語で最も信頼される情報を活用する必要性を強調します。我々のベンチマークと評価フレームワークを公開し、将来の多言語知識の移行に関する研究を推進することを目指しています。",
      "upvotes": 9,
      "discussionId": "67bff528ca6e3c22b6e89ddd"
    },
    "publishedAt": "2025-02-27T00:17:58.262Z",
    "title": "Language Models' Factuality Depends on the Language of Inquiry",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d2f1e0fe21569868393411",
      "avatarUrl": "/avatars/1401020e76d958bef3f33e7449773694.svg",
      "fullname": "Tushar Aggarwal",
      "name": "AggarwalTushar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.18864",
      "authors": [
        {
          "_id": "67bfd957c2a9b64ab3f97aa7",
          "name": "Juraj Gottweis",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97aa8",
          "name": "Wei-Hung Weng",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97aa9",
          "name": "Alexander Daryin",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97aaa",
          "name": "Tao Tu",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97aab",
          "name": "Anil Palepu",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97aac",
          "name": "Petar Sirkovic",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97aad",
          "name": "Artiom Myaskovsky",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97aae",
          "name": "Felix Weissenberger",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97aaf",
          "name": "Keran Rong",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ab0",
          "name": "Ryutaro Tanno",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ab1",
          "name": "Khaled Saab",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ab2",
          "name": "Dan Popovici",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ab3",
          "name": "Jacob Blum",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ab4",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ab5",
          "name": "Katherine Chou",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ab6",
          "name": "Avinatan Hassidim",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ab7",
          "name": "Burak Gokturk",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ab8",
          "name": "Amin Vahdat",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ab9",
          "name": "Pushmeet Kohli",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97aba",
          "name": "Yossi Matias",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97abb",
          "name": "Andrew Carroll",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97abc",
          "name": "Kavita Kulkarni",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97abd",
          "name": "Nenad Tomasev",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97abe",
          "name": "Yuan Guan",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97abf",
          "name": "Vikram Dhillon",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ac0",
          "name": "Eeshit Dhaval Vaishnav",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ac1",
          "name": "Byron Lee",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ac2",
          "name": "Tiago R D Costa",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ac3",
          "name": "José R Penadés",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ac4",
          "name": "Gary Peltz",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ac5",
          "name": "Yunhan Xu",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ac6",
          "name": "Annalisa Pawlosky",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ac7",
          "name": "Alan Karthikesalingam",
          "hidden": false
        },
        {
          "_id": "67bfd957c2a9b64ab3f97ac8",
          "name": "Vivek Natarajan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T06:17:13.000Z",
      "title": "AIとの協力を目指して",
      "summary": "科学発見は、科学者が新しい仮説を生成し、厳密な実験的な検証を受けることに依存します。このプロセスを支援するために、私たちは、ジェミニ2.0に基づいた多エージェントシステムで構築されたAIコサイエンティストを紹介します。AIコサイエンティストは、新しい、元のものの知識を発見し、先行の証拠に基づいて科学者が提供した研究目的とガイドに合わせた示実的に新しい研究仮説と提案を形成することを目指しています。システムの設計は、科学方法によるインスピレーションと、テスト時の計算量のスケーリングによる加速をもたらし、仮説の生成に対して生成、議論、進化のアプローチを採用しています。主な貢献点は、(1) 対象の仮説の生成に対して効果的な計算量のスケーリングを可能にする非同期タスク実行フレームワークを構築した多エージェントアーキテクチャ、(2) 自我改善の仮説の生成を図るトーナメント進化プロセスです。自動評価は、テスト時の計算量のベースの継続的な利益を示し、仮説の質を向上させています。一般的な用途であるが、薬品再利用、新たなターゲットの発見、バクテリアの進化と抗微生物抵抗性の機構の解明の3つの生物医学領域での開発と検証を焦点としています。薬品再利用においては、臨床的に有効な濃度でバリアントの発生を示すような候補を提案し、急性髓性白血病の候補について特に体外での腫瘍抑制を示しています。新たなターゲットの発見においては、肝臓発疡の新しい表達遗传的ターゲットをAIコサイエンティストが提案し、抗発瘍活性と人間の肝臓オーガノイドでの肝細胞の再生を検証しています。最後に、AIコサイエンティストは、バクテリアの進化の新しい遺伝子移動機構の並行なシネマルディスカバリーで未発表の実験結果を再現しました。これらの結果は、別々の同期ポストで詳細に記載されているが、これらは、生物医学と科学発見を支援することができ、AIを支えた科学者の新しい時代を迎えることができることを示しています。",
      "upvotes": 9,
      "discussionId": "67bfd958c2a9b64ab3f97afa"
    },
    "publishedAt": "2025-02-26T22:18:06.494Z",
    "title": "Towards an AI co-scientist",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6232
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.19414",
      "authors": [
        {
          "_id": "67c01587925b73feaf61ac41",
          "name": "Shiven Sinha",
          "hidden": false
        },
        {
          "_id": "67c01587925b73feaf61ac42",
          "name": "Shashwat Goel",
          "hidden": false
        },
        {
          "_id": "67c01587925b73feaf61ac43",
          "name": "Ponnurangam Kumaraguru",
          "hidden": false
        },
        {
          "_id": "67c01587925b73feaf61ac44",
          "name": "Jonas Geiping",
          "hidden": false
        },
        {
          "_id": "67c01587925b73feaf61ac45",
          "name": "Matthias Bethge",
          "hidden": false
        },
        {
          "_id": "67c01587925b73feaf61ac46",
          "name": "Ameya Prabhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T18:58:13.000Z",
      "title": "言語モデルが偽造できるか？ 反例作成によるアルゴリズム的推理評価",
      "summary": "言語モデル（LMs）で科学の発見を加速する可能性について、興味が増えています。仮説の否定は科学の進歩の鍵で、これにより主張が時間の経過によって反復的に改良できます。このプロセスは、研究者の大きな努力、理由、そして独创性を必要とします。しかし、現在のLMsの評価基準は主に解決策の生成能力を評価し、その逆の能力を評価するものは少ないです。私たちは、逆の能力を評価する基準を開発することを主張します。具体的には、アルゴリズム問題解決の領域に焦点を当て、コード実行で自動的に評価できるカウンターエグゼンスを作成することを目的とします。特に、REFUTEという動的に更新される評価基準を介して、最近の問題と不正な提出を含むプログラミングコンペティションからカウンターエグゼンスを成功に識別した人間の専門家の経験を参考にします。私たちの分析によると、REFUTEの不正な解決策に対してカウンターエグゼンスを作成できる最良の理由アジェント、OpenAI o3-mini（高）によりも、コード実行のフィードバックを受けても、<9%の不正な解決策に対してカウンターエグゼンスを作成できます。しかし、レーティングは、これらの問題をショートカットで解決できる能力を示していることを示しています。私たちは、LMsの不正な解決策を否定する能力の評価と向上について進展を促し、この能力は、両方でも研究の加速とモデルの信頼性のある反省的な理由による自己改善に重要であることを希望しています。",
      "upvotes": 8,
      "discussionId": "67c01588925b73feaf61ad2c"
    },
    "publishedAt": "2025-02-27T02:36:29.037Z",
    "title": "Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19414.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6506832221ac448013f94995",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg",
      "fullname": "Shashwat Goel",
      "name": "shash42",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18906",
      "authors": [
        {
          "_id": "67bfd5d2381f8fcb67e5ad36",
          "name": "Jiani Zheng",
          "hidden": false
        },
        {
          "_id": "67bfd5d2381f8fcb67e5ad37",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "67bfd5d2381f8fcb67e5ad38",
          "user": {
            "_id": "669dcf6200970c3b27aafa5d",
            "avatarUrl": "/avatars/bb9ed5ff86326fdaeb184c6b0e40f74f.svg",
            "isPro": false,
            "fullname": "kaikai yang",
            "user": "keanudicap",
            "type": "user"
          },
          "name": "Fangkai Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:57.452Z",
          "hidden": false
        },
        {
          "_id": "67bfd5d2381f8fcb67e5ad39",
          "user": {
            "_id": "654dbac9938fbf1e696be8aa",
            "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
            "isPro": false,
            "fullname": "Chaoyun Zhang",
            "user": "vyokky",
            "type": "user"
          },
          "name": "Chaoyun Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:59.653Z",
          "hidden": false
        },
        {
          "_id": "67bfd5d2381f8fcb67e5ad3a",
          "name": "Lingrui Mei",
          "hidden": false
        },
        {
          "_id": "67bfd5d2381f8fcb67e5ad3b",
          "name": "Wenjie Yin",
          "hidden": false
        },
        {
          "_id": "67bfd5d2381f8fcb67e5ad3c",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "67bfd5d2381f8fcb67e5ad3d",
          "name": "Dongmei Zhang",
          "hidden": false
        },
        {
          "_id": "67bfd5d2381f8fcb67e5ad3e",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "67bfd5d2381f8fcb67e5ad3f",
          "name": "Qi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T07:52:02.000Z",
      "title": "VEM: 環境無しの訓練によるGUIアガントの探索と価値の環境モデル",
      "summary": "GUIのための視覚言語モデル（VLMs）を強化学習（RL）によって訓練する際には重要な課題がある。環境に基づくRLは高額な相互作用が必要であり、環境無しの方法は分布変換と報酬の一般化に苦労する。私たちは、予った値環境モデル（VEM）を利用して価値評価と政策最適化を分離する環境無しのRLフレームワークを提案します。VEMは、オフラインデータから直接状態アクション価値を予測し、GUIの交互作用の結果に関する人間のような先入れを結構化します。これは次状態予測や環境のフィードバックが必要とならないことで誤差を重ね合わせることを避け、UIの変更に対する強固性を高めます。フレームワークは2段階で動作します：（1）VEMの予っちまして、長期的アクションの価値を評価する、（2）フローズンドVEMのシグナルを用いて政策の検索をガイドする、レイアウト無関係のGUI自動化を可能にします。Android-in-the-Wildベンチマークで評価した結果、VEMはオフラインおよびオンライン設定で最先端の性能を達成し、環境無しのベースラインを大幅に超え、環境に基づくアプローチと同じレベルの性能を達成します。重要なことに、VEMは語義認識に基づく価値評価がオンライン訓練された方法と比較的性能を達成できることを示します。",
      "upvotes": 5,
      "discussionId": "67bfd5d7381f8fcb67e5ae3d"
    },
    "publishedAt": "2025-02-26T22:02:50.690Z",
    "title": "VEM: Environment-Free Exploration for Training GUI Agent with Value Environment Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18906.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.18772",
      "authors": [
        {
          "_id": "67bfc297ca6e3c22b6d99c78",
          "name": "Xueqing Peng",
          "hidden": false
        },
        {
          "_id": "67bfc297ca6e3c22b6d99c79",
          "name": "Triantafillos Papadopoulos",
          "hidden": false
        },
        {
          "_id": "67bfc297ca6e3c22b6d99c7a",
          "name": "Efstathia Soufleri",
          "hidden": false
        },
        {
          "_id": "67bfc297ca6e3c22b6d99c7b",
          "name": "Polydoros Giannouris",
          "hidden": false
        },
        {
          "_id": "67bfc297ca6e3c22b6d99c7c",
          "name": "Ruoyu Xiang",
          "hidden": false
        },
        {
          "_id": "67bfc297ca6e3c22b6d99c7d",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "67bfc297ca6e3c22b6d99c7e",
          "name": "Lingfei Qian",
          "hidden": false
        },
        {
          "_id": "67bfc297ca6e3c22b6d99c7f",
          "user": {
            "_id": "63b58ed5889aa6707f0bb0f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/znl74_aMswlV8VtHrfj3G.jpeg",
            "isPro": true,
            "fullname": "Jimin Huang",
            "user": "jiminHuang",
            "type": "user"
          },
          "name": "Jimin Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-27T01:40:40.189Z",
          "hidden": false
        },
        {
          "_id": "67bfc297ca6e3c22b6d99c80",
          "name": "Qianqian Xie",
          "hidden": false
        },
        {
          "_id": "67bfc297ca6e3c22b6d99c81",
          "name": "Sophia Ananiadou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T03:04:01.000Z",
      "title": "プルトス：低資源ギリシャ金融での大規模言語モデルのベンチマーク",
      "summary": "ギリシャの世界経済における重要な役割にもかかわらず、ギリシャの財政コンテキストにおける大規模言語モデル（LLMs）は、ギリシャ語の言語複雑性とドメイン専門的データの不足により、調査が進まない。これまでの多言語財務自然言語処理（NLP）の努力では、相当な性能の差異が明らかにされたが、これまでにギリシャ財務用の専門的ベンチマークやギリシャ語言特有のLLMsは開発されていなかった。この隙を埋めるために、私たちはPlutus-benという最初のギリシャ財務評価ベンチマークとPlutus-8Bという先駆的なギリシャ財務用LLMを紹介します。Plutus-8Bは、ギリシャ語言特有のデータで微調節されています。Plutus-benは、ギリシャ語での5つの核心的な金融NLPタスクを対処し、LLMのシステマティックで再現可能な評価を促進します。これらのタスクに基づいて、私たちは3つの新しい、高品質のギリシャ財務データセットを提供し、それらはネイティブギリシャ語者の専門家によって厳密に注釈され、2つの既存のリソースを追加しています。22つのLLMのコンプリーティブな評価により、ギリシャ財務NLPは、言語複雑性、ドメイン専門的な用語、財務計算の欠陥により難しいことが明らかになりました。これらの発見は、言語間のトランスフェーサーの限界、ギリシャ語言を学ぶモデルにおける財務知識の必要性、および財務LLMをギリシャ語言に適用する難しさを強調します。私たちは、Plutus-ben、Plutus-8B、そしてすべての関連データセットを公開し、再現可能な研究を推進し、ギリシャ財務NLPの進歩を促進し、金融分野での広範囲の多言語インクルジョンを促進します。",
      "upvotes": 4,
      "discussionId": "67bfc298ca6e3c22b6d99caa"
    },
    "publishedAt": "2025-02-27T00:08:09.082Z",
    "title": "Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18772.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b58ed5889aa6707f0bb0f4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/znl74_aMswlV8VtHrfj3G.jpeg",
      "fullname": "Jimin Huang",
      "name": "jiminHuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.16776",
      "authors": [
        {
          "_id": "67bfd8d546083445aacb4605",
          "name": "Zhexin Zhang",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb4606",
          "name": "Leqi Lei",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb4607",
          "name": "Junxiao Yang",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb4608",
          "name": "Xijie Huang",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb4609",
          "name": "Yida Lu",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb460a",
          "name": "Shiyao Cui",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb460b",
          "name": "Renmiao Chen",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb460c",
          "name": "Qinglin Zhang",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb460d",
          "name": "Xinyuan Wang",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb460e",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb460f",
          "user": {
            "_id": "653f1ef4aabbf15fc76a259c",
            "avatarUrl": "/avatars/94e569999d913e961266394ea2875965.svg",
            "isPro": false,
            "fullname": "LLLeo Li",
            "user": "LLLeo612",
            "type": "user"
          },
          "name": "Hao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:45.366Z",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb4610",
          "name": "Xianqi Lei",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb4611",
          "name": "Chengwei Pan",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb4612",
          "name": "Lei Sha",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb4613",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "67bfd8d546083445aacb4614",
          "name": "Minlie Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T02:11:52.000Z",
      "title": "AISafetyLab: AIセキュリティ評価と向上の一括フレームワーク",
      "summary": "AIモデルが多様なリアルウェアスケーラーで日々拡大している中、その安全性の確保は重要な難題であり、まだ十分に研究されていない。AIセキュリティの評価と向上にための大きな努力があるが、標準化されたフレームワークと完全なツールキットの欠如は、システム的研究と実用的な採用に重大な障壁をつけている。この隙を埋めるために、AISafetyLabという統一されたフレームワークとツールキットを紹介します。AISafetyLabは、代表的な攻撃、防御、評価手法を統合したもので、開発者が直感的なインターフェースを用いて様々な技術を無難に適用できるように設計されています。また、Vicunaに対して実験的な研究を行い、攻撃と防御の戦略を分析し、比較的効果性についての有價値なコンプライアンスを提供します。AIセキュリティの研究と開発の進行を促進するために、AISafetyLabは公開的に利用可能です（https://github.com/thu-coai/AISafetyLab）し、その継続的な保守と向上に取り組みます。",
      "upvotes": 4,
      "discussionId": "67bfd8d646083445aacb464f"
    },
    "publishedAt": "2025-02-26T22:16:03.582Z",
    "title": "AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61b58aa0d65058ce70beb98c",
      "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
      "fullname": "Zhexin Zhang",
      "name": "nonstopfor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.19204",
      "authors": [
        {
          "_id": "67bfd735ca6e3c22b6de43c7",
          "name": "Xiankang He",
          "hidden": false
        },
        {
          "_id": "67bfd735ca6e3c22b6de43c8",
          "name": "Dongyan Guo",
          "hidden": false
        },
        {
          "_id": "67bfd735ca6e3c22b6de43c9",
          "name": "Hongji Li",
          "hidden": false
        },
        {
          "_id": "67bfd735ca6e3c22b6de43ca",
          "name": "Ruibo Li",
          "hidden": false
        },
        {
          "_id": "67bfd735ca6e3c22b6de43cb",
          "name": "Ying Cui",
          "hidden": false
        },
        {
          "_id": "67bfd735ca6e3c22b6de43cc",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T15:10:05.000Z",
      "title": "どんな深さでも結果を得る：ディスタイルーションは強力なモノラル深さ推定器を作成します。",
      "summary": "モノカラー深さ推定（MDE）は、1枚のRGB画像からスケーンの深さを予測する目的を持ち、3次元スケーン理解に重要な役割を果たします。最近のゼロシートMDEの進展は、正規化された深さ表現と、学習に基づく結果の収納を利用して、多様なスケーンに対する一般化能力を向上させています。しかし、現在の結果の収納に用いられる深さ正規化方法は、グローバル正規化を基にしているため、ノイズを増幅させ、結果の収納の効果を低下させます。本論文では、異なる深さ正規化戦略の影響をシステマ的に分析します。その結果に基づき、Cross-Context Distillationを提案します。これは、グローバルと局所的な深さコードを統合して、プセフォールラベルの質を向上させます。また、補間的な強みを活用した多ターガー結果の収納フレームワークを導入します。これにより、深さ予測がより強固で正確になることを実現します。標準データセット上での拡張的な実験により、私たちのアプローチは状態の達の方法に比べて、定量的および定性的に显著に優れていることが示されます。",
      "upvotes": 4,
      "discussionId": "67bfd736ca6e3c22b6de441e"
    },
    "publishedAt": "2025-02-26T22:10:20.646Z",
    "title": "Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64196320ed725fef64419c2a/k13rSuJPlDkMtzwdHXCXm.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19204.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64196320ed725fef64419c2a",
      "avatarUrl": "/avatars/96feb22fb5e8931d6c9e0ea06148266f.svg",
      "fullname": "Chi Zhang",
      "name": "DrChiZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.19279",
      "authors": [
        {
          "_id": "67bffaca3f838c1e33e074e7",
          "user": {
            "_id": "638ef0b0c67af472d31674a6",
            "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg",
            "isPro": false,
            "fullname": "Honglin Guo",
            "user": "KYLN24",
            "type": "user"
          },
          "name": "Honglin Guo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:13:52.094Z",
          "hidden": false
        },
        {
          "_id": "67bffaca3f838c1e33e074e8",
          "name": "Kai Lv",
          "hidden": false
        },
        {
          "_id": "67bffaca3f838c1e33e074e9",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "67bffaca3f838c1e33e074ea",
          "name": "Tianyi Liang",
          "hidden": false
        },
        {
          "_id": "67bffaca3f838c1e33e074eb",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "67bffaca3f838c1e33e074ec",
          "name": "Demin Song",
          "hidden": false
        },
        {
          "_id": "67bffaca3f838c1e33e074ed",
          "name": "Qiuyinzhe Zhang",
          "hidden": false
        },
        {
          "_id": "67bffaca3f838c1e33e074ee",
          "name": "Yu Sun",
          "hidden": false
        },
        {
          "_id": "67bffaca3f838c1e33e074ef",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "67bffaca3f838c1e33e074f0",
          "name": "Xipeng Qiu",
          "hidden": false
        },
        {
          "_id": "67bffaca3f838c1e33e074f1",
          "name": "Tao Gui",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T16:33:41.000Z",
      "title": "キャリックス：ヒューマンの好みからデータの品質の基準を開発する",
      "summary": "言語モデルは、最適な性能を発揮するために高品質のデータが非常に重要です。現在のアプローチは、手動で設計されたヒューリスティック、現有モデルの揺らぎ、分類器の訓練、または誘導文の調整に依存しています。これらは、大幅な専門知識や人間の記号努力が必要で、データへのバイアスを引き起こします。我々は、CritiQという新しいデータ選択方法を紹介します。CritiQは、sim30の人間記号ペアのみを使用して、データの品質に関する人間の好みから自動的に規則を採掘し、効率的なデータ選択を行います。主な構成要素であるCritiQ Flowは、管理者アガントが品質の規則を進化させ、ワーカーアガントが二つのジャンドルを判断するものです。我々は、以前の仕事から品質の規則を抽出し、CritiQ Flowを強化する知識ベースを構築します。揺らぎと分類器に基づく方法に比べ、語的な規則は解釈可能で再利用の価値があります。規則を得た後、CritiQスコアーを訓練し、品質スコアを与え、効率的なデータ選択を行います。コード、数学、ロジック領域での効果を示し、人間記号テストセット上で高い精度を達成します。選択されたデータの品質を確認するために、Llama 3.1モデルを継続的に訓練し、下流タスクに対する性能向上を観察します。Ablation studiesでは、知識ベースと反省プロセスの利益を証明します。規則の進化と多数決の効果を分析します。",
      "upvotes": 3,
      "discussionId": "67bffacc3f838c1e33e075a2"
    },
    "publishedAt": "2025-02-27T00:47:02.948Z",
    "title": "CritiQ: Mining Data Quality Criteria from Human Preferences",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638ef0b0c67af472d31674a6",
      "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg",
      "fullname": "Honglin Guo",
      "name": "KYLN24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.19413",
      "authors": [
        {
          "_id": "67c02d6aa15ac71dcf1c754e",
          "name": "Christoph Schuhmann",
          "hidden": false
        },
        {
          "_id": "67c02d6aa15ac71dcf1c754f",
          "name": "Gollam Rabby",
          "hidden": false
        },
        {
          "_id": "67c02d6aa15ac71dcf1c7550",
          "name": "Ameya Prabhu",
          "hidden": false
        },
        {
          "_id": "67c02d6aa15ac71dcf1c7551",
          "name": "Tawsif Ahmed",
          "hidden": false
        },
        {
          "_id": "67c02d6aa15ac71dcf1c7552",
          "name": "Andreas Hochlehnert",
          "hidden": false
        },
        {
          "_id": "67c02d6aa15ac71dcf1c7553",
          "name": "Huu Nguyen",
          "hidden": false
        },
        {
          "_id": "67c02d6aa15ac71dcf1c7554",
          "name": "Nick Akinci Heidrich",
          "hidden": false
        },
        {
          "_id": "67c02d6aa15ac71dcf1c7555",
          "name": "Ludwig Schmidt",
          "hidden": false
        },
        {
          "_id": "67c02d6aa15ac71dcf1c7556",
          "name": "Robert Kaczmarczyk",
          "hidden": false
        },
        {
          "_id": "67c02d6aa15ac71dcf1c7557",
          "name": "Sören Auer",
          "hidden": false
        },
        {
          "_id": "67c02d6aa15ac71dcf1c7558",
          "name": "Jenia Jitsev",
          "hidden": false
        },
        {
          "_id": "67c02d6aa15ac71dcf1c7559",
          "name": "Matthias Bethge",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T18:56:52.000Z",
      "title": "アレクシデュリアプロジェクト：LLMsをよって科学知識から著作権の負担から解放するために",
      "summary": "ペイウォール、リセントと著作権規則は、科学知識の広範囲の普及と再利用を制限します。私たちは、学術文書に含まれる科学知識を抽出することが法則的におおよそ技術的に可能であるという立場を採用しています。現在の方法、例えばテキスト埋め込みは、事実的な内容を信頼的に保存することができません。簡単な再語化は法則的に正当であるかもしれません。私たちは、コミュニティに新しいアイデアを採用することを促します：学術文書をLLMを使用してKnowledge Unitsに変換する。これらのユニットは、スタイリスティックな内容を含まないための構造化されたデータを使用して、エンティティ、属性と関係を捉えます。私たちは、Knowledge Unitsが以下のことを示すことを証明しています： (1) 著作権の研究文書からの知識の共有に基づく法則的に立得住腳なフレームワークを形成することができることを、ドイツの著作権法と米国の「Fair Use」理論の法則的分析に基づいて示しています。 (2) 原稿のほとんどのこと（約95%）の事実的な知識を保存することができ、原稿の著作権の文書からの事実をMCQで測定した結果を示しています。科学知識は、言語モデルが著作権の文書から重要な事実を再利用することを許可することで、科学研究と教育に変革的な利益を収得することができることを予想します。これを支援するため、研究文書をKnowledge Unitsに変換するためのオープンソースツールを共有しています。まとめて、我々の仕事は、著作権を尊重することとともに科学知識の民主的なアクセスの可能性を主張しています。",
      "upvotes": 2,
      "discussionId": "67c02d6ba15ac71dcf1c7596"
    },
    "publishedAt": "2025-02-27T04:18:26.724Z",
    "title": "Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19413.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6464a0d41683d3c81f51924a",
      "avatarUrl": "/avatars/bfa89f568302fa34a641e0d8744bf8b5.svg",
      "fullname": "Ameya Prabhu",
      "name": "AmeyaPrabhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18417",
      "authors": [
        {
          "_id": "67c02b2eb14cf3cbc800c292",
          "name": "Alexander Groshev",
          "hidden": false
        },
        {
          "_id": "67c02b2eb14cf3cbc800c293",
          "user": {
            "_id": "67aafccd7517c92ba71142f2",
            "avatarUrl": "/avatars/ef4b5c6867250b8b7af2c995dd7ad740.svg",
            "isPro": false,
            "fullname": "Anastasiia Iashchenko",
            "user": "nastasia-y",
            "type": "user"
          },
          "name": "Anastasiia Iashchenko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:13:49.896Z",
          "hidden": false
        },
        {
          "_id": "67c02b2eb14cf3cbc800c294",
          "name": "Pavel Paramonov",
          "hidden": false
        },
        {
          "_id": "67c02b2eb14cf3cbc800c295",
          "name": "Denis Dimitrov",
          "hidden": false
        },
        {
          "_id": "67c02b2eb14cf3cbc800c296",
          "name": "Andrey Kuznetsov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-25T18:13:55.000Z",
      "title": "GHOST 2.0: ヘッドの生成高品質1回転送",
      "summary": "最近、顔交換の課題が研究コミュニティに注目された間、頭交換の問題は大きく調査されていません。顔色の移行を除き、頭交換は合成中に全頭の構造情報を保存し、交換した頭と背景の間の欠けを補完する必要があるという特別な課題があります。本論文では、GHOST 2.0という2つの問題に特化したモジュールを用いてこれらの問題に対処します。まず、頭再演出に適したアライナモデルを強化し、多尺度でアイデンティティ情報を保存し、極端な姿勢の変化に対して強健であるようにします。次に、ブレンダーモジュールを使用し、顔色の移行と間違った領域の補完を行い、目標の背景に順転させることができます。両モジュールは対応するタスクで基準に比べてより高い性能を示し、頭交換で最先端の結果を達成することができます。また、源と目標の髪型の大きな違いなどの複雑なケースも解決します。コードは、https://github.com/ai-forever/ghost-2.0 にアクセスできます。",
      "upvotes": 2,
      "discussionId": "67c02b31b14cf3cbc800c34b"
    },
    "publishedAt": "2025-02-27T04:15:43.126Z",
    "title": "GHOST 2.0: generative high-fidelity one shot transfer of heads",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18417.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67aafccd7517c92ba71142f2",
      "avatarUrl": "/avatars/ef4b5c6867250b8b7af2c995dd7ad740.svg",
      "fullname": "Anastasiia Iashchenko",
      "name": "nastasia-y",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.19187",
      "authors": [
        {
          "_id": "67c01747e8c7d56a8e0cbdc3",
          "name": "Mehran Kazemi",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdc4",
          "user": {
            "_id": "654e97ef5da3196a78409341",
            "avatarUrl": "/avatars/1a5ea7351ca21960891cf9721b9f4667.svg",
            "isPro": false,
            "fullname": "Bahare Fatemi",
            "user": "baharefatemi",
            "type": "user"
          },
          "name": "Bahare Fatemi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-27T07:42:00.525Z",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdc5",
          "name": "Hritik Bansal",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdc6",
          "name": "John Palowitch",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdc7",
          "name": "Chrysovalantis Anastasiou",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdc8",
          "name": "Sanket Vaibhav Mehta",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdc9",
          "name": "Lalit K. Jain",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdca",
          "name": "Virginia Aglietti",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdcb",
          "name": "Disha Jindal",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdcc",
          "name": "Peter Chen",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdcd",
          "name": "Nishanth Dikkala",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdce",
          "name": "Gladys Tyen",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdcf",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdd0",
          "name": "Uri Shalit",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdd1",
          "name": "Silvia Chiappa",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdd2",
          "name": "Kate Olszewska",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdd3",
          "name": "Yi Tay",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdd4",
          "name": "Vinh Q. Tran",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdd5",
          "name": "Quoc V. Le",
          "hidden": false
        },
        {
          "_id": "67c01747e8c7d56a8e0cbdd6",
          "name": "Orhan Firat",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T14:50:50.000Z",
      "title": "BIG-Bench エクストラ ハード",
      "summary": "大語言モデル（LLMs）は日常的なアプリケーションで日々増えており、強固な一般的な理由論の能力と多様な理由論スキルセットを求めています。しかし、現在のLLMの理由論ベンチマークは主に数学的およびコーディング能力に焦点を当てているため、広範囲の理由論の能力を評価する隙間があります。特に例外として、BIG-Benchデータセットがあり、それが多様な難しいタスクのセットを持っていて、一つの統一的なフレームワークで広範囲の理由論を評価することができることにより、LLMの一般的な理由論能力を評価する重要なベンチマークとして役立ちました。しかし、LLMの最近の進展はBIG-Benchにおいてサタージュ（サタージュ）として、その難しいバージョンBIG-Bench Hard（BBH）においてもその効用が低下しました。最先端のモデルはBBHの多くのタスクで近準絶対的なスコアを達成し、その効用を低下させています。この制限を解決するために、BIG-Bench Extra Hard（BBEH）という新たなベンチマークを導入し、LLMの理由論評価の境界を突き抜けることを目指しています。BBEHは、BBHの各タスクを新しいタスクに置き換え、類似な理由論能力を調べるが、大幅に難しくなったものに置き換えています。BBEHでは、各種のモデルを評価し、最良の一般的なモデルの平均正確率は9.8％、最良の理由論専門モデルの平均正確率は44.8％であり、大幅な向上の余地があり、LLMで強固な一般的な理由論を達成する課題の続きを明らかにしています。BBEHは公開的にリリースされています：https://github.com/google-deepmind/bbeh。",
      "upvotes": 2,
      "discussionId": "67c01748e8c7d56a8e0cbe0b"
    },
    "publishedAt": "2025-02-27T02:43:05.341Z",
    "title": "BIG-Bench Extra Hard",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.19187.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 773
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.16284",
      "authors": [
        {
          "_id": "67bfdbd0302c06f220658e9d",
          "user": {
            "_id": "64e84ec6d41a68b065bf78a7",
            "avatarUrl": "/avatars/bae3c5e3210b40af6e4f113e85f3e206.svg",
            "isPro": false,
            "fullname": "Liang Wang",
            "user": "AzureLeon1",
            "type": "user"
          },
          "name": "Liang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:14:42.802Z",
          "hidden": false
        },
        {
          "_id": "67bfdbd0302c06f220658e9e",
          "name": "Shaozhen Liu",
          "hidden": false
        },
        {
          "_id": "67bfdbd0302c06f220658e9f",
          "name": "Yu Rong",
          "hidden": false
        },
        {
          "_id": "67bfdbd0302c06f220658ea0",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "67bfdbd0302c06f220658ea1",
          "name": "Qiang Liu",
          "hidden": false
        },
        {
          "_id": "67bfdbd0302c06f220658ea2",
          "name": "Shu Wu",
          "hidden": false
        },
        {
          "_id": "67bfdbd0302c06f220658ea3",
          "name": "Liang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-22T16:34:32.000Z",
      "title": "MolSpectra: 3次元分子表現の予習学習をエネルギースペクトルの多様性によって実現する",
      "summary": "3D構造と分子システムのエネルギー状態の関係を確立することは、3D分子の表現を学習するための有望なアプローチであることが証明されました。しかし、現在の方法は古典力学からの分子のエネルギー状態のモデリングに限られています。この制限は、量子力学の効果、例えば量子化（離散）のエネルギーレベル構造など、分子のエネルギーの詳細な評価を提供し、エネルギースペクトルを通じて実験的に測定できるようなものを過眼してしまいます。この論文では、エネルギースペクトルを利用して3D分子の表現の事前訓練を強化する方法（MolSpectra）を提案し、分子表現に量子力学の知識をフィルマーに入れることを目的とします。特に、マスク付きパッチ再構成を通じて分子スペクトルをエンコードするための多スペクトルエンコーダーであるSpecFormerを提案します。3Dエンコーダーの分子の理解を高めるために、3Dエンコーダーとスペクトルエンコーダーの出力を対比的なオブジェクティブを用いて調整します。公開ベンチマーク上での評価により、私たちの事前訓練された表現は、分子の性質の予測とダイナミクスのモデリングにおいて現在の方法を超えていることが明らかになりました。",
      "upvotes": 2,
      "discussionId": "67bfdbd1302c06f220658ece"
    },
    "publishedAt": "2025-02-26T22:29:40.056Z",
    "title": "MolSpectra: Pre-training 3D Molecular Representation with Multi-modal Energy Spectra",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.16284.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e84ec6d41a68b065bf78a7",
      "avatarUrl": "/avatars/bae3c5e3210b40af6e4f113e85f3e206.svg",
      "fullname": "Liang Wang",
      "name": "AzureLeon1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.17540",
      "authors": [
        {
          "_id": "67bff9608d761fc6a75e24ad",
          "user": {
            "_id": "657ccbf2869d5bb0e53b482f",
            "avatarUrl": "/avatars/2eae5a10bdc14814a04d9f255f16de6b.svg",
            "isPro": false,
            "fullname": "Rohit Saxena",
            "user": "rohitsaxena",
            "type": "user"
          },
          "name": "Rohit Saxena",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-27T09:13:54.284Z",
          "hidden": false
        },
        {
          "_id": "67bff9608d761fc6a75e24ae",
          "name": "Pasquale Minervini",
          "hidden": false
        },
        {
          "_id": "67bff9608d761fc6a75e24af",
          "name": "Frank Keller",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-24T18:35:39.000Z",
      "title": "ポスターサマリーバンチャーム：科学ポスターの多モーダルベンチャーム",
      "summary": "マルチモーダルドキュメントから正確かつ簡潔な文字要約を生成することは難しい、特に科学ポスターのような視覚的に複雑な内容を対処する場合は。ポスターサマリーを用いて、科学ポスターを研究論文の要約にして理解し、要約することができるビジョン・言語モデルの開発に貢献するために、ポスターサマリーを用いて新しいベンチマークを紹介します。データセットには16,305件の会議ポスターが含まれ、それぞれのポスターは画像フォーマットで提供され、複雑なラウター、稠密なテキスト領域、テーブル、フィギュアなどの多様な視覚的理解的課題を提示します。ポスターサマリーにベンチマークし、最先端のマルチモーダル大語言モデル（MLLMs）を評価し、科学ポスターの正確な解釈と要約に難しいことを示します。Segment & Summarizeという階層的な方法を提案し、現在のMLLMsを超えることを示し、ROUGE-Lで3.14%の収益を達成します。これは将来のポスター要約研究の起点として役立つことを目指します。",
      "upvotes": 1,
      "discussionId": "67bff96d8d761fc6a75e27a0"
    },
    "publishedAt": "2025-02-27T00:37:24.965Z",
    "title": "PosterSum: A Multimodal Benchmark for Scientific Poster Summarization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.17540.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657ccbf2869d5bb0e53b482f",
      "avatarUrl": "/avatars/2eae5a10bdc14814a04d9f255f16de6b.svg",
      "fullname": "Rohit Saxena",
      "name": "rohitsaxena",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  }
]