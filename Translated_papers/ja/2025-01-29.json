[
  {
    "paper": {
      "id": "2501.17161",
      "authors": [
        {
          "_id": "6799b39b15f4661561c22968",
          "name": "Tianzhe Chu",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c22969",
          "name": "Yuexiang Zhai",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296a",
          "name": "Jihan Yang",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296b",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296c",
          "name": "Saining Xie",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296d",
          "name": "Dale Schuurmans",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296e",
          "name": "Quoc V. Le",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c2296f",
          "name": "Sergey Levine",
          "hidden": false
        },
        {
          "_id": "6799b39b15f4661561c22970",
          "name": "Yi Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T18:59:44.000Z",
      "title": "SFT記憶、RL一般化：基盤モデルの補強後の比較的な研究",
      "summary": "Supervised fine-tuning (SFT)とreinforcement learning (RL)は、基盤モデルの訓練後の技術で広く使用されていますが、それらがモデルの汎化能力を向上させる役割は明確ではありません。本論文では、SFTとRLの汎化と記憶過程の違いを調査し、文脈ベースのルール変体と視覚変体に焦点を当てています。我々は、GeneralPoints（一般的な点）という算術推理カードゲームを介し、V-IRL（実世界的なナビゲーション環境）を用いて、SFTとRLで訓練されたモデルが文脈的および視覚的な未見の変体にどのように汎化するかを評価します。我々は、RL、特に結果ベースの報酬を用いて訓練された場合、ルールベースの文脈的および視覚的な変体でも汎化することを示します。SFTと比べて、訓練データを記憶する傾向があり、分布外のシナリオで汎化することが難しいことを示します。進み端の分析では、RLがモデルの視覚認識能力を向上させ、視覚的領域での汎化を促進することを示します。RLの優れた汎化性にもかかわらず、SFTは効果的なRL訓練において重要であることを示し、SFTはモデルの出力フォーマットを安定させ、後続のRLが性能の向上を達成することを可能にします。これらの発見は、複雑な多モーダルタスクで汎化可能な知識を学習するRLの能力を示しています。",
      "upvotes": 11,
      "discussionId": "6799b39d15f4661561c229e6"
    },
    "publishedAt": "2025-01-28T23:50:56.664Z",
    "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17161.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.17116",
      "authors": [
        {
          "_id": "6799b367d30dc065a2d51592",
          "name": "Ruizhe Wang",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51593",
          "name": "Yeyun Gong",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51594",
          "name": "Xiao Liu",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51595",
          "name": "Guoshuai Zhao",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51596",
          "name": "Ziyue Yang",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51597",
          "name": "Baining Guo",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51598",
          "name": "Zhengjun Zha",
          "hidden": false
        },
        {
          "_id": "6799b367d30dc065a2d51599",
          "user": {
            "_id": "653feb7ccf1f9c88f4928910",
            "avatarUrl": "/avatars/23a6a6818116683ea9485e1470a0062f.svg",
            "isPro": false,
            "fullname": "Peng Cheng",
            "user": "cp5555",
            "type": "user"
          },
          "name": "Peng Cheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-29T04:49:44.372Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T18:04:50.000Z",
      "title": "FP4 データ型を使用して大規模言語モデルの訓練を最適化する方法",
      "summary": "LLMの学習に伴う増加する計算負担に対して、より効率的な方法が必要となります。Quantized trainingは、低ビット演算を可能にしてこれらのコストを減らすために望ましい解決策となります。FP8精度は実行の可能性を示していますが、FP4の活用は、大きな量化誤差と限定的な表現能力により難しい問題となっています。本稿では、LLMの最初のFP4学習フレームワークを介して、これらの問題を解決するために2つのキーのイノベーションを提案します：微分可能な量化評価器を用いた重み更新と活性の崩壊を防ぐ異常値のクラップと補正戦略。安定性を確保するために、フレームワークは混合精度学習シナーズとベクターワイズ量化を組み合わせています。実験結果によると、我々のFP4フレームワークは、最小限の損失でBF16とFP8と同等の精度を達成し、13BパラメータのLLMを100Bトークンで学習することができます。次世代のハードウェアがFP4をサポートすることにより、我々のフレームワークは、効率的な超低精度学習の基盤を提供します。",
      "upvotes": 9,
      "discussionId": "6799b368d30dc065a2d515bf"
    },
    "publishedAt": "2025-01-28T23:50:12.472Z",
    "title": "Optimizing Large Language Model Training Using FP4 Quantization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17116.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16975",
      "authors": [
        {
          "_id": "6799b345a66ae6b357bef986",
          "name": "Hongzhi Huang",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef987",
          "name": "Defa Zhu",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef988",
          "name": "Banggu Wu",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef989",
          "name": "Yutao Zeng",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef98a",
          "name": "Ya Wang",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef98b",
          "name": "Qiyang Min",
          "hidden": false
        },
        {
          "_id": "6799b345a66ae6b357bef98c",
          "name": "Xun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T14:15:42.000Z",
      "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
      "summary": "トークナイゼーションは、大規模言語モデル（LLMs）の基本的な構成要素であり、その影響が完全に調査されていません。本論文では、入力と出力の単語ベクトルを分離し、言語モデリングの性能を改善するために新しいフレームワーク「Over-Tokenized Transformers」を紹介します。特に、我々のアプローチは入力の単語ベクトルを拡大し、多グラムトークンを活用します。拡大した実験を通じて、入力の単語ベクトルのサイズと訓練損失の間にログライナー関係を明らかにし、それらのサイズがモデルのサイズに関係なく、モデルの性能を一貫して向上させることを示しました。大きな入力の単語ベクトルを使用して、追加のコストを負担しないで、ベースラインの2倍サイズと同等の性能を達成しました。我々の発見は、スケーリング法則の重要性を強調し、トークナイゼーションの設計に実用的なインサイトを提供し、より効率的かつ強力なLLMsのための道が開かれます。",
      "upvotes": 6,
      "discussionId": "6799b346a66ae6b357bef9e3"
    },
    "publishedAt": "2025-01-28T23:49:26.959Z",
    "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16975.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16764",
      "authors": [
        {
          "_id": "6799aa5a311dbfe3c96724cd",
          "name": "Chenguo Lin",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724ce",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724cf",
          "name": "Bangbang Yang",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724d0",
          "name": "Zeming Li",
          "hidden": false
        },
        {
          "_id": "6799aa5a311dbfe3c96724d1",
          "name": "Yadong Mu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T07:38:59.000Z",
      "title": "DiffSplat: 画像ディフューションモデルをスケーラブルなガウススプラット生成に再利用する",
      "summary": "最近の3Dコンテンツ生成の進展は、限られた高品質な3Dデータセットと2D多角度生成の不確実性に転落しています。ここでは、DiffSplatという新しい3D生成フレームワークを紹介します。DiffSplatは、大規模な文字から画像への拡散モデルを利用して、3Dガウススプラットを原生で生成することを目指しています。以前の3D生成モデルと異なり、ウェブスケールの2Dプロイヤーを効果的に利用しながら、統一的なモデルで3Dの一貫性を維持しています。トレーニングをスタートアップするためには、軽量の再構成モデルが提案され、スケーラブルなデータセットの整備に対して、複数角度のガウススプラットグリッドをすぐに生成することができます。これらのグリッドに対しての通常の拡散損失と結合し、任意の角度での3Dの一貫性を促進するために3Dレンダリング損失を導入します。画像拡散モデルとの相性があり、画像生成のための多数の技術を3D領域に無難に適用することができます。拡張された実験は、DiffSplatの文字と画像に基づく生成タスクとその下流アプリケーションでの優れた性能を明らかにしています。詳細な消去研究は、各重要な設計選択の効果性を証明し、裏技を理解することができます。",
      "upvotes": 4,
      "discussionId": "6799aa5c311dbfe3c9672542"
    },
    "publishedAt": "2025-01-29T01:12:02.839Z",
    "title": "DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/654866e8cd0a5621395f8287/TFJMeKzXxMLOnq8NH8ltZ.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/654866e8cd0a5621395f8287/6kn1RLEUsUV-W6S0Taylo.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16764.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "654866e8cd0a5621395f8287",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654866e8cd0a5621395f8287/4Bccwd1ehn-Ee4T1rId5S.jpeg",
      "fullname": "Panwang Pan",
      "name": "paulpanwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16496",
      "authors": [
        {
          "_id": "6799b2fbfe3c29ec219d7d99",
          "name": "Lee Sharkey",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9a",
          "user": {
            "_id": "64ad563f4beffa272de6efac",
            "avatarUrl": "/avatars/f1a4902a95830cc3936058449626f8e4.svg",
            "isPro": false,
            "fullname": "Bilal Chughtai",
            "user": "bilalchughtai",
            "type": "user"
          },
          "name": "Bilal Chughtai",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-29T04:47:56.702Z",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9b",
          "name": "Joshua Batson",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9c",
          "name": "Jack Lindsey",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9d",
          "name": "Jeff Wu",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9e",
          "name": "Lucius Bushnaq",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7d9f",
          "name": "Nicholas Goldowsky-Dill",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da0",
          "name": "Stefan Heimersheim",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da1",
          "name": "Alejandro Ortega",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da2",
          "name": "Joseph Bloom",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da3",
          "name": "Stella Biderman",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da4",
          "name": "Adria Garriga-Alonso",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da5",
          "name": "Arthur Conmy",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da6",
          "name": "Neel Nanda",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da7",
          "name": "Jessica Rumbelow",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da8",
          "name": "Martin Wattenberg",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7da9",
          "name": "Nandi Schoots",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7daa",
          "name": "Joseph Miller",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dab",
          "name": "Eric J. Michaud",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dac",
          "name": "Stephen Casper",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dad",
          "name": "Max Tegmark",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7dae",
          "name": "William Saunders",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7daf",
          "name": "David Bau",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db0",
          "name": "Eric Todd",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db1",
          "name": "Atticus Geiger",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db2",
          "name": "Mor Geva",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db3",
          "name": "Jesse Hoogland",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db4",
          "name": "Daniel Murfet",
          "hidden": false
        },
        {
          "_id": "6799b2fbfe3c29ec219d7db5",
          "name": "Tom McGrath",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T20:57:18.000Z",
      "title": "機械的説明の中の開放的な問題",
      "summary": "機械的説明性は、神経ネットワークの機能を理解し、具体的な科学的および工学的目標を達成するための計算的機構を明確にすることを目的としています。この分野の進歩は、AIシステムの行動における信頼性を高め、知能の本質に関する興奮的な科学的な問題に光をつけることを承諾しています。最近の進歩に伴い、この目標に向けて進むことができるが、計算的な機構の理解に必要な多くの開放的な問題が存在し、これらの問題が解決されるまで、多くの科学的および実用的な利益を実現することはできません: 私たちの方法は、概念的および実用的な改良が必要で、深い見解を明らかにすることができるようになる必要があります; 私たちは、特定の目標を達成するために私たちの方法を最適的に適用する方法を明確にする必要があります; そして、私たちの仕事に影響を与えるさらには、私たちの仕事に影響を受ける社会技術的な課題について、分野は対応する必要があります。この先進的なレビューは、機械的な説明性の現在の前線と、分野が優先しておけばよい開放的な問題について議論しています。",
      "upvotes": 4,
      "discussionId": "6799b2fcfe3c29ec219d7dca"
    },
    "publishedAt": "2025-01-28T23:48:30.888Z",
    "title": "Open Problems in Mechanistic Interpretability",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16496.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.15747",
      "authors": [
        {
          "_id": "6799946c18cb282841d42639",
          "name": "Sankalp KJ",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263a",
          "name": "Ashutosh Kumar",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263b",
          "user": {
            "_id": "66707b60405252abeefd4c50",
            "avatarUrl": "/avatars/ee2728f115376e234e96820b8b376849.svg",
            "isPro": false,
            "fullname": "Laxmaan Balaji",
            "user": "laxmaanb",
            "type": "user"
          },
          "name": "Laxmaan Balaji",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-29T02:37:34.240Z",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263c",
          "name": "Nikunj Kotecha",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263d",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263e",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-29T08:55:14.312Z",
          "hidden": false
        },
        {
          "_id": "6799946c18cb282841d4263f",
          "name": "Sreyoshi Bhaduri",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T03:19:03.000Z",
      "title": "IndicMMLU-Pro: マルチタスク語言理解におけるIndic大語言モデルのベンチマーク",
      "summary": "インドサブコンティニエンの15億人以上が知られるインド諸語は、文化の豊かな伝統、言語の多様性と複雑な構造により、自然言語処理（NLP）研究に特徴的な課題と機会を提供しています。IndicMMLU-Proは、インド諸語の幅広い言語にわたる大規模言語モデル（LLMs）の評価を行うために設計された詳細なベンチマークです。マジック・マルチタスク言語理解（Massive Multitask Language Understanding）のフレームワークに基づいて、ハンディ、ベンガリ、ギャラタイ、マラタイ、カナダ、パンチバイ、タミル、テルグ、ウーダーなどの主な言語を対象として、インドサブコンティニエンの言語多様性による特徴的な課題と機会を解決します。このベンチマークは、言語理解、理由論理、生成などの幅広いタスクを含む言語処理の複雑な複雑性を捉え、インド諸語の特徴を極めてよく捉えるものです。IndicMMLU-Proは、インド諸語のAI研究のブランチを推進するために標準化された評価フレームワークを提供し、正確な、効率的な、文化的に敏感なモデルの開発を促進します。この論文は、ベンチマークの設計原則、タスクタクノロジー、データの収集方法を説明し、最先端の多言語モデルからの基準的な結果を提供します。",
      "upvotes": 3,
      "discussionId": "6799946e18cb282841d426d6"
    },
    "publishedAt": "2025-01-28T21:38:17.182Z",
    "title": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15747.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.16372",
      "authors": [
        {
          "_id": "67999c3dc1e34886f90320ee",
          "name": "J. Pablo Muñoz",
          "hidden": false
        },
        {
          "_id": "67999c3dc1e34886f90320ef",
          "name": "Jinjie Yuan",
          "hidden": false
        },
        {
          "_id": "67999c3dc1e34886f90320f0",
          "name": "Nilesh Jain",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-23T02:14:08.000Z",
      "title": "低ランクアダプターとニューラルアーキテクチャサーチがLLMコンパクションに会い合う",
      "summary": "LLMの急速な拡大は、微調節と部署に必要な計算コンピューティングリソースに関する重要な課題を見出しました。低レンキングアダプタの最近の進展は、これらのモデルのパラメータ効率的な微調節（PEFT）において効果的であることを示しました。本記録論文では、低レンキング表現と神経アーキテクチャ検索（NAS）手法、特に重み共有スーパーネットワークとの相互作用をもとにした創新的なアプローチを全面的に議論します。これらの手法を統合して、大規模な事前学習モデルの圧縮と微調節の強固な解決策を開発しました。我々の分析は、これらの組み合わせの戦略がLLMの利用を民主化し、資源制限付き環境での部署によりもっとアクセス可能にする可能性を明らかにします。結果として得られたモデルはメモリフットプリントを減らし、推論時間を短縮し、LLMの実用的なおよびスケーラブルなアプリケーションの実現に道が開かれます。モデルとコードは以下のURLで提供されています。\nhttps://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning",
      "upvotes": 2,
      "discussionId": "67999c3dc1e34886f9032140"
    },
    "publishedAt": "2025-01-28T22:11:04.472Z",
    "title": "Low-Rank Adapters Meet Neural Architecture Search for LLM Compression",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16372.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5850
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.17117",
      "authors": [
        {
          "_id": "6799e5f9121155210e4fa48c",
          "name": "Thibaud Leteno",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa48d",
          "name": "Irina Proskurina",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa48e",
          "name": "Antoine Gourru",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa48f",
          "name": "Julien Velcin",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa490",
          "name": "Charlotte Laclau",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa491",
          "name": "Guillaume Metzler",
          "hidden": false
        },
        {
          "_id": "6799e5f9121155210e4fa492",
          "name": "Christophe Gravier",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-28T18:07:30.000Z",
      "title": "モラルアライメントの評価に向けてのフランス語データセット「モラルノミティース」",
      "summary": "モデルが日常生活によりより多くの機能を担うことに伴い、モデルの語言を人類の価値観に合わせることは重要である。モデルは通常、ユーザーの好みに合わせられるが、実世界的な社会の倫理規範と行動に合わせることも同様に重要である。英語や中国語においては進歩が大きいが、フランス語ではこの領域には少しの関注があり、フランス語でLLMがモラル論理をどのように扱うかの理解に欠陥がある。この欠陥を解決するために、我々は、モラルストーリーから製作され、母語者の助言を通じて語法の正確性とフランス文化のコンテキストに適合させた「Histoires Morales」のフランス語データセットを紹介する。データセット内のモラル価値のアノテーションも利用して、フランスの価値観に合わせることを確保する。「Histoires Morales」は、チャンスの差、関係中の正直性の表現、動物への責任など、広い範囲の社会状況をカバーしている。将来の研究のために、我々は、フランス語と英語データの多言語モデルの合わせ方とその強固性についての初期実験も行う。これらの実験によると、LLMは一般的にユーザーの好みの最適化によってモラルや不道德なデータにも容易に影響されることがわかった。",
      "upvotes": 0,
      "discussionId": "6799e5fb121155210e4fa500"
    },
    "publishedAt": "2025-01-29T03:32:09.927Z",
    "title": "Histoires Morales: A French Dataset for Assessing Moral Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.17117.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "629a3dbcd496c6dcdebf41cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655113762275-629a3dbcd496c6dcdebf41cc.jpeg",
      "fullname": "Irina Proskurina",
      "name": "iproskurina",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]