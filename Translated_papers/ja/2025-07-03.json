[
  {
    "paper": {
      "id": "2507.01949",
      "authors": [
        {
          "_id": "6865e6218c83dab5f72d1e47",
          "name": "Kwai Keye Team",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e48",
          "name": "Biao Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e49",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4a",
          "name": "Changyi Liu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4b",
          "name": "Chenglong Chu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4c",
          "name": "Chengru Song",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4d",
          "name": "Chongling Rao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4e",
          "name": "Chuan Yi",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e4f",
          "name": "Da Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e50",
          "name": "Dunju Zang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e51",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e52",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e53",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e54",
          "name": "Haojie Ding",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e55",
          "name": "Jiaming Huang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e56",
          "user": {
            "_id": "65802e81b701ff85a37caba8",
            "avatarUrl": "/avatars/b2e9726893caa7e62aad83b1d02e5b41.svg",
            "isPro": false,
            "fullname": "jiangxia cao",
            "user": "caojiangxia",
            "type": "user"
          },
          "name": "Jiangxia Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:44.694Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e57",
          "name": "Jiankang Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e58",
          "user": {
            "_id": "61540338e5b9ae6774201e58",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61540338e5b9ae6774201e58/p_minqil1sdiqg5wEVxT5.jpeg",
            "isPro": false,
            "fullname": "jingyun",
            "user": "hjy",
            "type": "user"
          },
          "name": "Jingyun Hua",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:15:01.253Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e59",
          "name": "Jin Ouyang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5a",
          "name": "Kaibing Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5b",
          "name": "Kaiyu Jiang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5c",
          "name": "Kaiyu Tang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5d",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5e",
          "name": "Shengnan Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e5f",
          "name": "Siyang Mao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e60",
          "name": "Sui Huang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e61",
          "name": "Tianke Zhang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e62",
          "user": {
            "_id": "68652063e29f1407b58da60f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hTgS65zsKRPxELjSMDSNm.png",
            "isPro": false,
            "fullname": "tingting gao",
            "user": "TinaGao",
            "type": "user"
          },
          "name": "Tingting Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:47:43.679Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e63",
          "name": "Wei Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e64",
          "user": {
            "_id": "65423daba385933e812516d5",
            "avatarUrl": "/avatars/d18b85b4206ab5905ef5bc95622dff3e.svg",
            "isPro": false,
            "fullname": "wei yuan",
            "user": "yw95",
            "type": "user"
          },
          "name": "Wei Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:49.219Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e65",
          "name": "Xiangyu Wu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e66",
          "user": {
            "_id": "64a4dba8fe950993d2d89113",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a4dba8fe950993d2d89113/yukb9NNeVspIX7eFTreUq.jpeg",
            "isPro": false,
            "fullname": "Xiao Hu",
            "user": "huxiao09",
            "type": "user"
          },
          "name": "Xiao Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:42.172Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e67",
          "user": {
            "_id": "673597cfc2424474d12ca58c",
            "avatarUrl": "/avatars/f748f19619b07838a66bc419a7a6db9d.svg",
            "isPro": false,
            "fullname": "xingyulu",
            "user": "Xingyulu47",
            "type": "user"
          },
          "name": "Xingyu Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:46.882Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e68",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e69",
          "user": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "isPro": false,
            "fullname": "Yi-Fan Zhang",
            "user": "yifanzhang114",
            "type": "user"
          },
          "name": "Yi-Fan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:15:06.461Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6a",
          "name": "Yiping Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6b",
          "name": "Yulong Chen",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6c",
          "name": "Zhenhua Wu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6d",
          "name": "Zhenyu Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6e",
          "user": {
            "_id": "641948b7d13ffa40812eb239",
            "avatarUrl": "/avatars/65a0262fea6907bec48ddc1d966742da.svg",
            "isPro": false,
            "fullname": "Zhixin Ling",
            "user": "NamingIsTroublesome",
            "type": "user"
          },
          "name": "Zhixin Ling",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:15:03.454Z",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e6f",
          "name": "Ziming Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e70",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e71",
          "name": "Di Xu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e72",
          "name": "Haixuan Gao",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e73",
          "name": "Hang Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e74",
          "name": "Jiawei Guo",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e75",
          "name": "Jing Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e76",
          "name": "Lejian Ren",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e77",
          "name": "Muhao Wei",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e78",
          "name": "Qianqian Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e79",
          "name": "Qigen Hu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7a",
          "name": "Shiyao Wang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7b",
          "name": "Tao Yu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7c",
          "name": "Xinchen Luo",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7d",
          "name": "Yan Li",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7e",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e7f",
          "name": "Yuhang Hu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e80",
          "name": "Zeyi Lu",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e81",
          "name": "Zhuoran Yang",
          "hidden": false
        },
        {
          "_id": "6865e6218c83dab5f72d1e82",
          "name": "Zixing Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:57:28.000Z",
      "submittedOnDailyAt": "2025-07-03T00:40:58.759Z",
      "title": "Kwai Keye-VL 技術報告",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "マルチモーダル大語言モデル（MLLMs）は、静的画像では驚異的な能力を示すが、情報豊富な短形の動画に対して理解力が不足しています。この隙を埋めるために、私たちはKwai Keye-VLを紹介します。Kwai Keye-VLは、短形動画の理解に最先端の性能を維持しながら、強力的な一般的な視覚言語能力を持つ8バイリオンパラメーターマルチモーダルベースモデルです。Keye-VLの開発は、大規模な高品質データセットと独自の学習プロセスの2つの核心の支柱に基づいています。データセットは6000億トークンを超え、動画に特に重視されています。学習プロセスは、4段階の前学習プロセスと2段階の後学習プロセスからなり、最初の後学習段階では、指示従いや基盤的な能力を向上させ、第二段階では進歩的な理由論を促すことを焦点としています。第二段階では、「思考」、「非思考」、「自動思考」、「画像と同時に思考」、高品質の動画データを含む5モードの「冷やかなスタート」データミックスが重要な創新となっています。このミックスはモデルがどのようにどのタイミングで理由論を行うかを学ぶように訓練されます。後続の強化学習（RL）とアライメントステップは、これらの理由論能力を進め、重複のような異常なモデルの行動を修正することを通じて、さらに向上させています。Keye-VLのアプローチの有効性を証明するために、公共の動画ベンチマークで最先端の結果を達成し、一般的な画像ベースのタスクでも高度な競争力を保持していることを示します（図1）。また、KC-MMBenchという新しいベンチマークを開発し、実世界的な短形動画のシナリオに適したベンチマークを提供し、Keye-VLが顕著な優勢を示すことを示しています。",
      "upvotes": 85,
      "discussionId": "6865e6218c83dab5f72d1e83",
      "projectPage": "https://kwai-keye.github.io/",
      "githubRepo": "https://github.com/Kwai-Keye/Keye",
      "githubStars": 365
    },
    "publishedAt": "2025-07-02T13:57:28.000Z",
    "title": "Kwai Keye-VL Technical Report",
    "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce Kwai Keye-VL, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the KC-MMBench, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01945",
      "authors": [
        {
          "_id": "6865e4b88c83dab5f72d1e41",
          "user": {
            "_id": "6629d7c9fa14eaccf07d8633",
            "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
            "isPro": false,
            "fullname": "Nan Chen",
            "user": "CNcreator0331",
            "type": "user"
          },
          "name": "Nan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:51.437Z",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e42",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e43",
          "name": "Yihao Meng",
          "hidden": false
        },
        {
          "_id": "6865e4b88c83dab5f72d1e44",
          "name": "Zhendong Mao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6629d7c9fa14eaccf07d8633/0WDfsnDIDJ9hzI6iJym9L.mp4"
      ],
      "publishedAt": "2025-07-02T17:55:50.000Z",
      "submittedOnDailyAt": "2025-07-03T00:57:17.831Z",
      "title": "長アニメーション: 動的グローバル・ローカルメモリによる長アニメーション生成",
      "submittedOnDailyBy": {
        "_id": "6629d7c9fa14eaccf07d8633",
        "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
        "isPro": false,
        "fullname": "Nan Chen",
        "user": "CNcreator0331",
        "type": "user"
      },
      "summary": "アニメーションの彩色化は、実際のアニメーション業界の生産にとって重要な部分です。長期的なアニメーションの彩色化には高い労働費がかかります。そこで、ビデオ生成モデルに基づく自動化された長期的なアニメーションの彩色化は、大きな研究価値があります。既存の研究は、短期間の彩色化に限られています。これらの研究は、局所的なパラダイムを採用し、重なりの特徴量を融合させて局所的なセグメント間の平滑な転移を実現します。しかし、局所的なパラダイムは、グローバル的情報を無視し、長期的な彩色の一貫性を維持できないことに注意されています。本研究では、理想的な長期的な彩色の一貫性は、動的なグローバル・局所的なパラダイムにより実現できることを主張します。具体的には、我々はLongAnimationという新しいフレームワークを提案します。これは、SketchDiT、Dynamic Global-Local Memory（DGLM）、Color Consistency Rewardを主に含みます。SketchDiTは、統合するためのハイブリッド参照特徴量を捉え、DGLMモジュールをサポートします。DGLMモジュールは、長期的なビデオ理解モデルを使用して、グローバルの歴史特徴量を動的に圧縮し、現在の生成特徴量と適応的に融合させます。彩色の一貫性を精確化するために、Color Consistency Rewardを導入します。推論時には、彩色の一貫性を平滑化するための彩色の一貫性融合を提案します。短期間（14フレーム）と長期的な（平均500フレーム）アニメーションに対しての拡張的な実験は、LongAnimationが開放ドメインのアニメーションの彩色化タスクで短期的なと長期的な彩色の一貫性を維持する効果を示しています。コードは、https://cn-makers.github.io/long_animation_web/ に見つかります。",
      "upvotes": 52,
      "discussionId": "6865e4b88c83dab5f72d1e45",
      "projectPage": "https://cn-makers.github.io/long_animation_web/",
      "githubRepo": "https://github.com/CN-makers/LongAnimation",
      "githubStars": 65
    },
    "publishedAt": "2025-07-02T13:55:50.000Z",
    "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
    "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6629d7c9fa14eaccf07d8633/0WDfsnDIDJ9hzI6iJym9L.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01945.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6629d7c9fa14eaccf07d8633",
      "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
      "fullname": "Nan Chen",
      "name": "CNcreator0331",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01634",
      "authors": [
        {
          "_id": "6865e04b8c83dab5f72d1e2d",
          "user": {
            "_id": "66ef2611fcc1c455f8dce832",
            "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
            "isPro": false,
            "fullname": "Boyuan Sun",
            "user": "BBBBCHAN",
            "type": "user"
          },
          "name": "Boyuan Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:54.412Z",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e2e",
          "name": "Modi Jin",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e2f",
          "name": "Bowen Yin",
          "hidden": false
        },
        {
          "_id": "6865e04b8c83dab5f72d1e30",
          "name": "Qibin Hou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T12:05:57.000Z",
      "submittedOnDailyAt": "2025-07-03T00:16:17.577Z",
      "title": "Depth Anything at Any Condition\n\nどの条件でも、どのものでも深く扱うことができる。",
      "submittedOnDailyBy": {
        "_id": "66ef2611fcc1c455f8dce832",
        "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
        "isPro": false,
        "fullname": "Boyuan Sun",
        "user": "BBBBCHAN",
        "type": "user"
      },
      "summary": "DepthAnything at Any Condition (DepthAnything-AC) は、多様な環境条件を扱う基盤的なモノカメラ深さ推定モデルです。先行の基盤的なモデルは一般的な場所では驚異的な性能を示しますが、複雑な開放ワールド環境では、照明の変化、不利な天候、センサーによる異常な異常なものがあります。データの不足と破損画像から高品質のファルシーラベルの生成ができない問題を克服するために、無マニュアルの一貫性正規化微調パラダイムを提案します。また、空間距離制約を提案し、モデルがパッチレベルの相対的な関係を学習するように明記し、語意的な境界が明確になり、詳細が正確になるようにします。実験結果は、DepthAnything-AC のゼロショット能力を示し、実世界的な不利な天候ベンチマーク、合成的な破損ベンチマーク、一般的なベンチマークを含む多様なベンチマークで示しました。\n  プロジェクトページ：https://ghost233lism.github.io/depthanything-AC-page\n  コード：https://github.com/HVision-NKU/DepthAnythingAC",
      "upvotes": 27,
      "discussionId": "6865e04b8c83dab5f72d1e31",
      "projectPage": "https://ghost233lism.github.io/depthanything-AC-page/",
      "githubRepo": "https://github.com/HVision-NKU/DepthAnythingAC",
      "githubStars": 91
    },
    "publishedAt": "2025-07-02T08:05:57.000Z",
    "title": "Depth Anything at Any Condition",
    "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01634.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ef2611fcc1c455f8dce832",
      "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
      "fullname": "Boyuan Sun",
      "name": "BBBBCHAN",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.01925",
      "authors": [
        {
          "_id": "686600cf8c83dab5f72d1ed0",
          "name": "Yifan Zhong",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed1",
          "name": "Fengshuo Bai",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed2",
          "user": {
            "_id": "6578459d62d3ac1817ed79fe",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578459d62d3ac1817ed79fe/AXDJuwLUoEOb4Fj3U0Xxo.jpeg",
            "isPro": false,
            "fullname": "Shaofei Cai",
            "user": "phython96",
            "type": "user"
          },
          "name": "Shaofei Cai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:14:49.923Z",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed3",
          "user": {
            "_id": "66ee5cf1801ea45d7a44a542",
            "avatarUrl": "/avatars/04bafbcbf1aea3920a79bddbd1a18f42.svg",
            "isPro": false,
            "fullname": "XUCHUAN HUANG",
            "user": "Feernnn",
            "type": "user"
          },
          "name": "Xuchuan Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:14:52.141Z",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed4",
          "name": "Zhang Chen",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed5",
          "name": "Xiaowei Zhang",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed6",
          "name": "Yuanfei Wang",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed7",
          "name": "Shaoyang Guo",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed8",
          "name": "Tianrui Guan",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1ed9",
          "name": "Ka Nam Lui",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1eda",
          "name": "Zhiquan Qi",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1edb",
          "name": "Yitao Liang",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1edc",
          "name": "Yuanpei Chen",
          "hidden": false
        },
        {
          "_id": "686600cf8c83dab5f72d1edd",
          "name": "Yaodong Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:34:52.000Z",
      "submittedOnDailyAt": "2025-07-03T03:39:33.625Z",
      "title": "ビジョン・ラング・アクションモデルの調査：アクショントークナイションの視点から",
      "submittedOnDailyBy": {
        "_id": "655d9f43b5da99edaf3f2f81",
        "avatarUrl": "/avatars/c7225b3ed54d099a4fd87682427fb5bf.svg",
        "isPro": false,
        "fullname": "Yifan Zhong",
        "user": "Yifan-Zhong",
        "type": "user"
      },
      "summary": "視覚と言語の基盤モデルの驚人的な進展が多様的な理解、理由、生成に及ぼして、物理世界へのこれらの知能の拡張により、視覚-言語-行動（VLA）モデルの繁栄を促進しています。現在のVLAモデルは、それぞれ異なる手法を示しているが、我々は、これらを一つのフレームワークの下に統一できることを見出しました：視覚と言語の入力は、VLAモジュールの連続的な処理により、進歩的にもっと具体的で行動可能な情報を含む行動トークンの連鎖を生成し、最終的に実行可能な行動を生成します。また、VLAモデルの主な設計選択肢は、行動トークンの構成方法であることを明らかにしました。これらは、言語の説明、コード、アフフォードドン、軌跡、ゴール状態、潜在的表現、裸の行動、理由と分類されます。しかし、行動トークンについての詳細な理解が欠如しており、VLAの開発において効果的な進展を抑え、将来の方向を隠しています。そこで、この調査は、行動トークン化の視点から既存のVLA研究を分類し、各トークンの強みと限界を抽出し、改善の可能性のある領域を特定することを目的としています。このシステム的な評価と分析を通じて、VLAモデルの広い進化を合成的な視点から見て、調査していないけれど潜力のある方向を明らかにし、将来の研究における指導を提供し、一般的な目的の知能への近づきを望むことを希望しています。",
      "upvotes": 13,
      "discussionId": "686600cf8c83dab5f72d1ede",
      "githubRepo": "https://github.com/Psi-Robot/Awesome-VLA-Papers",
      "githubStars": 21
    },
    "publishedAt": "2025-07-02T13:34:52.000Z",
    "title": "A Survey on Vision-Language-Action Models: An Action Tokenization\n  Perspective",
    "summary": "The remarkable advancements of vision and language foundation models in\nmultimodal understanding, reasoning, and generation has sparked growing efforts\nto extend such intelligence to the physical world, fueling the flourishing of\nvision-language-action (VLA) models. Despite seemingly diverse approaches, we\nobserve that current VLA models can be unified under a single framework: vision\nand language inputs are processed by a series of VLA modules, producing a chain\nof action tokens that progressively encode more grounded and\nactionable information, ultimately generating executable actions. We further\ndetermine that the primary design choice distinguishing VLA models lies in how\naction tokens are formulated, which can be categorized into language\ndescription, code, affordance, trajectory, goal state, latent representation,\nraw action, and reasoning. However, there remains a lack of comprehensive\nunderstanding regarding action tokens, significantly impeding effective VLA\ndevelopment and obscuring future directions. Therefore, this survey aims to\ncategorize and interpret existing VLA research through the lens of action\ntokenization, distill the strengths and limitations of each token type, and\nidentify areas for improvement. Through this systematic review and analysis, we\noffer a synthesized outlook on the broader evolution of VLA models, highlight\nunderexplored yet promising directions, and contribute guidance for future\nresearch, hoping to bring the field closer to general-purpose intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655d9f43b5da99edaf3f2f81",
      "avatarUrl": "/avatars/c7225b3ed54d099a4fd87682427fb5bf.svg",
      "fullname": "Yifan Zhong",
      "name": "Yifan-Zhong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01953",
      "authors": [
        {
          "_id": "686601648c83dab5f72d1ee0",
          "name": "Yukang Cao",
          "hidden": false
        },
        {
          "_id": "686601648c83dab5f72d1ee1",
          "name": "Chenyang Si",
          "hidden": false
        },
        {
          "_id": "686601648c83dab5f72d1ee2",
          "name": "Jinghao Wang",
          "hidden": false
        },
        {
          "_id": "686601648c83dab5f72d1ee3",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:58:20.000Z",
      "submittedOnDailyAt": "2025-07-03T02:40:23.949Z",
      "title": "FreeMorph: ディフューションモデルを用いた無調整済み拡張画像変形技術",
      "submittedOnDailyBy": {
        "_id": "63a07c3ab5515dccd40fdb71",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
        "isPro": false,
        "fullname": "Yukang Cao",
        "user": "yukangcao",
        "type": "user"
      },
      "summary": "FreeMorphは、最初のチューニング無制限な画像モーフィング手法で、異なるセマンティクスやレイアウトの入力を扱うことができます。現在の方法は、予め学習済みのディフュージョンモデルを微調節する必要があり、時間制約やセマンティクスやレイアウトの違いに制限されていますが、FreeMorphはこれらの制限を超え、プロジェクト単位のトレーニングが必要なく高品質な画像モーフィングを提供します。チューニング無制限な方法は、多段階ディフュージョンプロセスの非線形性と予め学習済みモデルから継承されたバイアスによる高品質の維持に課題がありますが、この論文では、2つのキーイノベーションを組み合わせてこれらの課題を解決するFreeMorphを紹介します。1) 入力画像からの明示的なガイドニングを受け入れる球面インタープライザの設計を提案し、自己注意モジュールを改良して識別性の損失を補正し、生成されるシーケンス全体で方向的なチャンスを保証します。2) 各入力画像から得られる自己注意モジュールを融合させ、両方の入力に対応した制御的で一貫したチャンスを実現するステップベースの変化趨勢を導入します。広範囲の評価により、FreeMorphは現在の方法を上回り、10倍～50倍速く画像モーフィングの新たな最先端技術を創り出しました。",
      "upvotes": 9,
      "discussionId": "686601658c83dab5f72d1ee4",
      "projectPage": "https://yukangcao.github.io/FreeMorph/",
      "githubRepo": "https://github.com/yukangcao/FreeMorph",
      "githubStars": 7
    },
    "publishedAt": "2025-07-02T13:58:20.000Z",
    "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
    "summary": "We present FreeMorph, the first tuning-free method for image morphing that\naccommodates inputs with different semantics or layouts. Unlike existing\nmethods that rely on finetuning pre-trained diffusion models and are limited by\ntime constraints and semantic/layout discrepancies, FreeMorph delivers\nhigh-fidelity image morphing without requiring per-instance training. Despite\ntheir efficiency and potential, tuning-free methods face challenges in\nmaintaining high-quality results due to the non-linear nature of the multi-step\ndenoising process and biases inherited from the pre-trained diffusion model. In\nthis paper, we introduce FreeMorph to address these challenges by integrating\ntwo key innovations. 1) We first propose a guidance-aware spherical\ninterpolation design that incorporates explicit guidance from the input images\nby modifying the self-attention modules, thereby addressing identity loss and\nensuring directional transitions throughout the generated sequence. 2) We\nfurther introduce a step-oriented variation trend that blends self-attention\nmodules derived from each input image to achieve controlled and consistent\ntransitions that respect both inputs. Our extensive evaluations demonstrate\nthat FreeMorph outperforms existing methods, being 10x ~ 50x faster and\nestablishing a new state-of-the-art for image morphing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01953.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a07c3ab5515dccd40fdb71",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
      "fullname": "Yukang Cao",
      "name": "yukangcao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01957",
      "authors": [
        {
          "_id": "686633d28c83dab5f72d1f39",
          "name": "Zhuoyang Zhang",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3a",
          "name": "Luke J. Huang",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3b",
          "name": "Chengyue Wu",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3c",
          "name": "Shang Yang",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3d",
          "name": "Kelly Peng",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3e",
          "name": "Yao Lu",
          "hidden": false
        },
        {
          "_id": "686633d28c83dab5f72d1f3f",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:59:23.000Z",
      "submittedOnDailyAt": "2025-07-03T06:13:07.255Z",
      "title": "地域情報を知ることで効率的な自動単語連鎖画像生成の並列解確定法",
      "submittedOnDailyBy": {
        "_id": "650e6ab08f3228d807707735",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650e6ab08f3228d807707735/yFo6aLuyGH06t9yG8AOp7.png",
        "isPro": false,
        "fullname": "Zhuoyang Zhang",
        "user": "zhuoyang20",
        "type": "user"
      },
      "summary": "ロカリティに関心のある並列解像 (LPD) を紹介し、自動帰納的画像生成を加速します。伝統的な自動帰納的画像生成は、次のパッチ予測をもとに、メモリバンドプロセスであり、高いラテンシーを招致します。既存の研究は、次のパッチ予測を並列化するために、多パッチ予測に移行して処理を加速しようとしましたが、限られた並列化を達成しました。高い並列化を達成しながら生成質量を維持するために、私たちは2つの鍵の技術を導入します： (1) 柔軟な並列化された自動帰納的モデリング、任意の生成順序と並列化の程度を可能にする新しいアーキテクチャです。学習可能な位置クエリトークンを使用して、目標位置での生成をガイドし、同時に生成されるトークンの相互視覚を確保し、一致的な並列解像を実現します。 (2) ロカリティに関心のある生成順序、グループを形成し、グループ内の依存関係を最小化し、コンテキストフォードを最大化し、生成質量を向上させます。これらの設計により、ImageNetクラス条件付き生成では、生成ステップを256から20（256×256レジズ）と1024から48（512×512レジズ）に減少させ、質量を維持しながら、以前の並列化された自動帰納的モデルより3.4倍以下のラテンシーを達成します。",
      "upvotes": 7,
      "discussionId": "686633d28c83dab5f72d1f40"
    },
    "publishedAt": "2025-07-02T13:59:23.000Z",
    "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image\n  Generation",
    "summary": "We present Locality-aware Parallel Decoding (LPD) to accelerate\nautoregressive image generation. Traditional autoregressive image generation\nrelies on next-patch prediction, a memory-bound process that leads to high\nlatency. Existing works have tried to parallelize next-patch prediction by\nshifting to multi-patch prediction to accelerate the process, but only achieved\nlimited parallelization. To achieve high parallelization while maintaining\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\nordering and degrees of parallelization. It uses learnable position query\ntokens to guide generation at target positions while ensuring mutual visibility\namong concurrently generated tokens for consistent parallel decoding. (2)\nLocality-aware Generation Ordering, a novel schedule that forms groups to\nminimize intra-group dependencies and maximize contextual support, enhancing\ngeneration quality. With these designs, we reduce the generation steps from 256\nto 20 (256times256 res.) and 1024 to 48 (512times512 res.) without\ncompromising quality on the ImageNet class-conditional generation, and\nachieving at least 3.4times lower latency than previous parallelized\nautoregressive models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01957.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650e6ab08f3228d807707735",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650e6ab08f3228d807707735/yFo6aLuyGH06t9yG8AOp7.png",
      "fullname": "Zhuoyang Zhang",
      "name": "zhuoyang20",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.23552",
      "authors": [
        {
          "_id": "6865e0148c83dab5f72d1e26",
          "name": "Mingi Kwon",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e27",
          "user": {
            "_id": "631074d895c34b95407945f0",
            "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
            "isPro": false,
            "fullname": "Joonghyuk Shin",
            "user": "alex4727",
            "type": "user"
          },
          "name": "Joonghyuk Shin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:16:56.605Z",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e28",
          "name": "Jaeseok Jung",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e29",
          "name": "Jaesik Park",
          "hidden": false
        },
        {
          "_id": "6865e0148c83dab5f72d1e2a",
          "name": "Youngjung Uh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T06:51:40.000Z",
      "submittedOnDailyAt": "2025-07-03T00:15:16.476Z",
      "title": "JAM-Flow: フローマッチングを用いた結合アウディオ・モーション合成",
      "submittedOnDailyBy": {
        "_id": "631074d895c34b95407945f0",
        "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
        "isPro": false,
        "fullname": "Joonghyuk Shin",
        "user": "alex4727",
        "type": "user"
      },
      "summary": "ファジカル・モーションとスピーチの固有の連結関係は、生成モデリングで通常は関係がなく見落とされています。この論文では、ファジカル・モーションとスピーチの両方を同時に合成し、条件に基づけるための一連のフレームワークを紹介します。私たちのアプローチは、flow matchingと新しいMulti-Modal Diffusion Transformer（MM-DiT）アーキテクチャを利用し、特別化されたMotion-DiTとAudio-DiTモジュールを組み合わせています。これらは、選択的な共通注意層で結合され、時間的にアラインされた位置ベクトルと局所的な共通注意マスクを挟むことで、モード間の相互作用を効果的に促進し、同時にモード固有の強みを保持することができます。ペンチンタイプの目標で訓練されたJAM-Flowは、幅広い範囲の条件入力をサポートし、テキスト、参照音声、参照モーションを含むことで、テキストからの同期されたテーブルヘッド生成、音声駆動アニメーションなどの多様なタスクを実現できます。JAM-Flowは、ホリスティックな音声ビュージョン合成の実用的な解決策を提供し、多モード生成モデリングに大きな進展を遂げます。プロジェクトページ：https://joonghyuk.com/jamflow-web",
      "upvotes": 3,
      "discussionId": "6865e0148c83dab5f72d1e2b"
    },
    "publishedAt": "2025-06-30T02:51:40.000Z",
    "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
    "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23552.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631074d895c34b95407945f0",
      "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
      "fullname": "Joonghyuk Shin",
      "name": "alex4727",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.22868",
      "authors": [
        {
          "_id": "68637f0d588cea0da970c95e",
          "user": {
            "_id": "6719de3235b6494469ab69f6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
            "isPro": false,
            "fullname": "Junsung Lee",
            "user": "jslee525",
            "type": "user"
          },
          "name": "Junsung Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-03T07:32:55.680Z",
          "hidden": false
        },
        {
          "_id": "68637f0d588cea0da970c95f",
          "name": "Junoh Kang",
          "hidden": false
        },
        {
          "_id": "68637f0d588cea0da970c960",
          "name": "Bohyung Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-28T12:36:19.000Z",
      "submittedOnDailyAt": "2025-07-03T06:08:50.749Z",
      "title": "STR-Match: 空間時間的関連性スコアのマッチング（トレーニング不要のビデオ編集向け）",
      "submittedOnDailyBy": {
        "_id": "6719de3235b6494469ab69f6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
        "isPro": false,
        "fullname": "Junsung Lee",
        "user": "jslee525",
        "type": "user"
      },
      "summary": "以前のテキストガイドされたビデオ編集手法は、時間的な不連続性、動きの歪み、特に限られた領域変換による制限がありました。これらの制限を原因として、編集プロセス中の空間時間的なピクセルの関連性のモデリングが不足していることを説明します。これらの問題を解決するために、ビデオ編集アルゴリズム「STR-Match」を提案します。これは、新しい「STRスコア」による潜在計画をガイドして、視覚的に楽しいかつ空間時間的に一貫したビデオを生成します。スコアは、2D空間注意と1D時間モジュールを活用して、テキストからビデオ（T2V）の拡散モデルで隣接フレーム間の空間時間的なピクセルの関連性を捉えます。計算量の高い3D注意機構のオーバーヘッドを避けることで、このスコアは効果的です。潜在計画フレームワークと潜在マスクを組み合わせたSTR-Matchは、時間的に一貫したかつ視覚的に忠実なビデオを生成し、領域変換による大きな影響を受けない限り強い性能を維持します。拡散モデルでは、隣接フレーム間の空間時間的なピクセルの関連性を捉えることで、視覚質量と空間時間的な一貫性において現在の手法を一貫して優れていることが詳細な実験により示されました。",
      "upvotes": 3,
      "discussionId": "68637f0e588cea0da970c961",
      "projectPage": "https://jslee525.github.io/str-match",
      "githubRepo": "https://github.com/jslee525/STR-Match_official",
      "ai_summary": "STR-Match uses latent optimization and a novel STR score to produce spatiotemporally coherent and visually appealing edited videos by leveraging 2D spatial and 1D temporal attention in T2V diffusion models.",
      "ai_keywords": [
        "T2V diffusion models",
        "latent optimization",
        "spatiotemporal pixel relevance",
        "latent mask",
        "2D spatial attention",
        "1D temporal modules"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-06-28T08:36:19.000Z",
    "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free\n  Video Editing",
    "summary": "Previous text-guided video editing methods often suffer from temporal\ninconsistency, motion distortion, and-most notably-limited domain\ntransformation. We attribute these limitations to insufficient modeling of\nspatiotemporal pixel relevance during the editing process. To address this, we\npropose STR-Match, a training-free video editing algorithm that produces\nvisually appealing and spatiotemporally coherent videos through latent\noptimization guided by our novel STR score. The score captures spatiotemporal\npixel relevance across adjacent frames by leveraging 2D spatial attention and\n1D temporal modules in text-to-video (T2V) diffusion models, without the\noverhead of computationally expensive 3D attention mechanisms. Integrated into\na latent optimization framework with a latent mask, STR-Match generates\ntemporally consistent and visually faithful videos, maintaining strong\nperformance even under significant domain transformations while preserving key\nvisual attributes of the source. Extensive experiments demonstrate that\nSTR-Match consistently outperforms existing methods in both visual quality and\nspatiotemporal consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22868.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6719de3235b6494469ab69f6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
      "fullname": "Junsung Lee",
      "name": "jslee525",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]