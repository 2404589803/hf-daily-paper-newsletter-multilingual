[
  {
    "paper": {
      "id": "2507.13334",
      "authors": [
        {
          "_id": "6879aad021b37e676c8e406b",
          "user": {
            "_id": "63120517ae8896941da4c5da",
            "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
            "isPro": false,
            "fullname": "Lingrui Mei",
            "user": "Chevalier",
            "type": "user"
          },
          "name": "Lingrui Mei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:58.423Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406c",
          "user": {
            "_id": "671f9cd9ff056a1b49444f37",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/B3Z9oiFb79Gi-_YXKP13u.png",
            "isPro": false,
            "fullname": "duoduo yao",
            "user": "Theodyy",
            "type": "user"
          },
          "name": "Jiayu Yao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:48.326Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406d",
          "user": {
            "_id": "656ad93853703dd78f3de7b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/r6VB6ICND1td3wFOy8pnz.jpeg",
            "isPro": false,
            "fullname": "YuyaoGe",
            "user": "YuyaoGe",
            "type": "user"
          },
          "name": "Yuyao Ge",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:54.689Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406e",
          "name": "Yiwei Wang",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e406f",
          "name": "Baolong Bi",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4070",
          "name": "Yujun Cai",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4071",
          "name": "Jiazhi Liu",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4072",
          "user": {
            "_id": "6720cf97a0396f933ec93ab8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NAdPIISe9-TYGGM0nAgsb.png",
            "isPro": false,
            "fullname": "Li Max",
            "user": "LImax72",
            "type": "user"
          },
          "name": "Mingyu Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:56.619Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4073",
          "name": "Zhong-Zhi Li",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4074",
          "user": {
            "_id": "662383a20edeabfe3b64a6a5",
            "avatarUrl": "/avatars/a76da726002d853dd08a51a6af6311d9.svg",
            "isPro": false,
            "fullname": "Duzhen Zhang",
            "user": "ShowerMaker",
            "type": "user"
          },
          "name": "Duzhen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:50.488Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4075",
          "user": {
            "_id": "669e27902dbf53ccd23ae47f",
            "avatarUrl": "/avatars/5c193b542dcfe2e64467fa5c686f3e20.svg",
            "isPro": false,
            "fullname": "chenlin",
            "user": "tvstfe",
            "type": "user"
          },
          "name": "Chenlin Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:52.410Z",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4076",
          "name": "Jiayi Mao",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4077",
          "name": "Tianze Xia",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4078",
          "name": "Jiafeng Guo",
          "hidden": false
        },
        {
          "_id": "6879aad021b37e676c8e4079",
          "name": "Shenghua Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:50:36.000Z",
      "submittedOnDailyAt": "2025-07-18T00:52:14.092Z",
      "title": "ラージェット言語モデルのコンテキスト工学の調査",
      "submittedOnDailyBy": {
        "_id": "63120517ae8896941da4c5da",
        "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
        "isPro": false,
        "fullname": "Lingrui Mei",
        "user": "Chevalier",
        "type": "user"
      },
      "summary": "LLMの性能は、推論時に提供されるコンテキスト情報に基づき根本的に決定される。本調査では、簡単なプロンプト設計を超え、LLMの情報負荷の系統的な最適化を統合する正式な学問である「コンテキスト工学」を紹介します。コンテキスト工学を基盤的な要素と複雑な実装に分解した詳細なカテゴリーを提案します。まず、基盤的な要素について検討します：コンテキスト検索と生成、コンテキスト処理と管理。次に、これらの要素が構造的に統合されて複雑なシステム実装を作成する方法について調査します：検索アウガーデド生成（RAG）、メモリシステムとツール統合論理、マルチアガントシステム。1300点以上の研究論文をシステマティックに分析した上で、本調査は、技術的なプログラムを構築し、コンテキストエンジニアリングによる先進的なモデルが複雑なコンテキストを理解するための優れた効能性を示しながら、同等の複雑な長文の生成において明らかな制限があることを明らかにします。この隙間を解決することは、将来的な研究の定義的な優先事項です。最終的に、この調査は、コンテキスト情報を読み取るAIの進展を支える研究者やエンジニアにとっての統一的なフレームワークを提供します。",
      "upvotes": 65,
      "discussionId": "6879aad021b37e676c8e407a",
      "githubRepo": "https://github.com/Meirtz/Awesome-Context-Engineering",
      "ai_summary": "Context Engineering systematically optimizes information payloads for Large Language Models, addressing gaps in generating sophisticated, long-form outputs.",
      "ai_keywords": [
        "Context Engineering",
        "context retrieval",
        "context generation",
        "context processing",
        "context management",
        "retrieval-augmented generation",
        "memory systems",
        "tool-integrated reasoning",
        "multi-agent systems"
      ],
      "githubStars": 164
    },
    "publishedAt": "2025-07-17T13:50:36.000Z",
    "title": "A Survey of Context Engineering for Large Language Models",
    "summary": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1300 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13334.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63120517ae8896941da4c5da",
      "avatarUrl": "/avatars/10e1be026035f3e24225e6782a710083.svg",
      "fullname": "Lingrui Mei",
      "name": "Chevalier",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.13348",
      "authors": [
        {
          "_id": "6879ba2021b37e676c8e40c9",
          "name": "Senqiao Yang",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40ca",
          "name": "Junyi Li",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40cb",
          "name": "Xin Lai",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40cc",
          "name": "Bei Yu",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40cd",
          "name": "Hengshuang Zhao",
          "hidden": false
        },
        {
          "_id": "6879ba2021b37e676c8e40ce",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:59:55.000Z",
      "submittedOnDailyAt": "2025-07-18T01:47:35.200Z",
      "title": "VisionThink: レイノルス学によるスマートなおよび効率的な視覚言語モデル",
      "submittedOnDailyBy": {
        "_id": "6527b7280ae663e384eb8499",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b7280ae663e384eb8499/73yF3eu2cUx7jVZrhXnXx.jpeg",
        "isPro": false,
        "fullname": "Senqiao Yang",
        "user": "Senqiao",
        "type": "user"
      },
      "summary": "最近の視覚言語モデル（VLMs）の進歩は、視覚トークンの数を増やし、それらが文脈トークンよりも長くなり、性能を向上させています。しかし、私たちは、ほとんどの本格的なシナリオでは、そのような長い視覚トークンが必要となっていません。OCR関連のタスクの一部において性能が大幅に低下することを見出しましたが、ほとんどの一般的なVQAタスクでは、1/4の解像度でも正確に動作します。そこで、私たちは、異なる解像度で異なるサンプルを動的に処理する新しいパラダイムを提案し、VisionThinkという視覚トークンの圧縮の新しい方法を提案します。これは、ダウンサンプリーされた画像から始め、問題解決に十分なものかどうかをスマートに判断します。そうでない場合、モデルは高解像度の画像を要求する特殊トークンを出力します。既存のEfficient VLM方法と比較して、VisionThinkは個別にトークンを圧縮するかどうかを自動的に判断します。OCR関連のタスクでは、この能力が強く、より簡単なタスクでは視覚トークンを大幅に削減できます。強化学習を用いて、LLM-as-Judgeの戦略を提案し、一般的なVQAタスクに実際的に応用しました。また、穩定した解像度のリサイズの呼び出し割合を達成するために、報酬関数と課金機構を謹めて設計しました。拡張検証は、私たちの方法の上位性、効率性、効果性を示しました。コードは、https://github.com/dvlab-research/VisionThinkにアクセスできます。",
      "upvotes": 38,
      "discussionId": "6879ba2121b37e676c8e40cf",
      "githubRepo": "https://github.com/dvlab-research/VisionThink",
      "ai_summary": "VisionThink dynamically adjusts image resolution and visual token processing for efficient and effective vision-language tasks, improving performance on OCR tasks while reducing token usage in simpler tasks.",
      "ai_keywords": [
        "vision-language models",
        "visual tokens",
        "text tokens",
        "downsampled image",
        "smart decision-making",
        "special token",
        "Efficient VLM",
        "token compression",
        "reinforcement learning",
        "LLM-as-Judge",
        "reward function",
        "penalty mechanism",
        "image resize call ratio"
      ],
      "githubStars": 25
    },
    "publishedAt": "2025-07-17T13:59:55.000Z",
    "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning",
    "summary": "Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13348.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6527b7280ae663e384eb8499",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b7280ae663e384eb8499/73yF3eu2cUx7jVZrhXnXx.jpeg",
      "fullname": "Senqiao Yang",
      "name": "Senqiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.13347",
      "authors": [
        {
          "_id": "6879b78a21b37e676c8e40b1",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b2",
          "name": "Jianjun Zhou",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b3",
          "name": "Haoyi Zhu",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b4",
          "name": "Wenzheng Chang",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b5",
          "name": "Yang Zhou",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b6",
          "name": "Zizun Li",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b7",
          "name": "Junyi Chen",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b8",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40b9",
          "name": "Chunhua Shen",
          "hidden": false
        },
        {
          "_id": "6879b78a21b37e676c8e40ba",
          "name": "Tong He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:59:53.000Z",
      "submittedOnDailyAt": "2025-07-18T01:26:57.410Z",
      "title": "π^3: 可揃える並び替え同値性の視覚的な幾何学的な学習",
      "submittedOnDailyBy": {
        "_id": "6747ede3a9c72aebe1322382",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/inILqQ05sESbYLdsEldJ_.png",
        "isPro": false,
        "fullname": "Tong He",
        "user": "tonghe90",
        "type": "user"
      },
      "summary": "pi^3は、決定的固定参照点からの依存性を破り、視覚的な幾何的再構造に新しいアプローチを提供する前向きニューラルネットワークです。先行の方法では、再構造は特定の視点に固定され、これは参照が最適でない場合に不穩定さや失敗を招く導入バイアスです。相反的に、pi^3は、参照フレームを使用しないように、完全な置換対称性のアーキテクチャを用いて、アフィン不変性のカメラの姿勢とスケール不変性の局所点マップを予測します。この設計は、入力の順番に固有の強固性を持ち、高度なスケーラビリティを持ちます。これらの優れた点をもつことで、我々の簡単でバイアスなしのアプローチは、撮影姿勢の推定、モノカラー/ビデオの測深、および密集点マップの再構造などの幅広いタスクで最先端の性能を収めます。コードとモデルは公開的に提供されています。",
      "upvotes": 30,
      "discussionId": "6879b78b21b37e676c8e40bb",
      "projectPage": "https://yyfz.github.io/pi3/",
      "githubRepo": "https://github.com/yyfz/Pi3",
      "ai_summary": "A permutation-equivariant neural network, $\\pi^3$, reconstructs visual geometry without a fixed reference view, achieving state-of-the-art performance in camera pose estimation, depth estimation, and point map reconstruction.",
      "ai_keywords": [
        "feed-forward neural network",
        "permutation-equivariant architecture",
        "affine-invariant",
        "scale-invariant",
        "camera pose estimation",
        "monocular depth estimation",
        "video depth estimation",
        "dense point map reconstruction"
      ],
      "githubStars": 104
    },
    "publishedAt": "2025-07-17T13:59:53.000Z",
    "title": "π^3: Scalable Permutation-Equivariant Visual Geometry Learning",
    "summary": "We introduce pi^3, a feed-forward neural network that offers a novel\napproach to visual geometry reconstruction, breaking the reliance on a\nconventional fixed reference view. Previous methods often anchor their\nreconstructions to a designated viewpoint, an inductive bias that can lead to\ninstability and failures if the reference is suboptimal. In contrast, pi^3\nemploys a fully permutation-equivariant architecture to predict\naffine-invariant camera poses and scale-invariant local point maps without any\nreference frames. This design makes our model inherently robust to input\nordering and highly scalable. These advantages enable our simple and bias-free\napproach to achieve state-of-the-art performance on a wide range of tasks,\nincluding camera pose estimation, monocular/video depth estimation, and dense\npoint map reconstruction. Code and models are publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13347.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6747ede3a9c72aebe1322382",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/inILqQ05sESbYLdsEldJ_.png",
      "fullname": "Tong He",
      "name": "tonghe90",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.13332",
      "authors": [
        {
          "_id": "6879b01621b37e676c8e40a8",
          "user": {
            "_id": "66214b4e4991d64ad0e28675",
            "avatarUrl": "/avatars/2574928aab7e45bc581c567d556a4cfd.svg",
            "isPro": false,
            "fullname": "Zhouqi Hua",
            "user": "ZhouqiHUA",
            "type": "user"
          },
          "name": "Zhouqi Hua",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:38.507Z",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40a9",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40aa",
          "name": "Chengqi Lyu",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ab",
          "user": {
            "_id": "6601196cc91ba4c08ad6e270",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
            "isPro": false,
            "fullname": "yuzhe gu",
            "user": "vanilla1116",
            "type": "user"
          },
          "name": "Yuzhe Gu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:36.203Z",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ac",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ad",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "6879b01621b37e676c8e40ae",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:50:07.000Z",
      "submittedOnDailyAt": "2025-07-18T00:59:46.053Z",
      "title": "「製造ゲーム：トーリングマシンの製造者は、長さに一般化可能です」",
      "submittedOnDailyBy": {
        "_id": "6601196cc91ba4c08ad6e270",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
        "isPro": false,
        "fullname": "yuzhe gu",
        "user": "vanilla1116",
        "type": "user"
      },
      "summary": "長さの一般化能力、即、トレーニング期間に見つからない長いシーケンスの問題を解決する能力は、Transformerベースの大規模な言語モデル（LLM）にとって核心的な課題です。既存の研究は主に数値計算や符号記号操作のデータ駆動アプローチに集中しており、これらのアプローチはタスク特有で全体的な性能が限定されています。より一般的な解決策を見つけるために、本研究はアルゴリズムが解くことができる可計算な理由の問題の広い場合を焦点としています。この観点から、本研究ではトーラインマシンの実行プロセスをコンピュータプログラムで模倣するコンジョニングのデータを統合し、トーラインマシンイマイション学習（TAIL）を提案して、LLMの長さの一般化能力を向上させることを目指しています。TAILは、コンジョニングを原子的状態に線形的に拡張し、短絡学習を軽減し、動的な長距離データアクセスの難易度を減らすために明示的なメモリフェッチメカニズムを導入しています。TAILの信頼性と普遍性を証明するために、8種類のアルゴリズムと18タスクを含む難しい合成データセットを構築しました。合成データのみを使用して、TAILはQwen2.5-7Bの長さの一般化能力と多様なタスクの性能を大幅に向上させ、先行の方法やDeepSeek-R1を超えました。実験結果から明らかになったことは、TAILの長さの一般化には、トーラインマシンの基本概念が、思考のスタイルよりも不可欠であることです。このモデルは、その注意層でトーラインマシンの特性に一致する読み書き行為を示しています。本研究は、LLMの理由の学習における将来の研究の望ましい方向を提供しています。",
      "upvotes": 30,
      "discussionId": "6879b01721b37e676c8e40af",
      "ai_summary": "TAIL, a method that imitates Turing Machine execution processes, enhances the length generalization and performance of LLMs by synthesizing chain-of-thought data and reducing shortcut learning.",
      "ai_keywords": [
        "Transformer-based large language models",
        "length generalization",
        "Turing MAchine Imitation Learning",
        "TAIL",
        "chain-of-thoughts",
        "Turing Machine",
        "synthetic dataset",
        "Qwen2.5-7B",
        "read-and-write behaviors",
        "attention layers"
      ]
    },
    "publishedAt": "2025-07-17T13:50:07.000Z",
    "title": "The Imitation Game: Turing Machine Imitator is Length Generalizable\n  Reasoner",
    "summary": "Length generalization, the ability to solve problems of longer sequences than\nthose observed during training, poses a core challenge of Transformer-based\nlarge language models (LLM). Although existing studies have predominantly\nfocused on data-driven approaches for arithmetic operations and symbolic\nmanipulation tasks, these approaches tend to be task-specific with limited\noverall performance. To pursue a more general solution, this paper focuses on a\nbroader case of reasoning problems that are computable, i.e., problems that\nalgorithms can solve, thus can be solved by the Turing Machine. From this\nperspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to\nimprove the length generalization ability of LLMs. TAIL synthesizes\nchain-of-thoughts (CoT) data that imitate the execution process of a Turing\nMachine by computer programs, which linearly expands the reasoning steps into\natomic states to alleviate shortcut learning and explicit memory fetch\nmechanism to reduce the difficulties of dynamic and long-range data access in\nelementary operations. To validate the reliability and universality of TAIL, we\nconstruct a challenging synthetic dataset covering 8 classes of algorithms and\n18 tasks. Without bells and whistles, TAIL significantly improves the length\ngeneralization ability as well as the performance of Qwen2.5-7B on various\ntasks using only synthetic data, surpassing previous methods and DeepSeek-R1.\nThe experimental results reveal that the key concepts in the Turing Machine,\ninstead of the thinking styles, are indispensable for TAIL for length\ngeneralization, through which the model exhibits read-and-write behaviors\nconsistent with the properties of the Turing Machine in their attention layers.\nThis work provides a promising direction for future research in the learning of\nLLM reasoning from synthetic data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13332.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12841",
      "authors": [
        {
          "_id": "6879bba621b37e676c8e4195",
          "name": "Yiming Ren",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4196",
          "name": "Zhiqiang Lin",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4197",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4198",
          "name": "Gao Meng",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e4199",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419a",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419b",
          "name": "Zicheng Lin",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419c",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419d",
          "name": "Yujiu Yang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419e",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "6879bba621b37e676c8e419f",
          "user": {
            "_id": "642e3bcb958faf258a40e89c",
            "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
            "isPro": false,
            "fullname": "Ruihang Chu",
            "user": "Ruihang",
            "type": "user"
          },
          "name": "Ruihang Chu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:30.538Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T07:04:05.000Z",
      "submittedOnDailyAt": "2025-07-18T02:04:48.913Z",
      "title": "AnyCapプロジェクト：一つの統一的なフレームワーク、データセット、ベンチマークですが、制御可能な全方位カプチング",
      "submittedOnDailyBy": {
        "_id": "642e3bcb958faf258a40e89c",
        "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
        "isPro": false,
        "fullname": "Ruihang Chu",
        "user": "Ruihang",
        "type": "user"
      },
      "summary": "制御可能なキャピティングは、精密な多モデルアライメントと指示従いに必要なものですが、現在のモデルは通常、細かい制御と信頼性のある評価プロトコルが不足しています。この空間を填ぐために、私たちはAnyCapプロジェクトを紹介します。これはモデル、データセット、評価をまとめた統合的な解決策です。AnyCapModel（ACM）を紹介します。これは軽量フレームワークであり、現在の基盤モデルの制御可能性を向上させるために再学習を避けることができます。ACMは、基盤モデルの元のキャピティングを再利用し、ユーザ指示とモデル特徴を組み合わせて改善されたキャピティングを生成します。制御可能な多モデルキャピティングのデータ不足を補い、AnyCapDataset（ACD）を構築しました。これは3つのモデル、28つのユーザ指示タイプ、300kの高品質データ入力を収録しています。また、AnyCapEvalを提案しました。これは、内容の正確性とスタイルの忠実性を分離して、制御可能なキャピティングの信頼性のある評価指標を提供します。ACMは、AnyCapEvalでの多様な基盤モデルのキャピティング質を明らかに向上させます。特に、ACM-8BはGPT-4oの内容スコアを45%上げ、スタイルスコアを12%上げ、MIA-BenchとVidCapBenchなどの一般的なベンチマークでも大幅な効果を収めました。",
      "upvotes": 27,
      "discussionId": "6879bba721b37e676c8e41a0",
      "githubRepo": "https://github.com/qishisuren123/AnyCap",
      "ai_summary": "The AnyCap Project introduces a framework, dataset, and evaluation protocol to enhance controllability and reliability in multimodal captioning.",
      "ai_keywords": [
        "AnyCapModel",
        "ACM",
        "omni-modal captioning",
        "AnyCapDataset",
        "ACD",
        "AnyCapEval",
        "content accuracy",
        "stylistic fidelity",
        "MIA-Bench",
        "VidCapBench"
      ],
      "githubStars": 27
    },
    "publishedAt": "2025-07-17T03:04:05.000Z",
    "title": "AnyCap Project: A Unified Framework, Dataset, and Benchmark for\n  Controllable Omni-modal Captioning",
    "summary": "Controllable captioning is essential for precise multimodal alignment and\ninstruction following, yet existing models often lack fine-grained control and\nreliable evaluation protocols. To address this gap, we present the AnyCap\nProject, an integrated solution spanning model, dataset, and evaluation. We\nintroduce AnyCapModel (ACM), a lightweight plug-and-play framework that\nenhances the controllability of existing foundation models for omni-modal\ncaptioning without retraining the base model. ACM reuses the original captions\nfrom base models while incorporating user instructions and modality features to\ngenerate improved captions. To remedy the data scarcity in controllable\nmultimodal captioning, we build AnyCapDataset (ACD), covering three modalities,\n28 user-instruction types, and 300\\,k high-quality data entries. We further\npropose AnyCapEval, a new benchmark that provides more reliable evaluation\nmetrics for controllable captioning by decoupling content accuracy and\nstylistic fidelity. ACM markedly improves caption quality across a diverse set\nof base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores\nby 45\\% and style scores by 12\\%, and it also achieves substantial gains on\nwidely used benchmarks such as MIA-Bench and VidCapBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12841.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e3bcb958faf258a40e89c",
      "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
      "fullname": "Ruihang Chu",
      "name": "Ruihang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.13344",
      "authors": [
        {
          "_id": "6879f3aa21b37e676c8e4202",
          "user": {
            "_id": "649958942ca6f96c8b8c1076",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649958942ca6f96c8b8c1076/olfLIqNryaog1nAnQPwkN.jpeg",
            "isPro": false,
            "fullname": "Yudong Jin",
            "user": "krahets",
            "type": "user"
          },
          "name": "Yudong Jin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:22.520Z",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4203",
          "name": "Sida Peng",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4204",
          "name": "Xuan Wang",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4205",
          "name": "Tao Xie",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4206",
          "name": "Zhen Xu",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4207",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4208",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e4209",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "6879f3aa21b37e676c8e420a",
          "name": "Xiaowei Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649958942ca6f96c8b8c1076/G5xpU1DSr8smrbqbom9hU.mp4"
      ],
      "publishedAt": "2025-07-17T17:59:17.000Z",
      "submittedOnDailyAt": "2025-07-18T06:34:43.522Z",
      "title": "Diffuman4D: 4D一致的人間視点合成従稀少ビュービデオ\nスペクトラルタイムディフューションモデルを用いて",
      "submittedOnDailyBy": {
        "_id": "649958942ca6f96c8b8c1076",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649958942ca6f96c8b8c1076/olfLIqNryaog1nAnQPwkN.jpeg",
        "isPro": false,
        "fullname": "Yudong Jin",
        "user": "krahets",
        "type": "user"
      },
      "summary": "この論文は、稀少なビデオを入力として、人間の高品質な視点合成の挑戦を課題としています。先行の方法は、4D ディフュージョンモデルを利用して新しい視点でのビデオを生成して、観察不足の問題を解決しています。しかし、これらのモデルから生成されるビデオは、スペクトラルタイム的一致性を欠くことが多いため、視点合成の品質が低下します。本論文では、4D ディフュージョンモデルのスペクトラルタイム的一致性を高めるために、新しいスライディングイテレーショナルなデノイズプロセスを提案しています。具体的には、各ビデオ、カメラの姿勢、人間の姿勢を含む潜在グリッドを定義し、スペクトラルタイムの両方にスライディングウィンドウを使用してデノイズを交換し、最終的に目標の視点からのビデオをデコードします。イテレーション的なスライディングでは、情報が潜在グリッドの幅に十分に流れ、ディフュージョンモデルが大きな受容野を得ることができ、出力の4D一致性を高め、同時にGPUのメモリ消費を許容することができます。DNA-RenderingおよびActorsHQデータセットの実験は、私たちの方法が高品質で一貫した新視点ビデオを合成でき、現在の手法を大幅に超えることを示しています。プロジェクトページには、相互作用デモとビデオ結果を見ることができます：https://diffuman4d.github.io/。",
      "upvotes": 18,
      "discussionId": "6879f3ab21b37e676c8e420b",
      "projectPage": "https://diffuman4d.github.io/",
      "githubRepo": "https://github.com/zju3dv/Diffuman4D",
      "ai_summary": "A sliding iterative denoising process is proposed to enhance spatio-temporal consistency in 4D diffusion models for high-fidelity view synthesis from sparse-view videos.",
      "ai_keywords": [
        "4D diffusion models",
        "sliding iterative denoising",
        "latent grid",
        "image",
        "camera pose",
        "human pose",
        "spatio-temporal consistency",
        "GPU memory consumption",
        "DNA-Rendering",
        "ActorsHQ"
      ],
      "githubStars": 48
    },
    "publishedAt": "2025-07-17T13:59:17.000Z",
    "title": "Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos\n  with Spatio-Temporal Diffusion Models",
    "summary": "This paper addresses the challenge of high-fidelity view synthesis of humans\nwith sparse-view videos as input. Previous methods solve the issue of\ninsufficient observation by leveraging 4D diffusion models to generate videos\nat novel viewpoints. However, the generated videos from these models often lack\nspatio-temporal consistency, thus degrading view synthesis quality. In this\npaper, we propose a novel sliding iterative denoising process to enhance the\nspatio-temporal consistency of the 4D diffusion model. Specifically, we define\na latent grid in which each latent encodes the image, camera pose, and human\npose for a certain viewpoint and timestamp, then alternately denoising the\nlatent grid along spatial and temporal dimensions with a sliding window, and\nfinally decode the videos at target viewpoints from the corresponding denoised\nlatents. Through the iterative sliding, information flows sufficiently across\nthe latent grid, allowing the diffusion model to obtain a large receptive field\nand thus enhance the 4D consistency of the output, while making the GPU memory\nconsumption affordable. The experiments on the DNA-Rendering and ActorsHQ\ndatasets demonstrate that our method is able to synthesize high-quality and\nconsistent novel-view videos and significantly outperforms the existing\napproaches. See our project page for interactive demos and video results:\nhttps://diffuman4d.github.io/ .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649958942ca6f96c8b8c1076/G5xpU1DSr8smrbqbom9hU.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13344.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649958942ca6f96c8b8c1076",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649958942ca6f96c8b8c1076/olfLIqNryaog1nAnQPwkN.jpeg",
      "fullname": "Yudong Jin",
      "name": "krahets",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12508",
      "authors": [
        {
          "_id": "6879af4f21b37e676c8e409b",
          "user": {
            "_id": "65e919332fd9300c7eb96556",
            "avatarUrl": "/avatars/a826f7e14acf34603aa68e4fc27f12af.svg",
            "isPro": false,
            "fullname": "Yuncong Yang",
            "user": "yyuncong",
            "type": "user"
          },
          "name": "Yuncong Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:42.036Z",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409c",
          "name": "Jiageng Liu",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409d",
          "name": "Zheyuan Zhang",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409e",
          "name": "Siyuan Zhou",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e409f",
          "name": "Reuben Tan",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e40a0",
          "name": "Jianwei Yang",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e40a1",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "6879af4f21b37e676c8e40a2",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65e919332fd9300c7eb96556/sRj5beNxspccadVwKC_vj.mp4"
      ],
      "publishedAt": "2025-07-16T17:59:36.000Z",
      "submittedOnDailyAt": "2025-07-18T00:56:28.122Z",
      "title": "ミンドジョurney: ワールドモデルを用いた検証時スケーリングによる空間論理",
      "submittedOnDailyBy": {
        "_id": "65e919332fd9300c7eb96556",
        "avatarUrl": "/avatars/a826f7e14acf34603aa68e4fc27f12af.svg",
        "isPro": false,
        "fullname": "Yuncong Yang",
        "user": "yyuncong",
        "type": "user"
      },
      "summary": "3D空間の空間的推理は、人類の認知の中心的な部分であり、歩行可能なタスクのようにナビゲーションや操作に欠かせない。しかし、最先端の視覚言語モデル（VLMs）は、自動視点の動きの後のスケーブルさを予測するような簡単なタスクでも苦労します：2D画像を見るだけで、3D動力学の内部モデルを持っていません。そこで、私たちは、ビデオディフュージョンに基づく制御可能な世界モデルと結合し、この欠けた能力をVLMに提供するためのテストタイムスケーリングフレームワーク「MindJourney」を提案します。VLMは、世界モデルが各ステップで合成した相対的な視点を繰り返し描画し、その間に交互的な探索で集められた多点の証拠によって理由をつくります。ファイナルチューニングを行わずに、我々のMindJourneyは代表的な空間的推理ベンチマークSATで平均8%以上の性能向上を達成し、VLMと世界モデルの組み合わせでテストタイムスケーリングを行うことが簡単でプラグとプレイングのルートとして強固な3D推理を可能にすることを示します。また、我々の方法は、強化学習でトレーニングされたテストタイム推論VLMsを改善し、世界モデルをテストタイムスケーリングに利用する方法の可能性を示します。",
      "upvotes": 10,
      "discussionId": "6879af5021b37e676c8e40a3",
      "projectPage": "https://umass-embodied-agi.github.io/MindJourney/",
      "githubRepo": "https://github.com/UMass-Embodied-AGI/MindJourney",
      "ai_summary": "MindJourney enhances vision-language models with 3D reasoning by coupling them with a video diffusion-based world model, achieving improved performance on spatial reasoning tasks without fine-tuning.",
      "ai_keywords": [
        "vision-language models",
        "VLMs",
        "world model",
        "video diffusion",
        "camera trajectory",
        "multi-view evidence",
        "spatial reasoning",
        "SAT benchmark",
        "reinforcement learning"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-07-16T13:59:36.000Z",
    "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning",
    "summary": "Spatial reasoning in 3D space is central to human cognition and indispensable\nfor embodied tasks such as navigation and manipulation. However,\nstate-of-the-art vision-language models (VLMs) struggle frequently with tasks\nas simple as anticipating how a scene will look after an egocentric motion:\nthey perceive 2D images but lack an internal model of 3D dynamics. We therefore\npropose MindJourney, a test-time scaling framework that grants a VLM with this\nmissing capability by coupling it to a controllable world model based on video\ndiffusion. The VLM iteratively sketches a concise camera trajectory, while the\nworld model synthesizes the corresponding view at each step. The VLM then\nreasons over this multi-view evidence gathered during the interactive\nexploration. Without any fine-tuning, our MindJourney achieves over an average\n8% performance boost on the representative spatial reasoning benchmark SAT,\nshowing that pairing VLMs with world models for test-time scaling offers a\nsimple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also\nimproves upon the test-time inference VLMs trained through reinforcement\nlearning, which demonstrates the potential of our method that utilizes world\nmodels for test-time scaling.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65e919332fd9300c7eb96556/sRj5beNxspccadVwKC_vj.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e919332fd9300c7eb96556",
      "avatarUrl": "/avatars/a826f7e14acf34603aa68e4fc27f12af.svg",
      "fullname": "Yuncong Yang",
      "name": "yyuncong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.13300",
      "authors": [
        {
          "_id": "6879c27e21b37e676c8e41a9",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41aa",
          "name": "Weiyuan Chen",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ab",
          "name": "Zhijian Xu",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ac",
          "name": "Manasi Patwardhan",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ad",
          "name": "Yixin Liu",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41ae",
          "name": "Chengye Wang",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41af",
          "name": "Lovekesh Vig",
          "hidden": false
        },
        {
          "_id": "6879c27e21b37e676c8e41b0",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T17:09:22.000Z",
      "submittedOnDailyAt": "2025-07-18T02:12:09.802Z",
      "title": "AbGen: 科学研究の消去試験デザインと評価における大規模言語モデルの評価",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "ここでは、科学研究の消滅調査設計能力を評価するための最初のベンチマーク「AbGen」を紹介します。AbGenは、807篇のNLP論文からの1,500件の専門家注釈された例を構成しています。このベンチマークでは、与えられた研究コンテキストに基づいて特定のモジュールまたはプロセスの詳細な消滅調査設計を生成することをLLMの課題としております。DeepSeek-R1-0528とo4-miniなどの先進的なLLMの評価により、消滅調査設計の重要性、忠実性、そして妥当性においてモデルと人間の専門家の間では明らかに性能の間違いがあります。また、現在の自動評価方法は、人間の評価と比較して明らかに信頼性がありません。これをより詳細に調査するために、自動評価システムの信頼性を評価するためのメタ評価ベンチマーク「AbGen-Eval」を開発しました。AbGen-Evalでは、複雑な科学タスクのLLMの性能を測定する通常の自動評価システムの信頼性を評価するために、各種のLLM-as-Judgeシステムを調査し、将来の研究においてより有效率で信頼性のあるLLMベースの評価システムの開発につながるようなヒントを提供します。",
      "upvotes": 9,
      "discussionId": "6879c27f21b37e676c8e41b1",
      "githubRepo": "https://github.com/yale-nlp/AbGen",
      "ai_summary": "AbGen evaluates LLMs in designing ablation studies for scientific research, revealing performance gaps compared to human experts and highlighting the unreliability of current automated evaluation methods.",
      "ai_keywords": [
        "LLMs",
        "ablation studies",
        "NLP papers",
        "DeepSeek-R1-0528",
        "o4-mini",
        "AbGen-Eval",
        "LLM-as-Judge"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-07-17T13:09:22.000Z",
    "title": "AbGen: Evaluating Large Language Models in Ablation Study Design and\n  Evaluation for Scientific Research",
    "summary": "We introduce AbGen, the first benchmark designed to evaluate the capabilities\nof LLMs in designing ablation studies for scientific research. AbGen consists\nof 1,500 expert-annotated examples derived from 807 NLP papers. In this\nbenchmark, LLMs are tasked with generating detailed ablation study designs for\na specified module or process based on the given research context. Our\nevaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a\nsignificant performance gap between these models and human experts in terms of\nthe importance, faithfulness, and soundness of the ablation study designs.\nMoreover, we demonstrate that current automated evaluation methods are not\nreliable for our task, as they show a significant discrepancy when compared to\nhuman assessment. To better investigate this, we develop AbGen-Eval, a\nmeta-evaluation benchmark designed to assess the reliability of commonly used\nautomated evaluation systems in measuring LLM performance on our task. We\ninvestigate various LLM-as-Judge systems on AbGen-Eval, providing insights for\nfuture research on developing more effective and reliable LLM-based evaluation\nsystems for complex scientific tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13300.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.12956",
      "authors": [
        {
          "_id": "6879df6521b37e676c8e41cd",
          "user": {
            "_id": "653b195c5f1703225b2fd571",
            "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
            "isPro": false,
            "fullname": "wangqiang",
            "user": "wangqiang9",
            "type": "user"
          },
          "name": "Qiang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T07:49:24.746Z",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41ce",
          "name": "Mengchao Wang",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41cf",
          "name": "Fan Jiang",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41d0",
          "name": "Yaqi Fan",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41d1",
          "name": "Yonggang Qi",
          "hidden": false
        },
        {
          "_id": "6879df6521b37e676c8e41d2",
          "name": "Mu Xu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/653b195c5f1703225b2fd571/vkwYxJilAke5PetjLV8lm.mp4"
      ],
      "publishedAt": "2025-07-17T09:50:43.000Z",
      "submittedOnDailyAt": "2025-07-18T04:28:50.046Z",
      "title": "ファンタジーポートレット：表現を増強したディフュージョントランスフォーマターを用いた多キャラクターポートレットアニメーションの向上",
      "submittedOnDailyBy": {
        "_id": "653b195c5f1703225b2fd571",
        "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
        "isPro": false,
        "fullname": "wangqiang",
        "user": "wangqiang9",
        "type": "user"
      },
      "summary": "静止画から表現的な顔のアニメーションを生成することは難しい任務です。先行の方法は明示的な幾何学的な先入観（例：顔のランドマークや3DMM）を基にしていることで、再演出の際にアーティファクトが発生し、微妙な感情を捉えやすくなりません。また、現在のアプローチは多キャラクターアニメーションのサポートを持ちません。異なる個体からのドライバーフィーチャーが相互に干渉し、課題を複雑化します。これらの課題に対処するために、私たちはFantasyPortraitを提案します。これは、高品質な感情豊富なアニメーションを生成できる、拡散チャネルベースのフレームワークです。私たちの方法は、表現を拡張した学習戦略を導入し、隠れ表現を利用して、顔の動作を捉え、モデルの感情の詳細表現能力を向上させます。多キャラクターの制御において、私たちはマスク付きクロスアテンション機構を設計し、独立しながらコオライドした表現生成を確保し、特に特徴の干渉を防ぎます。この分野の研究に貢献するために、私たちはMulti-ExprデータセットとExprBenchを提案します。これらは、多キャラクターのポートレイトアニメーションの訓練と評価に特に設計されたデータセットとベンチマークです。拡張した実験は、FantasyPortraitが状態の最先端の方法を超え、定量的評価と定性的評価で特に優れていることを示し、再演出と多キャラクターの複雑なコンテキストで特に優れています。私たちのプロジェクトページはhttps://fantasy-amap.github.io/fantasy-portrait/です。",
      "upvotes": 9,
      "discussionId": "6879df6521b37e676c8e41d3",
      "projectPage": "https://fantasy-amap.github.io/fantasy-portrait/",
      "githubRepo": "https://github.com/Fantasy-AMAP/fantasy-portrait",
      "ai_summary": "FantasyPortrait, a diffusion transformer framework, generates high-fidelity and emotion-rich facial animations for single and multi-character scenarios using implicit representations and a masked cross-attention mechanism.",
      "ai_keywords": [
        "diffusion transformer",
        "expression-augmented learning",
        "implicit representations",
        "masked cross-attention mechanism",
        "Multi-Expr dataset",
        "ExprBench"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-07-17T05:50:43.000Z",
    "title": "FantasyPortrait: Enhancing Multi-Character Portrait Animation with\n  Expression-Augmented Diffusion Transformers",
    "summary": "Producing expressive facial animations from static images is a challenging\ntask. Prior methods relying on explicit geometric priors (e.g., facial\nlandmarks or 3DMM) often suffer from artifacts in cross reenactment and\nstruggle to capture subtle emotions. Furthermore, existing approaches lack\nsupport for multi-character animation, as driving features from different\nindividuals frequently interfere with one another, complicating the task. To\naddress these challenges, we propose FantasyPortrait, a diffusion transformer\nbased framework capable of generating high-fidelity and emotion-rich animations\nfor both single- and multi-character scenarios. Our method introduces an\nexpression-augmented learning strategy that utilizes implicit representations\nto capture identity-agnostic facial dynamics, enhancing the model's ability to\nrender fine-grained emotions. For multi-character control, we design a masked\ncross-attention mechanism that ensures independent yet coordinated expression\ngeneration, effectively preventing feature interference. To advance research in\nthis area, we propose the Multi-Expr dataset and ExprBench, which are\nspecifically designed datasets and benchmarks for training and evaluating\nmulti-character portrait animations. Extensive experiments demonstrate that\nFantasyPortrait significantly outperforms state-of-the-art methods in both\nquantitative metrics and qualitative evaluations, excelling particularly in\nchallenging cross reenactment and multi-character contexts. Our project page is\nhttps://fantasy-amap.github.io/fantasy-portrait/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/653b195c5f1703225b2fd571/vkwYxJilAke5PetjLV8lm.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12956.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b195c5f1703225b2fd571",
      "avatarUrl": "/avatars/b7f376225cef6c13952c9c5540dd43be.svg",
      "fullname": "wangqiang",
      "name": "wangqiang9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12990",
      "authors": [
        {
          "_id": "6879fdc821b37e676c8e422b",
          "name": "Nikita Koriagin",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422c",
          "name": "Yaroslav Aksenov",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422d",
          "user": {
            "_id": "634c5f8cfb80cc6bcaf42c03",
            "avatarUrl": "/avatars/1f37db0e70cbaf9707f4c8cbcee37ca0.svg",
            "isPro": false,
            "fullname": "Daniil Laptev",
            "user": "dlaptev",
            "type": "user"
          },
          "name": "Daniil Laptev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T08:32:16.310Z",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422e",
          "name": "Gleb Gerasimov",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e422f",
          "user": {
            "_id": "60b364e7f88532cd79eaff7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
            "isPro": false,
            "fullname": "Nikita Balagansky",
            "user": "elephantmipt",
            "type": "user"
          },
          "name": "Nikita Balagansky",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T08:32:14.730Z",
          "hidden": false
        },
        {
          "_id": "6879fdc821b37e676c8e4230",
          "user": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "isPro": false,
            "fullname": "Daniil Gavrilov",
            "user": "kefirski",
            "type": "user"
          },
          "name": "Daniil Gavrilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-18T08:32:17.917Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60b364e7f88532cd79eaff7b/puU_3A7T0uk5E0fGkoc1k.png"
      ],
      "publishedAt": "2025-07-17T10:57:49.000Z",
      "submittedOnDailyAt": "2025-07-18T06:31:25.084Z",
      "title": "Teach Old SAEs New Domain Tricks with Boosting",
      "submittedOnDailyBy": {
        "_id": "60b364e7f88532cd79eaff7b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
        "isPro": false,
        "fullname": "Nikita Balagansky",
        "user": "elephantmipt",
        "type": "user"
      },
      "summary": "Sparse Autoencodersは、Large Language Modelsの内部表現を解釈するための強力なツールとして現れたが、それらは通常、訓練データコーパスに特徴的な特徴を捉えずに失敗する。本論文では、この特徴の見落としを解決するために残差学習アプローチを提案し、完全な再訓練が必要とならないようにする。次のSAEを特に訓練することを提案し、事前学習されたSAEの特徴的な文章に対する再構成誤差をモデル化し、主なモデルが見落とした特徴を有効に捉えることができる。推論時に両モデルの出力を合計することで、LLMの交差エントロピーと説明可能な変異メトリックにおいて显著な改善を示す。実験結果から、この方法は既存のSAEに新しいドライブンスキーを効率的に組み込みながら、一般的なタスクに対する性能を維持することを示している。このアプローチは、特定のドライブンスキーに適した機械的な解釈性を選択的に向上させることを可能にし、LLMの標的的な機械的な解釈性の新しい可能性を開拓する。",
      "upvotes": 4,
      "discussionId": "6879fdc821b37e676c8e4231",
      "ai_summary": "A residual learning approach enhances Sparse Autoencoders to capture domain-specific features without retraining, improving interpretability and performance on specialized domains.",
      "ai_keywords": [
        "Sparse Autoencoders",
        "residual learning",
        "reconstruction error",
        "cross-entropy",
        "explained variance",
        "targeted mechanistic interpretability",
        "Large Language Models"
      ]
    },
    "publishedAt": "2025-07-17T06:57:49.000Z",
    "title": "Teach Old SAEs New Domain Tricks with Boosting",
    "summary": "Sparse Autoencoders have emerged as powerful tools for interpreting the\ninternal representations of Large Language Models, yet they often fail to\ncapture domain-specific features not prevalent in their training corpora. This\npaper introduces a residual learning approach that addresses this feature\nblindness without requiring complete retraining. We propose training a\nsecondary SAE specifically to model the reconstruction error of a pretrained\nSAE on domain-specific texts, effectively capturing features missed by the\nprimary model. By summing the outputs of both models during inference, we\ndemonstrate significant improvements in both LLM cross-entropy and explained\nvariance metrics across multiple specialized domains. Our experiments show that\nthis method efficiently incorporates new domain knowledge into existing SAEs\nwhile maintaining their performance on general tasks. This approach enables\nresearchers to selectively enhance SAE interpretability for specific domains of\ninterest, opening new possibilities for targeted mechanistic interpretability\nof LLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60b364e7f88532cd79eaff7b/puU_3A7T0uk5E0fGkoc1k.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60b364e7f88532cd79eaff7b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
      "fullname": "Nikita Balagansky",
      "name": "elephantmipt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12720",
      "authors": [
        {
          "_id": "68799d0521b37e676c8e4060",
          "name": "Abraham Toluase Owodunni",
          "hidden": false
        },
        {
          "_id": "68799d0521b37e676c8e4061",
          "name": "Orevaoghene Ahia",
          "hidden": false
        },
        {
          "_id": "68799d0521b37e676c8e4062",
          "name": "Sachin Kumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-17T01:55:41.000Z",
      "submittedOnDailyAt": "2025-07-18T00:36:57.429Z",
      "title": "FLEXITOKENS: 変化する言語モデル向けの柔軟なトークナイション",
      "submittedOnDailyBy": {
        "_id": "626d1e1e72169e781945bf44",
        "avatarUrl": "/avatars/6bf9f35042b6d939f2ab525816ad0423.svg",
        "isPro": false,
        "fullname": "Abraham  Owodunni",
        "user": "Owos",
        "type": "user"
      },
      "summary": "言語モデル（LMs）は、簡単な微調製で新しいデータ分布に適応することが難しい。これは、そのサブウォードトークナイザーの剛性によることです。この不変性は、通常、適応期間中変化しないため、この不変性は、新しいデータ分布、見たことのない言語、またはスクリプトの過度分割による無効なトークナイゼーションを招くことが多い。本研究では、学習可能なトークナイザーを持つバイトレベルのLMsを開発し、トークナイゼーションを適応可能にします。モデルには、入力バイトシーケンスの境界を予測するサブモジュールが含まれ、それを変長セグメントに変換します。現在のトークナイザー無しの方法は、トークナイザーの境界予測を補助損失を使用して訓練し、トレーニングコーパス全体で固定の圧縮率を強制し、新しい種類の剛性を引き起こします。FLEXITOKENSを提案します。FLEXITOKENSは、適応期間中で大幅に高まる柔軟性を課題とします。複数の多言語ベンチマーク、構造的に多様なタスク、データ領域を検証し、FLEXITOKENSは、サブウォードトークナイザーや他の勾配ベースのトークナイザーと比較して、トークンの過度分割を絶えずに減少し、下流タスクの性能については10％程度の向上を実現します。実験のコードとデータは、https://github.com/owos/flexitokens からリリースされます。",
      "upvotes": 4,
      "discussionId": "68799d0521b37e676c8e4063",
      "ai_summary": "FLEXITOKENS, a byte-level language model with a learnable tokenizer, reduces token over-fragmentation and improves performance across multilingual and morphologically diverse tasks.",
      "ai_keywords": [
        "byte-level LMs",
        "learnable tokenizers",
        "boundary predictor",
        "FLEXITOKENS",
        "token over-fragmentation",
        "subword tokenizers",
        "gradient-based tokenizers"
      ]
    },
    "publishedAt": "2025-07-16T21:55:41.000Z",
    "title": "FLEXITOKENS: Flexible Tokenization for Evolving Language Models",
    "summary": "Language models (LMs) are challenging to adapt to new data distributions by\nsimple finetuning. This is due to the rigidity of their subword tokenizers,\nwhich typically remain unchanged during adaptation. This inflexibility often\nleads to inefficient tokenization, causing overfragmentation of\nout-of-distribution domains, unseen languages, or scripts. In this work, we\ndevelop byte-level LMs with learnable tokenizers to make tokenization adaptive.\nOur models include a submodule that learns to predict boundaries between the\ninput byte sequence, encoding it into variable-length segments. Existing\ntokenizer-free methods train this boundary predictor using an auxiliary loss\nthat enforces a fixed compression rate across the training corpus, introducing\na new kind of rigidity. We propose FLEXITOKENS, a simplified training objective\nthat enables significantly greater flexibility during adaptation. Evaluating\nacross multiple multilingual benchmarks, morphologically diverse tasks, and\ndomains, we demonstrate that FLEXITOKENS consistently reduces token\nover-fragmentation and achieves up to 10\\% improvements on downstream task\nperformance compared to subword and other gradient-based tokenizers. Code and\ndata for our experiments will be released at\nhttps://github.com/owos/flexitokens",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "626d1e1e72169e781945bf44",
      "avatarUrl": "/avatars/6bf9f35042b6d939f2ab525816ad0423.svg",
      "fullname": "Abraham  Owodunni",
      "name": "Owos",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04984",
      "authors": [
        {
          "_id": "68783633001546c83aa4f928",
          "user": {
            "_id": "67abb26debe64eaa3a624bd7",
            "avatarUrl": "/avatars/39259308f5aeda7f80a5489335554913.svg",
            "isPro": false,
            "fullname": "Zonglin Lyu",
            "user": "ucfzl",
            "type": "user"
          },
          "name": "Zonglin Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:35.041Z",
          "hidden": false
        },
        {
          "_id": "68783633001546c83aa4f929",
          "name": "Chen Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T13:25:32.000Z",
      "submittedOnDailyAt": "2025-07-18T01:45:16.974Z",
      "title": "TLB-VFI: 時間認識潜在ブラウンブリッジ拡散法のビデオフレームインタープロープション",
      "submittedOnDailyBy": {
        "_id": "67abb26debe64eaa3a624bd7",
        "avatarUrl": "/avatars/39259308f5aeda7f80a5489335554913.svg",
        "isPro": false,
        "fullname": "Zonglin Lyu",
        "user": "ucfzl",
        "type": "user"
      },
      "summary": "ビデオフレームインタープレーション（VFI）は、2つの連続する隣接フレームI_0とI_1に基づいて、時間tと記号負荷を回避するためにnを使用して、中間フレームI_nを予測することを目的としています。最近のアプローチは、この任務で画像基底およびビデオ基底の拡散モデルを適用し、強力な性能を達成しています。しかし、画像基底の拡散モデルは時間情報を抽出できないことにより、非拡散方法と比較して相対的にディープです。ビデオ基底の拡散モデルは時間情報を抽出できることにより、それらは学習スケール、モデルサイズ、推論時間において過大なものとなります。これらの問題を軽減するために、私たちは、3D-wavelet gatingと時間情報を知る自動エンコーダーを提案したTemporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation（TLB-VFI）を提案します。この方法では、ビデオ入力から豊富な時間情報を抽出し、最近の画像基底の拡散モデルのSOTAに比べてFIDに20%の改善を達成します。また、豊富な時間情報の存在により、この方法はパラメータ数が3倍少なくても強力な性能を達成します。このようなパラメータ数の減少は2.3倍のスピードアップを収得します。オープンカラーグイドライングを採用することで、この方法は9000倍少ない学習データを必要とし、ビデオ基底の拡散モデルに比べて20倍少ないパラメータ数を達成します。コードと結果は、以下のプロジェクトページにアクセス可能です：https://zonglinl.github.io/tlbvfi_page。",
      "upvotes": 4,
      "discussionId": "68783634001546c83aa4f92a",
      "projectPage": "https://zonglinl.github.io/tlbvfi_page/",
      "githubRepo": "https://github.com/ZonglinL/TLBVFI",
      "ai_summary": "Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) improves video frame interpolation by efficiently extracting temporal information, reducing parameters, and requiring less training data compared to existing methods.",
      "ai_keywords": [
        "diffusion models",
        "video frame interpolation",
        "temporal information",
        "3D-wavelet gating",
        "temporal-aware autoencoder",
        "FID",
        "optical flow guidance"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-07-07T09:25:32.000Z",
    "title": "TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame\n  Interpolation",
    "summary": "Video Frame Interpolation (VFI) aims to predict the intermediate frame I_n\n(we use n to denote time in videos to avoid notation overload with the timestep\nt in diffusion models) based on two consecutive neighboring frames I_0 and\nI_1. Recent approaches apply diffusion models (both image-based and\nvideo-based) in this task and achieve strong performance. However, image-based\ndiffusion models are unable to extract temporal information and are relatively\ninefficient compared to non-diffusion methods. Video-based diffusion models can\nextract temporal information, but they are too large in terms of training\nscale, model size, and inference time. To mitigate the above issues, we propose\nTemporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation\n(TLB-VFI), an efficient video-based diffusion model. By extracting rich\ntemporal information from video inputs through our proposed 3D-wavelet gating\nand temporal-aware autoencoder, our method achieves 20% improvement in FID on\nthe most challenging datasets over recent SOTA of image-based diffusion models.\nMeanwhile, due to the existence of rich temporal information, our method\nachieves strong performance while having 3times fewer parameters. Such a\nparameter reduction results in 2.3x speed up. By incorporating optical flow\nguidance, our method requires 9000x less training data and achieves over 20x\nfewer parameters than video-based diffusion models. Codes and results are\navailable at our project page: https://zonglinl.github.io/tlbvfi_page.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67abb26debe64eaa3a624bd7",
      "avatarUrl": "/avatars/39259308f5aeda7f80a5489335554913.svg",
      "fullname": "Zonglin Lyu",
      "name": "ucfzl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.11589",
      "authors": [
        {
          "_id": "687902bccc15e42a72b01ad6",
          "name": "Sandeep Suresh Cranganore",
          "hidden": false
        },
        {
          "_id": "687902bccc15e42a72b01ad7",
          "user": {
            "_id": "66bdb0025bdd611f9a008bec",
            "avatarUrl": "/avatars/468ec3aebcf798bff601583adbc3bf88.svg",
            "isPro": false,
            "fullname": "Bodnar",
            "user": "AndreiB137",
            "type": "user"
          },
          "name": "Andrei Bodnar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T14:59:15.181Z",
          "hidden": false
        },
        {
          "_id": "687902bccc15e42a72b01ad8",
          "name": "Arturs Berzins",
          "hidden": false
        },
        {
          "_id": "687902bccc15e42a72b01ad9",
          "name": "Johannes Brandstetter",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-15T14:55:39.000Z",
      "submittedOnDailyAt": "2025-07-18T08:27:07.754Z",
      "title": "エインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティフ\n\nエインシュタインフィールド：計算的普遍的相対論のニューラルポーティ",
      "submittedOnDailyBy": {
        "_id": "66bdb0025bdd611f9a008bec",
        "avatarUrl": "/avatars/468ec3aebcf798bff601583adbc3bf88.svg",
        "isPro": false,
        "fullname": "Bodnar",
        "user": "AndreiB137",
        "type": "user"
      },
      "summary": "エインテインフィールドズ、計算量の効率的な4次元数値相対論シミュレーションを組織的な隠れニューラルネットワーク重みに圧縮するためのニューラル表現を紹介します。一般相対論の核心テンソルフィールドであるメトリックをモデル化することで、物理量の計算を自動微分によって行うことができます。しかし、通常のニューラルフィールド（例：符号付き距離、占有、ラディエンスフィールド）と異なり、エインテインフィールドズは、一般相対論のスペースタイム幾何をニューラルフィールド表現に変換する際に自然と動力学が出現するニューラルテンソルフィールドです。エインテインフィールドズは、4次元スペースタイムの連続モデリング、メッシュ無依存性、ストレージエフィシェンス、微分の精度、および使用の容易性について驚異的な可能性を示します。一般相対論の標準テストベッドでのこれらの挑戦に対処し、JAXに基づくオープンソースライブラリをリリースし、数値相対論のよりスケーラブルおよび表現力のあるアプローチのための道を鋭く開けます。コードは、https://github.com/AndreiB137/EinFields から利用可能です。",
      "upvotes": 0,
      "discussionId": "687902bdcc15e42a72b01ada",
      "githubRepo": "https://github.com/AndreiB137/EinFields",
      "ai_summary": "Einstein Fields, a neural tensor field representation, compresses four-dimensional numerical relativity simulations into neural network weights, enabling automatic differentiation and natural emergence of dynamics.",
      "ai_keywords": [
        "Einstein Fields",
        "neural representation",
        "implicit neural network",
        "metric",
        "general relativity",
        "neural tensor fields",
        "spacetime geometry",
        "automatic differentiation",
        "numerical relativity",
        "JAX-based library"
      ],
      "githubStars": 23
    },
    "publishedAt": "2025-07-15T10:55:39.000Z",
    "title": "Einstein Fields: A Neural Perspective To Computational General\n  Relativity",
    "summary": "We introduce Einstein Fields, a neural representation that is designed to\ncompress computationally intensive four-dimensional numerical relativity\nsimulations into compact implicit neural network weights. By modeling the\nmetric, which is the core tensor field of general relativity, Einstein\nFields enable the derivation of physical quantities via automatic\ndifferentiation. However, unlike conventional neural fields (e.g., signed\ndistance, occupancy, or radiance fields), Einstein Fields are Neural\nTensor Fields with the key difference that when encoding the spacetime\ngeometry of general relativity into neural field representations, dynamics\nemerge naturally as a byproduct. Einstein Fields show remarkable potential,\nincluding continuum modeling of 4D spacetime, mesh-agnosticity, storage\nefficiency, derivative accuracy, and ease of use. We address these challenges\nacross several canonical test beds of general relativity and release an open\nsource JAX-based library, paving the way for more scalable and expressive\napproaches to numerical relativity. Code is made available at\nhttps://github.com/AndreiB137/EinFields",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66bdb0025bdd611f9a008bec",
      "avatarUrl": "/avatars/468ec3aebcf798bff601583adbc3bf88.svg",
      "fullname": "Bodnar",
      "name": "AndreiB137",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]