[
  {
    "paper": {
      "id": "2503.03601",
      "authors": [
        {
          "_id": "67cbfff12cc05acaab147f07",
          "name": "Kristian Kuznetsov",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f08",
          "user": {
            "_id": "636254dc2691058b19d9276a",
            "avatarUrl": "/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg",
            "isPro": false,
            "fullname": "Kushnareva",
            "user": "Kushnareva",
            "type": "user"
          },
          "name": "Laida Kushnareva",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:23:18.630Z",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f09",
          "name": "Polina Druzhinina",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0a",
          "user": {
            "_id": "6172aaeec8e66e2aa84c06b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6172aaeec8e66e2aa84c06b9/ZdRZSp3P1SU6CIDbvQwkv.jpeg",
            "isPro": false,
            "fullname": "Anton Razzhigaev",
            "user": "razzant",
            "type": "user"
          },
          "name": "Anton Razzhigaev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:23:21.197Z",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0b",
          "name": "Anastasia Voznyuk",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0c",
          "name": "Irina Piontkovskaya",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0d",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "67cbfff12cc05acaab147f0e",
          "name": "Serguei Barannikov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T15:33:52.000Z",
      "title": "特徴レベルでの人工的な文章検出の知見を持つスパース自動エンコーダーについて",
      "summary": "人工テキスト検出（ATD）は、高度な大規模言語モデル（LLMs）の増加に伴い重要性が高まります。ただし、複数の努力を重ねても、異なるタイプの見たことのないテキストに対して一貫して良い性能を示すことはできず、新しいLLMsへの有効な拡張を保証することもできません。解釈性はこの目標を達成するために重要な役割を果たします。本研究では、Sparse Autoencoders（SAE）を使用して、Gemma-2-2bの残差ストリームから特徴量を抽出し、ATDの解釈性を向上させます。両方の解釈的で効率的な特徴量を特定し、ドメインおよびモデル特有の統計、ステアリングアプローチ、手動またはLLMベースの解釈を通じて、それらの語意と関連性を分析します。我々の方法は、異なるモデルからのテキストが人間の書き物とどのように異なるかについて有價値なヒントを提供します。また、現代のLLMsは、情報密集なドメインで特に異なる書き方を持っていることを示し、プロマティシャルなプロンプトを使用して人間のような出力を生成することもできます。",
      "upvotes": 85,
      "discussionId": "67cbfff22cc05acaab147f4d",
      "ai_keywords": [
        "Sparse Autoencoders",
        "Gemma-2-2b",
        "residual stream",
        "interpretability",
        "domain-specific statistics",
        "model-specific statistics",
        "steering approach",
        "LLM-based interpretation",
        "writing style",
        "information-dense domains",
        "human-like outputs"
      ]
    },
    "publishedAt": "2025-03-05T10:33:52.000Z",
    "title": "Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders",
    "summary": "Artificial Text Detection (ATD) is becoming increasingly important with the\nrise of advanced Large Language Models (LLMs). Despite numerous efforts, no\nsingle algorithm performs consistently well across different types of unseen\ntext or guarantees effective generalization to new LLMs. Interpretability plays\na crucial role in achieving this goal. In this study, we enhance ATD\ninterpretability by using Sparse Autoencoders (SAE) to extract features from\nGemma-2-2b residual stream. We identify both interpretable and efficient\nfeatures, analyzing their semantics and relevance through domain- and\nmodel-specific statistics, a steering approach, and manual or LLM-based\ninterpretation. Our methods offer valuable insights into how texts from various\nmodels differ from human-written content. We show that modern LLMs have a\ndistinct writing style, especially in information-dense domains, even though\nthey can produce human-like outputs with personalized prompts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03601.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07365",
      "authors": [
        {
          "_id": "67cf9cd037bc7273882147a3",
          "name": "Fanqing Meng",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a4",
          "name": "Lingxiao Du",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a5",
          "name": "Zongkai Liu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a6",
          "name": "Zhixiang Zhou",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a7",
          "name": "Quanfeng Lu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a8",
          "name": "Daocheng Fu",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147a9",
          "name": "Botian Shi",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147aa",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ab",
          "name": "Junjun He",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ac",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ad",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147ae",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147af",
          "name": "Qiaosheng Zhang",
          "hidden": false
        },
        {
          "_id": "67cf9cd037bc7273882147b0",
          "name": "Wenqi Shao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T14:23:12.000Z",
      "title": "MM-Eureka: ルールベースの大規模な強化学習による視覚的な「アホモメント」の探索",
      "summary": "MM-Eurekaは、多モデル論理モデルであり、大規模なルールベースの強化学習（RL）を多モデル論理に拡張して成功したものです。ルールベースのRLは、LLMの論理能力を文脈領域で向上させるために驚異的な成功を示しましたが、多モデル設定に応用することは難しかった。我々の研究は、DeepSeek-R1のような文脈ベースのRLシステムの主要な特徴を多モデル空間に再現し、精度賞と回答長の穩定な上昇、反省行動の発生などを含むものです。我々は、ルールベースのRLを用いて、教師制限なしでインストラクションチューニングモデルと予習モデルが強い多モデル論理能力を開発できることを示し、他のアプローチに比べてより高いデータエフィシェンスを示します。我々は、https://github.com/ModalMinds/MM-EUREKAで完全なパイプラインをオープンソース化し、この領域の進展を促進します。我々のすべてのコード、モデル、データなどをリリースしています。",
      "upvotes": 38,
      "discussionId": "67cf9cd137bc7273882147e2",
      "ai_keywords": [
        "multimodal reasoning",
        "rule-based reinforcement learning (RL)",
        "large-scale rule-based reinforcement learning (RL)",
        "DeepSeek-R1",
        "multimodal space",
        "accuracy reward",
        "response length",
        "reflection behaviors",
        "instruction-tuned",
        "pre-trained models",
        "multimodal reasoning capabilities",
        "rule-based RL",
        "supervised fine-tuning",
        "data efficiency"
      ]
    },
    "publishedAt": "2025-03-10T10:23:12.000Z",
    "title": "MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning",
    "summary": "We present MM-Eureka, a multimodal reasoning model that successfully extends\nlarge-scale rule-based reinforcement learning (RL) to multimodal reasoning.\nWhile rule-based RL has shown remarkable success in improving LLMs' reasoning\nabilities in text domains, its application to multimodal settings has remained\nchallenging. Our work reproduces key characteristics of text-based RL systems\nlike DeepSeek-R1 in the multimodal space, including steady increases in\naccuracy reward and response length, and the emergence of reflection behaviors.\nWe demonstrate that both instruction-tuned and pre-trained models can develop\nstrong multimodal reasoning capabilities through rule-based RL without\nsupervised fine-tuning, showing superior data efficiency compared to\nalternative approaches. We open-source our complete pipeline to foster further\nresearch in this area. We release all our codes, models, data, etc. at\nhttps://github.com/ModalMinds/MM-EUREKA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07365.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07605",
      "authors": [
        {
          "_id": "67cfa0c1edb742caa3572982",
          "name": "Xun Liang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572983",
          "user": {
            "_id": "669e0b93c7cb0568dac6e92e",
            "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
            "isPro": false,
            "fullname": "hanyu Wang",
            "user": "UglyToilet",
            "type": "user"
          },
          "name": "Hanyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:46.104Z",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572984",
          "name": "Huayi Lai",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572985",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572986",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572987",
          "name": "Jiawei Yang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572988",
          "name": "Jihao Zhao",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa3572989",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa357298a",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67cfa0c1edb742caa357298b",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:59:03.000Z",
      "title": "SEAP: トレーニング無しスパースエクスプーラーアクティベーションプリニングで大規模言語モデルの能力を解放する",
      "summary": "大語言モデルは、多様な自然言語処理タスクで驚異的な成功を収めていますが、推論時の高い計算コストは主なブロックとして残っています。本論文では、トレーニング無しのパラメータ削減手法であるSparse Expert Activation Pruning (SEAP)を介して、推論オーバーヘッドを減少させるために、タスク関連のパラメータを選択的に残します。LLMの隠れ状態と活性化のクラスタリングパターンによりインスピレーションを受け、SEAPはタスク特有のエキスパート活性化パターンを特定し、モデルを削減しながらタスク性能を維持し、計算効率を向上させます。実験結果から、SEAPは計算オーバーヘッドを大幅に減少しながら、相対的な精度を維持していることが示されます。特に、50%の削減率では、WandAとFLAPを超える20%以上の効果を示し、20%の削減率では、密なモデルに対する2.2%の性能低下しか見られません。これらの発見は、SEAPのスケーラビリティと効果性を明らかにし、大規模なLLMの最適化において望ましいアプローチとしての可能性を示しています。",
      "upvotes": 36,
      "discussionId": "67cfa0c2edb742caa35729dc",
      "githubRepo": "https://github.com/IAAR-Shanghai/SEAP",
      "ai_keywords": [
        "Sparse Expert Activation Pruning (SEAP)",
        "hidden states",
        "activations",
        "task-specific expert activation patterns",
        "computational efficiency"
      ]
    },
    "publishedAt": "2025-03-10T13:59:03.000Z",
    "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models",
    "summary": "Large Language Models have achieved remarkable success across various natural\nlanguage processing tasks, yet their high computational cost during inference\nremains a major bottleneck. This paper introduces Sparse Expert Activation\nPruning (SEAP), a training-free pruning method that selectively retains\ntask-relevant parameters to reduce inference overhead. Inspired by the\nclustering patterns of hidden states and activations in LLMs, SEAP identifies\ntask-specific expert activation patterns and prunes the model while preserving\ntask performance and enhancing computational efficiency. Experimental results\ndemonstrate that SEAP significantly reduces computational overhead while\nmaintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both\nWandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2%\nperformance drop compared to the dense model. These findings highlight SEAP's\nscalability and effectiveness, making it a promising approach for optimizing\nlarge-scale LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07605.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07002",
      "authors": [
        {
          "_id": "67cfa814d212c9c5048845a0",
          "name": "Jiazheng Liu",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a1",
          "name": "Sipeng Zheng",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a2",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:32.095Z",
          "hidden": false
        },
        {
          "_id": "67cfa814d212c9c5048845a3",
          "name": "Zongqing Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T07:32:53.000Z",
      "title": "ノートを取ると集中力が上がるか？　多ターン多モーダルダイアロジーへの向けて\n学習",
      "summary": "多ターン多モデルデータセットMMDiagを紹介します。このデータセットは、特別に設計されたルールとGPTの助けを通じて共同生成され、問い合わせ間、問い合わせと画像間、そして異なる画像領域間に強い関連性を持つことで、実世界的なシナリオによりもっと近いものとしています。MMDiagは、多ターン多モデルデータセットの学習に強いベンチマークとして役立ちますし、MLLMの基礎と理由論の能力においてもさらなる挑戦を提供します。また、人間の視覚処理によりモデルを受け継ぎ、DiagNoteというMLLMを紹介します。DiagNoteは、Chain-of-Thoughtと注釈の両方を行うための2つのモジュール（計画と視点）が相互作用しています。DiagNoteは、現在のMLLMによりもっと優れた基礎と視覚情報と言語情報の共に処理して理由論を行うことを実験的に示します。",
      "upvotes": 29,
      "discussionId": "67cfa818d212c9c504884689",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "vision towers",
        "multi-turn vision question-answering tasks",
        "multi-turn multimodal dialogue dataset (MMDiag)",
        "GPT assistant",
        "multimodal dialogue learning",
        "grounding",
        "reasoning capabilities",
        "Deliberate module",
        "Gaze module",
        "Chain-of-Thought"
      ]
    },
    "publishedAt": "2025-03-10T03:32:53.000Z",
    "title": "Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue\n  Learning",
    "summary": "Multimodal large language models (MLLMs), built on large-scale pre-trained\nvision towers and language models, have shown great capabilities in multimodal\nunderstanding. However, most existing MLLMs are trained on single-turn vision\nquestion-answering tasks, which do not accurately reflect real-world human\nconversations. In this paper, we introduce MMDiag, a multi-turn multimodal\ndialogue dataset. This dataset is collaboratively generated through\ndeliberately designed rules and GPT assistance, featuring strong correlations\nbetween questions, between questions and images, and among different image\nregions; thus aligning more closely with real-world scenarios. MMDiag serves as\na strong benchmark for multi-turn multimodal dialogue learning and brings more\nchallenges to the grounding and reasoning capabilities of MLLMs. Further,\ninspired by human vision processing, we present DiagNote, an MLLM equipped with\nmultimodal grounding and reasoning capabilities. DiagNote consists of two\nmodules (Deliberate and Gaze) interacting with each other to perform\nChain-of-Thought and annotations respectively, throughout multi-turn dialogues.\nWe empirically demonstrate the advantages of DiagNote in both grounding and\njointly processing and reasoning with vision and language information over\nexisting MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07002.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07314",
      "authors": [
        {
          "_id": "67cfa750c8f2a661dc9798fe",
          "name": "Weijia Wu",
          "hidden": false
        },
        {
          "_id": "67cfa750c8f2a661dc9798ff",
          "name": "Zeyu Zhu",
          "hidden": false
        },
        {
          "_id": "67cfa750c8f2a661dc979900",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T13:33:27.000Z",
      "title": "自動化映画生成による多エージェントコンテキスト計画",
      "summary": "現在の長ビデオ生成フレームワークは自動計画が欠落しており、物語、場面、映像設計、キャラクターの相互作用において手動での入力が必要となり、高いコストと不適切な効率を招く。これらの課題に対処するために、我々はMovieAgentを紹介します。MovieAgentは多効機関のChain of Thought（CoT）計画を用いた自動映画生成フレームワークです。MovieAgentは2つの主な優れ点を提供します：1）我々は最初に自動化された映画/長ビデオ生成のパラダイムを探求し定義します。スクリプトとキャラクターバンクを与えると、我々のMovieAgentはコネクティブなナログを保ちながら、キャラクターの一貫性、同期された字幕、ビデオ全体のサブアナログを確保し、多場面、多角度の長ビデオを生成します。2）MovieAgentは、場面の構成、カメラ設定、映像設計を自動的に構築するためのヒューリスティックなCoTベースの理由過程を導入し、人間の努力を大幅に減らします。映画監督、スクリーンリティング、ストーリーボードアーティスト、ロケーションマネージャーの役割を模倣するために、複数のLLMエージェントを使用し、MovieAgentは生産プロセスをストリームライン化します。実験はスクリプト忠実性、キャラクターの一貫性、ナロジーのコネクティブさに新しい最先端の結果を達成したことを示します。我々のヒューリスティックなフレームワークは、完全な自動化された映画生成について新しいインサイトを提供します。コードとプロジェクトウェブサイトは以下のURLで利用可能です：https://github.com/showlab/MovieAgent と https://weijiawu.github.io/MovieAgent。",
      "upvotes": 24,
      "discussionId": "67cfa752c8f2a661dc9799b8",
      "ai_keywords": [
        "MovieAgent",
        "Chain of Thought (CoT)",
        "automated movie/long-video generation",
        "multi-scene, multi-shot long-form videos",
        "coherent narrative",
        "character consistency",
        "synchronized subtitles",
        "stable audio",
        "hierarchical CoT-based reasoning",
        "multiple LLM agents",
        "director",
        "screenwriter",
        "storyboard artist",
        "location manager",
        "script faithfulness",
        "narrative coherence",
        "fully automated movie generation"
      ]
    },
    "publishedAt": "2025-03-10T09:33:27.000Z",
    "title": "Automated Movie Generation via Multi-Agent CoT Planning",
    "summary": "Existing long-form video generation frameworks lack automated planning,\nrequiring manual input for storylines, scenes, cinematography, and character\ninteractions, resulting in high costs and inefficiencies. To address these\nchallenges, we present MovieAgent, an automated movie generation via\nmulti-agent Chain of Thought (CoT) planning. MovieAgent offers two key\nadvantages: 1) We firstly explore and define the paradigm of automated\nmovie/long-video generation. Given a script and character bank, our MovieAgent\ncan generates multi-scene, multi-shot long-form videos with a coherent\nnarrative, while ensuring character consistency, synchronized subtitles, and\nstable audio throughout the film. 2) MovieAgent introduces a hierarchical\nCoT-based reasoning process to automatically structure scenes, camera settings,\nand cinematography, significantly reducing human effort. By employing multiple\nLLM agents to simulate the roles of a director, screenwriter, storyboard\nartist, and location manager, MovieAgent streamlines the production pipeline.\nExperiments demonstrate that MovieAgent achieves new state-of-the-art results\nin script faithfulness, character consistency, and narrative coherence. Our\nhierarchical framework takes a step forward and provides new insights into\nfully automated movie generation. The code and project website are available\nat: https://github.com/showlab/MovieAgent and\nhttps://weijiawu.github.io/MovieAgent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07314.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07216",
      "authors": [
        {
          "_id": "67cfa6fcd77496ce0c154bdc",
          "name": "Sangwoo Park",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bdd",
          "name": "Seanie Lee",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bde",
          "name": "Byungjoo Kim",
          "hidden": false
        },
        {
          "_id": "67cfa6fcd77496ce0c154bdf",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T11:55:50.000Z",
      "title": "FedRand: ランダム化LoRAによる連携学習のプライバシー向上による更新ユニット",
      "summary": "Federated Learning (FL)は、分散的なモデル訓練を行うために広く使用されているフレームワークです。この方法では、中央サーバーは局所コンピューターからのデータに直接アクセスしないように設計されています。しかし、このアプローチは、局所コンピューターからのモデルが中央サーバーに集約される際にデータプライバシーを完全に保護することはできないこともあります。この問題は、ビジョン言語モデル（VLM）をFLで訓練する場合にさらに重要になります。VLMは、訓練データのインスタンスを記憶することが容易であり、メンバーシップ推論攻撃（MIAs）に脆弱になることがあります。この挑戦に対処するために、FedRandフレームワークを提案します。このフレームワークでは、全てのコンピューターパラメーターを公開することを避けるように設計されています。このフレームワークでは、各コンピューターは、サーバーからLow-Rank Adaptation（LoRA）のサブパラメーターをランダムに選択し、残りのLoRAの重みのコンペナントをプライベートパラメーターとして保持します。このフレームワークでは、コンピューターのプライベートデータセットで両方のパラメーターを訓練した後、非公開なコンピューターパラメーターだけはサーバーに送信されて集約されます。このアプローチは、コンピューター側のVLMパラメーターを暴露するリスクを減らし、データプライバシーを向上させます。実験的には、FedRandは、関連する基準と比較してMIAsに対する強固性を向上させ、訓練データを完全にLoRAパラメーターを送信する方法と比較しても、様々なベンチマークデータセットで精度が相当していることを実証しました。",
      "upvotes": 22,
      "discussionId": "67cfa6fdd77496ce0c154c18",
      "ai_keywords": [
        "Federated Learning (FL)",
        "vision-language models (VLMs)",
        "membership inference attacks (MIAs)",
        "FedRand framework",
        "Low-Rank Adaptation (LoRA)",
        "subparameters",
        "non-private client parameters",
        "client parameters",
        "aggregation",
        "robustness"
      ]
    },
    "publishedAt": "2025-03-10T07:55:50.000Z",
    "title": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
    "summary": "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07216.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07067",
      "authors": [
        {
          "_id": "67cfa99b7c95194db8d75468",
          "user": {
            "_id": "64b7628af902508f0d7ae112",
            "avatarUrl": "/avatars/83c155254486e80c1dfd14676fdf9215.svg",
            "isPro": false,
            "fullname": "Jongwoo Ko",
            "user": "jongwooko",
            "type": "user"
          },
          "name": "Jongwoo Ko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:29.622Z",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d75469",
          "user": {
            "_id": "64ad94f05a4a60156925ec96",
            "avatarUrl": "/avatars/643bdb076e703bfcc89cec6fccb756c6.svg",
            "isPro": false,
            "fullname": "Tianyi Chen",
            "user": "tianyic",
            "type": "user"
          },
          "name": "Tianyi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:27.139Z",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546a",
          "name": "Sungnyun Kim",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546b",
          "name": "Tianyu Ding",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546c",
          "name": "Luming Liang",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546d",
          "name": "Ilya Zharkov",
          "hidden": false
        },
        {
          "_id": "67cfa99b7c95194db8d7546e",
          "name": "Se-Young Yun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T08:51:32.000Z",
      "title": "DistiLLM-2: 対比的アプローチでLLMのディスタイル化を向上させる",
      "summary": "その成功を収めた大規模言語モデル（LLMs）のディスティル（Distillation）にもかかわらず、ほとんどすべての先行研究は、教師モデルと学生モデルの生成データに同じ損失関数を適用しています。これらの戦略は、損失表現とデータタイプの連携を見落とし、学生モデルの性能向上において最適な結果を得ることができないことを見落としています。これに対して、私たちは、教師の回答の確率を増やし、学生の回答の確率を減らすことでこの連携を活用した対照的なアプローチを提案します。私たちの広範囲の実験により、DistiLLM-2は、指示従いやコード生成などの幅広い課題で高性能の学生モデルを構築できることを示し、好みの調整や視覚言語拡張などの多様なアプリケーションをサポートします。これらの発見は、対照的なアプローチが、教師モデルと学生モデルの連携を効果的に実現し、多様なデータタイプにおいてLLMのディスティルの効果を向上させることの可能性を明らかにしています。",
      "upvotes": 19,
      "discussionId": "67cfa99c7c95194db8d754bf",
      "githubRepo": "https://github.com/jongwooko/distillm-2",
      "ai_keywords": [
        "contrastive approach",
        "likelihood",
        "DistiLLM-2",
        "instruction-following",
        "code generation",
        "preference alignment",
        "vision-language extensions"
      ]
    },
    "publishedAt": "2025-03-10T04:51:32.000Z",
    "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs",
    "summary": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07067.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06680",
      "authors": [
        {
          "_id": "67cf94d9f2b1fe815db6db40",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db41",
          "user": {
            "_id": "641a9a4b05290a135041a3ed",
            "avatarUrl": "/avatars/95d66ac607973abe95bd3558c6c93739.svg",
            "isPro": false,
            "fullname": "Pluto",
            "user": "CharonBony",
            "type": "user"
          },
          "name": "Xin Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T01:41:47.194Z",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db42",
          "name": "Zhongxin Guo",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db43",
          "name": "Shaoguang Mao",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db44",
          "name": "Wen Luo",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db45",
          "name": "Guangyue Peng",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db46",
          "name": "Yangyu Huang",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db47",
          "name": "Houfeng Wang",
          "hidden": false
        },
        {
          "_id": "67cf94d9f2b1fe815db6db48",
          "name": "Scarlett Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T16:11:57.000Z",
      "title": "FEA-Bench: フィーチャー実装のリポジトリレベルコード生成を評価するためのベンチマーク",
      "summary": "リポジトリレベルのコードベースで新機能の実装は、コード生成モデルの重要な応用として挙げられます。しかし、現在のベンチマークはこの能力に対する専門的な評価フレームワークを欠くことがあります。この空間を埋めるために、FEA-Benchというベンチマークを紹介します。FEA-Benchは、大規模な言語モデル（LLMs）がリポジトリ内での進行開発を行う能力を評価するために設計されています。83ページのGitHubリポジトリからプルリクエストを集め、ルールベースとインテントベースのフィルタリングを用いて新機能開発に焦点を当てたタスクインスタンスを構築しました。各タスクインスタンスに含まれるコード変更は、関連するユニットテストファイルと組み合わせ、解決策の検証が可能にします。新機能の実装には、LLMsは新しいコンポーネントのコード完成機能とリポジトリ内の他の関連部分のコード編集能力を同時に持つ必要があり、LLMsの自動軽量製作ソフトウェア能力を更に詳細に評価する方法を提供します。実験結果によると、LLMsはFEA-Benchで显著に悪い性能を示し、リポジトリレベルの進行開発における相当の課題が明らかにされました。",
      "upvotes": 16,
      "discussionId": "67cf94dbf2b1fe815db6db9e"
    },
    "publishedAt": "2025-03-09T12:11:57.000Z",
    "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation",
    "summary": "Implementing new features in repository-level codebases is a crucial\napplication of code generation models. However, current benchmarks lack a\ndedicated evaluation framework for this capability. To fill this gap, we\nintroduce FEA-Bench, a benchmark designed to assess the ability of large\nlanguage models (LLMs) to perform incremental development within code\nrepositories. We collect pull requests from 83 GitHub repositories and use\nrule-based and intent-based filtering to construct task instances focused on\nnew feature development. Each task instance containing code changes is paired\nwith relevant unit test files to ensure that the solution can be verified. The\nfeature implementation requires LLMs to simultaneously possess code completion\ncapabilities for new components and code editing abilities for other relevant\nparts in the code repository, providing a more comprehensive evaluation method\nof LLMs' automated software engineering capabilities. Experimental results show\nthat LLMs perform significantly worse in the FEA-Bench, highlighting\nconsiderable challenges in such repository-level incremental code development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06680.png",
    "numComments": 5,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07027",
      "authors": [
        {
          "_id": "67cf98fd59dbba733d8c531e",
          "user": {
            "_id": "636b3f9ce3ad78bc68b67541",
            "avatarUrl": "/avatars/2b7e745953ae39e01222e99fb63b279e.svg",
            "isPro": false,
            "fullname": "yuxuan",
            "user": "zzyx",
            "type": "user"
          },
          "name": "Yuxuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:57.021Z",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c531f",
          "name": "Yirui Yuan",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5320",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5321",
          "user": {
            "_id": "637745113a63a2983ffbde13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
            "isPro": false,
            "fullname": "Haofan Wang",
            "user": "wanghaofan",
            "type": "user"
          },
          "name": "Haofan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:54.821Z",
          "hidden": false
        },
        {
          "_id": "67cf98fd59dbba733d8c5322",
          "name": "Jiaming Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T08:07:17.000Z",
      "title": "EasyControl: 拡散モデルに効率的かつ柔軟な制御を追加するシステム\nTransformer",
      "summary": "最近、Unetベースのdiffusionモデルの進展で、ControlNetとIP-Adapterなどの効果的な空間と主題の制御機構が導入されましたが、DiT（Diffusion Transformer）アーキテクチャは効率的かつ柔軟な制御に難しくなっています。この問題を解決するために、EasyControlという新しいフレームワークを提案します。このフレームワークは高い効率性と柔軟性を持つ条件ガイドされたdiffusion transformerを統一しています。フレームワークは3つのキーのイノベーションに基づいて構築されています。まず、ラウンドウェイトのCondition Injection LoRAモジュールを導入します。このモジュールは条件シグナルを孤立して処理し、プラグインとプレイングソリューションとして機能します。基盤モデルの重みを変更しないように、カスタマイズされたモデルとの相性を保証し、多様な条件の柔軟な注入を可能にします。特に、このモジュールは、単一条件データでも学習されている場合にも、調和的で強固なゼロシート多条件の拡張性をサポートします。次に、Position-Aware Training Paradigmを提案します。このアプローチは入力条件を固定レジジョンに標準化し、任意のアスペクト比と柔軟なレジジョンの画像の生成を可能にします。同時に、計算効率を最適化し、実世界的なアプリケーションに対してプラクティカルになります。最後に、Causal Attention MechanismとKV Cacheテクニックを組み合わせた条件付き生成タスクに適用します。このイノベーションは画像合成の遅延を大幅に減少し、フレームワーク全体の効率を向上させます。拡張された実験では、EasyControlは様々なアプリケーションシナリオで出色な性能を収めています。これらのイノベーションは、フレームワークの高度な効率性、柔軟性と、広範囲のタスクに適した性質を共有しています。",
      "upvotes": 15,
      "discussionId": "67cf990359dbba733d8c545d",
      "ai_keywords": [
        "Unet-based diffusion models",
        "ControlNet",
        "IP-Adapter",
        "DiT (Diffusion Transformer)",
        "Condition Injection LoRA Module",
        "Condition Injection",
        "zero-shot multi-condition generalization",
        "Position-Aware Training Paradigm",
        "Position-Aware",
        "Causal Attention Mechanism",
        "KV Cache",
        "conditional generation tasks",
        "image synthesis"
      ]
    },
    "publishedAt": "2025-03-10T04:07:17.000Z",
    "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
    "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07027.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07602",
      "authors": [
        {
          "_id": "67cfb2efb77bc8e7d415f904",
          "name": "Yujie Wei",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f905",
          "name": "Shiwei Zhang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f906",
          "user": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "isPro": false,
            "fullname": "Hangjie Yuan",
            "user": "JacobYuan",
            "type": "user"
          },
          "name": "Hangjie Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:32.780Z",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f907",
          "name": "Biao Gong",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f908",
          "user": {
            "_id": "6492a0d8d4ae24c933ace44d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DXIky2sdPwmiCOR9p-JBQ.png",
            "isPro": false,
            "fullname": "Longxiang Tang",
            "user": "lloong",
            "type": "user"
          },
          "name": "Longxiang Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:30.700Z",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f909",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90a",
          "name": "Haonan Qiu",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90b",
          "name": "Hengjia Li",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90c",
          "name": "Shuai Tan",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90d",
          "name": "Yingya Zhang",
          "hidden": false
        },
        {
          "_id": "67cfb2efb77bc8e7d415f90e",
          "name": "Hongming Shan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:03.000Z",
      "title": "DreamRelation: 関係センターのビデオカスタマイズ",
      "summary": "関係ビデオカスタマイズは、ユーザー指定の2つの主題の関係を描く個別化ビデオの作成であり、写真内容の理解に重要なタスクです。既存の方法は主題の外観と動きをプロフェッショナル化できますが、複雑な関係ビデオカスタマイズに適応できないことが見られます。関係の精確なモデリングと主題カテゴリの高い一般化が重要です。主な課題は、関係に固有の複雑な空間配置、並び順の変化、そして時系列的な動きの複雑さから来ます。現在のモデルは、意味のある相互作用を捉えることではなく、関係の無関心な視覚的な詳細を過度に重み付けしています。これらの課題を解決するために、私たちはDreamRelationを提案します。DreamRelationは、小さなサンプルビデオを使用して関係をプロフェッショナル化する新しいアプローチです。2つのキーコンポーネントを利用します：関係デコープリング学習と関係ダイナミクスアプライズメント。まず、関係デコープリング学習では、関係ロラータプルと組み合わせたマスクトレーニングスタラテジーを使用して主題の外観から関係を分離し、多様な関係でのより良い一般化を確保します。また、MM-DiTのアタンション機構のクエリ、キー、バリュー特徴の異なる役割を分析して関係ロラータプルの最適設計を決定し、DreamRelationは説明可能なコンポーネントを持つ最初の関係ビデオ生成フレームワークとなります。次に、関係ダイナミクスアプライズメントでは、スペースタイム関係的な対比的損失を導入し、関係ダイナミクスを優先し、主題の詳細な外観に依存しないようにします。拡張検証は、DreamRelationが状態の最先端の方法に優れて関係ビデオカスタマイズで表現します。コードとモデルは公開的に提供されます。",
      "upvotes": 10,
      "discussionId": "67cfb2f1b77bc8e7d415f96b",
      "ai_keywords": [
        "Relational Decoupling Learning",
        "Relational Dynamics Enhancement",
        "relation LoRA triplet",
        "hybrid mask training strategy",
        "attention mechanism",
        "space-time relational contrastive loss",
        "MM-DiT"
      ]
    },
    "publishedAt": "2025-03-10T13:58:03.000Z",
    "title": "DreamRelation: Relation-Centric Video Customization",
    "summary": "Relational video customization refers to the creation of personalized videos\nthat depict user-specified relations between two subjects, a crucial task for\ncomprehending real-world visual content. While existing methods can personalize\nsubject appearances and motions, they still struggle with complex relational\nvideo customization, where precise relational modeling and high generalization\nacross subject categories are essential. The primary challenge arises from the\nintricate spatial arrangements, layout variations, and nuanced temporal\ndynamics inherent in relations; consequently, current models tend to\noveremphasize irrelevant visual details rather than capturing meaningful\ninteractions. To address these challenges, we propose DreamRelation, a novel\napproach that personalizes relations through a small set of exemplar videos,\nleveraging two key components: Relational Decoupling Learning and Relational\nDynamics Enhancement. First, in Relational Decoupling Learning, we disentangle\nrelations from subject appearances using relation LoRA triplet and hybrid mask\ntraining strategy, ensuring better generalization across diverse relationships.\nFurthermore, we determine the optimal design of relation LoRA triplet by\nanalyzing the distinct roles of the query, key, and value features within\nMM-DiT's attention mechanism, making DreamRelation the first relational video\ngeneration framework with explainable components. Second, in Relational\nDynamics Enhancement, we introduce space-time relational contrastive loss,\nwhich prioritizes relational dynamics while minimizing the reliance on detailed\nsubject appearances. Extensive experiments demonstrate that DreamRelation\noutperforms state-of-the-art methods in relational video customization. Code\nand models will be made publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07602.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06580",
      "authors": [
        {
          "_id": "67cfa71827c7f0b2db19f7c2",
          "user": {
            "_id": "645b4a2978730bcc103dfe4d",
            "avatarUrl": "/avatars/de544de899897fd0a83506ff287123bc.svg",
            "isPro": false,
            "fullname": "Yuxiang Zhang",
            "user": "TokerZ",
            "type": "user"
          },
          "name": "Yuxiang Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:40.336Z",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c3",
          "name": "Yuqi Yang",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c4",
          "name": "Jiangming Shu",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c5",
          "name": "Xinyan Wen",
          "hidden": false
        },
        {
          "_id": "67cfa71827c7f0b2db19f7c6",
          "name": "Jitao Sang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T12:19:47.000Z",
      "title": "アウトプット:\nAgentモデル: 行動生成の連鎖を理由論理に内包するモデル",
      "summary": "傳統的アジェントワークフローは、外部のプロンプトを使用してツールと環境との相互作用を管理し、これにより推理モデルの自律性が制限されています。我々は、Chain-of-Action（CoA）の生成を内部化することで自律的に決定した時間と方法で外部ツールを使用することを可能にするラージェットエージェントモデル（LAMs）を提案しています。我々の提案のAutoCoAフレームワークは、規範的学習（SFT）と強化学習（RL）を組み合わせ、モデルが推理と行動を流れ切りながら環境の相互作用を効率的に管理することを可能にします。主要な構成要素は、ステップレベルの行動トリガー、トラジェトレベルのCoA最適化、および内部のワールドモデルです。開放ドメインQAタスクにおける評価により、AutoCoAにより訓練されたアジェントモデルは、特に長期の理由と多段階の行動が必要なタスクでReActベースのワークフローを大幅に超えることを示しています。コードとデータセットは、https://github.com/ADaM-BJTU/AutoCoA から利用できます。",
      "upvotes": 10,
      "discussionId": "67cfa71927c7f0b2db19f817",
      "githubRepo": "https://github.com/ADaM-BJTU/AutoCoA",
      "ai_keywords": [
        "Large Agent Models (LAMs)",
        "Chain-of-Action (CoA)",
        "AutoCoA framework",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "step-level action triggering",
        "trajectory-level CoA optimization",
        "internal world model"
      ]
    },
    "publishedAt": "2025-03-09T08:19:47.000Z",
    "title": "Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models",
    "summary": "Traditional agentic workflows rely on external prompts to manage interactions\nwith tools and the environment, which limits the autonomy of reasoning models.\nWe position Large Agent Models (LAMs) that internalize the generation of\nChain-of-Action (CoA), enabling the model to autonomously decide when\nand how to use external tools. Our proposed AutoCoA framework combines\nsupervised fine-tuning (SFT) and reinforcement learning (RL), allowing the\nmodel to seamlessly switch between reasoning and action while efficiently\nmanaging environment interactions. Main components include step-level action\ntriggering, trajectory-level CoA optimization, and an internal world model to\nreduce real-environment interaction costs. Evaluations on open-domain QA tasks\ndemonstrate that AutoCoA-trained agent models significantly outperform\nReAct-based workflows in task completion, especially in tasks that require\nlong-term reasoning and multi-step actions. Code and dataset are available at\nhttps://github.com/ADaM-BJTU/AutoCoA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06580.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07608",
      "authors": [
        {
          "_id": "67cfa5bcb17ca92d24da9033",
          "user": {
            "_id": "65a4a180c8a09bd5e8e900b8",
            "avatarUrl": "/avatars/c135db68f6ff2c40119acd2e9ddce968.svg",
            "isPro": false,
            "fullname": "Bo Jiang",
            "user": "rb93dett",
            "type": "user"
          },
          "name": "Bo Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:43.665Z",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9034",
          "name": "Shaoyu Chen",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9035",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9036",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67cfa5bcb17ca92d24da9037",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:59:42.000Z",
      "title": "AlphaDrive: 自動運転におけるVLMsの力を解放するための強化学習と理由論",
      "summary": "OpenAI o1とDeepSeek R1は、数学や科学の複雑な分野では、強化学習（RL）と理由論が重要な役割を果たして、人間の専門家レベルの性能を達成したり、超えたりすることができる。自動運転においては、最近の端末から端末までのモデルは計画性能を大幅に向上させたが、限られた常識と理由論能力により、長尾問題に対応できない問題が残っている。一部の研究は、自動運転にビジョン言語モデル（VLMs）を統合しているが、通常は、簡単な監督学習（SFT）による驅転データのトレーニングを依存し、トレーニング戦略や計画に特に適した最適化を試みていない。本論文では、自動運転のVLMsに対するRLと理由論のフレームワークを提案しています。AlphaDriveは、計画に適したGRPOベースのRL報酬を4つ導入し、SFTとRLの組み合わせの2段階計画理由論トレーニング戦略を用いています。これにより、AlphaDriveはSFTだけであるからこその場合や理由論を含まない場合に比べて、計画性能とトレーニング効率を大幅に向上させています。また、RLトレーニング後にAlphaDriveは、駆転安全性と効率向上に重要な多様的な計画能力を発見することができることを興奮しています。私たちの知識の限りでは、AlphaDriveはGRPOベースのRLと計画理由論を自動運転に統合したものであることを知ります。コードは将来の研究を促進するために公開されます。",
      "upvotes": 9,
      "discussionId": "67cfa5bdb17ca92d24da9064",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "reasoning",
        "end-to-end models",
        "vision-language models (VLMs)",
        "supervised fine-tuning (SFT)",
        "GRPO-based RL rewards",
        "two-stage planning reasoning training strategy",
        "emergent multimodal planning capabilities"
      ]
    },
    "publishedAt": "2025-03-10T13:59:42.000Z",
    "title": "AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via\n  Reinforcement Learning and Reasoning",
    "summary": "OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level\nperformance in complex domains like mathematics and science, with reinforcement\nlearning (RL) and reasoning playing a crucial role. In autonomous driving,\nrecent end-to-end models have greatly improved planning performance but still\nstruggle with long-tailed problems due to limited common sense and reasoning\nabilities. Some studies integrate vision-language models (VLMs) into autonomous\ndriving, but they typically rely on pre-trained models with simple supervised\nfine-tuning (SFT) on driving data, without further exploration of training\nstrategies or optimizations specifically tailored for planning. In this paper,\nwe propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous\ndriving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning\nand employs a two-stage planning reasoning training strategy that combines SFT\nwith RL. As a result, AlphaDrive significantly improves both planning\nperformance and training efficiency compared to using only SFT or without\nreasoning. Moreover, we are also excited to discover that, following RL\ntraining, AlphaDrive exhibits some emergent multimodal planning capabilities,\nwhich is critical for improving driving safety and efficiency. To the best of\nour knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning\nreasoning into autonomous driving. Code will be released to facilitate future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07608.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05244",
      "authors": [
        {
          "_id": "67cfebe18a4265f3656a50aa",
          "user": {
            "_id": "642d430a7f9efee76b8713c0",
            "avatarUrl": "/avatars/4981f166a6df8e2ea60cd4c41c2f44d4.svg",
            "isPro": false,
            "fullname": "YuningWu",
            "user": "AQuarterMile",
            "type": "user"
          },
          "name": "Yuning Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:24:29.900Z",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ab",
          "name": "Jiahao Mei",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ac",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ad",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50ae",
          "name": "SHaopeng Lai",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50af",
          "name": "Yuran Ren",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b0",
          "name": "Zijia Wang",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b1",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b2",
          "name": "Mengyue Wu",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b3",
          "name": "Qin Jin",
          "hidden": false
        },
        {
          "_id": "67cfebe18a4265f3656a50b4",
          "name": "Fei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T08:56:20.000Z",
      "title": "WritingBench: ジェネレータ写真のためのコンプレックスなベンチマーク",
      "summary": "最近の大規模言語モデル（LLMs）の進歩は、テキスト生成能力を大幅に向上させたが、生成書話の性能評価は難しい問題です。現在のベンチマークは主に一般的なテキスト生成や限定的な書話タスクに焦点を当てていますが、高品質の書話内容の多様な要求を捉えずにいます。この隙を埋めるために、私たちはWritingBenchを紹介します。これは、6つの核心書話ディレインと100つのサブディレインを構成し、創造的的、説得的な、情報的な、技術的な書話を含む、詳細なベンチマークです。また、LLMsがインスタンス特有の評価基準を動的に生成することを可能にするクエリ依存性評価フレームワークを提案します。このフレームワークは、評価基準に関する評価モデルとして補完され、スタイル、フォーマット、長さの評価を可能にします。このフレームワークの有効性は、7Bパラメータモデルが最先端（SOTA）の性能に近づくことを示しています。私たちは、このベンチマーク、評価ツール、モジュール化フレームコンポーネントを公開し、LLMsの書話開発に貢献します。",
      "upvotes": 9,
      "discussionId": "67cfebe38a4265f3656a5136",
      "githubRepo": "https://github.com/X-PLUG/WritingBench",
      "ai_keywords": [
        "large language models (LLMs)",
        "text generation",
        "generative writing",
        "benchmarks",
        "writing domains",
        "subdomains",
        "creative writing",
        "persuasive writing",
        "informative writing",
        "technical writing",
        "query-dependent evaluation framework",
        "instance-specific assessment criteria",
        "critic model",
        "criteria-aware scoring",
        "data curation"
      ]
    },
    "publishedAt": "2025-03-07T03:56:20.000Z",
    "title": "WritingBench: A Comprehensive Benchmark for Generative Writing",
    "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05244.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04629",
      "authors": [
        {
          "_id": "67cfbab6607797f40c6d4164",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4165",
          "name": "Shiyang Feng",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4166",
          "name": "Jiakang Yuan",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4167",
          "name": "Renqiu Xia",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4168",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d4169",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67cfbab6607797f40c6d416a",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T17:15:48.000Z",
      "title": "スライドフォージング：オフラインヒューリスティクス、メモリードライブンジェンレーション、および自動化調査書作成の多次元評価",
      "summary": "調査論文は科学研究に重要な役割を果たし、特に研究論文の急速な増加の背景においてもその重要性が高まっています。最近、研究者はLLMを使用して調査の生成を自動化し、より効率的な研究を実現することに取り組んでいます。しかし、LLMによって生成された調査論文と人間が書いた調査論文の間には、特に綱要の質と引用の正確性において明らかな質の間違いが残っています。これらの間違いを埋めるために、私たちはSurveyForgeを紹介します。SurveyForgeは、人間が書いた綱要の論理的構造を分析し、検索された領域関連論文を参照して綱要を生成します。次に、私たちの学術ナビゲーターから検索された高品質の論文を活用して、生成された論文の内容を自動的に生成し、改良します。また、全体的な評価を実現するために、私たちはSurveyBenchを構築し、100件の人間が書いた調査論文を含む、勝率比較を行うことで、AIが生成した調査論文を参照、綱要、内容の質の3つの次元で評価します。実験は、SurveyForgeが以前のワークよりも上回ることを示しています。",
      "upvotes": 9,
      "discussionId": "67cfbab9607797f40c6d4206",
      "ai_keywords": [
        "LLMs (Large Language Models)",
        "SurveyForge",
        "SurveyBench",
        "AutoSurvey"
      ]
    },
    "publishedAt": "2025-03-06T12:15:48.000Z",
    "title": "SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing",
    "summary": "Survey paper plays a crucial role in scientific research, especially given\nthe rapid growth of research publications. Recently, researchers have begun\nusing LLMs to automate survey generation for better efficiency. However, the\nquality gap between LLM-generated surveys and those written by human remains\nsignificant, particularly in terms of outline quality and citation accuracy. To\nclose these gaps, we introduce SurveyForge, which first generates the outline\nby analyzing the logical structure of human-written outlines and referring to\nthe retrieved domain-related articles. Subsequently, leveraging high-quality\npapers retrieved from memory by our scholar navigation agent, SurveyForge can\nautomatically generate and refine the content of the generated article.\nMoreover, to achieve a comprehensive evaluation, we construct SurveyBench,\nwhich includes 100 human-written survey papers for win-rate comparison and\nassesses AI-generated survey papers across three dimensions: reference,\noutline, and content quality. Experiments demonstrate that SurveyForge can\noutperform previous works such as AutoSurvey.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04629.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04812",
      "authors": [
        {
          "_id": "67ce5542818e1825dea7440b",
          "user": {
            "_id": "6626449503e1f561573d30e9",
            "avatarUrl": "/avatars/e7f9720ccd01bae32d0a03a1b0dacab5.svg",
            "isPro": false,
            "fullname": "Zhibin Lan",
            "user": "zhibinlan",
            "type": "user"
          },
          "name": "Zhibin Lan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:54.535Z",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440c",
          "user": {
            "_id": "635239137d071f23d083b056",
            "avatarUrl": "/avatars/1f1a0ed38d8de499d4b78922801c6d95.svg",
            "isPro": false,
            "fullname": "liqiang niu",
            "user": "lqniu",
            "type": "user"
          },
          "name": "Liqiang Niu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:00:51.713Z",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440d",
          "name": "Fandong Meng",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440e",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5542818e1825dea7440f",
          "name": "Jinsong Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T10:21:57.000Z",
      "title": "LLaVE: 大規模言語と視覚のエンベッディングモデルにおける難易度重み付きの比較学習",
      "summary": "Universal multimodal embedding models play a critical role in tasks such as interleaved image-text retrieval, multimodal RAG, and multimodal clustering. However, our empirical results indicate that existing LMM-based embedding models trained with the standard InfoNCE loss exhibit a high degree of overlap in similarity distribution between positive and negative pairs, making it challenging to distinguish hard negative pairs effectively. To deal with this issue, we propose a simple yet effective framework that dynamically improves the embedding model's representation learning for negative pairs based on their discriminative difficulty. Within this framework, we train a series of models, named LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks and 36 datasets. Experimental results show that LLaVE establishes stronger baselines that achieve state-of-the-art (SOTA) performance while demonstrating strong scalability and efficiency. Specifically, LLaVE-2B surpasses the previous SOTA 7B models, while LLaVE-7B achieves a further performance improvement of 6.2 points. Although LLaVE is trained on image-text data, it can generalize to text-video retrieval tasks in a zero-shot manner and achieve strong performance, demonstrating its remarkable potential for transfer to other embedding tasks.",
      "upvotes": 9,
      "discussionId": "67ce5543818e1825dea74480",
      "githubRepo": "https://github.com/DeepLearnXMU/LLaVE",
      "ai_keywords": [
        "multimodal embedding models",
        "interleaved image-text retrieval",
        "multimodal RAG",
        "multimodal clustering",
        "LMM-based embedding models",
        "InfoNCE loss",
        "similarity distribution",
        "hard negative pairs",
        "representation learning",
        "LLaVE",
        "MMEB benchmark",
        "meta-tasks",
        "datasets",
        "state-of-the-art (SOTA)",
        "scalability",
        "efficiency",
        "text-video retrieval tasks",
        "zero-shot manner",
        "transfer"
      ]
    },
    "publishedAt": "2025-03-04T05:21:57.000Z",
    "title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted\n  Contrastive Learning",
    "summary": "Universal multimodal embedding models play a critical role in tasks such as\ninterleaved image-text retrieval, multimodal RAG, and multimodal clustering.\nHowever, our empirical results indicate that existing LMM-based embedding\nmodels trained with the standard InfoNCE loss exhibit a high degree of overlap\nin similarity distribution between positive and negative pairs, making it\nchallenging to distinguish hard negative pairs effectively. To deal with this\nissue, we propose a simple yet effective framework that dynamically improves\nthe embedding model's representation learning for negative pairs based on their\ndiscriminative difficulty. Within this framework, we train a series of models,\nnamed LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks\nand 36 datasets. Experimental results show that LLaVE establishes stronger\nbaselines that achieve state-of-the-art (SOTA) performance while demonstrating\nstrong scalability and efficiency. Specifically, LLaVE-2B surpasses the\nprevious SOTA 7B models, while LLaVE-7B achieves a further performance\nimprovement of 6.2 points. Although LLaVE is trained on image-text data, it can\ngeneralize to text-video retrieval tasks in a zero-shot manner and achieve\nstrong performance, demonstrating its remarkable potential for transfer to\nother embedding tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04812.png",
    "numComments": 2,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07459",
      "authors": [
        {
          "_id": "67cfd1934fed2b7e3e4cbb34",
          "user": {
            "_id": "63357c608adfa81faf2ac180",
            "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
            "isPro": false,
            "fullname": "Xiangru Tang",
            "user": "RTT1",
            "type": "user"
          },
          "name": "Xiangru Tang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T06:00:52.457Z",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb35",
          "name": "Daniel Shao",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb36",
          "name": "Jiwoong Sohn",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb37",
          "name": "Jiapeng Chen",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb38",
          "name": "Jiayi Zhang",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb39",
          "name": "Jinyu Xiang",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3a",
          "name": "Fang Wu",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3b",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3c",
          "name": "Chenglin Wu",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3d",
          "user": {
            "_id": "65cae89119683f9817c049ea",
            "avatarUrl": "/avatars/b08b10d7c72e2cf1108147e659411b32.svg",
            "isPro": false,
            "fullname": "Wenqi Shi",
            "user": "wshi83",
            "type": "user"
          },
          "name": "Wenqi Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:16.321Z",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3e",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "67cfd1934fed2b7e3e4cbb3f",
          "name": "Mark Gerstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T15:38:44.000Z",
      "title": "MedAgentsBench: 複雑な医学論理を評価する思考モデルとアガントフレームワークのベンチマーク",
      "summary": "大語言モデル（LLMs）は、現在の医学問答ベンチマークで驚人的な性能を示しています。この高い性能により、意味的に評価および先進的な方法の違いを明確にすることが難しくなります。私たちは、現在のモデルが標準テストで強い性能を示しているが、複数ステップの臨床的な理由、診断の構成、治療計画のシナリオに焦点を当てたベンチマーク「MedAgentsBench」を紹介します。このベンチマークは7つの既存の医学データセットから構成され、既存の評価における3つの主要な制限を解決します：（1）簡単な問題の多く、それほど基礎モデルでも高い性能を達成する場合、（2）研究間の不均一なサンプリングと評価プロトコル、（3）性能、コスト、推論時間の相互作用のシステマティックな分析の欠如。基礎モデルと理由方法の様々な実験を通じて、私たちは最新のモデル、DeepSeek R1とOpenAI o3が複雑な医学的な理由タスクで特別な性能を示していることを示します。また、先進的な検索ベースアグエント方法は、伝統的なアプローチに比べて望ましい性能とコストの比率を示しています。私たちの分析は、複雑な問題でのモデルフamiliesの間の大きな性能間違いを明らかにし、異なる計算制約に対する最適なモデル選択を特定します。私たちのベンチマークと評価フレームワークは、https://github.com/gersteinlab/medagents-benchmark で公開しています。",
      "upvotes": 7,
      "discussionId": "67cfd1944fed2b7e3e4cbb81",
      "ai_keywords": [
        "MedAgentsBench",
        "multi-step clinical reasoning",
        "diagnosis formulation",
        "treatment planning",
        "MedAgentsBench",
        "DeepSeek R1",
        "OpenAI o3",
        "advanced search-based agent methods"
      ]
    },
    "publishedAt": "2025-03-10T11:38:44.000Z",
    "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning",
    "summary": "Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https://github.com/gersteinlab/medagents-benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07459.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06749",
      "authors": [
        {
          "_id": "67cfb6495944a8e54f24cd9a",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9b",
          "name": "Bohan Jia",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9c",
          "name": "Zijie Zhai",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9d",
          "name": "Shaosheng Cao",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9e",
          "name": "Zheyu Ye",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cd9f",
          "name": "Fei Zhao",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cda0",
          "name": "Yao Hu",
          "hidden": false
        },
        {
          "_id": "67cfb6495944a8e54f24cda1",
          "name": "Shaohui Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T20:06:45.000Z",
      "title": "ビジョン-R1: 多モデル大語言モデルの理由能力を奨励する",
      "summary": "DeepSeek-R1-Zeroは、強化学習(RL)によってLLMの理由論的認知能力の発見を成功しました。この突破から受け続け、RLがMLLMの理由論的認知能力を向上させることができるかを調査しました。しかし、直接RLを用いた学習は、MLLMにおける複雑な理由論的認知能力（例えば、質問と反省）を活性化することが難しいことに気付いています。この問題を解決するために、理由論的MLLM、Vision-R1を提案しました。特に、既存のMLLMとDeepSeek-R1をモデル橋渡しとデータフィルタリングを用いて、高品質な多タイプ理由論データを構築し、200Kの多タイプ理由論データセット、Vision-R1-coldデータセットを作成しました。これはVision-R1の冷開始初期化データとして使用されます。冷開始後の過度思考による最適化の課題を軽減するために、進歩的な思考抑制学習(PTST)戦略を提案し、グループ相対的ポリシー最適化(GRPO)と厳格な形式化結果報酬関数を用いて、10Kの多タイプ数学データセットで正しい複雑な理由論プロセスの学習能力を徐々に鍛錬しました。詳細な実験は、我々のモデルが多タイプ数学理由論ベンチマーク上で平均6%程度の向上を達成しました。Vision-R1-7Bは、広く使用されているMathVistaベンチマークで73.5%の精度を達成し、リーディングの理由論モデル、OpenAI O1よりも0.4%だけ低いことを示しました。データセットとコードは以下のURLで公開されます: https://github.com/Osilly/Vision-R1",
      "upvotes": 7,
      "discussionId": "67cfb64f5944a8e54f24cf33",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "MLLMs",
        "multimodal reasoning",
        "CoT dataset",
        "modality bridging",
        "data filtering",
        "Vision-R1-cold dataset",
        "Progressive Thinking Suppression Training (PTST)",
        "Group Relative Policy Optimization (GRPO)",
        "hard formatting result reward function",
        "multimodal math dataset",
        "MathVista benchmark",
        "OpenAI O1"
      ]
    },
    "publishedAt": "2025-03-09T16:06:45.000Z",
    "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models",
    "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning\ncapabilities in LLMs purely through Reinforcement Learning (RL). Inspired by\nthis breakthrough, we explore how RL can be utilized to enhance the reasoning\ncapability of MLLMs. However, direct training with RL struggles to activate\ncomplex reasoning capabilities such as questioning and reflection in MLLMs, due\nto the absence of substantial high-quality multimodal reasoning data. To\naddress this issue, we propose the reasoning MLLM, Vision-R1, to improve\nmultimodal reasoning capability. Specifically, we first construct a\nhigh-quality multimodal CoT dataset without human annotations by leveraging an\nexisting MLLM and DeepSeek-R1 through modality bridging and data filtering to\nobtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as\ncold-start initialization data for Vision-R1. To mitigate the optimization\nchallenges caused by overthinking after cold start, we propose Progressive\nThinking Suppression Training (PTST) strategy and employ Group Relative Policy\nOptimization (GRPO) with the hard formatting result reward function to\ngradually refine the model's ability to learn correct and complex reasoning\nprocesses on a 10K multimodal math dataset. Comprehensive experiments show our\nmodel achieves an average improvement of sim6% across various multimodal\nmath reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely\nused MathVista benchmark, which is only 0.4% lower than the leading reasoning\nmodel, OpenAI O1. The datasets and code will be released in:\nhttps://github.com/Osilly/Vision-R1 .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06749.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07507",
      "authors": [
        {
          "_id": "67cfa44c3a9d50150f59ffe1",
          "name": "Jie Hu",
          "hidden": false
        },
        {
          "_id": "67cfa44c3a9d50150f59ffe2",
          "name": "Shizun Wang",
          "hidden": false
        },
        {
          "_id": "67cfa44c3a9d50150f59ffe3",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T16:29:10.000Z",
      "title": "PE3R: 観測効率的3次元再構築",
      "summary": "最近の2Dから3Dの認識の進展は、2D画像から3Dスケーンを理解することができるように大幅に改善しました。しかし、現在の方法は、スケーン間の有限な一般化、認識精度の低さ、重建速度の遅さなどの重要な課題を見せています。これらの制限を解決するために、私たちは、精度と効率を両立させるための新しいフレームワーク「Perception-Efficient 3D Reconstruction (PE3R)」を提案します。PE3Rは、3D語義フィールドの高速重建を可能にするための前向きアーキテクチャを使用しています。このフレームワークは、多様なスケーンと物体に対して強固なゼロショット一般化を示し、重建速度を大幅に向上させることができます。2Dから3Dの開放ボキャブラリー分割と3D重建においての拡散的な実験は、PE3Rの効果および広泛性を証明しました。このフレームワークは、3D語義フィールドの重建速度に9倍以上のスピードアップを達成し、認識精度と重建精度にも大幅な効果を見出し、この分野で新しいベンチマークを設定しました。コードは、https://github.com/hujiecpp/PE3R で公開しています。",
      "upvotes": 5,
      "discussionId": "67cfa4503a9d50150f5a0137",
      "ai_keywords": [
        "feed-forward architecture",
        "3D semantic field reconstruction",
        "zero-shot generalization",
        "2D-to-3D open-vocabulary segmentation",
        "perception accuracy",
        "reconstruction precision",
        "speedup"
      ]
    },
    "publishedAt": "2025-03-10T12:29:10.000Z",
    "title": "PE3R: Perception-Efficient 3D Reconstruction",
    "summary": "Recent advancements in 2D-to-3D perception have significantly improved the\nunderstanding of 3D scenes from 2D images. However, existing methods face\ncritical challenges, including limited generalization across scenes, suboptimal\nperception accuracy, and slow reconstruction speeds. To address these\nlimitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel\nframework designed to enhance both accuracy and efficiency. PE3R employs a\nfeed-forward architecture to enable rapid 3D semantic field reconstruction. The\nframework demonstrates robust zero-shot generalization across diverse scenes\nand objects while significantly improving reconstruction speed. Extensive\nexperiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction\nvalidate the effectiveness and versatility of PE3R. The framework achieves a\nminimum 9-fold speedup in 3D semantic field reconstruction, along with\nsubstantial gains in perception accuracy and reconstruction precision, setting\nnew benchmarks in the field. The code is publicly available at:\nhttps://github.com/hujiecpp/PE3R.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07507.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07197",
      "authors": [
        {
          "_id": "67cfa76cf36e4221c5009654",
          "user": {
            "_id": "624f909eac5dd186b01ac3f5",
            "avatarUrl": "/avatars/71a5c93c491064ef9e1eda80fda90665.svg",
            "isPro": false,
            "fullname": "Zebin You",
            "user": "yyyou",
            "type": "user"
          },
          "name": "Zebin You",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:22:38.059Z",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009655",
          "name": "Jingyang Ou",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009656",
          "name": "Xiaolu Zhang",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009657",
          "name": "Jun Hu",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009658",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "67cfa76cf36e4221c5009659",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T11:27:12.000Z",
      "title": "効果的であるかつ効率的なマスク画像生成モデル",
      "summary": "隠れコードモデルと隠れ拡散モデルは異なる動機と目的で設計されていますが、私たちはそれらを一つのフレームワーク内に統合できることを見出しました。この見解に基づき、訓練とサンプリングの設計空間を詳しく調べ、性能と効率に寄与する要因を特定しました。この調査中の改善を基に、私たちのモデルを eMIGM として開発しました。実験的には、eMIGM は ImageNet 生成の性能において強力な効果を示しました。特に、ImageNet 256x256 で、類似の関数評価回数（NFEs）とモデルパラメータ数で eMIGM は先進的な VAR を上回りました。NFE とモデルパラメータ数が増加するにつれて、eMIGM は連続的な拡散モデルと同等の性能を達成しますが、40% 未満の NFE を必要とします。また、ImageNet 512x512 では、NFE の約 60% で eMIGM は連続的な拡散モデルを上回ります。",
      "upvotes": 5,
      "discussionId": "67cfa76df36e4221c5009686",
      "ai_keywords": [
        "masked image generation models",
        "masked diffusion models",
        "training and sampling",
        "Fr\\'echet Inception Distance (FID)",
        "function evaluations (NFEs)",
        "VAR",
        "continuous diffusion models"
      ]
    },
    "publishedAt": "2025-03-10T07:27:12.000Z",
    "title": "Effective and Efficient Masked Image Generation Models",
    "summary": "Although masked image generation models and masked diffusion models are\ndesigned with different motivations and objectives, we observe that they can be\nunified within a single framework. Building upon this insight, we carefully\nexplore the design space of training and sampling, identifying key factors that\ncontribute to both performance and efficiency. Based on the improvements\nobserved during this exploration, we develop our model, referred to as eMIGM.\nEmpirically, eMIGM demonstrates strong performance on ImageNet generation, as\nmeasured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet\n256x256, with similar number of function evaluations (NFEs) and model\nparameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model\nparameters increase, eMIGM achieves performance comparable to the\nstate-of-the-art continuous diffusion models while requiring less than 40% of\nthe NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE,\neMIGM outperforms the state-of-the-art continuous diffusion models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07197.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06520",
      "authors": [
        {
          "_id": "67cf990ca80a73999cc816c3",
          "name": "Yuqi Liu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c4",
          "name": "Bohao Peng",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c5",
          "name": "Zhisheng Zhong",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c6",
          "name": "Zihao Yue",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c7",
          "name": "Fanbin Lu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c8",
          "name": "Bei Yu",
          "hidden": false
        },
        {
          "_id": "67cf990ca80a73999cc816c9",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T08:48:51.000Z",
      "title": "Seg-Zero: 認知強化による理由連鎖ガイドされた分割",
      "summary": "傳統的推論分割手法は、分類ラベルと簡単な説明を用いた規範付きの微調節に依存し、領域外の一般化能力を制限し、明示的な推論プロセスを欠く。これらの制限を解決するために、私たちはSeg-Zeroという新しいフレームワークを提案します。このフレームワークは、驚異的な一般化能力を示し、認知強化を通じて明示的な推論チェーンを得ることができます。Seg-Zeroは、理由モデルと分割モデルからなるデコープレードアーキテクチャを導入しています。理由モデルは、ユーザーの意図を理解し、明示的な推論チェーンを生成し、次に分割モデルがピクセルレベルのマスクを生成するための位置プロンプトを生成します。私たちは、フォーマットと精度のペナルティーを統合した複雑な報酬機構を設計し、GRPOを用いた強化学習によって独自に訓練し、明示的な推論データを必要としないことで、Seg-Zeroは強固なゼロショットの一般化能力を示し、検証時の推論能力を発見します。実験は、Seg-Zero-7BはReasonSegベンチマークでゼロショット性能が57.5となり、先週のLISA-7Bを18%超えたことを示します。この顕著な向上は、Seg-Zeroが領域を横断して一般化する能力を示し、明示的な推論プロセスを提供することを示しています。コードは、https://github.com/dvlab-research/Seg-Zeroから利用可能です。",
      "upvotes": 5,
      "discussionId": "67cf990da80a73999cc81723",
      "ai_keywords": [
        "Seg-Zero",
        "decoupled architecture",
        "reasoning model",
        "segmentation model",
        "positional prompts",
        "pixel-level masks",
        "cognitive reinforcement",
        "reward mechanism",
        "reinforcement learning",
        "GRPO",
        "zero-shot generalization",
        "ReasonSeg benchmark",
        "emergent test-time reasoning"
      ]
    },
    "publishedAt": "2025-03-09T04:48:51.000Z",
    "title": "Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement",
    "summary": "Traditional methods for reasoning segmentation rely on supervised fine-tuning\nwith categorical labels and simple descriptions, limiting its out-of-domain\ngeneralization and lacking explicit reasoning processes. To address these\nlimitations, we propose Seg-Zero, a novel framework that demonstrates\nremarkable generalizability and derives explicit chain-of-thought reasoning\nthrough cognitive reinforcement. Seg-Zero introduces a decoupled architecture\nconsisting of a reasoning model and a segmentation model. The reasoning model\ninterprets user intentions, generates explicit reasoning chains, and produces\npositional prompts, which are subsequently used by the segmentation model to\ngenerate precious pixel-level masks. We design a sophisticated reward mechanism\nthat integrates both format and accuracy rewards to effectively guide\noptimization directions. Trained exclusively via reinforcement learning with\nGRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot\ngeneralization and exhibits emergent test-time reasoning capabilities.\nExperiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on\nthe ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant\nimprovement highlights Seg-Zero's ability to generalize across domains while\npresenting an explicit reasoning process. Code is available at\nhttps://github.com/dvlab-research/Seg-Zero.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06520.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06121",
      "authors": [
        {
          "_id": "67cfa436d37b8309603da1ee",
          "user": {
            "_id": "66de61d7174e9c6971dbb253",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sM0xfS7HAkf_6GmkEGjDk.png",
            "isPro": false,
            "fullname": "Alic Li",
            "user": "Alic-Li",
            "type": "user"
          },
          "name": "Li weile",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T04:29:57.207Z",
          "hidden": false
        },
        {
          "_id": "67cfa436d37b8309603da1ef",
          "user": {
            "_id": "6176b32847ee6431f632981e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg",
            "isPro": false,
            "fullname": "IvanD",
            "user": "xiaol",
            "type": "user"
          },
          "name": "Liu Xiao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T03:33:06.087Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-08T08:31:18.000Z",
      "title": "ブラックゴーストリマー：RWKV-7を単純で優れているTransformersの代替として、大規模な時系列モデリングに適用する方法",
      "summary": "時系列モデルは、大規模な複雑なデータセットを扱うために、大規模な言語モデル（LLMs）と同様のスケーリングを実現するために重大な課題を見せています。時系列データの特徴とモデルスケーリングの計算要求により、新しいアプローチが必要となります。研究者は、Transformers、LSTMs、GRUsなどの様々なアーキテクチャを用いてこれらの課題を解決するために調査を行いましたが、RWKV-7を用いてメタラーニングを採用した新しい解決策を提案します。RWKV-7の時間ミックスとチャネルミックスコンポーネントをTransformerベースの時系列モデルTimerに統合し、パラメータ数の1/23を使用しながら、約1.13倍から43.3倍の性能向上と訓練時間の4.5倍減少を実現しました。我々のコードとモデル重みは、https://github.com/Alic-Li/BlackGoose_Rimer で公開しており、進める研究と開発に供給できます。",
      "upvotes": 5,
      "discussionId": "67cfa437d37b8309603da253",
      "ai_keywords": [
        "Transformers",
        "LSTMs",
        "GRUs",
        "RWKV-7",
        "meta-learning",
        "state update mechanism",
        "time mix",
        "channel mix",
        "Timer",
        "performance improvement",
        "training time",
        "parameters"
      ]
    },
    "publishedAt": "2025-03-08T03:31:18.000Z",
    "title": "BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement\n  for Transformers in Large-Scale Time Series Modeling",
    "summary": "Time series models face significant challenges in scaling to handle large and\ncomplex datasets, akin to the scaling achieved by large language models (LLMs).\nThe unique characteristics of time series data and the computational demands of\nmodel scaling necessitate innovative approaches. While researchers have\nexplored various architectures such as Transformers, LSTMs, and GRUs to address\nthese challenges, we propose a novel solution using RWKV-7, which incorporates\nmeta-learning into its state update mechanism. By integrating RWKV-7's time mix\nand channel mix components into the transformer-based time series model Timer,\nwe achieve a substantial performance improvement of approximately 1.13 to 43.3x\nand a 4.5x reduction in training time with 1/23 parameters, all while utilizing\nfewer parameters. Our code and model weights are publicly available for further\nresearch and development at https://github.com/Alic-Li/BlackGoose_Rimer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06121.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03499",
      "authors": [
        {
          "_id": "67cb02680a2a716f25805cb4",
          "name": "Wonjun Kang",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb5",
          "name": "Kevin Galim",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb6",
          "name": "Yuchen Zeng",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb7",
          "name": "Minjae Lee",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb8",
          "name": "Hyung Il Koo",
          "hidden": false
        },
        {
          "_id": "67cb02680a2a716f25805cb9",
          "name": "Nam Ik Cho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T13:44:42.000Z",
      "title": "ステートオフセットチューニング：ステートベースパラメーター効率的な微調整のためのステートスペースモデル",
      "summary": "ステートスペースモデル（SSMs）は、Transformersの二次的な計算コストを軽減するために、より効率的な代替手段として現れてきました。しかし、Parameter-Efficient Fine-Tuning（PEFT）メソッドのSSMsへの応用は、大きく調査されていません。特に、Transformersで広く使用されるプロンプトベースの方法であるPrompt TuningとPrefix-Tuningは、SSMsではより良い性能を示せません。これに対して、私たちは、プロンプトベースの方法に対して優れた代替手段として、ステートベースの方法を提案します。この新しいメソッドの家族は、SSMsの構造的な特徴から自然に生まれてきます。ステートベースの方法は、外部のプロンプトに依存しないで、直接ステート関連の特徴を調整します。また、私たちは、新しいステートベースのPEFTメソッド、State-offset Tuningを紹介します。毎時間ステップで、私たちの方法は現在のステップのステートを直接影響し、より効果的な適応を実現します。多様なデータセットを構成する試験を通じて、私たちの方法の効果性を示します。コードは、https://github.com/furiosa-ai/ssm-state-tuning にアクセスできます。",
      "upvotes": 3,
      "discussionId": "67cb02690a2a716f25805cfd",
      "githubRepo": "https://github.com/furiosa-ai/ssm-state-tuning",
      "ai_keywords": [
        "State Space Models (SSMs)",
        "Parameter-Efficient Fine-Tuning (PEFT)",
        "Prompt Tuning",
        "Prefix-Tuning",
        "State-based methods",
        "State-offset Tuning",
        "timesteps",
        "state-related features",
        "state-at-the-current-step"
      ]
    },
    "publishedAt": "2025-03-05T08:44:42.000Z",
    "title": "State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models",
    "summary": "State Space Models (SSMs) have emerged as efficient alternatives to\nTransformers, mitigating their quadratic computational cost. However, the\napplication of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains\nlargely unexplored. In particular, prompt-based methods like Prompt Tuning and\nPrefix-Tuning, which are widely used in Transformers, do not perform well on\nSSMs. To address this, we propose state-based methods as a superior alternative\nto prompt-based methods. This new family of methods naturally stems from the\narchitectural characteristics of SSMs. State-based methods adjust state-related\nfeatures directly instead of depending on external prompts. Furthermore, we\nintroduce a novel state-based PEFT method: State-offset Tuning. At every\ntimestep, our method directly affects the state at the current step, leading to\nmore effective adaptation. Through extensive experiments across diverse\ndatasets, we demonstrate the effectiveness of our method. Code is available at\nhttps://github.com/furiosa-ai/ssm-state-tuning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03499.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07603",
      "authors": [
        {
          "_id": "67cfc310f2b1fe815dc24ebf",
          "name": "Sedrick Keh",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec0",
          "name": "Jean Mercat",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec1",
          "name": "Samir Yitzhak Gadre",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec2",
          "name": "Kushal Arora",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec3",
          "name": "Igor Vasiljevic",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec4",
          "name": "Benjamin Burchfiel",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec5",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec6",
          "name": "Russ Tedrake",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec7",
          "name": "Thomas Kollar",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec8",
          "name": "Ludwig Schmidt",
          "hidden": false
        },
        {
          "_id": "67cfc310f2b1fe815dc24ec9",
          "name": "Achal Dave",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:19.000Z",
      "title": "VLMsは画像データとして予習訓練するべきですか？",
      "summary": "前処理されたLLMsは、画像データとの進一部学習により、視覚言語タスクでは優れた性能を示す。第二の学習ステップに画像を追加することで、この能力を開放することは効果的であるが、この2ステップパイプラインがそれほど早い画像の統合を行ったVLMsよりどのような効果や損失があるかは明確ではない。これについて調査するために、データセット、スケール、画像テキスト比、前処理を行ったことの量を範囲に制限してモデルを学習し、その後、これらのモデルを微調節し、視覚言語タスクとテキストだけのタスクの下流性能を評価する。そして、画像とテキストデータの混合での前処理を行ったモデルは、テキストだけの評価でも強い性能を維持しながら、視覚言語タスクではより良い性能を示すことがわかる。6種類の多様なタスクの平均値では、1Bモデルでは、前処理の80%の段階で画像トークンを追加することにより、完全に前処理されたモデルに画像トークンを追加することによる平均的な改善率は2%となる。",
      "upvotes": 2,
      "discussionId": "67cfc314f2b1fe815dc24fe3",
      "ai_keywords": [
        "pre-trained LLMs",
        "vision-language tasks",
        "fine-tune",
        "vision tokens"
      ]
    },
    "publishedAt": "2025-03-10T13:58:19.000Z",
    "title": "Should VLMs be Pre-trained with Image Data?",
    "summary": "Pre-trained LLMs that are further trained with image data perform well on\nvision-language tasks. While adding images during a second training phase\neffectively unlocks this capability, it is unclear how much of a gain or loss\nthis two-step pipeline gives over VLMs which integrate images earlier into the\ntraining process. To investigate this, we train models spanning various\ndatasets, scales, image-text ratios, and amount of pre-training done before\nintroducing vision tokens. We then fine-tune these models and evaluate their\ndownstream performance on a suite of vision-language and text-only tasks. We\nfind that pre-training with a mixture of image and text data allows models to\nperform better on vision-language tasks while maintaining strong performance on\ntext-only evaluations. On an average of 6 diverse tasks, we find that for a 1B\nmodel, introducing visual tokens 80% of the way through pre-training results in\na 2% average improvement over introducing visual tokens to a fully pre-trained\nmodel.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07603.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06885",
      "authors": [
        {
          "_id": "67cfc1c5182d970d40896a5e",
          "user": {
            "_id": "655b813476e4fad5529f3256",
            "avatarUrl": "/avatars/73d83e45d921531f9830a0ea80f76491.svg",
            "isPro": false,
            "fullname": "Yan Yang",
            "user": "HelloKKMe",
            "type": "user"
          },
          "name": "Yan Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T08:21:28.574Z",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a5f",
          "user": {
            "_id": "6357362f811ee2fa05070f64",
            "avatarUrl": "/avatars/2cf37efb80f5cfb3e4e9d08674de6dd1.svg",
            "isPro": false,
            "fullname": "Dongxu Li",
            "user": "dxli1",
            "type": "user"
          },
          "name": "Dongxu Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T04:53:27.837Z",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a60",
          "name": "Haoning Wu",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a61",
          "name": "Bei Chen",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a62",
          "name": "Liu Liu",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a63",
          "name": "Liyuan Pan",
          "hidden": false
        },
        {
          "_id": "67cfc1c5182d970d40896a64",
          "name": "Junnan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T03:29:18.000Z",
      "title": "ProBench: 開放終端の多モデル基盤モデルの評価をめぐる多領域専門家タスクにおける評価",
      "summary": "解決専門家レベルの多モーダルタスクは、一般的な知能の鍵的なマークスです。多モーダル大語言モデル（MLLMs）の能力が継続的に向上している中、その先進的な多モーダル知能の評価は必要であり、難しいです。本稿では、専門家の専門的知識と進歩的な理由を必要とする開放的なユーザークエリーのベンチマークを介して、ProBenchを紹介します。ProBenchは、10分野と56サブ分野の幅広い範囲で、科学、芸術、人文学、コーディング、数学、そして創造的な著書にわたります。実験的に、24最新モデルをMLLM-as-a-Judgeを用いて評価し、比較しました。結果から、開放ソースモデルの最良ものは、プロプライエーションモデルと試合していますが、ProBenchは、視覚的知識、文脈的理解、分野の知識、進歩的な理由において、重要な課題を提示し、将来の多モーダルAI研究の方向を提供します。",
      "upvotes": 2,
      "discussionId": "67cfc1c7182d970d40896b1d",
      "projectPage": "https://yan98.github.io/ProBench/",
      "githubRepo": "https://github.com/Yan98/ProBench_eval",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "benchmark",
        "user queries",
        "professional expertise",
        "advanced reasoning",
        "high-quality samples",
        "daily productivity demands",
        "fields",
        "sub-fields",
        "visual perception",
        "textual understanding",
        "domain knowledge"
      ]
    },
    "publishedAt": "2025-03-09T23:29:18.000Z",
    "title": "ProBench: Judging Multimodal Foundation Models on Open-ended\n  Multi-domain Expert Tasks",
    "summary": "Solving expert-level multimodal tasks is a key milestone towards general\nintelligence. As the capabilities of multimodal large language models (MLLMs)\ncontinue to improve, evaluation of such advanced multimodal intelligence\nbecomes necessary yet challenging. In this work, we introduce ProBench, a\nbenchmark of open-ended user queries that require professional expertise and\nadvanced reasoning. ProBench consists of 4,000 high-quality samples\nindependently submitted by professionals based on their daily productivity\ndemands. It spans across 10 fields and 56 sub-fields, including science, arts,\nhumanities, coding, mathematics, and creative writing. Experimentally, we\nevaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal\nthat although the best open-source models rival the proprietary ones, ProBench\npresents significant challenges in visual perception, textual understanding,\ndomain knowledge and advanced reasoning, thus providing valuable directions for\nfuture multimodal AI research efforts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06885.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.02199",
      "authors": [
        {
          "_id": "67c90dad6f3ef3c2c77689b0",
          "name": "Ailin Deng",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b1",
          "name": "Tri Cao",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b2",
          "user": {
            "_id": "67cbb6ea2cc05acaab023f75",
            "avatarUrl": "/avatars/79272c8889a8c472cf75172ead72daea.svg",
            "isPro": false,
            "fullname": "Zhirui Chen",
            "user": "ryanchen42",
            "type": "user"
          },
          "name": "Zhirui Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:03:34.042Z",
          "hidden": false
        },
        {
          "_id": "67c90dad6f3ef3c2c77689b3",
          "name": "Bryan Hooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:21:07.000Z",
      "title": "Words or Vision: 言葉と視覚: 言葉を信じるのか、視覚を信じるのか？",
      "summary": "ビジョン・ラジオ言語モデル（VLMs）は、ビジョンセンター的なタスクで画像と文脈の情報を統合するところで優れていますが、モデル間の不対称性の調和は調査が少ないです。ビジョンセンター的な設定で、画像データと多様な文脈の入力に対するVLMsのモデル間の好みを調査します。4つのビジョンセンター的なタスクに文脈の変化を引き、10つのVision-Language Models（VLMs）を評価することで、「文脈に盲信」の現象を発見しました：VLMsは、不対称性がある場合に、画像データよりも文脈データに過度に信頼し、汚染された文脈による性能の低下と安全性の疑問を引き起こします。文脈のバイアスに影響する要因を分析します。指示プラント、言語モデルのサイズ、文脈の関連性、トークンの順番、画像と文脈の確信度の相互作用などを含む。言語モデルの位置バイアスを引き起こすため、トークンの順番などの要因は文脈のバイアスを悪化させることがあります。この問題を解決するために、文脈の拡張とスーパーバイダスファイナルチューニングを試み、その効果を示します。また、理論的な分析を提供し、文脈に盲信の現象は訓練中の純粋な文脈と多様モデルデータの不均衡から原因があることを示します。これらの発見は、VLMsでのモデル間の相互作用の調整と訓練のバランスの重要性を強調し、多様モデルデータの不対称性を処理することでの強固さと信頼性を向上させるために必要としています。",
      "upvotes": 2,
      "discussionId": "67c90dae6f3ef3c2c77689ec",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "blind faith in text",
        "modality preferences",
        "textual variations",
        "vision-centric tasks",
        "text bias",
        "instruction prompts",
        "language model size",
        "token order",
        "positional biases",
        "multi-modal data",
        "supervised fine-tuning",
        "text augmentation",
        "balanced training",
        "modality interactions"
      ]
    },
    "publishedAt": "2025-03-03T21:21:07.000Z",
    "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
    "summary": "Vision-Language Models (VLMs) excel in integrating visual and textual\ninformation for vision-centric tasks, but their handling of inconsistencies\nbetween modalities is underexplored. We investigate VLMs' modality preferences\nwhen faced with visual data and varied textual inputs in vision-centered\nsettings. By introducing textual variations to four vision-centric tasks and\nevaluating ten Vision-Language Models (VLMs), we discover a ``blind faith\nin text'' phenomenon: VLMs disproportionately trust textual data over visual\ndata when inconsistencies arise, leading to significant performance drops under\ncorrupted text and raising safety concerns. We analyze factors influencing this\ntext bias, including instruction prompts, language model size, text relevance,\ntoken order, and the interplay between visual and textual certainty. While\ncertain factors, such as scaling up the language model size, slightly mitigate\ntext bias, others like token order can exacerbate it due to positional biases\ninherited from language models. To address this issue, we explore supervised\nfine-tuning with text augmentation and demonstrate its effectiveness in\nreducing text bias. Additionally, we provide a theoretical analysis suggesting\nthat the blind faith in text phenomenon may stem from an imbalance of pure text\nand multi-modal data during training. Our findings highlight the need for\nbalanced training and careful consideration of modality interactions in VLMs to\nenhance their robustness and reliability in handling multi-modal data\ninconsistencies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02199.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07595",
      "authors": [
        {
          "_id": "67cfa440aff9c98bb3f45a56",
          "user": {
            "_id": "64b3fc1fa24816979609dcb3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b3fc1fa24816979609dcb3/cHRMs4YegRcgbZO8_bBaZ.jpeg",
            "isPro": false,
            "fullname": "Sinclair Schneider",
            "user": "SinclairSchneider",
            "type": "user"
          },
          "name": "Sinclair Schneider",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-11T02:48:03.261Z",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a57",
          "name": "Florian Steuber",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a58",
          "name": "Joao A. G. Schneider",
          "hidden": false
        },
        {
          "_id": "67cfa440aff9c98bb3f45a59",
          "name": "Gabi Dreo Rodosek",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:56:25.000Z",
      "title": "大語言モデルの検出回避テクニック",
      "summary": "大規模言語モデルの普及は、広泛な使用につながりましたが、その他のリスクも増加しており、ファクトの広がりを可能にしたりしています。このため、DetectGPTなどの分類システムの開発が重要になりました。これらのデタクターは、実験系列で示されたように、エバジョンテクニックに脆弱です。生成モデルの温度の系統的な変更は、浅い学習デタクターが最も信頼性の低いものになりました。再強化学習による生成モデルの微調節は、BERTベースデタクターを回避しました。最後に、再構文化は、DetectGPTなどのゼロショットデタクターに対して90%以上のエバジョンを実現しましたが、文章は元のものと高度に似ていました。既存の研究との比較で、提出された方法のより良い性能が明らかになりました。社会への影響や進める研究の可能性についても議論されています。",
      "upvotes": 1,
      "discussionId": "67cfa441aff9c98bb3f45a95",
      "ai_keywords": [
        "large language models",
        "fake news",
        "classification systems",
        "DetectGPT",
        "evasion techniques",
        "generative models",
        "temperature",
        "shallow learning-detectors",
        "fine-tuning",
        "reinforcement learning",
        "BERT-based-detectors",
        "zero-shot-detectors",
        "rephrasing"
      ]
    },
    "publishedAt": "2025-03-10T13:56:25.000Z",
    "title": "Detection Avoidance Techniques for Large Language Models",
    "summary": "The increasing popularity of large language models has not only led to\nwidespread use but has also brought various risks, including the potential for\nsystematically spreading fake news. Consequently, the development of\nclassification systems such as DetectGPT has become vital. These detectors are\nvulnerable to evasion techniques, as demonstrated in an experimental series:\nSystematic changes of the generative models' temperature proofed shallow\nlearning-detectors to be the least reliable. Fine-tuning the generative model\nvia reinforcement learning circumvented BERT-based-detectors. Finally,\nrephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT,\nalthough texts stayed highly similar to the original. A comparison with\nexisting work highlights the better performance of the presented methods.\nPossible implications for society and further research are discussed.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07595.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07465",
      "authors": [
        {
          "_id": "67cfaaed7f229132171f596b",
          "name": "Ao Wang",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596c",
          "name": "Lihao Liu",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596d",
          "name": "Hui Chen",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596e",
          "name": "Zijia Lin",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f596f",
          "name": "Jungong Han",
          "hidden": false
        },
        {
          "_id": "67cfaaed7f229132171f5970",
          "name": "Guiguang Ding",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T15:42:59.000Z",
      "title": "YOLOE: 時間効率的な、そのモノを見ることができる",
      "summary": "物体検出と分割はコンピュータビジョンアプリケーションで広く使用されていますが、YOLOシリーズなどの伝統的なモデルは、決定的なカテゴリによって制限され、開放シナリオでの適応性が低下します。最近の開放セット方法は、テキストプロンプト、視覚カット、またはプロンプト無しパラダイムを利用してこれを克服していますが、高い計算量要求や部署複雑性により性能と効率の補損を引き起こします。本稿では、YOLOEという一つの高い効率性を持つモデル内で多様な開放プロンプト機構に物体検出と分割を統合し、実時間で何も見られるようにします。テキストプロンプトにおいては、Re-parameterizable Region-Text Alignment (RepRTA) ステラタジーを提案します。これは、再パラメタ化可能な軽量な補助ネットワークを利用して事前学習されたテキストエンボーディングを精確化し、ゼロ推論コストと転送コストを伴って視覚テキストアラインメントを強化します。視覚プロンプトにおいては、Semantic-Activated Visual Prompt Encoder (SAVPE)を提示します。これは、分離された語意と活性バージンを利用して、最小限の複雑性で改善された視覚エンボーディングと精度を実現します。プロンプト無しシナリオにおいては、Lazy Region-Prompt Contrast (LRPC) ステラジーを提示します。これは、構築された大規模なボキャブラリーと専門的なエンボーディングを利用して、すべての物体を識別することを可能にし、費用の高い言語モデル依存性を避けます。拡張された実験は、YOLOEの例外的なゼロショット性能と高い推論効率と低い訓練コストを示します。特に、LVISでは、3倍の少ない訓練コストと1.4倍の推論スピードアップを設定し、YOLOE-v8-SはYOLO-Worldv2-Sを3.5AP超えます。COCOにおいては、YOLOE-v8-Lは閉めたセットのYOLOv8-Lを近い4倍の少ない訓練時間で0.6AP^bと0.4AP^mの効果を収めます。コードとモデルは、https://github.com/THU-MIG/yoloe に提供されています。",
      "upvotes": 0,
      "discussionId": "67cfaaf27f229132171f5ab4",
      "ai_keywords": [
        "Object detection",
        "Segmentation",
        "YOLO series",
        "Open-set methods",
        "Text prompts",
        "Visual cues",
        "Prompt-free paradigm",
        "YOLOE",
        "Rep-parameterizable Region-Text Alignment (RepRTA)",
        "Pretrained textual embeddings",
        "Re-parameterizable lightweight auxiliary network",
        "Semantic-Activated Visual Prompt Encoder (SAVPE)",
        "Decoupled semantic and activation branches",
        "Visual embedding",
        "Lazy Region-Prompt Contrast (LRPC)",
        "Large vocabulary",
        "Specialized embedding",
        "LVIS",
        "Zero-shot performance",
        "Transferability",
        "Inference efficiency",
        "Training cost",
        "AP",
        "COCO",
        "Closed-set YOLOv8-L",
        "Inference speedup",
        "Training time"
      ]
    },
    "publishedAt": "2025-03-10T11:42:59.000Z",
    "title": "YOLOE: Real-Time Seeing Anything",
    "summary": "Object detection and segmentation are widely employed in computer vision\napplications, yet conventional models like YOLO series, while efficient and\naccurate, are limited by predefined categories, hindering adaptability in open\nscenarios. Recent open-set methods leverage text prompts, visual cues, or\nprompt-free paradigm to overcome this, but often compromise between performance\nand efficiency due to high computational demands or deployment complexity. In\nthis work, we introduce YOLOE, which integrates detection and segmentation\nacross diverse open prompt mechanisms within a single highly efficient model,\nachieving real-time seeing anything. For text prompts, we propose\nRe-parameterizable Region-Text Alignment (RepRTA) strategy. It refines\npretrained textual embeddings via a re-parameterizable lightweight auxiliary\nnetwork and enhances visual-textual alignment with zero inference and\ntransferring overhead. For visual prompts, we present Semantic-Activated Visual\nPrompt Encoder (SAVPE). It employs decoupled semantic and activation branches\nto bring improved visual embedding and accuracy with minimal complexity. For\nprompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy.\nIt utilizes a built-in large vocabulary and specialized embedding to identify\nall objects, avoiding costly language model dependency. Extensive experiments\nshow YOLOE's exceptional zero-shot performance and transferability with high\ninference efficiency and low training cost. Notably, on LVIS, with 3times\nless training cost and 1.4times inference speedup, YOLOE-v8-S surpasses\nYOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6\nAP^b and 0.4 AP^m gains over closed-set YOLOv8-L with nearly 4times less\ntraining time. Code and models are available at\nhttps://github.com/THU-MIG/yoloe.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07465.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07426",
      "authors": [
        {
          "_id": "67cff321f2b1fe815dce3722",
          "name": "Junkang Wu",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3723",
          "name": "Kexin Huang",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3724",
          "name": "Xue Wang",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3725",
          "name": "Jinyang Gao",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3726",
          "name": "Bolin Ding",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3727",
          "name": "Jiancan Wu",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3728",
          "name": "Xiangnan He",
          "hidden": false
        },
        {
          "_id": "67cff321f2b1fe815dce3729",
          "user": {
            "_id": "65fca775fa59bdf4737b1a84",
            "avatarUrl": "/avatars/a161b510bde8f57e7686cbb0b4aa6a52.svg",
            "isPro": false,
            "fullname": "Xiang Wang",
            "user": "xiangwang1223",
            "type": "user"
          },
          "name": "Xiang Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-11T08:24:02.839Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T15:11:07.000Z",
      "title": "RePO: ReLUに基づく好み最適化",
      "summary": "LLMの人間の好みに合わせることは実世界的な機能のために重要ですが、現在の方法では計算と安定性の問題があります。DPOは単一のパラメーターbetaを用いてオフラインパラダイムを構築していますが、SimPOなどの後続の方法は再び複雑性を導入し、双重パラメーター(beta, gamma)を使用しています。私たちは、ReLUに基づく好み最適化(RePO)のストリーミングアルゴリズムを提案します。RePOは、(1)SimPOの参照無しのマージンを残しながら、グラフィックアナライズによってbetaを除去し、(2)ReLUに基づくmax-margin損失を採用して、自然に軽いパースのペアをフィルターすることで、betaを除去します。理論的には、RePOはSimPOの極限値として特徴され、ロジスティックの重み付けが二値閾値に収束し、0-1損失の凸包を形成します。AlpacaEval 2とArena-Hardの実験結果によれば、RePOは複数の基盤モデルでDPOとSimPOを超え、ひとつのパラメーターの調整しか必要ないことが示唆されています。",
      "upvotes": 0,
      "discussionId": "67cff322f2b1fe815dce3787",
      "ai_keywords": [
        "LLMS",
        "RLHF",
        "DPO",
        "SimPO",
        "ReLU-based Preference Optimization (RePO)",
        "reference-free margins",
        "gradient analysis",
        "ReLU-based max-margin loss",
        "convex envelope",
        "0-1 loss"
      ]
    },
    "publishedAt": "2025-03-10T11:11:07.000Z",
    "title": "RePO: ReLU-based Preference Optimization",
    "summary": "Aligning large language models (LLMs) with human preferences is critical for\nreal-world deployment, yet existing methods like RLHF face computational and\nstability challenges. While DPO establishes an offline paradigm with single\nhyperparameter beta, subsequent methods like SimPO reintroduce complexity\nthrough dual parameters (beta, gamma). We propose {ReLU-based Preference\nOptimization (RePO)}, a streamlined algorithm that eliminates beta via two\nadvances: (1) retaining SimPO's reference-free margins but removing beta\nthrough gradient analysis, and (2) adopting a ReLU-based max-margin loss that\nnaturally filters trivial pairs. Theoretically, RePO is characterized as\nSimPO's limiting case (beta to infty), where the logistic weighting\ncollapses to binary thresholding, forming a convex envelope of the 0-1 loss.\nEmpirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO\nand SimPO across multiple base models, requiring only one hyperparameter to\ntune.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07426.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06362",
      "authors": [
        {
          "_id": "67cffd119f703990a8e25925",
          "name": "Umberto Cappellazzo",
          "hidden": false
        },
        {
          "_id": "67cffd119f703990a8e25926",
          "name": "Minsu Kim",
          "hidden": false
        },
        {
          "_id": "67cffd119f703990a8e25927",
          "name": "Stavros Petridis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T00:02:10.000Z",
      "title": "マトリゾンハベースモノモーダルLLMsを用いた適応的音声視覚語音認識",
      "summary": "音声ビジュアル認識（AVSR）は、音声と視覚の両方のモデルを利用して、噪音のある環境での音声認識の強固性を向上させます。大規模言語モデル（LLMs）の最近の進展は、音声認識においても効果的であり、AVSRにおいても同様です。しかし、音声表現の長さが長いため、LLMsとの直接な統合は計算コストを大きくかけます。先行のアプローチは、これを解決するために、LLMsに入力する前に音声表現を圧縮しています。しかし、高い圧縮比が性能の低下を招き、計算コストと認識精度のトレードオフが必要となります。この挑戦を解決するために、私たちは、AVSRの最初のマトリオシカベースの多模態LLM、Llama-MTSKを提案します。これは、特定の計算制約に基づいて音声視覚トークンの割り当てを柔軟に調整できるように、高い性能を維持します。我々のアプローチは、マトリオシカ表現学習をモデルとして、モデル内で多重グラニューラリティで音声視覚表現をエンコードし、別の圧縮レベルに対応する異なるモデルを訓練する必要を消します。また、LLMの効率的な微調節について、グローバルおよびスケール特有のLoRAモジュールを用いた3つのLoRAベースのマトリオシカ戦略を導入します。2つの最大のAVSRデータセットでの検証は、Llama-MTSKが最先端の結果を実現し、固定の圧縮レベルで独立に訓練されたモデルとの結果を追い越すことを示します。",
      "upvotes": 0,
      "discussionId": "67cffd129f703990a8e25990",
      "ai_keywords": [
        "Audio-Visual Speech Recognition (AVSR)",
        "Large Language Models (LLMs)",
        "speech representations",
        "computational costs",
        "audio-visual token allocation",
        "Matryoshka-based Multimodal LLM",
        "Matryoshka Representation Learning",
        "global LoRA modules",
        "scale-specific LoRA modules",
        "LoRA-based Matryoshka strategies"
      ]
    },
    "publishedAt": "2025-03-08T19:02:10.000Z",
    "title": "Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs",
    "summary": "Audio-Visual Speech Recognition (AVSR) leverages both audio and visual\nmodalities to enhance speech recognition robustness, particularly in noisy\nenvironments. Recent advancements in Large Language Models (LLMs) have\ndemonstrated their effectiveness in speech recognition, including AVSR.\nHowever, due to the significant length of speech representations, direct\nintegration with LLMs imposes substantial computational costs. Prior approaches\naddress this by compressing speech representations before feeding them into\nLLMs. However, higher compression ratios often lead to performance degradation,\nnecessitating a trade-off between computational efficiency and recognition\naccuracy. To address this challenge, we propose Llama-MTSK, the first\nMatryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation of\nthe audio-visual token allocation based on specific computational constraints\nwhile preserving high performance. Our approach, inspired by Matryoshka\nRepresentation Learning, encodes audio-visual representations at multiple\ngranularities within a single model, eliminating the need to train separate\nmodels for different compression levels. Moreover, to efficiently fine-tune the\nLLM, we introduce three LoRA-based Matryoshka strategies using global and\nscale-specific LoRA modules. Extensive evaluations on the two largest AVSR\ndatasets demonstrate that Llama-MTSK achieves state-of-the-art results,\nmatching or surpassing models trained independently at fixed compression\nlevels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06362.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05283",
      "authors": [
        {
          "_id": "67cef721e5ab8ec0550b7a66",
          "name": "Souhail Hadgi",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a67",
          "name": "Luca Moschella",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a68",
          "user": {
            "_id": "5e8ef1f14957053f606489e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
            "isPro": false,
            "fullname": "Andrea Santilli",
            "user": "teelinsan",
            "type": "user"
          },
          "name": "Andrea Santilli",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T14:35:44.397Z",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a69",
          "name": "Diego Gomez",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6a",
          "name": "Qixing Huang",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6b",
          "name": "Emanuele Rodolà",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6c",
          "name": "Simone Melzi",
          "hidden": false
        },
        {
          "_id": "67cef721e5ab8ec0550b7a6d",
          "name": "Maks Ovsjanikov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T09:51:56.000Z",
      "title": "プラトの洞穴から逃げ出して：3Dとテキスト潜在空間のアラインメントへ",
      "summary": "最近の研究は、スケール上の訓練を行うと、単モーダル2D視覚エンコーダとテキストエンコーダが学習された特徴量が、異なる表現から生じることにも関わらず、驚異的な構造的な特徴を共有することを示しています。しかし、3Dエンコーダが他のモーダルに対する役割は調査されていません。また、現在の3Dファンダメンタルモデルは、大規模なデータセットを利用して訓練されているが、通常は、他の表現からのフリーズされたエンコーダと明示的なアラインメントオブジェクティブに基づいて訓練されています。本研究では、単モーダル3Dエンコーダから得られる表現とテキストベースの特徴空間との後置アラインメントの可能性を調査します。ネイフな後置訓練特徴アラインメントによる単モーダルテキストと3Dエンコーダの性能は限られています。その後、対応する特徴空間の部分空間を抽出し、学習された表現を選ばれた低次元部分空間に投射することでアラインメントの品質が大幅に高まり、マッチングと検索タスクにおいて精度が向上します。我々の分析は、これらの共有部分空間の性質を明らかにし、それらは大体的にセマンティックおよびギオメトリデータ表現と区別されることを示します。全体として、我々の研究は、後置訓練の3D単モーダルとテキスト特徴空間のアラインメントの基準を確立することを試み、3Dデータと他の表現と比較して共有されるおおよその特徴と異なる特徴を明らかにします。",
      "upvotes": 0,
      "discussionId": "67cef723e5ab8ec0550b7ac8",
      "ai_keywords": [
        "uni-modal 2D vision",
        "text encoders",
        "learned features",
        "3D encoders",
        "3D foundation models",
        "alignment objectives",
        "feature alignment",
        "subspaces",
        "lower-dimensional subspaces",
        "semantic data representations",
        "geometric data representations",
        "matching tasks",
        "retrieval tasks"
      ]
    },
    "publishedAt": "2025-03-07T04:51:56.000Z",
    "title": "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent\n  Spaces",
    "summary": "Recent works have shown that, when trained at scale, uni-modal 2D vision and\ntext encoders converge to learned features that share remarkable structural\nproperties, despite arising from different representations. However, the role\nof 3D encoders with respect to other modalities remains unexplored.\nFurthermore, existing 3D foundation models that leverage large datasets are\ntypically trained with explicit alignment objectives with respect to frozen\nencoders from other representations. In this work, we investigate the\npossibility of a posteriori alignment of representations obtained from\nuni-modal 3D encoders compared to text-based feature spaces. We show that naive\npost-training feature alignment of uni-modal text and 3D encoders results in\nlimited performance. We then focus on extracting subspaces of the corresponding\nfeature spaces and discover that by projecting learned representations onto\nwell-chosen lower-dimensional subspaces the quality of alignment becomes\nsignificantly higher, leading to improved accuracy on matching and retrieval\ntasks. Our analysis further sheds light on the nature of these shared\nsubspaces, which roughly separate between semantic and geometric data\nrepresentations. Overall, ours is the first work that helps to establish a\nbaseline for post-training alignment of 3D uni-modal and text feature spaces,\nand helps to highlight both the shared and unique properties of 3D data\ncompared to other representations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05283.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03511",
      "authors": [
        {
          "_id": "67cd7ace999766d8cd73fb18",
          "user": {
            "_id": "6732f2c24c2f18a60e76b915",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6732f2c24c2f18a60e76b915/W6oozAjM-zu7E3SL9uQ97.jpeg",
            "isPro": false,
            "fullname": "Fan",
            "user": "KianYale",
            "type": "user"
          },
          "name": "Qingyu Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T08:01:41.233Z",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb19",
          "name": "Yinghao Cai",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1a",
          "name": "Chao Li",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1b",
          "name": "Wenzhe He",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1c",
          "name": "Xudong Zheng",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1d",
          "name": "Tao Lu",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1e",
          "name": "Bin Liang",
          "hidden": false
        },
        {
          "_id": "67cd7ace999766d8cd73fb1f",
          "name": "Shuo Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T13:57:37.000Z",
      "title": "NeuGrasp: 背景プロファイルを用いた一般化可能なニューラル表面構築による物質無関係の物体のグラスプロデクション検出",
      "summary": "ロボットが透明と反射性の高い物体を捉える場合には、正確な深さ情報を基にした方法にとっては大きな課題があります。本論文では、背景プロジョードを利用して材料無関係的な捉え動作検出を行うニューラル表面再構成法NeuGraspを紹介します。NeuGraspは、transformersと全球的な先驚プロジョードを組み合わせて、空間エンコーディングを用いた多視点の特徴量を集約し、狭いや稀少な視点条件でも強固な表面再構成を可能にします。フォロコン物体を焦点に置いて残差特徴を強化し、占有先驚プロジョードを用いて空間認識を精確化することで、透明と反射性の高い物体を捉える場合にも優れています。記述の詳細は、https://neugrasp.github.io/ からご覧ください。",
      "upvotes": 0,
      "discussionId": "67cd7ad0999766d8cd73fb77",
      "ai_keywords": [
        "neural surface reconstruction",
        "background priors",
        "material-agnostic grasp detection",
        "transformers",
        "global prior volumes",
        "multi-view features",
        "spatial encoding",
        "narrow and sparse viewing conditions",
        "residual feature enhancement",
        "occupancy-prior volume",
        "transparent objects",
        "specular surfaces"
      ]
    },
    "publishedAt": "2025-03-05T08:57:37.000Z",
    "title": "NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection",
    "summary": "Robotic grasping in scenes with transparent and specular objects presents\ngreat challenges for methods relying on accurate depth information. In this\npaper, we introduce NeuGrasp, a neural surface reconstruction method that\nleverages background priors for material-agnostic grasp detection. NeuGrasp\nintegrates transformers and global prior volumes to aggregate multi-view\nfeatures with spatial encoding, enabling robust surface reconstruction in\nnarrow and sparse viewing conditions. By focusing on foreground objects through\nresidual feature enhancement and refining spatial perception with an\noccupancy-prior volume, NeuGrasp excels in handling objects with transparent\nand specular surfaces. Extensive experiments in both simulated and real-world\nscenarios show that NeuGrasp outperforms state-of-the-art methods in grasping\nwhile maintaining comparable reconstruction quality. More details are available\nat https://neugrasp.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03511.png",
    "numComments": 1,
    "isAuthorParticipating": true
  }
]