[
  {
    "paper": {
      "id": "2505.23747",
      "authors": [
        {
          "_id": "68391565d762b7c617b1ba81",
          "name": "Diankun Wu",
          "hidden": false
        },
        {
          "_id": "68391565d762b7c617b1ba82",
          "name": "Fangfu Liu",
          "hidden": false
        },
        {
          "_id": "68391565d762b7c617b1ba83",
          "name": "Yi-Hsin Hung",
          "hidden": false
        },
        {
          "_id": "68391565d762b7c617b1ba84",
          "name": "Yueqi Duan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6505a02f9310ce8c400edc63/eb5xv9-rlab_DrHLk4BMq.mp4"
      ],
      "publishedAt": "2025-05-29T17:59:04.000Z",
      "submittedOnDailyAt": "2025-05-30T00:56:58.237Z",
      "title": "スペクトル-MLLM: ビジュアルベースのスペクトル知能におけるMLLM能力の向上",
      "submittedOnDailyBy": {
        "_id": "6505a02f9310ce8c400edc63",
        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
        "isPro": false,
        "fullname": "Fangfu Liu",
        "user": "Liuff23",
        "type": "user"
      },
      "summary": "最近の多モデル大語言モデル（MLLM）の進歩は、2次元可視的タスクの性能を大幅に向上させました。しかし、その空間知識の向上は難しい問題です。現在の3次元MLLMは、3次元または2.5次元データを追加的に使用して空間認識を含めるため、グラフィックや映像やビデオのみの2次元入力の場合には役に立つことができません。本論文では、2次元の観測からの空間的推論のための新しいフレームワーク「Spatial-MLLM」を紹介します。単なるビデオMLLMと異なり、CLIPベースの視覚エンコーダーを用いて意味的理解に最適化されたものではありません。私たちの主なアイデアは、前向き的な視覚ジェネラル化モデルからの強い構造的な先入観を解放することです。特に、プレトレーンされた2次元視覚エンコーダーを用いて意味的な特徴を抽出し、視覚ジェネラル化モデルのバックボーンから初期化された空間エンコーダーを用いて3次元構造的な特徴を抽出するダブルエンコーダーアーキテクチャを提案します。その後、これらの特徴を統一的な視覚トークンに統合するコンネクタを用います。また、推論時に空間的に情報を持つフレームを選択する空間認識に関連付けたフレームサンプリング戦略を提案します。これにより、ビデオシーケンスの空間的に情報を持つフレームを選択することで、モデルが空間的な推論に必要なフレームに焦点を当てることを保証します。アーキテクチャの改善のほか、Spatial-MLLM-120kデータセットを構築し、これをサブプロバイジングとGRPOを用いてモデルを訓練します。多様な実世界的データセットでの検証により、我々の空間的MLLMは、広範囲の視覚的な空間的理解と推論タスクで最先端の性能を達成します。プロジェクトページ：https://diankun-wu.github.io/Spatial-MLLM/。",
      "upvotes": 39,
      "discussionId": "68391566d762b7c617b1bae5",
      "projectPage": "https://diankun-wu.github.io/Spatial-MLLM/",
      "githubRepo": "https://github.com/diankun-wu/Spatial-MLLM",
      "ai_summary": "Spatial-MLLM improves spatial reasoning in multimodal large language models using a dual-encoder architecture with pretrained 2D and 3D structure encoders, achieving state-of-the-art performance on visual spatial tasks.",
      "ai_keywords": [
        "spatial-mllm",
        "dual-encoder architecture",
        "visual geometry foundation model",
        "CLIP-based visual encoders",
        "semantic features",
        "3D structure features",
        "unified visual tokens",
        "space-aware frame sampling",
        "supervised fine-tuning",
        "GRPO",
        "spatial understanding",
        "spatial reasoning"
      ]
    },
    "publishedAt": "2025-05-29T13:59:04.000Z",
    "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial\n  Intelligence",
    "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6505a02f9310ce8c400edc63/eb5xv9-rlab_DrHLk4BMq.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23747.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6505a02f9310ce8c400edc63",
      "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
      "fullname": "Fangfu Liu",
      "name": "Liuff23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23621",
      "authors": [
        {
          "_id": "68391925d73e6015a1b0f305",
          "name": "Zheyuan Yang",
          "hidden": false
        },
        {
          "_id": "68391925d73e6015a1b0f306",
          "name": "Lyuhao Chen",
          "hidden": false
        },
        {
          "_id": "68391925d73e6015a1b0f307",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "68391925d73e6015a1b0f308",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:43.117Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:28:50.000Z",
      "submittedOnDailyAt": "2025-05-30T01:04:47.042Z",
      "title": "Table-R1: テーブル推論時のスケーリング",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "この研究では、推論時スケーリングを試みるための最初の研究を報告します。2つのトレーニング後の戦略を開発し、推論時スケーリングを可能にするために評価します：DeepSeek-R1からの先頭モデル推論トレースからのディスティルテーションと、可証明可能な報酬を持つ強化学習（RLVR）。ディスティルテーションにおいて、DeepSeek-R1から生成された大規模な推論トレースデータセットを用いて、LLMをTable-R1-SFTモデルに微調節します。RLVRにおいて、タスクに関係する可証明可能な報酬関数を提案し、GRPOアルゴリズムを適用してTable-R1-Zeroモデルを得ます。Table-R1-seriesモデルは、短文QA、事実検証、自由形式QAなどの多様なテーブル推論タスクにおいて評価されます。特に、Table-R1-Zeroモデルは、7BパラメータのLLMだけを使用してGPT-4.1とDeepSeek-R1の性能を匹敵または超えます。また、領域外データセットに強い一般化性能を示します。拡大された消去試験と質的解析により、指示調整、モデル構造の選択、タスク間の一般化、RLトレーニング中に重要なテーブル推論スキルの発生のベーナスを明らかにします。",
      "upvotes": 38,
      "discussionId": "68391928d73e6015a1b0f3a8",
      "githubRepo": "https://github.com/Table-R1/Table-R1",
      "ai_summary": "Two post-training strategies, distillation and RLVR, enable inference-time scaling in table reasoning tasks, resulting in a model (Table-R1-Zero) that matches GPT-4.1's performance using fewer parameters and shows strong generalization.",
      "ai_keywords": [
        "distillation",
        "reinforcement learning",
        "verifiable rewards",
        "RLVR",
        "reasoning traces",
        "DeepSeek-R1",
        "LLMs",
        "Table-R1-SFT",
        "GRPO",
        "Table-R1-Zero",
        "short-form QA",
        "fact verification",
        "free-form QA",
        "instruction tuning",
        "model architecture choices",
        "cross-task generalization",
        "table reasoning skills"
      ]
    },
    "publishedAt": "2025-05-29T12:28:50.000Z",
    "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
    "summary": "In this work, we present the first study to explore inference-time scaling on\ntable reasoning tasks. We develop and evaluate two post-training strategies to\nenable inference-time scaling: distillation from frontier model reasoning\ntraces and reinforcement learning with verifiable rewards (RLVR). For\ndistillation, we introduce a large-scale dataset of reasoning traces generated\nby DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For\nRLVR, we propose task-specific verifiable reward functions and apply the GRPO\nalgorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series\nmodels across diverse table reasoning tasks, including short-form QA, fact\nverification, and free-form QA. Notably, the Table-R1-Zero model matches or\nexceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a\n7B-parameter LLM. It also demonstrates strong generalization to out-of-domain\ndatasets. Extensive ablation and qualitative analyses reveal the benefits of\ninstruction tuning, model architecture choices, and cross-task generalization,\nas well as emergence of essential table reasoning skills during RL training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22653",
      "authors": [
        {
          "_id": "6838bb282b382ba50bdcddc4",
          "name": "Ang Lv",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc5",
          "user": {
            "_id": "6622443b9b0614a760dd8123",
            "avatarUrl": "/avatars/acb6c1c9c429af1112530dcf76a8e420.svg",
            "isPro": false,
            "fullname": "Ruobing Xie",
            "user": "Ruobing-Xie",
            "type": "user"
          },
          "name": "Ruobing Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:45.319Z",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc6",
          "name": "Xingwu Sun",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc7",
          "name": "Zhanhui Kang",
          "hidden": false
        },
        {
          "_id": "6838bb282b382ba50bdcddc8",
          "name": "Rui Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:59:03.000Z",
      "submittedOnDailyAt": "2025-05-30T00:44:54.555Z",
      "title": "登りの中に深く埋もれた知恵：学習する理由における騒がしい報酬について",
      "submittedOnDailyBy": {
        "_id": "64b8ca3c5067873176d4b436",
        "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg",
        "isPro": false,
        "fullname": "AngLv",
        "user": "AngLv",
        "type": "user"
      },
      "summary": "最近の後学習ラージュアル言語モデル（LLMs）の論理的計算を通じた強化学習（RL）に関する研究は、正確に検証できることができるタスクへの焦点を当てています。例えば数学問題の解決に焦点を当てています。対照的に、我々の研究は、実世界的な場合における後学習されたLLMsの場合の報酬モデルの影響を調査しています。我々は、LLMsは強い報酬ノイズに対して強固であることを見出しました。例えば、数学タスクの報酬関数の出力を手動で40%を反転させた場合、Qwen-2.5-7Bモデルは急速な収束を達成し、報酬ノイズがない場合の75%の正確性に比べて、数学タスクの性能が5%から72%に上がりました。驚くべきに、理由のフレーズの出現だけを報酬する（理由パターン報酬、RPR）ことで、答えの正確性を検証することはなくても、モデルは70%以上の正確性を達成し、厳格な正確性検証と正確な報酬を用いたモデルと比較しても、下流の性能が高まりました。理由のプロセスよりも最終的な結果に焦点を当てる重要性を認識し、RPRと報酬ノイズモデルを組み合わせました。RPRは報酬ノイズモデルを調整し、潜在的なフィールドネガティブを軽減し、LLMの開放的なタスクの性能を向上させました。これらの発見は、学習フェーズにおけるモデルの基盤的な能力の向上と後学習テクニックの進歩についてのヒントを提供します。我々のコードとスクリプトは、https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reasonに公開されています。",
      "upvotes": 36,
      "discussionId": "6838bb2a2b382ba50bdcde1b",
      "githubRepo": "https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason",
      "ai_summary": "LLMs exhibit robustness to reward noise during post-training and achieve high performance using reasoning pattern rewards (RPR) in conjunction with noisy reward models.",
      "ai_keywords": [
        "large language models (LLMs)",
        "post-training",
        "reinforcement learning (RL)",
        "reward noise",
        "reward models",
        "rapid convergence",
        "reasoning pattern reward (RPR)",
        "false negatives",
        "open-ended tasks"
      ]
    },
    "publishedAt": "2025-05-28T13:59:03.000Z",
    "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason",
    "summary": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22653.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b8ca3c5067873176d4b436",
      "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg",
      "fullname": "AngLv",
      "name": "AngLv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23693",
      "authors": [
        {
          "_id": "68390e95b85141ce6c11b50f",
          "user": {
            "_id": "64dc29d9b5d625e0e9a6ecb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
            "isPro": false,
            "fullname": "Tingyu Song",
            "user": "songtingyu",
            "type": "user"
          },
          "name": "Tingyu Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:12.429Z",
          "hidden": false
        },
        {
          "_id": "68390e95b85141ce6c11b510",
          "user": {
            "_id": "66e83ec5deb449d8d856e78d",
            "avatarUrl": "/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg",
            "isPro": false,
            "fullname": "Tongyan Hu",
            "user": "entropyhu",
            "type": "user"
          },
          "name": "Tongyan Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:15.154Z",
          "hidden": false
        },
        {
          "_id": "68390e95b85141ce6c11b511",
          "name": "Guo Gan",
          "hidden": false
        },
        {
          "_id": "68390e95b85141ce6c11b512",
          "name": "Yilun Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64dc29d9b5d625e0e9a6ecb9/ZnHMXas2Khv3qS0RIvYR2.png"
      ],
      "publishedAt": "2025-05-29T17:31:13.000Z",
      "submittedOnDailyAt": "2025-05-30T00:24:42.566Z",
      "title": "VF-Eval: AIGC のビデオに対するフィードバックを生成する多模構造LLMsの評価",
      "submittedOnDailyBy": {
        "_id": "64dc29d9b5d625e0e9a6ecb9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
        "isPro": false,
        "fullname": "Tingyu Song",
        "user": "songtingyu",
        "type": "user"
      },
      "summary": "MLLMsは最近、映画の質問回答によく研究されています。しかし、現在の評価は自然な映画を中心にしていて、AI生成された内容（AIGC）などの合成映画を飛ばしています。一方で、映画生成においては、MLLMsを用いて生成された映画の質を評価することがありますが、MLLMsがAIGC映画を理解する能力は大きく調査されていません。これに対して、私たちは新しいベンチマーク、VF-Evalを提案し、これには、AIGC映画の理解能力を詳細に評価するために、一致性検証、誤り意識、誤り種類検出、理由評価の4つのタスクを導入しました。VF-Evalで13つの先進的なMLLMsを評価し、最も良い性能を示すモデルでも、GPT-4.1はすべてのタスクで一致的に良い性能を達成することができません。これは、ベンチマークの難易度を明らかにしています。また、VF-Evalが映画生成にどのような実用的な応用を可能にするかを調査するために、RePromptの実験を行い、MLLMsを人間のフィードバックによりより近づけることが映画生成に利益を与えることを示しました。",
      "upvotes": 34,
      "discussionId": "68390e96b85141ce6c11b55c",
      "githubRepo": "https://github.com/SighingSnow/VF-EVAL",
      "ai_summary": "A new benchmark, VF-Eval, evaluates the capabilities of MLLMs in interpreting AI-generated content videos across four tasks, highlighting challenges and demonstrating benefits in video generation through human feedback alignment.",
      "ai_keywords": [
        "MLLMs",
        "video question answering",
        "synthetic videos",
        "AI-generated content (AIGC)",
        "VF-Eval",
        "coherence validation",
        "error awareness",
        "error type detection",
        "reasoning evaluation",
        "GPT-4.1",
        "RePrompt"
      ]
    },
    "publishedAt": "2025-05-29T13:31:13.000Z",
    "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC\n  Videos",
    "summary": "MLLMs have been widely studied for video question answering recently.\nHowever, most existing assessments focus on natural videos, overlooking\nsynthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in\nvideo generation rely on MLLMs to evaluate the quality of generated videos, but\nthe capabilities of MLLMs on interpreting AIGC videos remain largely\nunderexplored. To address this, we propose a new benchmark, VF-Eval, which\nintroduces four tasks-coherence validation, error awareness, error type\ndetection, and reasoning evaluation-to comprehensively evaluate the abilities\nof MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that\neven the best-performing model, GPT-4.1, struggles to achieve consistently good\nperformance across all tasks. This highlights the challenging nature of our\nbenchmark. Additionally, to investigate the practical applications of VF-Eval\nin improving video generation, we conduct an experiment, RePrompt,\ndemonstrating that aligning MLLMs more closely with human feedback can benefit\nvideo generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64dc29d9b5d625e0e9a6ecb9/ZnHMXas2Khv3qS0RIvYR2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23693.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dc29d9b5d625e0e9a6ecb9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
      "fullname": "Tingyu Song",
      "name": "songtingyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23762",
      "authors": [
        {
          "_id": "68391353d8c153d346e1ddb5",
          "user": {
            "_id": "637f347a52229c639211bee8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg",
            "isPro": false,
            "fullname": "Chenyu Yang",
            "user": "cyyang822",
            "type": "user"
          },
          "name": "Chenyu Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:52.043Z",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb6",
          "name": "Shiqian Su",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb7",
          "name": "Shi Liu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb8",
          "name": "Xuan Dong",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddb9",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddba",
          "name": "Weijie Su",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbb",
          "name": "Xuehui Wang",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbc",
          "name": "Zhaoyang Liu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbd",
          "name": "Jinguo Zhu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbe",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddbf",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddc0",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddc1",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "68391353d8c153d346e1ddc2",
          "name": "Jifeng Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:51.000Z",
      "submittedOnDailyAt": "2025-05-30T00:41:52.864Z",
      "title": "ZeroGUI: ゼロヒマンコストでのオンラインGUI学習の自動化",
      "submittedOnDailyBy": {
        "_id": "637f347a52229c639211bee8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg",
        "isPro": false,
        "fullname": "Chenyu Yang",
        "user": "cyyang822",
        "type": "user"
      },
      "summary": "大規模ビジョン言語モデル（VLMs）の急速な進歩は、プリビジョンベースのGUIアガントの開発を促進し、グラフィックユーザーインターフェース（GUI）を認識し、操作することで自動的にユーザーの指示を満たす能力を持つものである。しかし、現在のアプローチは通常、オフライン学習フレームワークを採用していますが、2つの核心的な制限があります：1）要素のジョージングとアクションのサブジェクションに対する高品質の手動注釈の重負だけでなく、2）動的なインタラクティブな環境に対する適応性が限られています。これらの制限に対処するために、ゼロコストでGUIアガントのトレーニングを自動化する可換性のあるオンライン学習フレームワークであるZeroGUIを提案します。特に、ZeroGUIは、1）VLMによる自動タスク生成を組み込み、現在の環境状態から多様なトレーニングゴールを生成し、2）VLMによる自動報酬評価を組み込み、手作業された評価関数を除くタスクの成功を評価し、3）2段階オンライン再励励学習を組み込み、GUI環境と継続的に相互作用し、学習することを特徴としています。UI-TARSとAguvisの2つの先進GUIアガントに対しての実験は、ZeroGUIがOSWorldとAndroidLab環境での性能を大幅に向上させることを示しています。コードは、https://github.com/OpenGVLab/ZeroGUIにアクセスできます。",
      "upvotes": 33,
      "discussionId": "68391354d8c153d346e1de1a",
      "githubRepo": "https://github.com/OpenGVLab/ZeroGUI",
      "ai_summary": "ZeroGUI is an online learning framework that uses Vision-Language Models for task generation and reward estimation, enhancing GUI Agents' performance with minimal human intervention.",
      "ai_keywords": [
        "Vision-Language Models",
        "GUI Agents",
        "element grounding",
        "action supervision",
        "offline learning",
        "automatic task generation",
        "automatic reward estimation",
        "reinforcement learning",
        "OSWorld",
        "AndroidLab"
      ]
    },
    "publishedAt": "2025-05-29T13:59:51.000Z",
    "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
    "summary": "The rapid advancement of large Vision-Language Models (VLMs) has propelled\nthe development of pure-vision-based GUI Agents, capable of perceiving and\noperating Graphical User Interfaces (GUI) to autonomously fulfill user\ninstructions. However, existing approaches usually adopt an offline learning\nframework, which faces two core limitations: (1) heavy reliance on high-quality\nmanual annotations for element grounding and action supervision, and (2)\nlimited adaptability to dynamic and interactive environments. To address these\nlimitations, we propose ZeroGUI, a scalable, online learning framework for\nautomating GUI Agent training at Zero human cost. Specifically, ZeroGUI\nintegrates (i) VLM-based automatic task generation to produce diverse training\ngoals from the current environment state, (ii) VLM-based automatic reward\nestimation to assess task success without hand-crafted evaluation functions,\nand (iii) two-stage online reinforcement learning to continuously interact with\nand learn from GUI environments. Experiments on two advanced GUI Agents\n(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance\nacross OSWorld and AndroidLab environments. The code is available at\nhttps://github.com/OpenGVLab/ZeroGUI.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23762.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f347a52229c639211bee8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg",
      "fullname": "Chenyu Yang",
      "name": "cyyang822",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23604",
      "authors": [
        {
          "_id": "68390da8f527444e97c4ab95",
          "name": "Guangtao Zeng",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab96",
          "user": {
            "_id": "6553c985a7aded0380b5f928",
            "avatarUrl": "/avatars/36109d6f536d2b34d98822b88eac9608.svg",
            "isPro": false,
            "fullname": "Maohao Shen",
            "user": "maohaos2",
            "type": "user"
          },
          "name": "Maohao Shen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T01:45:13.275Z",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab97",
          "name": "Delin Chen",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab98",
          "name": "Zhenting Qi",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab99",
          "name": "Subhro Das",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9a",
          "name": "Dan Gutfreund",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9b",
          "name": "David Cox",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9c",
          "name": "Gregory Wornell",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9d",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9e",
          "name": "Zhang-Wei Hong",
          "hidden": false
        },
        {
          "_id": "68390da8f527444e97c4ab9f",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:15:36.000Z",
      "submittedOnDailyAt": "2025-05-30T00:43:27.169Z",
      "title": "サトリ-SWE: サンプルエフエクティブなソフトウェアエンジニアリングの進化的テストタイムスケーリング",
      "submittedOnDailyBy": {
        "_id": "60ad0de755f970745d4ec28d",
        "avatarUrl": "/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg",
        "isPro": true,
        "fullname": "GtZeng",
        "user": "chaoscodes",
        "type": "user"
      },
      "summary": "言語モデル（LMs）は標準化コーディングベンチマークでは良い性能を示すが、実世界的なソフトウェア開発タスクに対しては、特にSWE-BenchでGitHubの問題を解決することが難しい。特にモデルパラメータが100B未満の場合。実践的には、計算コストが低いため、小さなモデルが好まれるが、その性能向上は難しい。現在のアプローチは主に高品質なデータを用いた規範的な微調校（SFT）に基づくが、スケール上でこれは高価である。代替策はテスト時スケーリングで、複数の出力を生成し、バリファイヤーでスコアを計算し、最良のものを選択することである。これは効果的であるが、過剰なサンプリングと高価なスコアを必要とするため、実用的な応用に限られている。私たちは、出力を進化プロセスとして扱うサンプル効率的な方法である進化的テスト時スケーリング（EvoScale）を提案している。選択と突然変異を用いて出力を反復的に改善し、EvoScaleは出力分布を高スコアの領域へと移動させ、正しい解を見つけるために必要なサンプル数を減らす。重複したサンプリングと選択のオーバーヘッドを減らすために、モデルを強化学習（RL）を用いて自己進化させる。推論時には外部のバリファイヤーを依存しないよう、モデルは自己改善したスコアを学習し、各イテレーションで自分の生成を改善する。SWE-Bench-Verified上で評価されたものとして、EvoScaleは私たちの32Bモデル、Satori-SWE-32Bが100Bパラメータ以上のモデルの性能を追い越すことができるようになった。コード、データとモデルは完全にオープンソースになる。",
      "upvotes": 18,
      "discussionId": "68390da9f527444e97c4abdf",
      "projectPage": "https://satori-reasoning.github.io/blog/satori-swe/",
      "ai_summary": "EvoScale, an evolutionary and reinforcement learning-based method, enhances small language models' performance on real-world software engineering tasks by iteratively improving and refining outputs.",
      "ai_keywords": [
        "supervised fine-tuning",
        "test-time scaling",
        "Evolutionary Test-Time Scaling",
        "EvoScale",
        "reinforcement learning",
        "SWE-Bench-Verified"
      ]
    },
    "publishedAt": "2025-05-29T12:15:36.000Z",
    "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software\n  Engineering",
    "summary": "Language models (LMs) perform well on standardized coding benchmarks but\nstruggle with real-world software engineering tasks such as resolving GitHub\nissues in SWE-Bench, especially when model parameters are less than 100B. While\nsmaller models are preferable in practice due to their lower computational\ncost, improving their performance remains challenging. Existing approaches\nprimarily rely on supervised fine-tuning (SFT) with high-quality data, which is\nexpensive to curate at scale. An alternative is test-time scaling: generating\nmultiple outputs, scoring them using a verifier, and selecting the best one.\nAlthough effective, this strategy often requires excessive sampling and costly\nscoring, limiting its practical application. We propose Evolutionary Test-Time\nScaling (EvoScale), a sample-efficient method that treats generation as an\nevolutionary process. By iteratively refining outputs via selection and\nmutation, EvoScale shifts the output distribution toward higher-scoring\nregions, reducing the number of samples needed to find correct solutions. To\nreduce the overhead from repeatedly sampling and selection, we train the model\nto self-evolve using reinforcement learning (RL). Rather than relying on\nexternal verifiers at inference time, the model learns to self-improve the\nscores of its own generations across iterations. Evaluated on\nSWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or\nexceed the performance of models with over 100B parameters while using a few\nsamples. Code, data, and models will be fully open-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23604.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60ad0de755f970745d4ec28d",
      "avatarUrl": "/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg",
      "fullname": "GtZeng",
      "name": "chaoscodes",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23359",
      "authors": [
        {
          "_id": "683909a29deef11aa625817c",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa625817d",
          "user": {
            "_id": "62cd7aca7a036fc9941bb2b0",
            "avatarUrl": "/avatars/17a4d27af0243fd7dccf06066f671461.svg",
            "isPro": false,
            "fullname": "kun ouyang",
            "user": "RUBBISHLIKE",
            "type": "user"
          },
          "name": "Kun Ouyang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:22.290Z",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa625817e",
          "name": "Haoning Wu",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa625817f",
          "name": "Yi Liu",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258180",
          "name": "Lin Sui",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258181",
          "name": "Xinhao Li",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258182",
          "name": "Yan Zhong",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258183",
          "name": "Y. Charles",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258184",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "683909a29deef11aa6258185",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6489761dcaea79f577897f98/48SxCF6GVyMdnPR7vZSgC.mp4"
      ],
      "publishedAt": "2025-05-29T11:33:43.000Z",
      "submittedOnDailyAt": "2025-05-30T01:03:01.950Z",
      "title": "VideoReasonBench: ビデオコンペンシャンベンチャーン：MLLMsは複雑なビデオの視覚中心的な複雑な理由論理を行うことができるか？",
      "submittedOnDailyBy": {
        "_id": "6489761dcaea79f577897f98",
        "avatarUrl": "/avatars/8f56dc9c08dc2b672555602d68509a03.svg",
        "isPro": false,
        "fullname": "Yuanxin Liu",
        "user": "lyx97",
        "type": "user"
      },
      "summary": "最近の研究は、長いチェーンオフサインコンティング（CoT）論理が複雑なタスクでの大規模言語モデル（LLMs）の性能を大幅に向上させることを示唆しています。しかし、この利益は映像理解の領域ではまだ示されていません。これは、現在のベンチマークが必要とする論理の深さを持っていないためです。最近の試みは、映像論理に向けたベンチマークを提案していますが、これらのタスクは通常知識を基にしていて、視覚内容に依存していません。この隙を埋めるために、私たちはVideoReasonBenchというベンチマークを紹介します。これは、視覚センタリックで複雑な映像論理を評価するベンチマークです。視覚豊富さと高い論理複雑性を確保するために、VideoReasonBenchの各映像は、映像の一部でみられる潜在的状態に対する細かい操作の順番を描くものです。質問は、視覚情報の記憶、潜在的状態の内容の推定、映像よりも情報を予測する3つの進歩的レベルの映像論理スキルを評価します。このタスク設定の下では、モデルは、これらの質問に対して正確な最終的な答えを得るために、映像の複数の操作を精密に記憶し、ステップごとの論理を行う必要があります。VideoReasonBenchを用いて、私たちは18つの最先端の多モデル言語モデル（MLLMs）を検討し、複雑な映像論理では多くのモデルが悪い性能を示しています。例えば、GPT-4oは6.9%の精度を達成し、思い出を強化したGemini-2.5-Proは56.0%の精度で他より显著に優れています。「テスト時スケーリング」についての調査は、現在の映像ベンチマークにおけるほとんどのベーチュームよりもベニューフェルムバッジバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッチバッ",
      "upvotes": 18,
      "discussionId": "683909a39deef11aa62581c2",
      "projectPage": "https://llyx97.github.io/video_reason_bench/",
      "githubRepo": "https://github.com/llyx97/video_reason_bench",
      "ai_summary": "A new benchmark, VideoReasonBench, evaluates complex vision-centric video reasoning, finding that extended thinking budgets are crucial for improved performance compared to existing benchmarks.",
      "ai_keywords": [
        "long chain-of-thought reasoning",
        "large language models",
        "video understanding",
        "VideoReasonBench",
        "vision-centric",
        "complex video reasoning",
        "latent state",
        "visual reasoning",
        "step-by-step reasoning",
        "multimodal language models",
        "thinking-enhanced models",
        "test-time scaling"
      ]
    },
    "publishedAt": "2025-05-29T07:33:43.000Z",
    "title": "VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video\n  Reasoning?",
    "summary": "Recent studies have shown that long chain-of-thought (CoT) reasoning can\nsignificantly enhance the performance of large language models (LLMs) on\ncomplex tasks. However, this benefit is yet to be demonstrated in the domain of\nvideo understanding, since most existing benchmarks lack the reasoning depth\nrequired to demonstrate the advantages of extended CoT chains. While recent\nefforts have proposed benchmarks aimed at video reasoning, the tasks are often\nknowledge-driven and do not rely heavily on visual content. To bridge this gap,\nwe introduce VideoReasonBench, a benchmark designed to evaluate vision-centric,\ncomplex video reasoning. To ensure visual richness and high reasoning\ncomplexity, each video in VideoReasonBench depicts a sequence of fine-grained\noperations on a latent state that is only visible in part of the video. The\nquestions evaluate three escalating levels of video reasoning skills: recalling\nobserved visual information, inferring the content of latent states, and\npredicting information beyond the video. Under such task setting, models have\nto precisely recall multiple operations in the video, and perform step-by-step\nreasoning to get correct final answers for these questions. Using\nVideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal\nLLMs (MLLMs), finding that most perform poorly on complex video reasoning,\ne.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced\nGemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our\ninvestigations on \"test-time scaling\" further reveal that extended thinking\nbudget, while offering none or minimal benefits on existing video benchmarks,\nis essential for improving the performance on VideoReasonBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6489761dcaea79f577897f98/48SxCF6GVyMdnPR7vZSgC.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23359.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "6489761dcaea79f577897f98",
      "avatarUrl": "/avatars/8f56dc9c08dc2b672555602d68509a03.svg",
      "fullname": "Yuanxin Liu",
      "name": "lyx97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23716",
      "authors": [
        {
          "_id": "6839217e95fedc63bb4ae475",
          "name": "Lihan Jiang",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae476",
          "user": {
            "_id": "65de9c6cf68c3d3bac330509",
            "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
            "isPro": false,
            "fullname": "Yucheng Mao",
            "user": "matthewmao",
            "type": "user"
          },
          "name": "Yucheng Mao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:34.206Z",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae477",
          "name": "Linning Xu",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae478",
          "name": "Tao Lu",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae479",
          "name": "Kerui Ren",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47a",
          "name": "Yichen Jin",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47b",
          "name": "Xudong Xu",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47c",
          "name": "Mulin Yu",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47d",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47e",
          "name": "Feng Zhao",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae47f",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "6839217e95fedc63bb4ae480",
          "name": "Bo Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:49:56.000Z",
      "submittedOnDailyAt": "2025-05-30T01:42:17.635Z",
      "title": "AnySplat: 無制限の視点からのFeed-forward 3D Gaussian Splatting",
      "submittedOnDailyBy": {
        "_id": "64a4ce8118f4e2529546daef",
        "avatarUrl": "/avatars/6d88aa68eccfa07d2009df405f957fd7.svg",
        "isPro": false,
        "fullname": "Jiang Lihan",
        "user": "lhjiang",
        "type": "user"
      },
      "summary": "AnySplatは、未測定画像コレクションからの新視点合成に向けた前向きネットワークです。従来のニューラルレンダリングパイプラインと比べ、カメラの姿勢とシーン毎の最適化が必要なことと、最近の前向き手法では、複数の複雑な視点に対する計算量の重みを負担することとは異なり、我々のモデルはすべてを一度のフォワードパスで予測します。入力画像の各枚に対して、3Dガウスプライミティーとしてのシーンの形状と外観を含むもの、そしてそれに対応するカメラの内観と外観をまとめて得られます。この統合的な設計は、ポジションの注釈がないようにして、カシャルに撮影された、複数の視点のデータセットにも簡単にスケーリングできます。広範囲のゼロショット評価では、AnySplatは、稀少な視点と複数の視点の両方でポジションに関心のあるベースラインと同じ品質を達成し、ポジション無制限のアプローチを超えます。また、最適化ベースのニューラルフィールドに比べて、渲染レンティングの遅延を大幅に減らし、無制限な撮影設定での実時間の新視点合成を可能にします。プロジェクトページ：https://city-super.github.io/anysplat/",
      "upvotes": 17,
      "discussionId": "6839217f95fedc63bb4ae4d0",
      "projectPage": "https://city-super.github.io/anysplat/",
      "ai_summary": "AnySplat is a feed forward network that performs novel view synthesis without camera poses, using 3D Gaussian primitives and unified design for efficiency and quality across sparse and dense datasets.",
      "ai_keywords": [
        "feed forward network",
        "novel view synthesis",
        "uncalibrated image collections",
        "3D Gaussian primitives",
        "camera intrinsics",
        "camera extrinsics",
        "neural rendering pipelines",
        "per scene optimization",
        "zero shot evaluations",
        "pose aware baselines",
        "pose free approaches",
        "rendering latency",
        "optimization based neural fields"
      ]
    },
    "publishedAt": "2025-05-29T13:49:56.000Z",
    "title": "AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views",
    "summary": "We introduce AnySplat, a feed forward network for novel view synthesis from\nuncalibrated image collections. In contrast to traditional neural rendering\npipelines that demand known camera poses and per scene optimization, or recent\nfeed forward methods that buckle under the computational weight of dense views,\nour model predicts everything in one shot. A single forward pass yields a set\nof 3D Gaussian primitives encoding both scene geometry and appearance, and the\ncorresponding camera intrinsics and extrinsics for each input image. This\nunified design scales effortlessly to casually captured, multi view datasets\nwithout any pose annotations. In extensive zero shot evaluations, AnySplat\nmatches the quality of pose aware baselines in both sparse and dense view\nscenarios while surpassing existing pose free approaches. Moreover, it greatly\nreduce rendering latency compared to optimization based neural fields, bringing\nreal time novel view synthesis within reach for unconstrained capture\nsettings.Project page: https://city-super.github.io/anysplat/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23716.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a4ce8118f4e2529546daef",
      "avatarUrl": "/avatars/6d88aa68eccfa07d2009df405f957fd7.svg",
      "fullname": "Jiang Lihan",
      "name": "lhjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23660",
      "authors": [
        {
          "_id": "683906adf85de1fc563957d8",
          "user": {
            "_id": "648ac3d53470b17ccc90deaf",
            "avatarUrl": "/avatars/35b678baa7a91959f40f704c5de2a3e1.svg",
            "isPro": false,
            "fullname": "Ziteng Gao",
            "user": "sebgao",
            "type": "user"
          },
          "name": "Ziteng Gao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T01:15:28.731Z",
          "hidden": false
        },
        {
          "_id": "683906adf85de1fc563957d9",
          "user": {
            "_id": "63a55320ce5763e06f78519c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671779060549-noauth.jpeg",
            "isPro": false,
            "fullname": "Mike Shou",
            "user": "mikeshou",
            "type": "user"
          },
          "name": "Mike Zheng Shou",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T01:15:28.731Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:09:25.000Z",
      "submittedOnDailyAt": "2025-05-30T01:28:22.938Z",
      "title": "D-AR: 自動回帰モデルによる拡散",
      "submittedOnDailyBy": {
        "_id": "648ac3d53470b17ccc90deaf",
        "avatarUrl": "/avatars/35b678baa7a91959f40f704c5de2a3e1.svg",
        "isPro": false,
        "fullname": "Ziteng Gao",
        "user": "sebgao",
        "type": "user"
      },
      "summary": "この論文では、Diffusion via Autoregressive models (D-AR) を紹介します。これは、画像の拡散プロセスを標準的な次のトークン予測フォーマットでのバーニーアノテーションプロセスとして再構成した新しいパラダイムです。まず、画像を離散トークンの列に変換するトークナイザを設計します。これらのトークンは、ピクセル空間の異なる位置で解像されることができます。拡散の性質により、これらのトークンは自然とコアストラフィーニューの順番に従います。これは、アノテーションモデリングに直接適しています。そこで、これらのトークンに対して標準的な次のトークン予測を適用し、原因性マスクや訓練/推論戦略の設計を変更しません。このような順次アノテーショントークン生成は、画像空間の拡散プロセスを直接反映します。つまり、アノテーションモデルがトークンの増分を生成したら、これらのトークンを直接フローミングモードで解像されることができます。我々のパイプラインは、トークンのみのサブセットを生成するときに継続的なプレビューをサポートし、ラウィト制御された合成をゼロショットで可能にします。標準のImageNetベンチマークでは、775MのLlamaバックボードと256の離散トークンを使用して2.09のFIDを達成しました。我々は、この研究が、視覚合成の統一アノテーションアーキテクチャの将来の研究にヒントを与えることを望むと思います。コードとモデルは、https://github.com/showlab/D-AR から利用可能です。",
      "upvotes": 17,
      "discussionId": "683906b0f85de1fc5639586b",
      "githubRepo": "https://github.com/showlab/D-AR",
      "ai_summary": "Diffusion via Autoregressive models (D-AR) recasts the image diffusion process as a standard autoregressive task, achieving high-quality image generation with consistent previews and layout control using a large language model backbone.",
      "ai_keywords": [
        "Diffusion via Autoregressive models",
        "autoregressive procedure",
        "next-token-prediction",
        "tokenizer",
        "discrete tokens",
        "diffusion denoising",
        "coarse-to-fine order",
        "autoregressive modeling",
        "autoregressive token generation",
        "FID",
        "Llama backbone"
      ]
    },
    "publishedAt": "2025-05-29T13:09:25.000Z",
    "title": "D-AR: Diffusion via Autoregressive Models",
    "summary": "This paper presents Diffusion via Autoregressive models (D-AR), a new\nparadigm recasting the image diffusion process as a vanilla autoregressive\nprocedure in the standard next-token-prediction fashion. We start by designing\nthe tokenizer that converts images into sequences of discrete tokens, where\ntokens in different positions can be decoded into different diffusion denoising\nsteps in the pixel space. Thanks to the diffusion properties, these tokens\nnaturally follow a coarse-to-fine order, which directly lends itself to\nautoregressive modeling. Therefore, we apply standard next-token prediction on\nthese tokens, without modifying any underlying designs (either causal masks or\ntraining/inference strategies), and such sequential autoregressive token\ngeneration directly mirrors the diffusion procedure in image space. That is,\nonce the autoregressive model generates an increment of tokens, we can directly\ndecode these tokens into the corresponding diffusion denoising step in the\nstreaming manner. Our pipeline naturally reveals several intriguing properties,\nfor example, it supports consistent previews when generating only a subset of\ntokens and enables zero-shot layout-controlled synthesis. On the standard\nImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone\nwith 256 discrete tokens. We hope our work can inspire future research on\nunified autoregressive architectures of visual synthesis, especially with large\nlanguage models. Code and models will be available at\nhttps://github.com/showlab/D-AR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23660.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648ac3d53470b17ccc90deaf",
      "avatarUrl": "/avatars/35b678baa7a91959f40f704c5de2a3e1.svg",
      "fullname": "Ziteng Gao",
      "name": "sebgao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23646",
      "authors": [
        {
          "_id": "683946c845636acda08ed401",
          "name": "Zijun Yao",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed402",
          "name": "Yantao Liu",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed403",
          "name": "Yanxu Chen",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed404",
          "name": "Jianhui Chen",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed405",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed406",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed407",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "683946c845636acda08ed408",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:53:41.000Z",
      "submittedOnDailyAt": "2025-05-30T04:31:18.157Z",
      "title": "理由モデルがホランショニングに関心を持ちやすいですか？",
      "submittedOnDailyBy": {
        "_id": "62e25e2247678ea5ce1b1786",
        "avatarUrl": "/avatars/1bb32e7597a9b1c89c434cbf550b5382.svg",
        "isPro": false,
        "fullname": "Yantao",
        "user": "RicardoL1u",
        "type": "user"
      },
      "summary": "最近開発された大規模な理由論モデル（LRMs）は、長いチェーンオフサインキング（CoT）論理能力を持って複雑なタスクを解決するときに強力な性能を示しています。これらのLRMsは主に形式的な論理タスクによる後学習によって開発されているため、それらの論理能力が事実探求タスクでのハウマニングを減らすことができるかどうかは明確ではなく、議論の中にあります。例えば、DeepSeek-R1は事実探求ベンチマークSimpleQAでの性能向上を報告していますが、OpenAI-o3はより厳しいハウマニングを観察しています。この差異は自然と次の研究問題を引き起こします：理由論モデルはハウマニングによりもっと脆弱ですか？本論文は3つの角度からこの問題を解決しています。（1）まず、LRMsのハウマニングに関する整體的な評価を行います。分析により、LRMsは冷やかなスタートのサブジェクト調節（SFT）と確認可能な報酬RLでハウマニングを減らすことができますが、そのみの熱学習や冷やかなスタートのSFTを除いたRL学習はより複雑なハウマニングを引き起こします。（2）LRMsのハウマニングにどのような影響を与えるかを調べるために、行動分析を行います。LRMsの事実性に直接影響を与える2つの重要な認知行動を特徴化します：Flaw Repetitionは、表面的な論理試みが同じ潜在的な誤り論理を繰り返して行うことであり、Think-Answer Mismatchは、最終的な答えが前のCoTプロセスに忠実に合わないことである。（3）さらに、LRMsのハウマニングの機構をモデルの不確実性から調べます。LRMsのハウマニングの増加は通常、モデルの不確実性と事実的な精度の不対称に関連付けられます。本論文はLRMsのハウマニングについて初めての理解を提供しています。",
      "upvotes": 17,
      "discussionId": "683946c845636acda08ed42a",
      "ai_summary": "Large reasoning models exhibit varying susceptibility to hallucination depending on post-training pipelines, revealing critical cognitive behaviors and uncertainty misalignment as contributing factors.",
      "ai_keywords": [
        "large reasoning models",
        "chain-of-thought",
        "post-training",
        "formal reasoning tasks",
        "fact-seeking tasks",
        "DeepSeek-R1",
        "OpenAI-o3",
        "hallucination",
        "cold start supervised fine-tuning",
        "verifiable reward RL",
        "distillation",
        "Flaw Repetition",
        "Think-Answer Mismatch",
        "model uncertainty",
        "factual accuracy"
      ]
    },
    "publishedAt": "2025-05-29T12:53:41.000Z",
    "title": "Are Reasoning Models More Prone to Hallucination?",
    "summary": "Recently evolved large reasoning models (LRMs) show powerful performance in\nsolving complex tasks with long chain-of-thought (CoT) reasoning capability. As\nthese LRMs are mostly developed by post-training on formal reasoning tasks,\nwhether they generalize the reasoning capability to help reduce hallucination\nin fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1\nreports increased performance on SimpleQA, a fact-seeking benchmark, while\nOpenAI-o3 observes even severer hallucination. This discrepancy naturally\nraises the following research question: Are reasoning models more prone to\nhallucination? This paper addresses the question from three perspectives. (1)\nWe first conduct a holistic evaluation for the hallucination in LRMs. Our\nanalysis reveals that LRMs undergo a full post-training pipeline with cold\nstart supervised fine-tuning (SFT) and verifiable reward RL generally alleviate\ntheir hallucination. In contrast, both distillation alone and RL training\nwithout cold start fine-tuning introduce more nuanced hallucinations. (2) To\nexplore why different post-training pipelines alters the impact on\nhallucination in LRMs, we conduct behavior analysis. We characterize two\ncritical cognitive behaviors that directly affect the factuality of a LRM: Flaw\nRepetition, where the surface-level reasoning attempts repeatedly follow the\nsame underlying flawed logic, and Think-Answer Mismatch, where the final answer\nfails to faithfully match the previous CoT process. (3) Further, we investigate\nthe mechanism behind the hallucination of LRMs from the perspective of model\nuncertainty. We find that increased hallucination of LRMs is usually associated\nwith the misalignment between model uncertainty and factual accuracy. Our work\nprovides an initial understanding of the hallucination in LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23646.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e25e2247678ea5ce1b1786",
      "avatarUrl": "/avatars/1bb32e7597a9b1c89c434cbf550b5382.svg",
      "fullname": "Yantao",
      "name": "RicardoL1u",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22914",
      "authors": [
        {
          "_id": "6839533ae7922379b361b3cb",
          "name": "Maksim Kolodiazhnyi",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3cc",
          "name": "Denis Tarasov",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3cd",
          "user": {
            "_id": "67d5a331eab66ce9cb01bae4",
            "avatarUrl": "/avatars/3ed437c874889e8a6db66c3ef88a60c0.svg",
            "isPro": false,
            "fullname": "DMITRII ZHEMCHUZHNIKOV",
            "user": "zhemchuzhnikov",
            "type": "user"
          },
          "name": "Dmitrii Zhemchuzhnikov",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T06:42:03.218Z",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3ce",
          "name": "Alexander Nikulin",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3cf",
          "name": "Ilya Zisman",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3d0",
          "name": "Anna Vorontsova",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3d1",
          "name": "Anton Konushin",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3d2",
          "name": "Vladislav Kurenkov",
          "hidden": false
        },
        {
          "_id": "6839533ae7922379b361b3d3",
          "name": "Danila Rukhovich",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T22:32:31.000Z",
      "submittedOnDailyAt": "2025-05-30T05:25:03.367Z",
      "title": "マルチモーダルカードレストラクションにオンライン強化学習を応用して",
      "submittedOnDailyBy": {
        "_id": "665b10fb270e47e678f2ddf1",
        "avatarUrl": "/avatars/1bc7a9211acf767f7bfca998c24315a0.svg",
        "isPro": false,
        "fullname": "max",
        "user": "maksimko123",
        "type": "user"
      },
      "summary": "コンピューターデザイン支援（CAD）は工学と製造工程に中心的な役割を果たし、精密な可設定可能な3Dモデルの作成が可能になっています。多様なセンサーやユーザー提供データをCAD再構成の入力として使用することで、デザインアプリケーションのアクセスが民主化されます。しかし、現在の方法は通常点センターや画像、もしくは本文などの1つの入力モデールを中心に焦点を当てていることで、一般化可能さり強固性が限られています。最近の視覚言語モデル（VLM）の進展を活用し、3つの入力モデールを同時に処理する多モデールCAD再構成モデルを提案します。大規模なプロセス生成データに対する規範的調整（SFT）とオンラインフィードバックを用いた強化学習（RL）調整の2段階パイプラインを採用します。また、LLMのRL調整をCADタスクに最初に探索し、オンラインRLアルゴリズムの例であるGroup Relative Preference Optimization（GRPO）がオフラインアルゴリズムを上回ることを示します。DeepCADベンチマークでは、SFTモデルは3つの入力モデールを同時に優れています。より重要なのは、RL調整後、cadrilleは3つの難しいデータセットにおいて新たな最先端となります。",
      "upvotes": 16,
      "discussionId": "6839533be7922379b361b418",
      "ai_summary": "A multi-modal CAD reconstruction model leveraging vision-language models and reinforcement learning achieves state-of-the-art performance across various datasets, including real-world ones.",
      "ai_keywords": [
        "vision-language models",
        "multi-modal",
        "large language model",
        "supervised fine-tuning",
        "reinforcement learning",
        "Group Relative Preference Optimization",
        "DeepCAD benchmark"
      ]
    },
    "publishedAt": "2025-05-28T18:32:31.000Z",
    "title": "cadrille: Multi-modal CAD Reconstruction with Online Reinforcement\n  Learning",
    "summary": "Computer-Aided Design (CAD) plays a central role in engineering and\nmanufacturing, making it possible to create precise and editable 3D models.\nUsing a variety of sensor or user-provided data as inputs for CAD\nreconstruction can democratize access to design applications. However, existing\nmethods typically focus on a single input modality, such as point clouds,\nimages, or text, which limits their generalizability and robustness. Leveraging\nrecent advances in vision-language models (VLM), we propose a multi-modal CAD\nreconstruction model that simultaneously processes all three input modalities.\nInspired by large language model (LLM) training paradigms, we adopt a two-stage\npipeline: supervised fine-tuning (SFT) on large-scale procedurally generated\ndata, followed by reinforcement learning (RL) fine-tuning using online\nfeedback, obtained programatically. Furthermore, we are the first to explore RL\nfine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such\nas Group Relative Preference Optimization (GRPO) outperform offline\nalternatives. In the DeepCAD benchmark, our SFT model outperforms existing\nsingle-modal approaches in all three input modalities simultaneously. More\nimportantly, after RL fine-tuning, cadrille sets new state-of-the-art on three\nchallenging datasets, including a real-world one.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22914.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "665b10fb270e47e678f2ddf1",
      "avatarUrl": "/avatars/1bc7a9211acf767f7bfca998c24315a0.svg",
      "fullname": "max",
      "name": "maksimko123",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20088",
      "authors": [
        {
          "_id": "683947458987f50a5ec45a01",
          "name": "Nitay Calderon",
          "hidden": false
        },
        {
          "_id": "683947458987f50a5ec45a02",
          "name": "Liat Ein-Dor",
          "hidden": false
        },
        {
          "_id": "683947458987f50a5ec45a03",
          "name": "Roi Reichart",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62d6a0c18faee0ac953c51fa/C8RHn7j61J4qQlZvXd52v.png"
      ],
      "publishedAt": "2025-05-26T15:01:56.000Z",
      "submittedOnDailyAt": "2025-05-30T04:25:05.621Z",
      "title": "多領域の好みの説明性",
      "submittedOnDailyBy": {
        "_id": "62d6a0c18faee0ac953c51fa",
        "avatarUrl": "/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg",
        "isPro": false,
        "fullname": "Nitay Calderon",
        "user": "nitay",
        "type": "user"
      },
      "summary": "偏好機制、その中でも人間の偏好、LLM-as-a-Judge（LaaJ）、報酬モデルなどが、大規模言語モデル（LLMs）の調整と評価に中心的です。しかし、これらの偏好を駆動する裏側の概念は、まだ理解されていません。本稿では、多様な領域での偏好の局所的的および全体的な概念ベースの説明を生成するための完全自動化された方法を提案します。我々の方法は、LLMを利用して選択されたとしては拒否されたレスポンスの間で区別する概念を識別し、それらを概念ベースのベクトルで表現します。概念と偏好の間の関係をモデル化するために、我々は、ドメイン一般的およびドメイン特別的な効果を捉えるための白箱ハイパーバイジョンマルチドメイン回帰モデルを提案します。このモデルを用いて、我々の方法を評価するために、8つの難しいおよび多様な領域を拡張したデータセットを選択し、12つの機制を説明します。我々の方法は、基線を超える強力な偏好予測性能を達成し、同時に説明可能です。また、2つのアプリケーション駆動された設定での説明を評価します。最初、LaaJの説明からの概念を用いてLLMの出力をガイドすることにより、その裁判者が一貫して好みのあるレスポンスを得ます。次に、人間の説明を解釈するための概念をLaaJに提示することにより、彼らの偏好予測を改善します。これらの成果は、LLMsの時代における説明性の新しいパラダイムを確立します。",
      "upvotes": 15,
      "discussionId": "683947488987f50a5ec45a95",
      "ai_summary": "A new automated method using concept-based vectors and a Hierarchical Multi-Domain Regression model improves preference explanations and predictions for large language models.",
      "ai_keywords": [
        "LLM-as-a-Judge",
        "reward models",
        "large language models",
        "concept-based explanations",
        "domain-general effects",
        "domain-specific effects",
        "Hierarchical Multi-Domain Regression model",
        "preference prediction",
        "explainability"
      ]
    },
    "publishedAt": "2025-05-26T11:01:56.000Z",
    "title": "Multi-Domain Explainability of Preferences",
    "summary": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated method for generating\nlocal and global concept-based explanations of preferences across multiple\ndomains. Our method utilizes an LLM to identify concepts that distinguish\nbetween chosen and rejected responses, and to represent them with concept-based\nvectors. To model the relationships between concepts and preferences, we\npropose a white-box Hierarchical Multi-Domain Regression model that captures\nboth domain-general and domain-specific effects. To evaluate our method, we\ncurate a dataset spanning eight challenging and diverse domains and explain\ntwelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two application-driven settings. First,\nguiding LLM outputs with concepts from LaaJ explanations yields responses that\nthose judges consistently prefer. Second, prompting LaaJs with concepts\nexplaining humans improves their preference predictions. Together, our work\nestablishes a new paradigm for explainability in the era of LLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62d6a0c18faee0ac953c51fa/C8RHn7j61J4qQlZvXd52v.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d6a0c18faee0ac953c51fa",
      "avatarUrl": "/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg",
      "fullname": "Nitay Calderon",
      "name": "nitay",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23380",
      "authors": [
        {
          "_id": "683906f81bf7a7a94309c5a5",
          "name": "Weijia Mao",
          "hidden": false
        },
        {
          "_id": "683906f81bf7a7a94309c5a6",
          "name": "Zhenheng Yang",
          "hidden": false
        },
        {
          "_id": "683906f81bf7a7a94309c5a7",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T12:00:15.000Z",
      "submittedOnDailyAt": "2025-05-30T02:41:07.217Z",
      "title": "UniRL: 自我改善するユニークモノモーダルモデルによる監督学習と強化学習",
      "submittedOnDailyBy": {
        "_id": "63f320ee0be81bdc5d8ecb88",
        "avatarUrl": "/avatars/9d08cff6ed23a51887c869947bc03228.svg",
        "isPro": false,
        "fullname": "Mao Weijia",
        "user": "benzweijia",
        "type": "user"
      },
      "summary": "統一多モデルの大規模な言語モデルであるShow-oやJanusは、生成タスクと理解タスクの両方で強力な性能を達成しています。しかし、これらのモデルは通常、大規模なデータセットを基に、予ち学習ステージでは計算量の多く必要となります。また、数多くの後学習方法が提案されていますが、それらは通常、外部データを依存しているか、タスク専用のカスタマイズに限定されています。本稿では、UniRLという自動改善の後学習アプローチを紹介します。我々のアプローチは、モデルがプロンプトから画像を生成し、それらを各イテレーションで学習データとして使用することを可能にします。また、これらの二つのタスクが互いに強化されることができます：生成された画像は理解に使用され、理解の結果は生成を制御することができます。我々は、規範的微調締め（SFT）とグループ相対策最適化（GRPO）を用いてモデルを最適化します。UniRLは3つの主な優みを提供します：（1）外部の画像データを依存しないことで、すべての学習サンプルは学習中にモデル自身が生成しています；（2）モデルの個々のタスクの性能を向上させることで、生成と理解の間の不均衡を減らします；（3）後学習ステージでは、ただ数の追加の学習ステップを必要とします。UniRLはShow-oとJanusの上で評価され、Show-oのGenEvalスコアは0.77、JanusのGenEvalスコアは0.65となりました。コードとモデルはhttps://github.com/showlab/UniRLで公開されます。",
      "upvotes": 14,
      "discussionId": "683906f91bf7a7a94309c5dd",
      "ai_summary": "UniRL is a self-improving post-training method for unified multimodal large language models that uses generated images as training data, enhancing both generation and understanding tasks without external data.",
      "ai_keywords": [
        "Unified multimodal large language models",
        "Show-o",
        "Janus",
        "self-improving post-training",
        "prompts",
        "training data",
        "supervised fine-tuning",
        "Group Relative Policy Optimization",
        "GenEval score"
      ]
    },
    "publishedAt": "2025-05-29T08:00:15.000Z",
    "title": "UniRL: Self-Improving Unified Multimodal Models via Supervised and\n  Reinforcement Learning",
    "summary": "Unified multimodal large language models such as Show-o and Janus have\nachieved strong performance across both generation and understanding tasks.\nHowever, these models typically rely on large-scale datasets and require\nsubstantial computation during the pretraining stage. In addition, several\npost-training methods have been proposed, but they often depend on external\ndata or are limited to task-specific customization. In this work, we introduce\nUniRL, a self-improving post-training approach. Our approach enables the model\nto generate images from prompts and use them as training data in each\niteration, without relying on any external image data. Moreover, it enables the\ntwo tasks to enhance each other: the generated images are used for\nunderstanding, and the understanding results are used to supervise generation.\nWe explore supervised fine-tuning (SFT) and Group Relative Policy Optimization\n(GRPO) to optimize the models. UniRL offers three key advantages: (1) it\nrequires no external image data, as all training samples are generated by the\nmodel itself during training; (2) it not only improves individual task\nperformance, but also reduces the imbalance between generation and\nunderstanding; and (3) it requires only several additional training steps\nduring the post-training stage. We evaluate UniRL on top of Show-o and Janus,\nachieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and\nmodels will be released in https://github.com/showlab/UniRL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23380.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f320ee0be81bdc5d8ecb88",
      "avatarUrl": "/avatars/9d08cff6ed23a51887c869947bc03228.svg",
      "fullname": "Mao Weijia",
      "name": "benzweijia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23419",
      "authors": [
        {
          "_id": "68391574f85de1fc563cd890",
          "name": "Linghao Zhang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd891",
          "name": "Shilin He",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd892",
          "name": "Chaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd893",
          "name": "Yu Kang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd894",
          "name": "Bowen Li",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd895",
          "name": "Chengxing Xie",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd896",
          "name": "Junhao Wang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd897",
          "name": "Maoquan Wang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd898",
          "name": "Yufan Huang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd899",
          "name": "Shengyu Fu",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89a",
          "name": "Elsie Nallipogu",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89b",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89c",
          "name": "Yingnong Dang",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89d",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "68391574f85de1fc563cd89e",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T13:09:44.000Z",
      "submittedOnDailyAt": "2025-05-30T00:49:19.429Z",
      "title": "SWE-bench サービスが開始しました！",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "問題解決タスクでは、モデルが実世界のバグを修正するためのパッチを生成することが、大規模な言語モデル（LLMs）の能力を評価するための重要なベンチマークとして出現しました。しかし、SWE-benchおよびその変体は、初期リリース以降更新されていない、狭いリポジトリセットをカバーし、インスタンス構築と環境設定において手動の努力を重く依存しているため、これらの要素はスケーラビリティを妨げ、過学習およびデータの汚染のリスクを引き起こします。本論文中では、これらの挑戦を克服するために、SWE-bench-Liveというリビーディング可能なベンチマークを紹介します。初期リリースは、2024年以降の実際のGitHub問題からの1,319タスクからなり、93ポジトリを収めています。各タスクは、再現性の確保を目的としたドキュメーブルイメージを付け加えています。本ベンチマークの中心的な部分は、インスタンスの作成から環境の設定までの全過程をストリームライン化する自動化カレーティブパイプラインです。これにより、手動のボトルネックを除去し、スケーラビリティと連続的な更新を可能にします。SWE-bench-Liveでは、種類の最新のアグェントフレームワークとLLMsを評価し、静的ベンチマークと比較しても、プログレス的な性能間隔が明らかになりました。この差異を理解するために、リポジトリの起源、問題の新しさ、タスクの難易度の詳細な分析を行いました。リビーディング可能なリポジトリ活動に基づく新しい、多様な、実行可能なベンチマークを提供することで、SWE-bench-Liveは、LLMsとアグェントの厳密な、汚染抵抗性のある評価を可能にし、動的な、実世界的なソフトウェア開発環境での評価を促進します。",
      "upvotes": 13,
      "discussionId": "68391574f85de1fc563cd8d4",
      "ai_summary": "SWE-bench-Live is a continuously updatable benchmark for evaluating LLMs in issue resolution, featuring live GitHub issues and automated curation to ensure scalability and contamination resistance.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "SWE-bench",
        "SWE-bench-Live",
        "live-updatable benchmark",
        "GitHub issues",
        "Docker image",
        "automated curation pipeline",
        "reproducible execution",
        "performance evaluation",
        "repository origin",
        "issue recency",
        "task difficulty"
      ]
    },
    "publishedAt": "2025-05-29T09:09:44.000Z",
    "title": "SWE-bench Goes Live!",
    "summary": "The issue-resolving task, where a model generates patches to fix real-world\nbugs, has emerged as a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs). While SWE-bench and its variants have become\nstandard in this domain, they suffer from key limitations: they have not been\nupdated since their initial releases, cover a narrow set of repositories, and\ndepend heavily on manual effort for instance construction and environment\nsetup. These factors hinder scalability and introduce risks of overfitting and\ndata contamination. In this work, we present SWE-bench-Live, a\nlive-updatable benchmark designed to overcome these challenges. Our\ninitial release consists of 1,319 tasks derived from real GitHub issues created\nsince 2024, spanning 93 repositories. Each task is accompanied by a dedicated\nDocker image to ensure reproducible execution. Central to our benchmark is\n\\method, an automated curation pipeline that streamlines the entire process\nfrom instance creation to environment setup, removing manual bottlenecks and\nenabling scalability and continuous updates. We evaluate a range of\nstate-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a\nsubstantial performance gap compared to static benchmarks like SWE-bench, even\nunder controlled evaluation conditions. To better understand this discrepancy,\nwe perform detailed analyses across repository origin, issue recency, and task\ndifficulty. By providing a fresh, diverse, and executable benchmark grounded in\nlive repository activity, SWE-bench-Live facilitates rigorous,\ncontamination-resistant evaluation of LLMs and agents in dynamic, real-world\nsoftware development settings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22618",
      "authors": [
        {
          "_id": "683931e1b6280677f75edf09",
          "name": "Chengyue Wu",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0a",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0b",
          "user": {
            "_id": "64cf5e81a2e7f9ff61eb3a0c",
            "avatarUrl": "/avatars/cbaa11daec9d4113bf7de93fe9b9ee86.svg",
            "isPro": false,
            "fullname": "scxue",
            "user": "Cauthyyy",
            "type": "user"
          },
          "name": "Shuchen Xue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:19.453Z",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0c",
          "user": {
            "_id": "650dac79b959b0e1d41d7378",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dac79b959b0e1d41d7378/mzbN0MFk3k8b94FQ40I7L.jpeg",
            "isPro": false,
            "fullname": "Zhijian Liu",
            "user": "zhijianliu",
            "type": "user"
          },
          "name": "Zhijian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:22.124Z",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0d",
          "name": "Shizhe Diao",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0e",
          "name": "Ligeng Zhu",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf0f",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf10",
          "name": "Song Han",
          "hidden": false
        },
        {
          "_id": "683931e1b6280677f75edf11",
          "name": "Enze Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:39:15.000Z",
      "submittedOnDailyAt": "2025-05-30T02:50:04.196Z",
      "title": "Fast-dLLM: 訓練不要なディフューションLLMの加速によるKVキャッシュと並列解確認",
      "submittedOnDailyBy": {
        "_id": "633bd54b00732349209a18fe",
        "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
        "isPro": false,
        "fullname": "Shizhe Diao",
        "user": "shizhediao",
        "type": "user"
      },
      "summary": "Diffusionベースの大規模言語モデル（Diffusion LLMs）は、並列解像能力を持つ非自動帰受文生成に優れています。しかし、Key-Value (KV) Cacheのないため、開源されたDiffusion LLMsの実用的な推論速度は、自動帰受文モデルよりも遅く、同時に複数のトークンを解像する際に品質が低下します。この隙を埋めるために、私たちは、両方向的なDiffusionモデルに適した新しいブロックごとの近似KV Cache機構を導入し、無視できるほどの性能低下を伴わずにキャッシュを再利用できるようにしました。また、並列解像での生成品質の低下の根因を見つけ、条件独立性の仮定によるトークンの依存関係の破続を原因としていることを認識しました。これに対して、私たちは、信頼度を超えたトークンを選択的に解像することで依存関係の違反を軽減し、生成品質を維持するコンフィデンスに基づく並列解像戦略を提案しました。LLaDAとDreamモデルの複数のLLMベンチマークでの実験結果によると、精度損失が最小限であり、自動帰受文モデルとの性能間隔を閉じ、Diffusion LLMsの実用的な採用につながるようにしました。",
      "upvotes": 11,
      "discussionId": "683931e2b6280677f75edf32",
      "ai_summary": "A novel block-wise approximate KV Cache and confidence-aware parallel decoding strategy improve the inference speed of diffusion-based large language models without significant quality loss.",
      "ai_keywords": [
        "diffusion-based large language models (Diffusion LLMs)",
        "non-autoregressive text generation",
        "parallel decoding",
        "Key-Value (KV) Cache",
        "bidirectional diffusion models",
        "token dependencies",
        "confidence-aware parallel decoding",
        "LLaDA",
        "Dream models",
        "LLM benchmarks"
      ]
    },
    "publishedAt": "2025-05-28T13:39:15.000Z",
    "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
    "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to 27.6times\nthroughput improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633bd54b00732349209a18fe",
      "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
      "fullname": "Shizhe Diao",
      "name": "shizhediao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23606",
      "authors": [
        {
          "_id": "6839189f4a3a71a917b0514e",
          "user": {
            "_id": "656724074f6ec72017754d33",
            "avatarUrl": "/avatars/e61de248f6f53719b2375077340dd033.svg",
            "isPro": false,
            "fullname": "QingyuShi",
            "user": "QingyuShi",
            "type": "user"
          },
          "name": "Qingyu Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:45.160Z",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b0514f",
          "user": {
            "_id": "63fccdac93b993a4ebd7789a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
            "isPro": false,
            "fullname": "Jinbin Bai",
            "user": "BryanW",
            "type": "user"
          },
          "name": "Jinbin Bai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:47.122Z",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05150",
          "name": "Zhuoran Zhao",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05151",
          "name": "Wenhao Chai",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05152",
          "name": "Kaidong Yu",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05153",
          "name": "Jianzong Wu",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05154",
          "name": "Shuangyong Song",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05155",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05156",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05157",
          "name": "Xuelong Li",
          "hidden": false
        },
        {
          "_id": "6839189f4a3a71a917b05158",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:15:48.000Z",
      "submittedOnDailyAt": "2025-05-30T01:06:53.607Z",
      "title": "ムドィット：文脈より画像を生成することを超えて、統一的な離散ディフフェレンションモデルを用いて次世代を解放する",
      "submittedOnDailyBy": {
        "_id": "63fccdac93b993a4ebd7789a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
        "isPro": false,
        "fullname": "Jinbin Bai",
        "user": "BryanW",
        "type": "user"
      },
      "summary": "統合生成モデルは、テキスト生成、画像生成、ビジョン言語推理などの多様なタスクを、1つのアーキテクチャと解碼パラダイムで処理することを目指しています。自動順次生成の統合モデルは、順次解碼により推論が遅く、非自動順次生成の統合モデルは、限定的な事前学習バックボーンによる弱い一般化に苦しみます。我々は、Muddit、テキストと画像の両方のモデルデータルや平行な高速生成を可能にする統合的な離散ディフフェレンシャルトランスフォーマーを紹介します。先行の統合ディフフェレンシャルモデルと異なり、Mudditは、強い画像プロイダーズを持つ事前学習テキストタウン画像バックボーンと軽量テキストデコーダーを統合し、1つのアーキテクチャで柔軟で高品質の多モデル生成を可能にします。実験結果から、Mudditは質と効率において、もっとも大きな自動順次モデルと比較して相対的または上位の性能を達成します。本作は、強い画像プロイダーズを持つ単に離散ディフフェレンシャルを、統合生成の可換性と効率的なバックボーンとしての可能性を高めています。",
      "upvotes": 10,
      "discussionId": "683918a14a3a71a917b051ea",
      "githubRepo": "https://github.com/M-E-AGI-Lab/Muddit",
      "ai_summary": "Muddit, a unified discrete diffusion transformer, achieves fast and high-quality generation across text and image modalities by integrating pretrained visual priors with a lightweight text decoder.",
      "ai_keywords": [
        "unified generation models",
        "autoregressive models",
        "non-autoregressive models",
        "discrete diffusion",
        "diffusion transformer",
        "pretrained backbones",
        "flexible generation",
        "multimodal generation",
        "text generation",
        "image generation",
        "vision-language reasoning",
        "quality",
        "efficiency",
        "visual priors",
        "scalable backbone"
      ]
    },
    "publishedAt": "2025-05-29T12:15:48.000Z",
    "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified\n  Discrete Diffusion Model",
    "summary": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23606.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63fccdac93b993a4ebd7789a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
      "fullname": "Jinbin Bai",
      "name": "BryanW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23585",
      "authors": [
        {
          "_id": "683916a7a2d6f83cf12552dd",
          "name": "Yaru Hao",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552de",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552df",
          "name": "Xun Wu",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552e0",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552e1",
          "name": "Zewen Chi",
          "hidden": false
        },
        {
          "_id": "683916a7a2d6f83cf12552e2",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T15:58:04.000Z",
      "submittedOnDailyAt": "2025-05-30T00:53:45.844Z",
      "title": "ポリシープロジェクトのRLに最適な報酬ベースラインフレームを使用してコンテキストを追加したもの",
      "submittedOnDailyBy": {
        "_id": "5df85abada6d0311fd3d5408",
        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
        "isPro": false,
        "fullname": "Li Dong",
        "user": "unilm",
        "type": "user"
      },
      "summary": "強化学習アルゴリズムは、大規模な言語モデルと人間の好みを一致させ、理由論の能力を向上させるために基盤的です。しかし、現在の強化学習アルゴリズムは、ポリシー制約の厳密性の低さによるトレーニング不穩定および補助モデルによる計算ネットワークの効率化の低さによるトレーニング不穩定に悩まされます。本論文では、これらの課題を解決するために新しい簡単化された強化学習アルゴリズム「On-Policy RL with Optimal reward baseline (OPO)」を提案します。OPOは、実験的にトレーニングプロセスの安定化と探索の向上を実現するための正確なポリシートレーニングの重要性を強調しています。また、OPOは理論的に勾配の分散を最小化する最適な報酬ベースラインを導入しています。数学的な理由論ベンチマーク上でOPOを評価しました。結果は、追加されたモデルや正則化項を含めずにも、上位の性能とトレーニングの安定化を示しました。また、OPOはより低いポリシーシフトとより高い出力ヒートベントを実現し、より多様なおよび再複製しないレスポンスを促成します。これらの結果は、大規模な言語モデルの対応と理由論タスクにおける安定したおよび効果的な強化学習の新しい方向をOPOが示していることを明らかにしています。実装は、https://github.com/microsoft/LMOps/tree/main/opo で提供されています。",
      "upvotes": 9,
      "discussionId": "683916a8a2d6f83cf1255310",
      "githubRepo": "https://github.com/microsoft/LMOps/tree/main/opo",
      "ai_summary": "A novel reinforcement learning algorithm, OPO, improves training stability and performance in large language model alignment and reasoning by emphasizing exact on-policy training and using an optimal reward baseline.",
      "ai_keywords": [
        "reinforcement learning",
        "on-policy constraints",
        "computational efficiency",
        "auxiliary models",
        "optimal reward baseline",
        "gradient variance",
        "policy shifts",
        "output entropy",
        "mathematical reasoning benchmarks"
      ]
    },
    "publishedAt": "2025-05-29T11:58:04.000Z",
    "title": "On-Policy RL with Optimal Reward Baseline",
    "summary": "Reinforcement learning algorithms are fundamental to align large language\nmodels with human preferences and to enhance their reasoning capabilities.\nHowever, current reinforcement learning algorithms often suffer from training\ninstability due to loose on-policy constraints and computational inefficiency\ndue to auxiliary models. In this work, we propose On-Policy RL with Optimal\nreward baseline (OPO), a novel and simplified reinforcement learning algorithm\ndesigned to address these challenges. OPO emphasizes the importance of exact\non-policy training, which empirically stabilizes the training process and\nenhances exploration. Moreover, OPO introduces the optimal reward baseline that\ntheoretically minimizes gradient variance. We evaluate OPO on mathematical\nreasoning benchmarks. The results demonstrate its superior performance and\ntraining stability without additional models or regularization terms.\nFurthermore, OPO achieves lower policy shifts and higher output entropy,\nencouraging more diverse and less repetitive responses. These results highlight\nOPO as a promising direction for stable and effective reinforcement learning in\nlarge language model alignment and reasoning tasks. The implementation is\nprovided at https://github.com/microsoft/LMOps/tree/main/opo.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23585.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5df85abada6d0311fd3d5408",
      "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
      "fullname": "Li Dong",
      "name": "unilm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 29
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22421",
      "authors": [
        {
          "_id": "68391dbb0653b6a3441a7f7e",
          "user": {
            "_id": "6311d9ee04f842f79916158c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PvOYgDEpe0x5ExLyj1BK-.png",
            "isPro": false,
            "fullname": "chen",
            "user": "antonio-c",
            "type": "user"
          },
          "name": "Anthony Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:38.940Z",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f7f",
          "name": "Wenzhao Zheng",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f80",
          "user": {
            "_id": "647068944be5cf1f33491cb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Aqudl2PTSKPAylBhlccWr.png",
            "isPro": false,
            "fullname": "Yida Wang",
            "user": "wangyida",
            "type": "user"
          },
          "name": "Yida Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:36.769Z",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f81",
          "name": "Xueyang Zhang",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f82",
          "name": "Kun Zhan",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f83",
          "name": "Peng Jia",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f84",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "68391dbb0653b6a3441a7f85",
          "name": "Shanghang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T14:46:51.000Z",
      "submittedOnDailyAt": "2025-05-30T01:26:00.575Z",
      "title": "GeoDrive: 3Dギャジメトリー情報付き運転ワールドモデルと精確なアクション制御",
      "submittedOnDailyBy": {
        "_id": "6311d9ee04f842f79916158c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PvOYgDEpe0x5ExLyj1BK-.png",
        "isPro": false,
        "fullname": "chen",
        "user": "antonio-c",
        "type": "user"
      },
      "summary": "最近の世界モデルの進歩は、動的な環境シミュレーションに革命的な影響を与え、システムが将来の状態を予測し、潜在的な行動を評価することができるようになりました。自動運転において、これらの能力は、車両が他の道路使用者の行動を予測し、リスクに関する計画を行い、シミュレーションのトレーニングを加速し、新しいシナリオに適応することを可能にし、安全性と信頼性を向上させます。現在のアプローチは、自動ナビゲーションタスクで信頼性のある安全性評価に必要な強固な3Dジェムトロニカルな一致性の維持または遮蔽処理時のアーティファクトの蓄積に欠陥を見出しています。これに対して、私たちはGeoDriveを紹介し、自動運転の世界モデルに強固な3Dジェムトロニカルな条件を明記して、空間理解と行動制御可能度を向上させることで、安全な自動運転のためにより写実的な、適応性のあるおよび信頼性のあるシーンモデリングを実現することを目指しています。特に、入力フレームから3D表現を抽出し、ユーザー指定の自動車の軌道に基づいて2Dレンダリングを取得します。動的なモデリングを可能にするために、トレーニング中に動的な編集モジュールを提案し、車両の位置を編集してレンダリングを改善します。拡張された実験により、我々の方法は、行動精度と3D空間認識の両方で既存のモデルよりも显著に優れてい、安全な自動運転におけるより写実的な、適応性のあるおよび信頼性のあるシーンモデリングを実現することを示します。また、我々のモデルは新しい軌道に一般化可能で、物体編集や物体軌道制御などの相互作用シーン編集機能を提供します。",
      "upvotes": 9,
      "discussionId": "68391dbc0653b6a3441a7fd1",
      "githubRepo": "https://github.com/antonioo-c/GeoDrive",
      "ai_summary": "GeoDrive integrates robust 3D geometry into driving world models to improve spatial understanding and action controllability in autonomous navigation, enhancing safety and reliability.",
      "ai_keywords": [
        "world models",
        "dynamic environment simulation",
        "autonomous driving",
        "3D geometric consistency",
        "occlusion handling",
        "3D representation",
        "2D rendering",
        "ego-car trajectory",
        "dynamic editing module",
        "action accuracy",
        "3D spatial awareness",
        "scene modeling",
        "object editing",
        "object trajectory control"
      ]
    },
    "publishedAt": "2025-05-28T10:46:51.000Z",
    "title": "GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action\n  Control",
    "summary": "Recent advancements in world models have revolutionized dynamic environment\nsimulation, allowing systems to foresee future states and assess potential\nactions. In autonomous driving, these capabilities help vehicles anticipate the\nbehavior of other road users, perform risk-aware planning, accelerate training\nin simulation, and adapt to novel scenarios, thereby enhancing safety and\nreliability. Current approaches exhibit deficiencies in maintaining robust 3D\ngeometric consistency or accumulating artifacts during occlusion handling, both\ncritical for reliable safety assessment in autonomous navigation tasks. To\naddress this, we introduce GeoDrive, which explicitly integrates robust 3D\ngeometry conditions into driving world models to enhance spatial understanding\nand action controllability. Specifically, we first extract a 3D representation\nfrom the input frame and then obtain its 2D rendering based on the\nuser-specified ego-car trajectory. To enable dynamic modeling, we propose a\ndynamic editing module during training to enhance the renderings by editing the\npositions of the vehicles. Extensive experiments demonstrate that our method\nsignificantly outperforms existing models in both action accuracy and 3D\nspatial awareness, leading to more realistic, adaptable, and reliable scene\nmodeling for safer autonomous driving. Additionally, our model can generalize\nto novel trajectories and offers interactive scene editing capabilities, such\nas object editing and object trajectory control.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22421.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6311d9ee04f842f79916158c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PvOYgDEpe0x5ExLyj1BK-.png",
      "fullname": "chen",
      "name": "antonio-c",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23758",
      "authors": [
        {
          "_id": "683925243a3289061eda69ee",
          "user": {
            "_id": "65454d7c117ecae648892170",
            "avatarUrl": "/avatars/83a7091a24bf86801176ca85234b417a.svg",
            "isPro": false,
            "fullname": "Yusuf Dalva",
            "user": "ydalva",
            "type": "user"
          },
          "name": "Yusuf Dalva",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:26.107Z",
          "hidden": false
        },
        {
          "_id": "683925243a3289061eda69ef",
          "name": "Hidir Yesiltepe",
          "hidden": false
        },
        {
          "_id": "683925243a3289061eda69f0",
          "name": "Pinar Yanardag",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:46.000Z",
      "submittedOnDailyAt": "2025-05-30T01:56:15.021Z",
      "title": "LoRAShop: トレーニング無しの多概念画像生成と編集における修正フロートランスフォーマー",
      "submittedOnDailyBy": {
        "_id": "65454d7c117ecae648892170",
        "avatarUrl": "/avatars/83a7091a24bf86801176ca85234b417a.svg",
        "isPro": false,
        "fullname": "Yusuf Dalva",
        "user": "ydalva",
        "type": "user"
      },
      "summary": "LoRAShopは、LoRAモデルを用いた多概念画像編集の最初のフレームワークです。LoRAShopは、Fluxスタイルのdiffusion transformers内部の特徴相互作用パターンについての重要な観察をベースに構築しています：概念固有のtransformer特徴がデノイズプロセス初期に空間的に一貫した領域に活性化されます。この観察を活用し、先にフォワードパスで各概念に対してディセンテルレーテントマスクを計算し、その概念を定義する領域内でのみ対応するLoRA重みをブレンドします。これにより、結果的な編集は、元のスケーンにおいて複数の主題やスタイルを無際的に統合し、グローバルなコンテキスト、照明、および詳細を保持します。私たちの実験は、LoRAShopがベースラインと比較してより良いアウトライン保持を提供していることを示しています。リトレーニングと外部制約を除去することで、LoRAShopは、個人化デフォーマションモデルを実用的な「LoRAを用いたphotoshop」ツールに変換し、構成的な視覚シュートリエティングおよび急速なクリエイティブイテレーションの新しい道を開きます。",
      "upvotes": 8,
      "discussionId": "683925263a3289061eda6a62",
      "projectPage": "https://lorashop.github.io/",
      "ai_summary": "LoRAShop, a framework for multi-concept image editing with LoRA models, leverages spatially coherent feature activation in Flux-style diffusion transformers to achieve seamless integration of multiple subjects or styles while preserving global context and identity.",
      "ai_keywords": [
        "Flux-style diffusion transformers",
        "concept-specific transformer features",
        "denoising process",
        "disentangled latent mask",
        "LoRA models",
        "identity preservation",
        "personalized diffusion models",
        "compositional visual storytelling",
        "rapid creative iteration"
      ]
    },
    "publishedAt": "2025-05-29T13:59:46.000Z",
    "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with\n  Rectified Flow Transformers",
    "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23758.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65454d7c117ecae648892170",
      "avatarUrl": "/avatars/83a7091a24bf86801176ca85234b417a.svg",
      "fullname": "Yusuf Dalva",
      "name": "ydalva",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23735",
      "authors": [
        {
          "_id": "6839158b56bcc85d9f92199b",
          "name": "Ali Behrouz",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199c",
          "name": "Zeman Li",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199d",
          "name": "Praneeth Kacham",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199e",
          "name": "Majid Daliri",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f92199f",
          "name": "Yuan Deng",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f9219a0",
          "name": "Peilin Zhong",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f9219a1",
          "name": "Meisam Razaviyayn",
          "hidden": false
        },
        {
          "_id": "6839158b56bcc85d9f9219a2",
          "name": "Vahab Mirrokni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:57:16.000Z",
      "submittedOnDailyAt": "2025-05-30T00:49:34.783Z",
      "title": "ATLAS: テストタイムで最適的にコンテキストを記憶する学習",
      "submittedOnDailyBy": {
        "_id": "65cccd5134a5d74cbaa9446c",
        "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
        "isPro": false,
        "fullname": "Ali Behrouz",
        "user": "AliBehrouz",
        "type": "user"
      },
      "summary": "Transformersは、順序モデリングの最も人気のあるベースコースとして確立されています。主に、そのIn-Context Retrievalタスクの効果性とスケールアップ可能な学習能力により、このような特徴を持っています。しかし、その二次元のメモリと時間複雑性は、長めのシーケンスに対する適用範囲を制限し、これにより、研究者たちは、現代のリカレントニューラルネットワーク（長期リカレントメモリモジュール）のような有効なアライターツクラチャーを探索することに励まされました。最近、多様な下流タスクでの成功にも加えて、長めのコンテキストの理解と長めのシーケンスにおける外挿が必要なタスクでは、これらのアライターツクラチャーは苦労しています。我々は、これらの欠点は、デザインの3つの異なる面で起きていることを見出しました。1. メモリ容量の制限は、入力のメモリ構造と特徴マッピングによって制限されています。2. メモリの更新は線上の性質であり、最後の入力にだけ対して最適化されています。3. 固定サイズのメモリの管理は表現力が低いです。これらの3つの面を改善するために、我々は、現在と過去のトークンに基づいてメモリを最適化し、長期メモリモデルの線上の性質を克服するための高容量の長期メモリモジュールを提案します。ATLAS。この洞察に基づいて、我々は、DeepTransformersというTransformerような新しいアーキテクチャーの家族を提案します。これらのアーキテクチャーは、元のTransformerアーキテクチャーの厳密な一般化です。言語モデリング、一般知識推理、呼び出し強調、長めのコンテキスト理解タスクにおける実験結果により、ATLASはTransformersと最近の線形リカレントモデルの性能を超えます。ATLASは、Titansの長めのコンテキスト性能を進め、BABILongベンチマークの10Mコンテキスト長では、精度を+80%向上させます。",
      "upvotes": 8,
      "discussionId": "6839158c56bcc85d9f9219fc",
      "ai_summary": "A new long-term memory module called ATLAS addresses limitations of Transformers in handling long contexts by optimizing memory based on current and past inputs, leading to improved performance in long-context understanding tasks.",
      "ai_keywords": [
        "Transformers",
        "sequence modeling",
        "in-context retrieval",
        "memory capacity",
        "online nature",
        "fixed-size memory",
        "long-term memory module",
        "DeepTransformers",
        "language modeling",
        "common-sense reasoning",
        "recall-intensive tasks",
        "long-context understanding",
        "ATLAS",
        "Titans",
        "BABILong benchmark"
      ]
    },
    "publishedAt": "2025-05-29T13:57:16.000Z",
    "title": "ATLAS: Learning to Optimally Memorize the Context at Test Time",
    "summary": "Transformers have been established as the most popular backbones in sequence\nmodeling, mainly due to their effectiveness in in-context retrieval tasks and\nthe ability to learn at scale. Their quadratic memory and time complexity,\nhowever, bound their applicability in longer sequences and so has motivated\nresearchers to explore effective alternative architectures such as modern\nrecurrent neural networks (a.k.a long-term recurrent memory module). Despite\ntheir recent success in diverse downstream tasks, they struggle in tasks that\nrequires long context understanding and extrapolation to longer sequences. We\nobserve that these shortcomings come from three disjoint aspects in their\ndesign: (1) limited memory capacity that is bounded by the architecture of\nmemory and feature mapping of the input; (2) online nature of update, i.e.,\noptimizing the memory only with respect to the last input; and (3) less\nexpressive management of their fixed-size memory. To enhance all these three\naspects, we present ATLAS, a long-term memory module with high capacity that\nlearns to memorize the context by optimizing the memory based on the current\nand past tokens, overcoming the online nature of long-term memory models.\nBuilding on this insight, we present a new family of Transformer-like\narchitectures, called DeepTransformers, that are strict generalizations of the\noriginal Transformer architecture. Our experimental results on language\nmodeling, common-sense reasoning, recall-intensive, and long-context\nunderstanding tasks show that ATLAS surpasses the performance of Transformers\nand recent linear recurrent models. ATLAS further improves the long context\nperformance of Titans, achieving +80\\% accuracy in 10M context length of\nBABILong benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cccd5134a5d74cbaa9446c",
      "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
      "fullname": "Ali Behrouz",
      "name": "AliBehrouz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23416",
      "authors": [
        {
          "_id": "68393d075711daf8cc919c04",
          "user": {
            "_id": "62e21907eda17fc126a15210",
            "avatarUrl": "/avatars/097879cf1eafa5e77bde419c7172fac6.svg",
            "isPro": false,
            "fullname": "Jang-Hyun Kim",
            "user": "Jang-Hyun",
            "type": "user"
          },
          "name": "Jang-Hyun Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:14.736Z",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c05",
          "user": {
            "_id": "60eb9074cc720726777d22a2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60eb9074cc720726777d22a2/aI-pHJRmnYOfC5fH7fFzD.jpeg",
            "isPro": false,
            "fullname": "Jinuk Kim",
            "user": "jusjinuk",
            "type": "user"
          },
          "name": "Jinuk Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:17.350Z",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c06",
          "name": "Sangwoo Kwon",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c07",
          "name": "Jae W. Lee",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c08",
          "name": "Sangdoo Yun",
          "hidden": false
        },
        {
          "_id": "68393d075711daf8cc919c09",
          "name": "Hyun Oh Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T13:05:47.000Z",
      "submittedOnDailyAt": "2025-05-30T03:37:32.573Z",
      "title": "KVzip: クエリ無知のKVキャッシュ圧縮にコンテキスト再構築を用いる",
      "submittedOnDailyBy": {
        "_id": "62e21907eda17fc126a15210",
        "avatarUrl": "/avatars/097879cf1eafa5e77bde419c7172fac6.svg",
        "isPro": false,
        "fullname": "Jang-Hyun Kim",
        "user": "Jang-Hyun",
        "type": "user"
      },
      "summary": "Transformerベースの大規模言語モデル（LLMs）は、推論時にコンテキストをキー値（KV）ペアとしてキャッシュします。コンテキストの長さが増えると、KVキャッシュサイズが拡大し、メモリオーバーヘッドと注意の遅延が増加します。本論文では、KVzipというクエリ無関係的なKVキャッシュの排除方法を紹介し、多様なクエリで効果的な圧縮されたKVキャッシュの再利用を可能にします。KVzipは、圧縮されたKVペアから元のコンテキストを再構築するための基礎的なLLMを使用して、重要度を評価し、重要度が低いペアを排除します。実験的評価により、KVzipはKVキャッシュサイズを3～4倍縮小し、FlashAttentionの解確定遅延を約2倍縮小し、問題解決、検索、理由、コード理解の課題に対しては微視的な性能損失を伴いません。評価には、LLaMA3.1-8B、Qwen2.5-14B、Gemma3-12Bなどの様々なモデルを含み、コンテキスト長が170Kトークンに達します。KVzipは、多クエリシナリオで90%のキャッシュバケット比率でも性能低下を受けないように、現在のクエリに依存するKV排除方法に比べて显著に優れています。",
      "upvotes": 8,
      "discussionId": "68393d075711daf8cc919c2b",
      "githubRepo": "https://github.com/snu-mllab/KVzip",
      "ai_summary": "KVzip, a query-agnostic KV cache eviction method for transformer-based LLMs, reduces KV cache size and decoding latency while maintaining performance across various tasks and models.",
      "ai_keywords": [
        "Transformer-based large language models (LLMs)",
        "KV cache",
        "context length",
        "query-agnostic",
        "KVzip",
        "query-aware",
        "KV eviction",
        "attention latency",
        "FlashAttention",
        "question-answering",
        "retrieval",
        "reasoning",
        "code comprehension",
        "LLaMA3.1-8B",
        "Qwen2.5-14B",
        "Gemma3-12B"
      ]
    },
    "publishedAt": "2025-05-29T09:05:47.000Z",
    "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
    "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4times and FlashAttention decoding latency by approximately\n2times, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23416.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e21907eda17fc126a15210",
      "avatarUrl": "/avatars/097879cf1eafa5e77bde419c7172fac6.svg",
      "fullname": "Jang-Hyun Kim",
      "name": "Jang-Hyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23559",
      "authors": [
        {
          "_id": "68390eebc260f5fa36eaa27c",
          "user": {
            "_id": "66554507e6ea63012f35824c",
            "avatarUrl": "/avatars/b82de75bd60890e7bb524fc3754b131c.svg",
            "isPro": false,
            "fullname": "Kunlun_Zhu",
            "user": "Leozkl",
            "type": "user"
          },
          "name": "Kunlun Zhu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T01:50:37.811Z",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa27d",
          "name": "Jiaxun Zhang",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa27e",
          "name": "Ziheng Qi",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa27f",
          "name": "Nuoxing Shang",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa280",
          "user": {
            "_id": "65d188a4aa309d842e438ef1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
            "isPro": false,
            "fullname": "Zijia Liu",
            "user": "m-serious",
            "type": "user"
          },
          "name": "Zijia Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:08.570Z",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa281",
          "user": {
            "_id": "638d601b5e14c2f38678fb3a",
            "avatarUrl": "/avatars/98d6151ec4261741eb4bd406685c07b5.svg",
            "isPro": false,
            "fullname": "韩沛煊",
            "user": "HakHan",
            "type": "user"
          },
          "name": "Peixuan Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:06.300Z",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa282",
          "name": "Yue Su",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa283",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "68390eebc260f5fa36eaa284",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T15:35:58.000Z",
      "submittedOnDailyAt": "2025-05-30T00:27:19.661Z",
      "title": "SafeScientist: LLMアガントによるリスク意識のある科学の発見へ",
      "submittedOnDailyBy": {
        "_id": "64c090a9f613170e7be93d2f",
        "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
        "isPro": false,
        "fullname": "KunlunZhu",
        "user": "KunlunZhu",
        "type": "user"
      },
      "summary": "最新の大規模言語モデル（LLM）アガントの進歩は、科学の発見自動化を大幅に加速したが、同時に重要な倫理的および安全性の懸念を引き起こしました。これらの挑戦をシステマティックに解決するために、私たちは、AIを駆動する科学の探索で安全性と倫理責任を高めるために特に設計された革新的なAIサイエンティストフレームワーク「SafeScientist」を紹介します。SafeScientistは、倫理的に適切でないまたは高リスクのタスクを主動的に拒否し、研究プロセス全体で厳格に安全性を強調しています。安全性の全面的な監視を実現するために、私たちは、プロンプトモニタリング、アガントコラボレーションモニタリング、ツール使用モニタリング、および倫理評価者コンポーネントを含めた複数の防御機構を統合しています。SafeScientistの補完として、私たちは、科学のコンテキストでのAIの安全性を評価するための新しいベンチマーク「SciSafetyBench」を提案します。これは、6つの領域からの240つの高リスクの科学タスク、特に設計された30つの科学ツール、および120つのツール関連リスクタスクからなるベンチマークです。拡大的な実験は、傳統的なAIサイエンティストフレームワークと比較して安全性性能を35％改善し、科学の出力品質を損なわずに実現したことを示しています。また、私たちは、多様な対抗的攻撃手法に対する安全パイプラインの強固性を厳格に検証し、我々の統合的アプローチの効果をさらに確認しています。コードとデータは、https://github.com/ulab-uiuc/SafeScientist から利用可能です。警告：この論文には、おかしいものや損害を招く可能性があるサンプルデータが含まれています。",
      "upvotes": 7,
      "discussionId": "68390eedc260f5fa36eaa319",
      "ai_summary": "SafeScientist is an AI framework that enhances safety in AI-driven scientific research through multiple defensive mechanisms and is validated using the SciSafetyBench benchmark.",
      "ai_keywords": [
        "SafeScientist",
        "AI scientist framework",
        "ethical responsibility",
        "prompt monitoring",
        "agent-collaboration monitoring",
        "tool-use monitoring",
        "ethical reviewer component",
        "SciSafetyBench",
        "benchmark",
        "scientific tasks",
        "scientific tools",
        "safety performance",
        "adversarial attack methods"
      ]
    },
    "publishedAt": "2025-05-29T11:35:58.000Z",
    "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
    "summary": "Recent advancements in large language model (LLM) agents have significantly\naccelerated scientific discovery automation, yet concurrently raised critical\nethical and safety concerns. To systematically address these challenges, we\nintroduce SafeScientist, an innovative AI scientist framework\nexplicitly designed to enhance safety and ethical responsibility in AI-driven\nscientific exploration. SafeScientist proactively refuses ethically\ninappropriate or high-risk tasks and rigorously emphasizes safety throughout\nthe research process. To achieve comprehensive safety oversight, we integrate\nmultiple defensive mechanisms, including prompt monitoring, agent-collaboration\nmonitoring, tool-use monitoring, and an ethical reviewer component.\nComplementing SafeScientist, we propose SciSafetyBench, a novel\nbenchmark specifically designed to evaluate AI safety in scientific contexts,\ncomprising 240 high-risk scientific tasks across 6 domains, alongside 30\nspecially designed scientific tools and 120 tool-related risk tasks. Extensive\nexperiments demonstrate that SafeScientist significantly improves safety\nperformance by 35\\% compared to traditional AI scientist frameworks, without\ncompromising scientific output quality. Additionally, we rigorously validate\nthe robustness of our safety pipeline against diverse adversarial attack\nmethods, further confirming the effectiveness of our integrated approach. The\ncode and data will be available at https://github.com/ulab-uiuc/SafeScientist.\nred{Warning: this paper contains example data that may be offensive\nor harmful.}",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23559.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c090a9f613170e7be93d2f",
      "avatarUrl": "/avatars/ccbdf444e1f2386d2281e8e42059ebb0.svg",
      "fullname": "KunlunZhu",
      "name": "KunlunZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22961",
      "authors": [
        {
          "_id": "68390b2ea26b4142b0578d05",
          "user": {
            "_id": "638d601b5e14c2f38678fb3a",
            "avatarUrl": "/avatars/98d6151ec4261741eb4bd406685c07b5.svg",
            "isPro": false,
            "fullname": "韩沛煊",
            "user": "HakHan",
            "type": "user"
          },
          "name": "Peixuan Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:17.916Z",
          "hidden": false
        },
        {
          "_id": "68390b2ea26b4142b0578d06",
          "user": {
            "_id": "65d188a4aa309d842e438ef1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
            "isPro": false,
            "fullname": "Zijia Liu",
            "user": "m-serious",
            "type": "user"
          },
          "name": "Zijia Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:20.240Z",
          "hidden": false
        },
        {
          "_id": "68390b2ea26b4142b0578d07",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T01:03:41.000Z",
      "submittedOnDailyAt": "2025-05-30T00:34:06.330Z",
      "title": "ToMAP: トライディング オペネントに親しむLLM説服者たちを、心の理論に基づく",
      "submittedOnDailyBy": {
        "_id": "65d188a4aa309d842e438ef1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
        "isPro": false,
        "fullname": "Zijia Liu",
        "user": "m-serious",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は說服において望ましいポテンシャルを示しているが、現在のLLM説服者の訓練に関する研究はまだ初步的である。特に、人間は対手の思いや考えを主動的にダイナミックにモデル化するスキルがあるが、現在のLLMsはこのTheory of Mind（ToM）の理由論を難しく認識し、対手の多様性と認識が限られている。この制限を解決するために、私たちはTheory of Mind Augmented Persuader（ToMAP）を導入し、2つのTheory of Mindモジュールを組み込み、説服者の対手の心の状態の認識と分析を強化する新しいアプローチを提案している。特に、私たちは、説服者を対手に対する中央主張に対する可能な反対意見を考慮させるようにプロンプトし、その反対意見に対する対手の現在の立場を予測するために、テキストエンコーダーと学習済みのMLP分類器を組み合わせる。説服者が学ぶことを可能にする、調整された再強化学習スキーマを設計し、対手に関連する情報を分析し、それを使ってより効果的な論点を生成することを学ぶことができる。実験は、ToMAP説服者は3Bパラメータを持つだけで、GPT-4oやその他の大きなベースラインを超え、複数の説服先モデルと多様なコーパスで39.4%の相対的な収益を示していることを示している。特に、ToMAPは複雑な理由鏈を示し、訓練中の再現を減らし、より多様的で効果的な論点を生成することができる。ToMAPの対手認識の機能も長いコンバーションに適して、より理性的で対手認識の戦略を採用することができる。これらの結果は、私たちの方法の効果性を強調し、その説服力のある言語アウトローカーの開発の可能性を明らかにしている。コードは以下のURLで利用可能です：https://github.com/ulab-uiuc/ToMAP。",
      "upvotes": 7,
      "discussionId": "68390b30a26b4142b0578d58",
      "githubRepo": "https://github.com/ulab-uiuc/ToMAP",
      "ai_summary": "ToMAP enhances LLM persuaders with Theory of Mind modules, improving opponent awareness and argument quality.",
      "ai_keywords": [
        "large language models",
        "theory of mind (ToM)",
        "reinforcement learning",
        "text encoder",
        "MLP classifier",
        "opponent awareness",
        "reasoning chains",
        "effective arguments",
        "logical strategies"
      ]
    },
    "publishedAt": "2025-05-28T21:03:41.000Z",
    "title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind",
    "summary": "Large language models (LLMs) have shown promising potential in persuasion,\nbut existing works on training LLM persuaders are still preliminary. Notably,\nwhile humans are skilled in modeling their opponent's thoughts and opinions\nproactively and dynamically, current LLMs struggle with such Theory of Mind\n(ToM) reasoning, resulting in limited diversity and opponent awareness. To\naddress this limitation, we introduce Theory of Mind Augmented Persuader\n(ToMAP), a novel approach for building more flexible persuader agents by\nincorporating two theory of mind modules that enhance the persuader's awareness\nand analysis of the opponent's mental state. Specifically, we begin by\nprompting the persuader to consider possible objections to the target central\nclaim, and then use a text encoder paired with a trained MLP classifier to\npredict the opponent's current stance on these counterclaims. Our carefully\ndesigned reinforcement learning schema enables the persuader learns how to\nanalyze opponent-related information and utilize it to generate more effective\narguments. Experiments show that the ToMAP persuader, while containing only 3B\nparameters, outperforms much larger baselines, like GPT-4o, with a relative\ngain of 39.4% across multiple persuadee models and diverse corpora. Notably,\nToMAP exhibits complex reasoning chains and reduced repetition during training,\nwhich leads to more diverse and effective arguments. The opponent-aware feature\nof ToMAP also makes it suitable for long conversations and enables it to employ\nmore logical and opponent-aware strategies. These results underscore our\nmethod's effectiveness and highlight its potential for developing more\npersuasive language agents. Code is available at:\nhttps://github.com/ulab-uiuc/ToMAP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22961.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d188a4aa309d842e438ef1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d188a4aa309d842e438ef1/t5zmBIytp-xnMoHUfYhEa.jpeg",
      "fullname": "Zijia Liu",
      "name": "m-serious",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20755",
      "authors": [
        {
          "_id": "6836a4401314d4ac39a526f3",
          "user": {
            "_id": "6838043c11b8b14a21f6ecd8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kzuNcUqFpRbM4rlHEfbrx.png",
            "isPro": false,
            "fullname": "Yifei Wang",
            "user": "smallAI",
            "type": "user"
          },
          "name": "Yifei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:43:51.221Z",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f4",
          "name": "Weimin Bai",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f5",
          "name": "Colin Zhang",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f6",
          "name": "Debing Zhang",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f7",
          "name": "Weijian Luo",
          "hidden": false
        },
        {
          "_id": "6836a4401314d4ac39a526f8",
          "name": "He Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6838043c11b8b14a21f6ecd8/y_4g5XJdwhTEjDjAMTQaS.png"
      ],
      "publishedAt": "2025-05-27T05:55:45.000Z",
      "submittedOnDailyAt": "2025-05-30T02:37:16.654Z",
      "title": "Uni-Instruct: 統一ディフフェーションを通じて一歩ディフフェーションモデル\n  ディフフェーション指示",
      "submittedOnDailyBy": {
        "_id": "6838043c11b8b14a21f6ecd8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kzuNcUqFpRbM4rlHEfbrx.png",
        "isPro": false,
        "fullname": "Yifei Wang",
        "user": "smallAI",
        "type": "user"
      },
      "summary": "この論文では、Diff-Instruct、DMD、SIM、SiD、f-distillなどの10以上の既存の一ステップディフフェレンスディステライションアプローチを理論的なフレームワーク「Uni-Instruct」で統合します。Uni-Instructは私たちが提案したf-divergence家族の拡大理論に基づいて考案されました。次に、拡大したf-divergenceの計算困難性問題を克服するための重要な理論を紹介し、拡大したf-divergence家族を最小化することで一ステップディフフェレンスモデルを効果的に訓練する等価で計算可能な損失を導出しました。Uni-Instructによる新しい統合は、既存のアプローチを高レベルから理解するための新しい理論的貢献を提供し、最先端の一ステップディフフェレンス生成性能を実現します。CIFAR10生成ベンチマークでは、無条件生成では1.46、条件生成では1.38の新たなレコードのFrechet Inception Distance (FID)値を達成します。ImageNet-64×64生成ベンチマークでは、79ステップの教師ディフフェレンスを大幅に改善し、1.02の新たな最先端の一ステップ生成FIDを達成します。また、Uni-Instructは3D生成やテキストから3D生成などの広範囲のタスクにも応用可能です。3D生成においては、前の方法と比べて生成質と多様性にも少しだけ優位を示します。Uni-Instructの堅固な理論的および実験的貢献は、将来の一ステップディフフェレンスディステライションとディフフェレンスモデルの知識伝達において潜在的に役立つことを期待します。",
      "upvotes": 6,
      "discussionId": "6836a4441314d4ac39a52803",
      "ai_summary": "Uni-Instruct unifies and enhances one-step diffusion distillation methods through a novel diffusion expansion theory, achieving state-of-the-art performance in unconditional and conditional image generation and text-to-3D generation.",
      "ai_keywords": [
        "Uni-Instruct",
        "diffusion expansion theory",
        "f-divergence",
        "one-step diffusion models",
        "Frechet Inception Distance (FID)",
        "CIFAR10",
        "ImageNet-64x64",
        "text-to-3D generation",
        "SDS",
        "VSD"
      ]
    },
    "publishedAt": "2025-05-27T01:55:45.000Z",
    "title": "Uni-Instruct: One-step Diffusion Model through Unified Diffusion\n  Divergence Instruction",
    "summary": "In this paper, we unify more than 10 existing one-step diffusion distillation\napproaches, such as Diff-Instruct, DMD, SIM, SiD, f-distill, etc, inside a\ntheory-driven framework which we name the \\emph{Uni-Instruct}.\nUni-Instruct is motivated by our proposed diffusion expansion theory of the\nf-divergence family. Then we introduce key theories that overcome the\nintractability issue of the original expanded f-divergence, resulting in an\nequivalent yet tractable loss that effectively trains one-step diffusion models\nby minimizing the expanded f-divergence family. The novel unification\nintroduced by Uni-Instruct not only offers new theoretical contributions that\nhelp understand existing approaches from a high-level perspective but also\nleads to state-of-the-art one-step diffusion generation performances. On the\nCIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet\nInception Distance (FID) values of \\emph{1.46} for unconditional\ngeneration and \\emph{1.38} for conditional generation. On the\nImageNet-64times 64 generation benchmark, Uni-Instruct achieves a new SoTA\none-step generation FID of \\emph{1.02}, which outperforms its 79-step\nteacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35).\nWe also apply Uni-Instruct on broader tasks like text-to-3D generation. For\ntext-to-3D generation, Uni-Instruct gives decent results, which slightly\noutperforms previous methods, such as SDS and VSD, in terms of both generation\nquality and diversity. Both the solid theoretical and empirical contributions\nof Uni-Instruct will potentially help future studies on one-step diffusion\ndistillation and knowledge transferring of diffusion models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6838043c11b8b14a21f6ecd8/y_4g5XJdwhTEjDjAMTQaS.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20755.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6838043c11b8b14a21f6ecd8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kzuNcUqFpRbM4rlHEfbrx.png",
      "fullname": "Yifei Wang",
      "name": "smallAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23742",
      "authors": [
        {
          "_id": "683917d0ecf59de6ef345e76",
          "name": "Yufan Deng",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e77",
          "name": "Xun Guo",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e78",
          "name": "Yuanyang Yin",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e79",
          "name": "Jacob Zhiyuan Fang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7a",
          "name": "Yiding Yang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7b",
          "name": "Yizhi Wang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7c",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:49.399Z",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7d",
          "name": "Angtian Wang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7e",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e7f",
          "name": "Haibin Huang",
          "hidden": false
        },
        {
          "_id": "683917d0ecf59de6ef345e80",
          "name": "Chongyang Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:58:15.000Z",
      "submittedOnDailyAt": "2025-05-30T00:58:48.386Z",
      "title": "MAGREF: マスク付きガイドフレームでの任意の参照のビデオ生成",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "ビデオ生成は、深層生成モデルの発展と特にディフュージョンベースのアプローチの出現により大幅に進展しています。しかし、複数の参照主題を基にしたビデオ生成は、多主題の一致性の維持と高い生成質の確保に関しては、大きな課題を見つけています。本論文では、多様な参照画像と文字提示に基づいたコホエントな多主題ビデオ合成を可能にするために、マスクされたガイドを挟むユニットフレームワークMAGREFを提案します。特に、(1) 領域に関する動的なマスク機構を提案し、モデルが人間、物体、背景などの多様な主題の推論を柔軟に対応できるように、構造的な変更を必要としません。また、(2) ピクセルごとのチャネル結合機構を提案し、チャネルの次元上でよりフィラーチャー特徴を保存することを効果的に行います。我々のモデルは、単一主題の訓練から複雑な多主題のシナリオへの拡張で最先端のビデオ生成質を提供し、現在の開放ソースや商業的なベースラインを超えます。評価のために、また複数主題のビデオベンチマークを紹介します。拡散的な実験は、我々のアプローチの効果性を示し、スケーラブル、制御可能で高品質な複数主題ビデオ合成のための道が開かれます。コードとモデルは以下のURLから見つかります: https://github.com/MAGREF-Video/MAGREF",
      "upvotes": 5,
      "discussionId": "683917d2ecf59de6ef345efd",
      "projectPage": "https://magref-video.github.io/magref.github.io/",
      "githubRepo": "https://github.com/MAGREF-Video/MAGREF",
      "ai_summary": "MAGREF is a unified framework for video generation that uses masked guidance and dynamic masking for coherent multi-subject synthesis from diverse references and text prompts.",
      "ai_keywords": [
        "diffusion-based approaches",
        "multi-subject consistency",
        "unified framework",
        "masked guidance",
        "region-aware dynamic masking",
        "pixel-wise channel concatenation",
        "appearance features",
        "multi-subject video benchmark",
        "scalable",
        "controllable",
        "high-fidelity video synthesis"
      ]
    },
    "publishedAt": "2025-05-29T13:58:15.000Z",
    "title": "MAGREF: Masked Guidance for Any-Reference Video Generation",
    "summary": "Video generation has made substantial strides with the emergence of deep\ngenerative models, especially diffusion-based approaches. However, video\ngeneration based on multiple reference subjects still faces significant\nchallenges in maintaining multi-subject consistency and ensuring high\ngeneration quality. In this paper, we propose MAGREF, a unified framework for\nany-reference video generation that introduces masked guidance to enable\ncoherent multi-subject video synthesis conditioned on diverse reference images\nand a textual prompt. Specifically, we propose (1) a region-aware dynamic\nmasking mechanism that enables a single model to flexibly handle various\nsubject inference, including humans, objects, and backgrounds, without\narchitectural changes, and (2) a pixel-wise channel concatenation mechanism\nthat operates on the channel dimension to better preserve appearance features.\nOur model delivers state-of-the-art video generation quality, generalizing from\nsingle-subject training to complex multi-subject scenarios with coherent\nsynthesis and precise control over individual subjects, outperforming existing\nopen-source and commercial baselines. To facilitate evaluation, we also\nintroduce a comprehensive multi-subject video benchmark. Extensive experiments\ndemonstrate the effectiveness of our approach, paving the way for scalable,\ncontrollable, and high-fidelity multi-subject video synthesis. Code and model\ncan be found at: https://github.com/MAGREF-Video/MAGREF",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23742.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 55
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23754",
      "authors": [
        {
          "_id": "68391c6dc4405ad056a9ff79",
          "name": "Ziyin Zhang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7a",
          "name": "Jiahao Xu",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7b",
          "name": "Zhiwei He",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7c",
          "name": "Tian Liang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7d",
          "name": "Qiuzhi Liu",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7e",
          "name": "Yansi Li",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff7f",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff80",
          "name": "Zhengwen Liang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff81",
          "name": "Zhuosheng Zhang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff82",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff83",
          "name": "Zhaopeng Tu",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff84",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "68391c6dc4405ad056a9ff85",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:39.000Z",
      "submittedOnDailyAt": "2025-05-30T01:18:32.068Z",
      "title": "DeepTheorem: 自然言語と強化学習を通じた定理証明のLLM推論の進歩",
      "submittedOnDailyBy": {
        "_id": "660399710f1fc2f16de18072",
        "avatarUrl": "/avatars/c22a749cc45db693c2d9ea877c7cace4.svg",
        "isPro": false,
        "fullname": "Jiahao Xu",
        "user": "Jiahao004",
        "type": "user"
      },
      "summary": "定理証明は、大規模言語モデル（LLMs）の複雑な推理能力を評価する重要なテストボックスとして役立つ。しかし、伝統的な自動化定理証明（ATP）アプローチは、LLMsが予練期間に自然語言から得られる非正式的な知識に基づく強さとは、形式的な証明システムとは密接に一致しない。本稿では、自然語言を活用してLLMsの数理論理能力を向上させるための厳密な非正式的な定理証明フレームワーク「DeepTheorem」を提案します。DeepTheoremは、121K件の高品質のIMOレベルの非正式的な定理と証明を含む大規模ベンチマークデータセットを構築し、正確性、難易度、トピックカテゴリーに厳密に注釈されており、可証明性のある定理の変体もシステマチックに構築されています。新しい強化学習戦略（RL-Zero）を提案し、可証明性のある定理の変体を活用して強固な数理推論を励まします。また、証明的正確性と推理ステップの質を評価する詳細な結果とプロセス評価指標を提案します。拡大的な実験分析により、DeepTheoremは既存のデータセットと監督学習のプロトコルと比較して、状態の最先端の精度と推理質を達成します。我々の発見は、DeepTheoremが自動化された非正式的な定理証明と数理探索に基盤的な進歩をもたらすことを明らかにします。",
      "upvotes": 4,
      "discussionId": "68391c6ec4405ad056a9ffb2",
      "ai_summary": "DeepTheorem enhances LLM theorem-proving through a large-scale natural language dataset and a tailored reinforcement learning strategy, achieving state-of-the-art results in informal theorem proving.",
      "ai_keywords": [
        "DeepTheorem",
        "automated theorem proving",
        "ATP",
        "natural language",
        "informal theorem-proving",
        "large-scale benchmark dataset",
        "IMO-level theorems",
        "reinforcement learning",
        "RL-Zero",
        "verified theorem variants",
        "proof correctness",
        "reasoning quality"
      ]
    },
    "publishedAt": "2025-05-29T13:59:39.000Z",
    "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement Learning",
    "summary": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23754.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "660399710f1fc2f16de18072",
      "avatarUrl": "/avatars/c22a749cc45db693c2d9ea877c7cace4.svg",
      "fullname": "Jiahao Xu",
      "name": "Jiahao004",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23387",
      "authors": [
        {
          "_id": "68397697a14020a996c30112",
          "name": "Mingzhe Du",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30113",
          "name": "Luu Tuan Tuan",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30114",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30115",
          "name": "Yuhao Qing",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30116",
          "name": "Dong Huang",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30117",
          "name": "Xinyi He",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30118",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c30119",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "68397697a14020a996c3011a",
          "name": "See-kiong Ng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T12:14:29.000Z",
      "submittedOnDailyAt": "2025-05-30T07:47:46.606Z",
      "title": "Afterburner: 強化学習による自動改善のコードの効率化アプローチ",
      "submittedOnDailyBy": {
        "_id": "61711f02e0b1ddb56eb9b526",
        "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
        "isPro": true,
        "fullname": "Mingzhe Du",
        "user": "Elfsong",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は機能的に正しい解決策を生成するが、コードの効率性に欠陥を見せ、実世界的な機械学習の実装において重要なバックロックとなる。本論文では、この問題を解決するために、実行サンドボックスからの実証的な実行性能のフィードバックに基づいた繰り返し最適化フレームワークを導入します。LLMsはこのフレームワークで、実行サンドボックスからの実証的な実行性能のフィードバックに基づいてコードを繰り返しに修正します。私たちは、3つの訓練戦略を検討します：Supervised Fine-Tuning（SFT）、Direct Preference Optimization（DPO）、Group Relative Policy Optimization（GRPO）。私たちのVenusデータセットとAPPSベンチマークにおける実験結果から、SFTとDPOは効率性の向上には急速にサタチします。対比的に、GRPOは強化学習（RL）と実行フィードバックを用いて、コードの効率性を継続的に最適化し、pass@1（47%から62%）と効率性において人間の提出よりも優れた結果を得る確率（31%から45%）を大幅に向上させます。私たちの研究は、テスト時のコードの効率性向上を効果的に実現し、特にLLMsをコードの効率性について自分で改善することを学ぶことの可能性を批判性に明らかにします。",
      "upvotes": 4,
      "discussionId": "68397698a14020a996c30176",
      "ai_summary": "A novel test-time iterative optimization framework using reinforcement learning continuously enhances code efficiency generated by large language models.",
      "ai_keywords": [
        "Large Language Models",
        "iterative optimization",
        "closed-loop system",
        "empirical performance feedback",
        "Supervised Fine-Tuning",
        "Direct Preference Optimization",
        "Group Relative Policy Optimization",
        "reinforcement learning",
        "pass@1",
        "Venus dataset",
        "APPS benchmark"
      ]
    },
    "publishedAt": "2025-05-29T08:14:29.000Z",
    "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code\n  Efficiency Optimization",
    "summary": "Large Language Models (LLMs) generate functionally correct solutions but\noften fall short in code efficiency, a critical bottleneck for real-world\ndeployment. In this paper, we introduce a novel test-time iterative\noptimization framework to address this, employing a closed-loop system where\nLLMs iteratively refine code based on empirical performance feedback from an\nexecution sandbox. We explore three training strategies: Supervised Fine-Tuning\n(SFT), Direct Preference Optimization (DPO), and Group Relative Policy\nOptimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark\nshow that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO,\nusing reinforcement learning (RL) with execution feedback, continuously\noptimizes code performance, significantly boosting both pass@1 (from 47% to\n62%) and the likelihood of outperforming human submissions in efficiency (from\n31% to 45%). Our work demonstrates effective test-time code efficiency\nimprovement and critically reveals the power of RL in teaching LLMs to truly\nself-improve code efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61711f02e0b1ddb56eb9b526",
      "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
      "fullname": "Mingzhe Du",
      "name": "Elfsong",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21114",
      "authors": [
        {
          "_id": "68366f4410fa22cd420ae295",
          "user": {
            "_id": "66615c855fd9d736e670e0a9",
            "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
            "isPro": false,
            "fullname": "wangshuai",
            "user": "wangsssssss",
            "type": "user"
          },
          "name": "Shuai Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T08:58:12.608Z",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae296",
          "name": "Zexian Li",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae297",
          "name": "Qipeng zhang",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae298",
          "user": {
            "_id": "649e7693a83143427691769c",
            "avatarUrl": "/avatars/d04f7b3d417423abaa053375212da21f.svg",
            "isPro": false,
            "fullname": "Tianhui Song",
            "user": "sthuihui",
            "type": "user"
          },
          "name": "Tianhui Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T08:58:10.255Z",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae299",
          "name": "Xubin Li",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae29a",
          "name": "Tiezheng Ge",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae29b",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "68366f4410fa22cd420ae29c",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T12:33:43.000Z",
      "submittedOnDailyAt": "2025-05-30T05:34:57.709Z",
      "title": "微分可能解法検索による高速なディフュージョンサンプリング",
      "submittedOnDailyBy": {
        "_id": "66615c855fd9d736e670e0a9",
        "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
        "isPro": false,
        "fullname": "wangshuai",
        "user": "wangsssssss",
        "type": "user"
      },
      "summary": "Diffusionモデルは、偉大的な生成質量を示しますが、その代わりに数多くの関数評価を必要としています。最近、逆ディフュージョン解法の計算負担を大幅に減らすために、進歩したODEベースのソルバーが開発されました。しかし、これらのソルバーは、Adamsみたいな多ステップ法をモデルとして、tに関係するLagrangeインタープライザーを単に依存しています。我々は、tに関係するLagrangeインタープライザーが、ディフュージョンモデルにとって最適ではありません、そして時間ステップとソルバー係数からなる狭い探索スペースを示します。我々の分析に基づき、新しい微分可能なソルバー探索アルゴリズムを提案し、より最適なソルバーを特定します。探索されたソルバーを採用した、例えばSiT-XL/2とFlowDCN-XL/2のrectified-flowモデルは、ImageNet256上で10ステップでそれぞれFIDスコア2.40と2.35を達成します。一方、DDPMモデルのDiT-XL/2は、10ステップでFIDスコア2.33を達成します。特に、我々が探索したソルバーは、伝統的なソルバーより大幅に優位を示しています。また、我々が探索したソルバーは、様々なモデル構造、解像度、モデルサイズにも一般的性を示しています。",
      "upvotes": 4,
      "discussionId": "68366f4a10fa22cd420ae43f",
      "githubRepo": "https://github.com/MCG-NJU/NeuralSolver",
      "ai_summary": "Researchers propose a novel differentiable solver search algorithm that optimizes the computational efficiency and quality of diffusion models for image generation tasks.",
      "ai_keywords": [
        "diffusion models",
        "ODE-based solvers",
        "Adams-like multistep methods",
        "t-related Lagrange interpolation",
        "differentiable solver search algorithm",
        "rectified-flow models",
        "SiT-XL/2",
        "FlowDCN-XL/2",
        "DDPM",
        "DiT-XL/2",
        "FID scores",
        "ImageNet256",
        "computational efficiency",
        "generality"
      ]
    },
    "publishedAt": "2025-05-27T08:33:43.000Z",
    "title": "Differentiable Solver Search for Fast Diffusion Sampling",
    "summary": "Diffusion models have demonstrated remarkable generation quality but at the\ncost of numerous function evaluations. Recently, advanced ODE-based solvers\nhave been developed to mitigate the substantial computational demands of\nreverse-diffusion solving under limited sampling steps. However, these solvers,\nheavily inspired by Adams-like multistep methods, rely solely on t-related\nLagrange interpolation. We show that t-related Lagrange interpolation is\nsuboptimal for diffusion model and reveal a compact search space comprised of\ntime steps and solver coefficients. Building on our analysis, we propose a\nnovel differentiable solver search algorithm to identify more optimal solver.\nEquipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and\nFlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256\nwith only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of\n2.33 with only 10 steps. Notably, our searched solver outperforms traditional\nsolvers by a significant margin. Moreover, our searched solver demonstrates\ngenerality across various model architectures, resolutions, and model sizes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21114.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66615c855fd9d736e670e0a9",
      "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
      "fullname": "wangshuai",
      "name": "wangsssssss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17818",
      "authors": [
        {
          "_id": "6837b52b1233747046cfa5df",
          "name": "Daeun Kyung",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e0",
          "name": "Hyunseung Chung",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e1",
          "name": "Seongsu Bae",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e2",
          "name": "Jiho Kim",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e3",
          "name": "Jae Ho Sohn",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e4",
          "name": "Taerim Kim",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e5",
          "name": "Soo Kyung Kim",
          "hidden": false
        },
        {
          "_id": "6837b52b1233747046cfa5e6",
          "name": "Edward Choi",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645cd00f5ebf379fd6d7a4c1/LP4mF9N1AGEXPzwV9jmbE.png"
      ],
      "publishedAt": "2025-05-23T12:34:48.000Z",
      "submittedOnDailyAt": "2025-05-30T01:25:38.142Z",
      "title": "PatientSim: プロフェッショナル駆動シミュレーターでは、リアルプレイの医師と患者のインタラクションを実現する。",
      "submittedOnDailyBy": {
        "_id": "645cd00f5ebf379fd6d7a4c1",
        "avatarUrl": "/avatars/be8375ed32ac234919e26a2450bf9d38.svg",
        "isPro": false,
        "fullname": "Daeun Kyung",
        "user": "dek924",
        "type": "user"
      },
      "summary": "医療師患者会話は、多ターンでコンテキストに関わったコミュニケーションであり、多様な患者ポセットに合わせたものです。このような設定で医療師LLMの訓練または評価を行うには、実際的な患者インタラクションシステムが必要です。しかし、現存するシミュレータは、臨床実践で見られる全範囲のポセットを反映していません。これに対して、我々はPatientSimを紹介します。PatientSimは、臨床スケーナーに基づく現実的で多様な患者ポセットを生成し、医学知識に基づいて動作します。PatientSimは以下の2つの方法を用いて動作します：1) 臨床プロフィール、症状と病歴を含む、MIMIC-EDとMIMIC-IVデータセットから得られる実世界的データから得られるものです。2) 性格、言語能力、病歴の記憶レベル、認知混淆レベルの4つの軸によって定義されたポセットで、37種類の独自の組み合わせを結果とします。我々は8つのLLMの事実的な精度とポセットの一致性について評価しました。最も優れた開放ソースモデルであるLlama 3.3は、4名の医師によってフレームワークの強固性を確認されました。PatientSimは、開放ソースでカスタマイズ可能なプラットフォームであり、特定の訓練の必要性に合わせてカスタマイズ可能です。プライバシーに合わせた環境を提供し、多様な患者の表現を通じて医療専用ダイアロジーシステムの評価のための強固なテストベンダーとして役立ち、健康ケアの教育ツールとしても望ましい成果を示しています。",
      "upvotes": 4,
      "discussionId": "6837b52c1233747046cfa614",
      "ai_summary": "PatientSim generates diverse and realistic patient personas using clinical data to evaluate LLMs in medical dialogue settings.",
      "ai_keywords": [
        "clinical profiles",
        "personas",
        "personality",
        "language proficiency",
        "medical history recall level",
        "cognitive confusion level",
        "patient simulator",
        "factual accuracy",
        "persona consistency",
        "privacy-compliant"
      ]
    },
    "publishedAt": "2025-05-23T08:34:48.000Z",
    "title": "PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient\n  Interactions",
    "summary": "Doctor-patient consultations require multi-turn, context-aware communication\ntailored to diverse patient personas. Training or evaluating doctor LLMs in\nsuch settings requires realistic patient interaction systems. However, existing\nsimulators often fail to reflect the full range of personas seen in clinical\npractice. To address this, we introduce PatientSim, a patient simulator that\ngenerates realistic and diverse patient personas for clinical scenarios,\ngrounded in medical expertise. PatientSim operates using: 1) clinical profiles,\nincluding symptoms and medical history, derived from real-world data in the\nMIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:\npersonality, language proficiency, medical history recall level, and cognitive\nconfusion level, resulting in 37 unique combinations. We evaluated eight LLMs\nfor factual accuracy and persona consistency. The top-performing open-source\nmodel, Llama 3.3, was validated by four clinicians to confirm the robustness of\nour framework. As an open-source, customizable platform, PatientSim provides a\nreproducible and scalable solution that can be customized for specific training\nneeds. Offering a privacy-compliant environment, it serves as a robust testbed\nfor evaluating medical dialogue systems across diverse patient presentations\nand shows promise as an educational tool for healthcare.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645cd00f5ebf379fd6d7a4c1/LP4mF9N1AGEXPzwV9jmbE.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17818.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645cd00f5ebf379fd6d7a4c1",
      "avatarUrl": "/avatars/be8375ed32ac234919e26a2450bf9d38.svg",
      "fullname": "Daeun Kyung",
      "name": "dek924",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23745",
      "authors": [
        {
          "_id": "683910d50df60182c0dd5b62",
          "user": {
            "_id": "6649fb62a460da1da20f66d0",
            "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
            "isPro": false,
            "fullname": "Hao Dong",
            "user": "hdong51",
            "type": "user"
          },
          "name": "Hao Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:56.719Z",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b63",
          "name": "Moru Liu",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b64",
          "name": "Jian Liang",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b65",
          "name": "Eleni Chatzi",
          "hidden": false
        },
        {
          "_id": "683910d50df60182c0dd5b66",
          "name": "Olga Fink",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:01.000Z",
      "submittedOnDailyAt": "2025-05-30T00:32:10.298Z",
      "title": "信頼するかどうか、視覚言語モデルの予測を信頼するかどうか",
      "submittedOnDailyBy": {
        "_id": "6649fb62a460da1da20f66d0",
        "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
        "isPro": false,
        "fullname": "Hao Dong",
        "user": "hdong51",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs)は、視覚と文字のモデルを正確に対応させる強力な能力を示し、多様的なアプリケーションにおいて広く利用できる。それにも関わらず、ゼロショットとトレンズ学習の場合でも、VLMsは間違った分類をやりやすい、自信があるが間違った予測を出すことがある。この制限は、間違った予測が厳しい結果を招く安全的な領域では、重大なリスクを見出す。この研究では、TrustVLMというトレーニング不要のフレームワークを紹介し、VLMの予測が信頼できるかどうかを評価する重要な挑戦に向けて解決策を提案する。観測されたモデル間の間違いと、特定の概念が画像埋め込み空間で明確に表現されることに基づいて、この空間を利用して新しい信頼度スコア関数を提案する。17種類の多様なデータセット、4つのアーキテクチャと2つのVLMsを使用して、厳密な評価を行い、現在の基準と比較してAURCで51.87%、AUROCで9.14%、FPR95で32.42%の改善を示し、最先端の性能を収めた。トレーニングを不要にしてモデルの信頼性を向上させることで、TrustVLMはVLMの実世界のアプリケーションでの安全な扱いのための道が開かれた。コードは、https://github.com/EPFL-IMOS/TrustVLMから利用できる。",
      "upvotes": 3,
      "discussionId": "683910d70df60182c0dd5bd3",
      "githubRepo": "https://github.com/EPFL-IMOS/TrustVLM",
      "ai_summary": "TrustVLM enhances the reliability of Vision-Language Models by estimating prediction trustworthiness without retraining, improving misclassification detection in multimodal tasks.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "modality gap",
        "image embedding space",
        "confidence-scoring function",
        "AURC",
        "AUROC",
        "FPR95"
      ]
    },
    "publishedAt": "2025-05-29T13:59:01.000Z",
    "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
    "summary": "Vision-Language Models (VLMs) have demonstrated strong capabilities in\naligning visual and textual modalities, enabling a wide range of applications\nin multimodal understanding and generation. While they excel in zero-shot and\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\noften yielding confident yet incorrect predictions. This limitation poses a\nsignificant risk in safety-critical domains, where erroneous predictions can\nlead to severe consequences. In this work, we introduce TrustVLM, a\ntraining-free framework designed to address the critical challenge of\nestimating when VLM's predictions can be trusted. Motivated by the observed\nmodality gap in VLMs and the insight that certain concepts are more distinctly\nrepresented in the image embedding space, we propose a novel confidence-scoring\nfunction that leverages this space to improve misclassification detection. We\nrigorously evaluate our approach across 17 diverse datasets, employing 4\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\ncompared to existing baselines. By improving the reliability of the model\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\nVLMs in real-world applications. The code will be available at\nhttps://github.com/EPFL-IMOS/TrustVLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6649fb62a460da1da20f66d0",
      "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
      "fullname": "Hao Dong",
      "name": "hdong51",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23253",
      "authors": [
        {
          "_id": "6839325ddbf608133c740556",
          "name": "Yixun Liang",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c740557",
          "name": "Kunming Luo",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c740558",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c740559",
          "name": "Rui Chen",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c74055a",
          "name": "Hongyu Yan",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c74055b",
          "name": "Weiyu Li",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c74055c",
          "name": "Jiarui Liu",
          "hidden": false
        },
        {
          "_id": "6839325ddbf608133c74055d",
          "name": "Ping Tan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647d8f65becb41a272907e7a/xUWTQ8gs16XOP8nrS-KNI.jpeg"
      ],
      "publishedAt": "2025-05-29T08:58:41.000Z",
      "submittedOnDailyAt": "2025-05-30T03:02:50.614Z",
      "title": "UniTEX: 3D形状の普遍的な高品質生成テクスチャー",
      "submittedOnDailyBy": {
        "_id": "647d8f65becb41a272907e7a",
        "avatarUrl": "/avatars/f891a1986a9361b23b7c83c23031cf26.svg",
        "isPro": false,
        "fullname": "yixunliang",
        "user": "lyxun",
        "type": "user"
      },
      "summary": "ユニテックス（UniTEX）は、新しい2段階3次元テクスチャ生成フレームワークです。これは、3次元アセットに高品質的で一貫したテクスチャを生成するために使用されます。現在のアプローチは、再投影された生成された多点像を3次元形状にかけるUV基のインパインティングによりテクスチャを精調化しますが、これはトポロジカルな不明確性に関する課題を引き起こします。これを解決するために、UVマッピングの制限を回避するために、統一的な3次元関数空間で処理することを提案します。特に、まずは、テクスチャ生成を3次元空間にリストアーを行うテクスチャ関数（TF）を提案します。これは、テクスチャ値をマッピングするために、ショートプロックに基づく、連続的な体積的な表現です。そして、これらのTFを画像とジオメトリー入力から直接予測するために、Transformerベースの大規模テクスチャモデル（LTM）を使用します。また、テクスチャの品質を進めることと強力な2次元プロイダーを活用するために、DiTsの大規模ディフュージョントランジャー（DiTs）の効率的な適応を行うためのアドバンスドなLoRAベースの戦略を開発します。これは、最初のステップで高品質の多点像テクスチャ合成を行います。実験は、UniTEXが現在のアプローチに比べて上品な視覚的な品質とテクスチャの整合性を実現し、自動化された3次元テクスチャ生成の一般化可能でスケーラブルな解決策を提供していることを示しています。コードは以下のURLで利用可能です：https://github.com/YixunLiang/UniTEX。",
      "upvotes": 3,
      "discussionId": "6839325fdbf608133c7405dc",
      "githubRepo": "https://github.com/YixunLiang/UniTEX",
      "ai_summary": "UniTEX generates high-quality, consistent 3D textures by using Texture Functions and adapting Diffusion Transformers directly from images and geometry without UV mapping.",
      "ai_keywords": [
        "Texture Functions",
        "transformer-based Large Texturing Model",
        "LoRA",
        "Diffusion Transformers"
      ]
    },
    "publishedAt": "2025-05-29T04:58:41.000Z",
    "title": "UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes",
    "summary": "We present UniTEX, a novel two-stage 3D texture generation framework to\ncreate high-quality, consistent textures for 3D assets. Existing approaches\npredominantly rely on UV-based inpainting to refine textures after reprojecting\nthe generated multi-view images onto the 3D shapes, which introduces challenges\nrelated to topological ambiguity. To address this, we propose to bypass the\nlimitations of UV mapping by operating directly in a unified 3D functional\nspace. Specifically, we first propose that lifts texture generation into 3D\nspace via Texture Functions (TFs)--a continuous, volumetric representation that\nmaps any 3D point to a texture value based solely on surface proximity,\nindependent of mesh topology. Then, we propose to predict these TFs directly\nfrom images and geometry inputs using a transformer-based Large Texturing Model\n(LTM). To further enhance texture quality and leverage powerful 2D priors, we\ndevelop an advanced LoRA-based strategy for efficiently adapting large-scale\nDiffusion Transformers (DiTs) for high-quality multi-view texture synthesis as\nour first stage. Extensive experiments demonstrate that UniTEX achieves\nsuperior visual quality and texture integrity compared to existing approaches,\noffering a generalizable and scalable solution for automated 3D texture\ngeneration. Code will available in: https://github.com/YixunLiang/UniTEX.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647d8f65becb41a272907e7a/xUWTQ8gs16XOP8nrS-KNI.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d8f65becb41a272907e7a",
      "avatarUrl": "/avatars/f891a1986a9361b23b7c83c23031cf26.svg",
      "fullname": "yixunliang",
      "name": "lyxun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18087",
      "authors": [
        {
          "_id": "68351b91037632c04211688d",
          "user": {
            "_id": "6837b9a63ed37b18326c7fff",
            "avatarUrl": "/avatars/7aaf5e1ff783aad45946c937ac36bdd8.svg",
            "isPro": false,
            "fullname": "Hyungyung Lee",
            "user": "ttumyche",
            "type": "user"
          },
          "name": "Hyungyung Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:44:18.221Z",
          "hidden": false
        },
        {
          "_id": "68351b91037632c04211688e",
          "name": "Geon Choi",
          "hidden": false
        },
        {
          "_id": "68351b91037632c04211688f",
          "name": "Jung-Oh Lee",
          "hidden": false
        },
        {
          "_id": "68351b91037632c042116890",
          "name": "Hangyul Yoon",
          "hidden": false
        },
        {
          "_id": "68351b91037632c042116891",
          "name": "Hyuk Gi Hong",
          "hidden": false
        },
        {
          "_id": "68351b91037632c042116892",
          "name": "Edward Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T16:44:21.000Z",
      "submittedOnDailyAt": "2025-05-30T00:18:09.180Z",
      "title": "CXReasonBench: 胸部X射线の構造的診断理由評価用ベンチマーク",
      "submittedOnDailyBy": {
        "_id": "6837b9a63ed37b18326c7fff",
        "avatarUrl": "/avatars/7aaf5e1ff783aad45946c937ac36bdd8.svg",
        "isPro": false,
        "fullname": "Hyungyung Lee",
        "user": "ttumyche",
        "type": "user"
      },
      "summary": "最近の大規模な視覚言語モデル（LVLMs）の進展は、医療タスクにおける報告書生成や可視化問答などの有望なアプリケーションを可能にしました。しかし、現在のベンチマークは主に最終的な診断回答に焦点を当て、モデルが臨床的に意味のある推理を行うかどうかについて限られた見解を提供しています。これに対して、私たちは、公開で利用可能なMIMIC-CXR-JPGデータセットに基づいた構造化されたパイプラインとベンチマーク、CheXStructとCXReasonBenchを紹介します。CheXStructは、胸部X線写真から直接的に中間的な推理ステップを自動的に抽出します。例えば、解剖学的領域の分割、解剖学的標識の抽出、診断的測定、診断的指数の計算、臨床的閾値の適用などを行います。CXReasonBenchはこのパイプラインを利用し、モデルが臨床的に正当な推理ステップを行うかどうか、そして構造化されたガイドラインからどの程度学習しているかを評価し、診断的推理の細かいさまざまな点を明確に評価することができます。ベンチマークは12つの診断タスクと1,200つのケースにわたる18,988つのQAペアを構成し、解剖学的領域選択による視覚的な基準化や診断的測定を含む多パス、多ステージ評価をサポートします。10つの評価された最強のLVLMも、構造化された推理と一般化に難しく、抽象的な知識と解剖学的に基づいた視覚的解釈の連結を失敗します。コードは、https://github.com/ttumyche/CXReasonBenchに公開されています。",
      "upvotes": 3,
      "discussionId": "68351b97037632c0421169d0",
      "githubRepo": "https://github.com/ttumyche/CXReasonBench",
      "ai_summary": "CheXStruct and CXReasonBench evaluate Large Vision-Language Models in clinical diagnosis by assessing structured reasoning, visual grounding, and generalization using the MIMIC-CXR-JPG dataset.",
      "ai_keywords": [
        "Large Vision-Language Models",
        "LVLMs",
        "CheXStruct",
        "CXReasonBench",
        "MIMIC-CXR-JPG",
        "structured reasoning",
        "visual question answering",
        "medical tasks",
        "report generation",
        "anatomical regions",
        "diagnostic measurements",
        "diagnostic indices",
        "clinical thresholds",
        "visual grounding",
        "diagnostic reasoning",
        "multi-path",
        "multi-stage evaluation"
      ]
    },
    "publishedAt": "2025-05-23T12:44:21.000Z",
    "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic\n  Reasoning in Chest X-rays",
    "summary": "Recent progress in Large Vision-Language Models (LVLMs) has enabled promising\napplications in medical tasks, such as report generation and visual question\nanswering. However, existing benchmarks focus mainly on the final diagnostic\nanswer, offering limited insight into whether models engage in clinically\nmeaningful reasoning. To address this, we present CheXStruct and CXReasonBench,\na structured pipeline and benchmark built on the publicly available\nMIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of\nintermediate reasoning steps directly from chest X-rays, such as segmenting\nanatomical regions, deriving anatomical landmarks and diagnostic measurements,\ncomputing diagnostic indices, and applying clinical thresholds. CXReasonBench\nleverages this pipeline to evaluate whether models can perform clinically valid\nreasoning steps and to what extent they can learn from structured guidance,\nenabling fine-grained and transparent assessment of diagnostic reasoning. The\nbenchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,\neach paired with up to 4 visual inputs, and supports multi-path, multi-stage\nevaluation including visual grounding via anatomical region selection and\ndiagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with\nstructured reasoning and generalization, often failing to link abstract\nknowledge with anatomically grounded visual interpretation. The code is\navailable at https://github.com/ttumyche/CXReasonBench",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18087.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6837b9a63ed37b18326c7fff",
      "avatarUrl": "/avatars/7aaf5e1ff783aad45946c937ac36bdd8.svg",
      "fullname": "Hyungyung Lee",
      "name": "ttumyche",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14321",
      "authors": [
        {
          "_id": "68396605ac00da416d094bbf",
          "name": "Bo Feng",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc0",
          "name": "Zhengfeng Lai",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc1",
          "name": "Shiyu Li",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc2",
          "name": "Zizhen Wang",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc3",
          "name": "Simon Wang",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc4",
          "name": "Ping Huang",
          "hidden": false
        },
        {
          "_id": "68396605ac00da416d094bc5",
          "name": "Meng Cao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T13:07:55.000Z",
      "submittedOnDailyAt": "2025-05-30T06:32:58.958Z",
      "title": "ビデオLLMベンチマークを分解する：知識、空間認識、または真の時系列理解？",
      "submittedOnDailyBy": {
        "_id": "66b5295f83425904fa7a1a6a",
        "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
        "isPro": false,
        "fullname": "Zhengfeng Lai",
        "user": "jefflai",
        "type": "user"
      },
      "summary": "現在の映像理解ベンチマークは、知識ベースと純粋の画像ベースの質問を混同し、モデルの時間的論理能力を明確に区別しないため、映像理解の他のモデルとの違いの鍵となる部分を明確に示せないことが多い。我々は、ベンチマークがモデルが映像の動的な内容を理解しているかどうかを明確に判断できない2つの主な制限を識別した：1) 強い言語先入観で、モデルが映像を見なくても質問を答えることができるものと、2) シャッフル不変性で、映像のフレームが時系列順序をシャッフルされても、特定の質問においてモデルの性能が同様に保持されるもの。これらの問題を解決するために、我々はVBenchCompという自動化プインパイプラインを提案し、質問をLLM-Answerable、Semantic、Temporalという異なるドメインに分類する。特に、LLM-Answerableの質問は映像を見なくても答えられるもの、Semanticの質問はフレームがシャッフルされても答えられるもの、Temporalの質問はフレームの正しい時系列順序を理解する必要があるものである。その他の質問はOthersとラベルされる。これにより、映像LLMの異なる能力を詳細に評価することが可能になる。我々の分析は、傳統的な全体的なスコアによって隠されているモデルの軽微な弱点を明らかにし、将来のベンチマークの設計におけるヒントと推薦を提供し、映像LLMの評価を更に正確にするための指導を提供する。",
      "upvotes": 3,
      "discussionId": "68396607ac00da416d094c6c",
      "ai_summary": "VBenchComp, an automated pipeline, categorizes video LLM questions into different domains to evaluate temporal reasoning and isolate model weaknesses beyond overall scores.",
      "ai_keywords": [
        "video understanding",
        "LLM-Answerable",
        "Semantic",
        "Temporal"
      ]
    },
    "publishedAt": "2025-05-20T09:07:55.000Z",
    "title": "Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or\n  True Temporal Understanding?",
    "summary": "Existing video understanding benchmarks often conflate knowledge-based and\npurely image-based questions, rather than clearly isolating a model's temporal\nreasoning ability, which is the key aspect that distinguishes video\nunderstanding from other modalities. We identify two major limitations that\nobscure whether higher scores truly indicate stronger understanding of the\ndynamic content in videos: (1) strong language priors, where models can answer\nquestions without watching the video; and (2) shuffling invariance, where\nmodels maintain similar performance on certain questions even when video frames\nare temporally shuffled. To alleviate these issues, we propose VBenchComp, an\nautomated pipeline that categorizes questions into different domains:\nLLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions\ncan be answered without viewing the video; Semantic questions remain answerable\neven when the video frames are shuffled; and Temporal questions require\nunderstanding the correct temporal order of frames. The rest of the questions\nare labeled as Others. This can enable fine-grained evaluation of different\ncapabilities of a video LLM. Our analysis reveals nuanced model weaknesses that\nare hidden by traditional overall scores, and we offer insights and\nrecommendations for designing future benchmarks that more accurately assess\nvideo LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14321.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b5295f83425904fa7a1a6a",
      "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
      "fullname": "Zhengfeng Lai",
      "name": "jefflai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.23761",
      "authors": [
        {
          "_id": "6839107ed762b7c617b0731b",
          "user": {
            "_id": "67578bf874cf42cdedbf00df",
            "avatarUrl": "/avatars/5981300ee0025c876973f94c81a85b19.svg",
            "isPro": false,
            "fullname": "Yunjae Won",
            "user": "yunjae-won",
            "type": "user"
          },
          "name": "Yunjae Won",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:04.071Z",
          "hidden": false
        },
        {
          "_id": "6839107ed762b7c617b0731c",
          "name": "Hyunji Lee",
          "hidden": false
        },
        {
          "_id": "6839107ed762b7c617b0731d",
          "name": "Hyeonbin Hwang",
          "hidden": false
        },
        {
          "_id": "6839107ed762b7c617b0731e",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T17:59:50.000Z",
      "submittedOnDailyAt": "2025-05-30T05:38:24.883Z",
      "title": "差分情報：情報理論的視点からの好み最適化",
      "submittedOnDailyBy": {
        "_id": "67578bf874cf42cdedbf00df",
        "avatarUrl": "/avatars/5981300ee0025c876973f94c81a85b19.svg",
        "isPro": false,
        "fullname": "Yunjae Won",
        "user": "yunjae-won",
        "type": "user"
      },
      "summary": "直接偏好優化（DPO）は、観測学習の方法で言語モデルを人間の好みに対応させる標準的な手法となりました。その実験的な成功にもかかわらず、ログ比率報酬パラメータ化の理論的な説明は完全ではありません。本研究では、この欠陥を補うために、微分情報分布（DID）を利用します：これは、政策更新の際に得られる情報を捉えるトークンシーケンスの分布です。まず、偏好ラベルが参照政策を対象政策に変換するために必要な微分情報を含む場合、DPOでのログ比率報酬が好み最適化により対象政策を学習するための最適な形式として現れます。この結果は、拒否された回答に対する最適なサンプリング分布の閉じた形の表現を自然に与えます。次に、偏好が微分情報を含む条件は、単調報酬関数に関するインデクタブルバイアスと基本的に関連しています。このインデクタブルバイアスは、好み最適化により広く使用されていますが、以前に認識されていません。最後に、DIDのエントロピーを分析し、低エントロピーの微分情報が政策分布を強化し、高エントロピーの微分情報がソームフェクトを引き起こすことを説明し、ログ似然率の移動現象を解釈します。合成的な実験で理論的な発見を検証し、実世界的な指示従いデータセットに拡張しました。結果は、一般的な指示従いにおいて高エントロピーの微分情報の学習が重要であること、知識関連の問題解答において低エントロピーの微分情報の学習が有利であることを示しました。全体として、本研究は微分情報の視点からDPOの目標、偏好データの構造、その結果として現れる政策行動を一貫した観点を提供します。",
      "upvotes": 2,
      "discussionId": "6839107fd762b7c617b07385",
      "ai_summary": "Theoretical analysis of Direct Preference Optimization (DPO) reveals that log-ratio reward parameterization is optimal for learning target policy via preference optimization, linked to log-margin ordered policies, and explains policy reinforcement and smoothing based on differential information entropy.",
      "ai_keywords": [
        "Direct Preference Optimization",
        "DPO",
        "log-ratio reward parameterization",
        "Differential Information Distribution",
        "DID",
        "token sequences",
        "policy updates",
        "preference labels",
        "target policy",
        "preference optimization",
        "log-margin ordered policies",
        "entropy",
        "differential information entropy",
        "log-likelihood displacement",
        "instruction-following datasets",
        "knowledge-intensive question answering"
      ]
    },
    "publishedAt": "2025-05-29T13:59:50.000Z",
    "title": "Differential Information: An Information-Theoretic Perspective on\n  Preference Optimization",
    "summary": "Direct Preference Optimization (DPO) has become a standard technique for\naligning language models with human preferences in a supervised manner. Despite\nits empirical success, the theoretical justification behind its log-ratio\nreward parameterization remains incomplete. In this work, we address this gap\nby utilizing the Differential Information Distribution (DID): a distribution\nover token sequences that captures the information gained during policy\nupdates. First, we show that when preference labels encode the differential\ninformation required to transform a reference policy into a target policy, the\nlog-ratio reward in DPO emerges as the uniquely optimal form for learning the\ntarget policy via preference optimization. This result naturally yields a\nclosed-form expression for the optimal sampling distribution over rejected\nresponses. Second, we find that the condition for preferences to encode\ndifferential information is fundamentally linked to an implicit assumption\nregarding log-margin ordered policies-an inductive bias widely used in\npreference optimization yet previously unrecognized. Finally, by analyzing the\nentropy of the DID, we characterize how learning low-entropy differential\ninformation reinforces the policy distribution, while high-entropy differential\ninformation induces a smoothing effect, which explains the log-likelihood\ndisplacement phenomenon. We validate our theoretical findings in synthetic\nexperiments and extend them to real-world instruction-following datasets. Our\nresults suggest that learning high-entropy differential information is crucial\nfor general instruction-following, while learning low-entropy differential\ninformation benefits knowledge-intensive question answering. Overall, our work\npresents a unifying perspective on the DPO objective, the structure of\npreference data, and resulting policy behaviors through the lens of\ndifferential information.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23761.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67578bf874cf42cdedbf00df",
      "avatarUrl": "/avatars/5981300ee0025c876973f94c81a85b19.svg",
      "fullname": "Yunjae Won",
      "name": "yunjae-won",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23625",
      "authors": [
        {
          "_id": "683924d1ac00da416df936dd",
          "name": "Chao Huang",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936de",
          "name": "Yuesheng Ma",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936df",
          "name": "Junxuan Huang",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e0",
          "name": "Susan Liang",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e1",
          "name": "Yunlong Tang",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e2",
          "name": "Jing Bi",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e3",
          "name": "Wenqiang Liu",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e4",
          "name": "Nima Mesgarani",
          "hidden": false
        },
        {
          "_id": "683924d1ac00da416df936e5",
          "name": "Chenliang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T16:31:45.000Z",
      "submittedOnDailyAt": "2025-05-30T01:54:58.337Z",
      "title": "ZeroSep: 音声内のどのものをも学習せずに分離できます。",
      "submittedOnDailyBy": {
        "_id": "67257ee0938e718957c9c100",
        "avatarUrl": "/avatars/db1f792ee1a5d860ea4e98c11a016a1b.svg",
        "isPro": false,
        "fullname": "Chao Huang",
        "user": "ChaoHuangCS",
        "type": "user"
      },
      "summary": "音源分離は、機械が複雑な音響環境を理解するための基本的な技術であり、複数の音響アプリケーションの基盤となっています。現在のサブジェクト付き深層学習アプローチは強力ですが、幅広い、タスク専門的な標準化されたデータの必要性により制限され、実世界的な音響スペースの巨大な変異性と開放セットの性質に対して一般化できない問題があります。生成ファンダメンティュモデルの成功をヒントに、これらの制限を克服できるかどうかを調査しています。その結果、どんなものでもよいタスクに対しても、テキストガイドディングの音響ディフュージョンモデルでゼロショットの音源分離が可能であることを驚きのほど発見しました。この方法は、ZeroSepとして名付けられ、混合音響をディフュージョンモデルの潜在空間に逆転し、テキスト条件付きでディノイズプロセスをガイドして個々のソースを復元することで働きます。タスク専門的な訓練や微調節を必要とさせず、ZeroSepは生成ディフュージョンモデルを分類的な分離タスクに再利用し、豊富なテキストプレイアバイを通じて開放セットのシナリオを内在的にサポートします。ZeroSepは、複数のテキストガイドディングの音響ディフュージョンベースモデルとの相性があり、複数の分離ベンチマークで強力な分離性能を提供し、サブジェクト付き方法を超えることもできます。",
      "upvotes": 2,
      "discussionId": "683924d6ac00da416df937f6",
      "ai_summary": "ZeroSep, a text-guided audio diffusion model, achieves zero-shot source separation through pre-trained models and text conditioning, outperforming supervised methods on various benchmarks.",
      "ai_keywords": [
        "audio source separation",
        "supervised deep learning",
        "generative foundation models",
        "text-guided audio diffusion models",
        "zero-shot source separation",
        "latent space",
        "denoising process",
        "discriminative separation task",
        "textual priors",
        "open-set scenarios"
      ]
    },
    "publishedAt": "2025-05-29T12:31:45.000Z",
    "title": "ZeroSep: Separate Anything in Audio with Zero Training",
    "summary": "Audio source separation is fundamental for machines to understand complex\nacoustic environments and underpins numerous audio applications. Current\nsupervised deep learning approaches, while powerful, are limited by the need\nfor extensive, task-specific labeled data and struggle to generalize to the\nimmense variability and open-set nature of real-world acoustic scenes. Inspired\nby the success of generative foundation models, we investigate whether\npre-trained text-guided audio diffusion models can overcome these limitations.\nWe make a surprising discovery: zero-shot source separation can be achieved\npurely through a pre-trained text-guided audio diffusion model under the right\nconfiguration. Our method, named ZeroSep, works by inverting the mixed audio\ninto the diffusion model's latent space and then using text conditioning to\nguide the denoising process to recover individual sources. Without any\ntask-specific training or fine-tuning, ZeroSep repurposes the generative\ndiffusion model for a discriminative separation task and inherently supports\nopen-set scenarios through its rich textual priors. ZeroSep is compatible with\na variety of pre-trained text-guided audio diffusion backbones and delivers\nstrong separation performance on multiple separation benchmarks, surpassing\neven supervised methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67257ee0938e718957c9c100",
      "avatarUrl": "/avatars/db1f792ee1a5d860ea4e98c11a016a1b.svg",
      "fullname": "Chao Huang",
      "name": "ChaoHuangCS",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22918",
      "authors": [
        {
          "_id": "68393b9ef0458c8bcb652d78",
          "user": {
            "_id": "6814e98abbe5b8a5d92fc335",
            "avatarUrl": "/avatars/83c1963280bc94fdc7a2b21f6e0f1c59.svg",
            "isPro": false,
            "fullname": "Ruichen Chen",
            "user": "crc5577",
            "type": "user"
          },
          "name": "Ruichen Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T05:01:23.240Z",
          "hidden": false
        },
        {
          "_id": "68393b9ef0458c8bcb652d79",
          "user": {
            "_id": "661c391720b47b0daddfcc5a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RwvDJRtlEEPch27xId9Cb.png",
            "isPro": false,
            "fullname": "Keith G. Mills",
            "user": "kgmills",
            "type": "user"
          },
          "name": "Keith G. Mills",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T06:54:23.854Z",
          "hidden": false
        },
        {
          "_id": "68393b9ef0458c8bcb652d7a",
          "user": {
            "_id": "64ac49ccb7d86b40fd60a8dd",
            "avatarUrl": "/avatars/e9f5482cffdd1d5917523a496a3805f0.svg",
            "isPro": false,
            "fullname": "Liyao Jiang",
            "user": "LiyaoJiang",
            "type": "user"
          },
          "name": "Liyao Jiang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T05:01:59.348Z",
          "hidden": false
        },
        {
          "_id": "68393b9ef0458c8bcb652d7b",
          "name": "Chao Gao",
          "hidden": false
        },
        {
          "_id": "68393b9ef0458c8bcb652d7c",
          "name": "Di Niu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T22:39:12.000Z",
      "submittedOnDailyAt": "2025-05-30T06:23:07.403Z",
      "title": "Re-ttention: ウルトラスパース可視記憶生成を通じてアテンション統計的にリシェイプ",
      "submittedOnDailyBy": {
        "_id": "6814e98abbe5b8a5d92fc335",
        "avatarUrl": "/avatars/83c1963280bc94fdc7a2b21f6e0f1c59.svg",
        "isPro": false,
        "fullname": "Ruichen Chen",
        "user": "crc5577",
        "type": "user"
      },
      "summary": "Diffusion Transformers (DiT)は、高品質の可視内容（例：映画と画像）の生成において事実上の標準モデルとなっています。大きなバックロックとなっているのは、複雑さが解像度と映画の長さに二次的に増加するアテンション機構です。この負担を減らす一つの合理的な方法として、スパースなアテンションがありますが、現在の技術は非常に高いスパースレベルで画質を保持することができないことがあり、または計算オーバーヘッドが見込まれます。この問題に対処するために、我々はRe-ttentionを提案します。Re-ttentionは、Diffusion Modelsの時間的な冗長性を利用してアテンション機構内の確率的正規化シフトを克服し、非常に高いスパースなアテンションを実現します。具体的には、Re-ttentionは、全二次アテンションの画質を保持するために、先ほどのソフトマックス分布の歴史に基づいてアテンションスコアをリサイズします。T2V/T2Iモデルの実験結果において、CogVideoXとPixArt DiTsなどでRe-ttentionは推論時に3.1%のトークンを必要とし、FastDiTAttn、Sparse VideoGen、MInferenceなどの現代的な方法を超えます。また、我々は、H100 GPUでの端末からのラテンシーの減少を計測し、無視できるコストで45%以上の端末からのラテンシーと92%以上の自動アテンションラテンシーの減少を実現できることを示します。コードは以下のリンクで公開されています。",
      "upvotes": 2,
      "discussionId": "68393ba3f0458c8bcb652e60",
      "ai_summary": "Re-ttention uses temporal redundancy in diffusion models to enable high sparse attention in visual generation, maintaining quality with minimal computational overhead.",
      "ai_keywords": [
        "Diffusion Transformers",
        "sparse attention",
        "visual generation models",
        "probabilistic normalization shift",
        "Re-ttention",
        "attention scores",
        "softmax distribution",
        "latency reduction",
        "H100 GPU"
      ]
    },
    "publishedAt": "2025-05-28T18:39:12.000Z",
    "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical\n  Reshape",
    "summary": "Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. % To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. % Experimental\nresults on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate\nthat Re-ttention requires as few as 3.1\\% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference. Further, we measure latency to show that our method can attain over\n45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU\nat negligible overhead cost.\n  Code available online here:\nhttps://github.com/cccrrrccc/Re-ttention{https://github.com/cccrrrccc/Re-ttention}",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22918.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6814e98abbe5b8a5d92fc335",
      "avatarUrl": "/avatars/83c1963280bc94fdc7a2b21f6e0f1c59.svg",
      "fullname": "Ruichen Chen",
      "name": "crc5577",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22765",
      "authors": [
        {
          "_id": "68394fb7f1e62a44f53ecf82",
          "user": {
            "_id": "658436f5c73f74776b19198a",
            "avatarUrl": "/avatars/3f1d76af6fc0405d663c9294318fe83e.svg",
            "isPro": false,
            "fullname": "Iddo Yosha",
            "user": "iyosha",
            "type": "user"
          },
          "name": "Iddo Yosha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:52:57.738Z",
          "hidden": false
        },
        {
          "_id": "68394fb7f1e62a44f53ecf83",
          "user": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "isPro": false,
            "fullname": "Gallil Maimon",
            "user": "gallilmaimon",
            "type": "user"
          },
          "name": "Gallil Maimon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:52:41.076Z",
          "hidden": false
        },
        {
          "_id": "68394fb7f1e62a44f53ecf84",
          "name": "Yossi Adi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T18:32:56.000Z",
      "submittedOnDailyAt": "2025-05-30T05:20:21.736Z",
      "title": "ストレステスト：あなたの言語モデルはストレスを受け入れることができますか？",
      "submittedOnDailyBy": {
        "_id": "66b9bc2dacdbc1d0b39c3b50",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
        "isPro": false,
        "fullname": "Gallil Maimon",
        "user": "gallilmaimon",
        "type": "user"
      },
      "summary": "文脈重音は、話し言葉中の特定の単語に重ねられ、アイデアをヒライトまたは対比することや新しい情報を紹介するための強調を指します。これは、明記されていない潜在的な目的を意味することも多くあります。最近の話聴知識の言語モデル（SLMs）の進歩は、音声の直接処理を可能にし、モデルが読み上げを通じてしていないように、音声信号の全ての豊富さを利用して音声論理タスクへの対応を行うことができます。文脈重音が意味の形成と話者の意図に関する重要な役割を果たしていることにもかかわらず、このようなモデルの評価と開発においては主に無視されています。この研究では、このギャップを解決するために、StressTestという、文脈重音パターンに基づいて話し言葉の解釈を区別するモデルの能力を評価するために設計されたベンチマークを導入します。これらのSLMsの性能を評価し、それらの総合的な能力に比べてこのタスクにおいては低い性能を示していることを発見しました。この制限を克服するために、新しい合成データ生成パイプラインを提案し、Stress17kという、重音の変化による意味の変化をシミュレートするトレーニングセットを作成しました。そして、この合成データセットを用いてモデルを最適化することにより、実世界的な録音との一致を示し、SLMsの効果的な微調節を可能にしました。結果は、我々の微調節モデルであるStresSLMが、文脈重音の理由と検出タスクにおいて現在のモデルと比べて显著に優れていることを示します。コード、モデル、データ、音声サンプル - pages.cs.huji.ac.il/adiyoss-lab/stresstest.",
      "upvotes": 2,
      "discussionId": "68394fb8f1e62a44f53ecfa4",
      "projectPage": "https://pages.cs.huji.ac.il/adiyoss-lab/stresstest/",
      "githubRepo": "https://github.com/slp-rl/StressTest",
      "ai_summary": "A StressTest benchmark and synthetic Stress17k dataset are introduced to improve speech-aware language models' ability to interpret sentence stress in spoken language.",
      "ai_keywords": [
        "speech-aware language models",
        "spoken question answering",
        "sentence stress",
        "benchmark",
        "synthetic data generation pipeline",
        "audio reasoning",
        "sentence stress reasoning and detection tasks"
      ]
    },
    "publishedAt": "2025-05-28T14:32:56.000Z",
    "title": "StressTest: Can YOUR Speech LM Handle the Stress?",
    "summary": "Sentence stress refers to emphasis, placed on specific words within a spoken\nutterance to highlight or contrast an idea, or to introduce new information. It\nis often used to imply an underlying intention that is not explicitly stated.\nRecent advances in speech-aware language models (SLMs) have enabled direct\nprocessing of audio, allowing models to bypass transcription and access the\nfull richness of the speech signal and perform audio reasoning tasks such as\nspoken question answering. Despite the crucial role of sentence stress in\nshaping meaning and speaker intent, it remains largely overlooked in evaluation\nand development of such models. In this work, we address this gap by\nintroducing StressTest, a benchmark specifically designed to evaluate a model's\nability to distinguish between interpretations of spoken sentences based on the\nstress pattern. We assess the performance of several leading SLMs and find\nthat, despite their overall capabilities, they perform poorly on such tasks. To\novercome this limitation, we propose a novel synthetic data generation\npipeline, and create Stress17k, a training set that simulates change of meaning\nimplied by stress variation. Then, we empirically show that optimizing models\nwith this synthetic dataset aligns well with real-world recordings and enables\neffective finetuning of SLMs. Results suggest, that our finetuned model,\nStresSLM, significantly outperforms existing models on both sentence stress\nreasoning and detection tasks. Code, models, data, and audio samples -\npages.cs.huji.ac.il/adiyoss-lab/stresstest.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22765.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b9bc2dacdbc1d0b39c3b50",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
      "fullname": "Gallil Maimon",
      "name": "gallilmaimon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20282",
      "authors": [
        {
          "_id": "683889f78c8e3b72170f6412",
          "user": {
            "_id": "641ddac5be3bd3a5a06ed4a4",
            "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
            "isPro": false,
            "fullname": "zitian gao",
            "user": "zgao3186",
            "type": "user"
          },
          "name": "Zitian Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:54:47.238Z",
          "hidden": false
        },
        {
          "_id": "683889f78c8e3b72170f6413",
          "name": "Lynx Chen",
          "hidden": false
        },
        {
          "_id": "683889f78c8e3b72170f6414",
          "name": "Joey Zhou",
          "hidden": false
        },
        {
          "_id": "683889f78c8e3b72170f6415",
          "name": "Bryan Dai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T17:58:30.000Z",
      "submittedOnDailyAt": "2025-05-30T01:51:16.066Z",
      "title": "一ピックエントロピー最小化",
      "submittedOnDailyBy": {
        "_id": "641ddac5be3bd3a5a06ed4a4",
        "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
        "isPro": false,
        "fullname": "zitian gao",
        "user": "zgao3186",
        "type": "user"
      },
      "summary": "ラージュ言語モデル13,440つを訓練し、エントロピー最小化は、単一の無ラベルデータと10ステップの最適化で、ルールベースの再励励効果学習で数千のデータと謹密に設計された報酬を使用した性能向上と比較的またはより大きな向上を実現することができた。この驚異な結果は、大規模な言語モデルの訓練後パラダイムの再考に促している。コードは、https://github.com/zitian-gao/one-shot-em に公開されています。",
      "upvotes": 2,
      "discussionId": "683889f78c8e3b72170f643d",
      "projectPage": "https://www.notion.so/One-shot-Entropy-Minimization-202606db813b80639773f850f39246a5",
      "githubRepo": "https://github.com/zitian-gao/one-shot-em",
      "ai_summary": "Entropy minimization with one sample and minimal optimization achieves significant performance improvements for large language models.",
      "ai_keywords": [
        "large language models",
        "entropy minimization",
        "unlabeled data",
        "optimization",
        "rule-based reinforcement learning",
        "post-training paradigms"
      ]
    },
    "publishedAt": "2025-05-26T13:58:30.000Z",
    "title": "One-shot Entropy Minimization",
    "summary": "We trained 13,440 large language models and found that entropy minimization\nrequires only a single unlabeled data and 10 steps optimization to achieve\nperformance improvements comparable to or even greater than those obtained\nusing thousands of data and carefully designed rewards in rule-based\nreinforcement learning. This striking result may prompt a rethinking of\npost-training paradigms for large language models. Our code is avaliable at\nhttps://github.com/zitian-gao/one-shot-em.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20282.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641ddac5be3bd3a5a06ed4a4",
      "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
      "fullname": "zitian gao",
      "name": "zgao3186",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19360",
      "authors": [
        {
          "_id": "68392279896eb9ceb71fac39",
          "name": "Manan Suri",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3a",
          "user": {
            "_id": "65c16444d4c3b8dff2f0d78d",
            "avatarUrl": "/avatars/4ed764c1657bd260d2a12ba61c111062.svg",
            "isPro": false,
            "fullname": "Puneet Mathur",
            "user": "puneetm",
            "type": "user"
          },
          "name": "Puneet Mathur",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-30T03:14:02.414Z",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3b",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3c",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:28.558Z",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3d",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "68392279896eb9ceb71fac3e",
          "name": "Dinesh Manocha",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T23:17:32.000Z",
      "submittedOnDailyAt": "2025-05-30T01:44:05.816Z",
      "title": "ChartLens: チャートの細かい可視化アトリビューション",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "多モデル大語言モデル（MLLMs）の機能の向上は、グラフの理解などのタスクを進めています。しかし、これらのモデルは、生成された文章が提供された可視化データと矛盾する「ホライゼーション」（hallucinations）に苦労します。これを解決するために、私たちは「後時点可視化責任」（Post-Hoc Visual Attribution for Charts）を導入します。これは、与えられたグラフ関連のレスポンスを正当化するために、細かいグラフ要素を特定します。また、私たちは、新しいグラフ責任アルゴリズム「ChartLens」を提案します。これは、セグメンテーションベースの手法を使用してグラフオブジェクトを特定し、MLLMsを用いて細かい可視化責任を行います。また、私たちは、合成チャートと実世界的チャートからなるベンチマーク「ChartVA-Eval」を提示します。これは、財政、政策、経済などの多様な領域からのチャートを含み、細かい責任記録付けを特徴とします。我々の評価により、ChartLensは細かい責任記録付けを26-66%向上させます。",
      "upvotes": 2,
      "discussionId": "6839227a896eb9ceb71fac99",
      "ai_summary": "ChartLens enhances multimodal language models with fine-grained visual attributions, improving the accuracy of chart understanding by 26-66%.",
      "ai_keywords": [
        "multimodal large language models",
        "MLLMs",
        "chart understanding",
        "hallucinations",
        "Post-Hoc Visual Attribution",
        "ChartLens",
        "segmentation-based techniques",
        "set-of-marks prompting",
        "ChartVA-Eval",
        "synthetic charts",
        "real-world charts",
        "fine-grained attribution annotations"
      ]
    },
    "publishedAt": "2025-05-25T19:17:32.000Z",
    "title": "ChartLens: Fine-grained Visual Attribution in Charts",
    "summary": "The growing capabilities of multimodal large language models (MLLMs) have\nadvanced tasks like chart understanding. However, these models often suffer\nfrom hallucinations, where generated text sequences conflict with the provided\nvisual data. To address this, we introduce Post-Hoc Visual Attribution for\nCharts, which identifies fine-grained chart elements that validate a given\nchart-associated response. We propose ChartLens, a novel chart attribution\nalgorithm that uses segmentation-based techniques to identify chart objects and\nemploys set-of-marks prompting with MLLMs for fine-grained visual attribution.\nAdditionally, we present ChartVA-Eval, a benchmark with synthetic and\nreal-world charts from diverse domains like finance, policy, and economics,\nfeaturing fine-grained attribution annotations. Our evaluations show that\nChartLens improves fine-grained attributions by 26-66%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19360.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19286",
      "authors": [
        {
          "_id": "6839220cf85de1fc56402b3d",
          "name": "Utkarsh Sahu",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b3e",
          "name": "Zhisheng Qi",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b3f",
          "name": "Yongjia Lei",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b40",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b41",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:31.804Z",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b42",
          "name": "Nesreen K. Ahmed",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b43",
          "user": {
            "_id": "637c6d95a8716d642050b50f",
            "avatarUrl": "/avatars/0955a10113807348f24db968c7bd7c7a.svg",
            "isPro": false,
            "fullname": "Mahantesh Halappanavar",
            "user": "mhalappa",
            "type": "user"
          },
          "name": "Mahantesh M Halappanavar",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T05:25:34.615Z",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b44",
          "name": "Yao Ma",
          "hidden": false
        },
        {
          "_id": "6839220cf85de1fc56402b45",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T19:34:15.000Z",
      "submittedOnDailyAt": "2025-05-30T01:42:17.692Z",
      "title": "グラフの視点からの知識の構造的パターンの検証",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "大語言モデルは、知識アクセス、編集可能性、推理、説明性において広く研究されていますが、知識の構造的パターンに焦点を当てて研究されているものは少ないです。この空白によって動機され、私たちはグラフの視点からこれらの構造的パターンを調査します。私たちは、LLMの知識をトリプレットレベルとエンティティレベルで定量化し、ノードの次数などのグラフ構造的性質との関係を分析します。さらに、知識ホモピリーを発見します。トポロジカルに近いエンティティは類似した知識レベルを示します。これにより、私たちはエンティティの知識をローカルの隣接ノードに基づいて推定するグラフ機械学習モデルを開発します。このモデルは、LLMにより知られていないトリプレットを選択して有價な知識検証を行うことができます。実験結果から、選択されたトリプレットを用いた微調節は、上位の性能を示します。",
      "upvotes": 2,
      "discussionId": "6839220ef85de1fc56402ba7",
      "ai_summary": "The study explores the structural patterns of knowledge in large language models from a graph perspective, uncovering knowledge homophily and developing models for graph machine learning to estimate entity knowledge.",
      "ai_keywords": [
        "large language models",
        "graph perspective",
        "triplet",
        "entity",
        "node degree",
        "knowledge homophily",
        "graph machine learning",
        "knowledge checking",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-05-25T15:34:15.000Z",
    "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large\n  Language Models",
    "summary": "Large language models have been extensively studied as neural knowledge bases\nfor their knowledge access, editability, reasoning, and explainability.\nHowever, few works focus on the structural patterns of their knowledge.\nMotivated by this gap, we investigate these structural patterns from a graph\nperspective. We quantify the knowledge of LLMs at both the triplet and entity\nlevels, and analyze how it relates to graph structural properties such as node\ndegree. Furthermore, we uncover the knowledge homophily, where topologically\nclose entities exhibit similar levels of knowledgeability, which further\nmotivates us to develop graph machine learning models to estimate entity\nknowledge based on its local neighbors. This model further enables valuable\nknowledge checking by selecting triplets less known to LLMs. Empirical results\nshow that using selected triplets for fine-tuning leads to superior\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19286.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23183",
      "authors": [
        {
          "_id": "683962e4cba8ce4f5ebced9c",
          "user": {
            "_id": "5e7749883d77a72421292d07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
            "isPro": false,
            "fullname": "Gabriele Sarti",
            "user": "gsarti",
            "type": "user"
          },
          "name": "Gabriele Sarti",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-30T07:54:16.127Z",
          "hidden": false
        },
        {
          "_id": "683962e4cba8ce4f5ebced9d",
          "name": "Vilém Zouhar",
          "hidden": false
        },
        {
          "_id": "683962e4cba8ce4f5ebced9e",
          "name": "Malvina Nissim",
          "hidden": false
        },
        {
          "_id": "683962e4cba8ce4f5ebced9f",
          "name": "Arianna Bisazza",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/1j5bJLYubeP6s5cGuTNzs.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/I2PXFiD8jN4odhmi5vrG6.png"
      ],
      "publishedAt": "2025-05-29T07:20:36.000Z",
      "submittedOnDailyAt": "2025-05-30T06:23:03.812Z",
      "title": "無サブバイアンスの単語レベルの質量評価ツールの機械翻訳を通じて (Dis)agreementのラベラーたちからの視点から",
      "submittedOnDailyBy": {
        "_id": "5e7749883d77a72421292d07",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
        "isPro": false,
        "fullname": "Gabriele Sarti",
        "user": "gsarti",
        "type": "user"
      },
      "summary": "Word-level quality estimation (WQE)は、機械翻訳の出力中の細かい誤りスパンを自動的に特定することを目的としています。これは翻訳者の後編集サポートなどにも多くの利用例があります。現代のWQE技術は、大規模な言語モデルの実行や大量の人間ラベルデータの実態的なトレーニングによって費用が高くなります。本稿では、言語モデルの説明性と不確実性評価の最近の進歩を活用した効率的な代替方法を検討し、翻訳モデルの内側の機能から翻訳誤りを特定することを目的としています。12方向の翻訳において14マーケットを掲載した評価で、複数の人間ラベルセットを使用してメトリックの性能に及ぼす人間ラベルの変化の影響を定量化します。この結果は、無マニュアルメトリックの未開発の可能性、ラベル不確実性に対するマニュアルメトリックの欠点、セカンドアノテーター評価の脆弱性を明らかにします。",
      "upvotes": 1,
      "discussionId": "683962e6cba8ce4f5ebcedfc",
      "githubRepo": "https://github.com/gsarti/labl",
      "ai_summary": "Evaluation of word-level quality estimation techniques leverages model interpretability and uncertainty quantification to identify translation errors with a focus on the impact of label variation and the performance of supervised versus unsupervised metrics.",
      "ai_keywords": [
        "language model interpretability",
        "uncertainty quantification",
        "word-level quality estimation",
        "translation errors",
        "human label variation",
        "unsupervised metrics",
        "supervised methods",
        "single-annotator evaluation practices"
      ]
    },
    "publishedAt": "2025-05-29T03:20:36.000Z",
    "title": "Unsupervised Word-level Quality Estimation for Machine Translation\n  Through the Lens of Annotators (Dis)agreement",
    "summary": "Word-level quality estimation (WQE) aims to automatically identify\nfine-grained error spans in machine-translated outputs and has found many uses,\nincluding assisting translators during post-editing. Modern WQE techniques are\noften expensive, involving prompting of large language models or ad-hoc\ntraining on large amounts of human-labeled data. In this work, we investigate\nefficient alternatives exploiting recent advances in language model\ninterpretability and uncertainty quantification to identify translation errors\nfrom the inner workings of translation models. In our evaluation spanning 14\nmetrics across 12 translation directions, we quantify the impact of human label\nvariation on metric performance by using multiple sets of human labels. Our\nresults highlight the untapped potential of unsupervised metrics, the\nshortcomings of supervised methods when faced with label uncertainty, and the\nbrittleness of single-annotator evaluation practices.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/1j5bJLYubeP6s5cGuTNzs.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/I2PXFiD8jN4odhmi5vrG6.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23183.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e7749883d77a72421292d07",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
      "fullname": "Gabriele Sarti",
      "name": "gsarti",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 225
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22943",
      "authors": [
        {
          "_id": "683912ee831cc04b0a3c0c6c",
          "user": {
            "_id": "64bb081c01f1983a863654dc",
            "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
            "isPro": false,
            "fullname": "Jaewoo Ahn",
            "user": "ahnpersie",
            "type": "user"
          },
          "name": "Jaewoo Ahn",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:54.580Z",
          "hidden": false
        },
        {
          "_id": "683912ee831cc04b0a3c0c6d",
          "name": "Heeseung Yun",
          "hidden": false
        },
        {
          "_id": "683912ee831cc04b0a3c0c6e",
          "name": "Dayoon Ko",
          "hidden": false
        },
        {
          "_id": "683912ee831cc04b0a3c0c6f",
          "name": "Gunhee Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T23:45:55.000Z",
      "submittedOnDailyAt": "2025-05-30T00:40:41.829Z",
      "title": "LLMsはCLIPを騙すことができるか？ テキスト更新による予め学習された多模態表現の敵対的な構成性をベンチマークする",
      "submittedOnDailyBy": {
        "_id": "64bb081c01f1983a863654dc",
        "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
        "isPro": false,
        "fullname": "Jaewoo Ahn",
        "user": "ahnpersie",
        "type": "user"
      },
      "summary": "クリップなどの事前学習マルチモーダル表現は、見事な能力を示しているが、構成的な脆弱性を示し、直感的な判断に反するものが多い。私たちは、マルチモーダルアドバーサリアリズム（MAC）というベンチマークを紹介し、大規模な言語モデル（LLMs）を使用して、これらの脆弱性を利用するための騙せられるテキストサンプルを生成し、サンプル毎の攻撃成功率とグループ毎のデバイスに基づく多様性を評価する。零ショットメソッドを改善するために、私たちは、騙されられるサンプルの多様性を促すフィルタリングを実行する拒否サンプリング微調チューニングを利用した自己学習アプローチを提案し、攻撃成功率とサンプルの多様性を両方向上に向上させる。Llama-3.1-8Bなどの小規模な言語モデルを使用して、私たちのアプローチは、画像、ビデオ、音声などの多様なマルチモーダル表現の構成的な脆弱性を明らかにする優れた性能を示している。",
      "upvotes": 1,
      "discussionId": "683912ef831cc04b0a3c0cc1",
      "githubRepo": "https://github.com/ahnjaewoo/MAC",
      "ai_summary": "A benchmark using deceptive text samples to evaluate compositional vulnerabilities in multimodal representations is introduced, and a self-training approach improves zero-shot methods by enhancing attack success and sample diversity.",
      "ai_keywords": [
        "Multimodal Adversarial Compositionality (MAC)",
        "multimodal representations",
        "large language models (LLMs)",
        "rejection-sampling fine-tuning",
        "diversity-promoting filtering",
        "images",
        "videos",
        "audios"
      ]
    },
    "publishedAt": "2025-05-28T19:45:55.000Z",
    "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of\n  Pre-trained Multimodal Representation via Text Updates",
    "summary": "While pre-trained multimodal representations (e.g., CLIP) have shown\nimpressive capabilities, they exhibit significant compositional vulnerabilities\nleading to counterintuitive judgments. We introduce Multimodal Adversarial\nCompositionality (MAC), a benchmark that leverages large language models (LLMs)\nto generate deceptive text samples to exploit these vulnerabilities across\ndifferent modalities and evaluates them through both sample-wise attack success\nrate and group-wise entropy-based diversity. To improve zero-shot methods, we\npropose a self-training approach that leverages rejection-sampling fine-tuning\nwith diversity-promoting filtering, which enhances both attack success rate and\nsample diversity. Using smaller language models like Llama-3.1-8B, our approach\ndemonstrates superior performance in revealing compositional vulnerabilities\nacross various multimodal representations, including images, videos, and\naudios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22943.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64bb081c01f1983a863654dc",
      "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
      "fullname": "Jaewoo Ahn",
      "name": "ahnpersie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22810",
      "authors": [
        {
          "_id": "68395c867f983113faf29005",
          "name": "Zhoufaran Yang",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf29006",
          "name": "Yan Shu",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf29007",
          "name": "Zhifei Yang",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf29008",
          "name": "Yan Zhang",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf29009",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900a",
          "name": "Keyang Lu",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900b",
          "name": "Gangyan Zeng",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900c",
          "name": "Shaohui Liu",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900d",
          "name": "Yu Zhou",
          "hidden": false
        },
        {
          "_id": "68395c867f983113faf2900e",
          "name": "Nicu Sebe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T19:39:35.000Z",
      "submittedOnDailyAt": "2025-05-30T05:52:32.917Z",
      "title": "VidText: 動画テキスト理解の完全な評価への向け方",
      "submittedOnDailyBy": {
        "_id": "65c4f99b27736b5b86c2cbda",
        "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
        "isPro": false,
        "fullname": "Yan Shu",
        "user": "sy1998",
        "type": "user"
      },
      "summary": "ビデオに埋め込まれた可視的テキストは、豊富なセマンティック情報を持ち、ビデオ全体の理解と局所的な人間の行動の詳細な推理に重要です。しかし、現在のビデオ理解ベンチマークは主にテキスト情報を無視していますが、OCR特化ベンチマークは静的画像に限られていて、テキストと動的な視覚的コンテキストの相互作用を捉える能力を制限しています。この空間を填えるために、VidTextという新しいベンチマークを提案します。VidTextは、次のポイントを挙げます：1）実世界的なシナリオの幅広い範囲をカバーし、多言語内容をサポートし、ビデオテキストが自然に出現する多様な設定を含む。2）ビデオレベル、クリップレベル、インスタンスレベルのタスクを挙げる階層的な評価フレームワークを導入し、グローバルな要約と局所的な検索能力を評価することができます。3）ベンチマークは、可視的テキストの視覚的認識から文字と視覚的コンテキストのクロスモード推理の範囲を広めたペアづけされた認知推理タスクを導入します。18つの最先端の大規模な多モデル（LMMs）に対しての拡大的な実験は、現在のモデルが多数のタスクで苦戦し、大幅な改善の余地があることを明らかにしました。進めた分析は、入力解像度とOCR能力のモデルの固有な要因、および助言情報の使用とChain-of-Thought推理戦略の外部要因の影響を主に示しました。VidTextは、現在のビデオ理解ベンチマークの空間を埋め、動的な環境でのビデオテキストを用いた多モデル推理の将来の研究の基盤として役立つことを望みます。",
      "upvotes": 1,
      "discussionId": "68395c897f983113faf290eb",
      "ai_summary": "VidText is a new benchmark that evaluates video text understanding across various tasks, covering global summarization and local retrieval, and highlights challenges for current multimodal models.",
      "ai_keywords": [
        "LMMs",
        "video text perception",
        "cross-modal reasoning",
        "Chain-of-Thought reasoning"
      ]
    },
    "publishedAt": "2025-05-28T15:39:35.000Z",
    "title": "VidText: Towards Comprehensive Evaluation for Video Text Understanding",
    "summary": "Visual texts embedded in videos carry rich semantic information, which is\ncrucial for both holistic video understanding and fine-grained reasoning about\nlocal human actions. However, existing video understanding benchmarks largely\noverlook textual information, while OCR-specific benchmarks are constrained to\nstatic images, limiting their ability to capture the interaction between text\nand dynamic visual contexts. To address this gap, we propose VidText, a new\nbenchmark designed for comprehensive and in-depth evaluation of video text\nunderstanding. VidText offers the following key features: 1) It covers a wide\nrange of real-world scenarios and supports multilingual content, encompassing\ndiverse settings where video text naturally appears. 2) It introduces a\nhierarchical evaluation framework with video-level, clip-level, and\ninstance-level tasks, enabling assessment of both global summarization and\nlocal retrieval capabilities. 3) The benchmark also introduces a set of paired\nperception reasoning tasks, ranging from visual text perception to cross-modal\nreasoning between textual and visual information. Extensive experiments on 18\nstate-of-the-art Large Multimodal Models (LMMs) reveal that current models\nstruggle across most tasks, with significant room for improvement. Further\nanalysis highlights the impact of both model-intrinsic factors, such as input\nresolution and OCR capability, and external factors, including the use of\nauxiliary information and Chain-of-Thought reasoning strategies. We hope\nVidText will fill the current gap in video understanding benchmarks and serve\nas a foundation for future research on multimodal reasoning with video text in\ndynamic environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22810.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c4f99b27736b5b86c2cbda",
      "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
      "fullname": "Yan Shu",
      "name": "sy1998",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22126",
      "authors": [
        {
          "_id": "683944793e5dd928f04e8431",
          "name": "Yifan Chang",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8432",
          "name": "Yukang Feng",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8433",
          "name": "Jianwen Sun",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8434",
          "name": "Jiaxin Ai",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8435",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8436",
          "name": "S. Kevin Zhou",
          "hidden": false
        },
        {
          "_id": "683944793e5dd928f04e8437",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T08:51:01.000Z",
      "submittedOnDailyAt": "2025-05-30T04:18:26.192Z",
      "title": "SridBench: 科学研究イラストレーションの描き方モデルのベンチマーク",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "近年、AI駆動の画像生成には急速な進歩が見られています。初期のディフュージョンモデルは視覚質量を優先し、新しい多モーダルモデルでは、高レベルの理由論を統合し、意味的理解と構造的な構成を改善しています。科学説明図の生成はこの進化の例です：一般的な画像合成と異なり、技術内容の正確な解釈と抽象的なアイデアを明確な、標準化された可視化に変換する必要があります。このタスクは知識的さや労力の多くの場合、手動で数時間かかり、特製のツールが必要です。これを制御的で知能的な方法で自動化することは実用的な価値を提供することができますが、現在はこの分野におけるAIの評価においてはベンチマークが存在しません。この空間を埋めるために、私たちはSridBenchを紹介します。これは科学説明図の生成に関する最初のベンチマークで、13分野の自然科学と計算科学の先進的な科学論文から選び出された1,120サンプルから構成されています。各サンプルは意味的な忠実性と構造的な正確性など6つの次元において評価されています。実験結果から、GPT-4o-imageなどのトップレベルモデルは、文/画像の明確性と科学性の正確性において人間の性能に追い抜かれていることが明らかになりました。これらの発見は、進歩的な理由論をライングアップする可視化生成能力の必要性を強調しています。",
      "upvotes": 1,
      "discussionId": "6839447c3e5dd928f04e84c1",
      "ai_summary": "The introduction of SridBench, a benchmark for scientific figure generation, reveals that current top-tier models, such as GPT-4o-image, fall short in semantic and structural accuracy compared to human performance, underscoring the need for more advanced multimodal reasoning-driven visual generation.",
      "ai_keywords": [
        "diffusion models",
        "multimodal models",
        "GPT-4o-image",
        "semantic understanding",
        "structural composition",
        "scientific illustration generation",
        "SridBench",
        "semantic fidelity",
        "structural accuracy"
      ]
    },
    "publishedAt": "2025-05-28T04:51:01.000Z",
    "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of\n  Image Generation Model",
    "summary": "Recent years have seen rapid advances in AI-driven image generation. Early\ndiffusion models emphasized perceptual quality, while newer multimodal models\nlike GPT-4o-image integrate high-level reasoning, improving semantic\nunderstanding and structural composition. Scientific illustration generation\nexemplifies this evolution: unlike general image synthesis, it demands accurate\ninterpretation of technical content and transformation of abstract ideas into\nclear, standardized visuals. This task is significantly more\nknowledge-intensive and laborious, often requiring hours of manual work and\nspecialized tools. Automating it in a controllable, intelligent manner would\nprovide substantial practical value. Yet, no benchmark currently exists to\nevaluate AI on this front. To fill this gap, we introduce SridBench, the first\nbenchmark for scientific figure generation. It comprises 1,120 instances\ncurated from leading scientific papers across 13 natural and computer science\ndisciplines, collected via human experts and MLLMs. Each sample is evaluated\nalong six dimensions, including semantic fidelity and structural accuracy.\nExperimental results reveal that even top-tier models like GPT-4o-image lag\nbehind human performance, with common issues in text/visual clarity and\nscientific correctness. These findings highlight the need for more advanced\nreasoning-driven visual generation capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22126.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.20199",
      "authors": [
        {
          "_id": "683919611186f2cbf3ed2267",
          "name": "Pengxiang Li",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed2268",
          "name": "Shilin Yan",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed2269",
          "name": "Joey Tsai",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226a",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226b",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226c",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "683919611186f2cbf3ed226d",
          "name": "Xiaowei Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T16:40:22.000Z",
      "submittedOnDailyAt": "2025-05-30T01:05:23.884Z",
      "title": "アダプティブなクラスファイザーフリーガイドニングによるダイナミックな低信頼性マスク",
      "submittedOnDailyBy": {
        "_id": "64245f2c089d5fae56b4549a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
        "isPro": false,
        "fullname": "Pengxiang Li",
        "user": "pengxiang",
        "type": "user"
      },
      "summary": "Classifier-Free Guidance (CFG)は、生成モデルの制御性を大幅に向上させるために、条件付きと非条件付き予測をインタープーレーションしています。しかし、標準的なCFGは、イテレーション的な生成プロセスでは、モデルの不確実性が動的に変化するために、静的な非条件付き入力を使用していますが、これは最適ではありません。私たちは、モデルの瞬時的な予測信頼性を活用して非条件付き入力をマスタードに製作する新しい方法、Adaptive Classifier-Free Guidance (A-CFG)を紹介します。イテレーション的な（マスク付き）ディフフォーション言語モデルの各ステップで、A-CFGは現在生成されたシーケンスのモデルが低い信頼性を示すトークンを特定します。これらのトークンは、動的な、局所的な非条件付き入力を作成するために一時的に再マスクされます。これにより、CFGの修正影響は不明確な領域に正確に集中し、より効果的なガイドを提供します。私たちは、最先端のマスク付きディフフォーション言語モデルにA-CFGを統合し、その効果性を示します。多様な言語生成ベンチマークの実験により、A-CFGは標準的なCFGより大幅な改善を収め、例えばGPQAでは3.9点の収益を収めました。私たちの研究は、イテレーション的な生成でモデルの不確実性に対してガイドメカニズムを動的に調整することのベースにあることを明らかにしています。",
      "upvotes": 1,
      "discussionId": "683919611186f2cbf3ed2292",
      "ai_summary": "Adaptive Classifier-Free Guidance (A-CFG) dynamically adjusts the guidance in masked diffusion language models by focusing on areas of low model confidence, leading to significant improvements in language generation performance.",
      "ai_keywords": [
        "Classifier-Free Guidance (CFG)",
        "Adaptive Classifier-Free Guidance (A-CFG)",
        "masked diffusion language model",
        "predictive confidence",
        "GPQA"
      ]
    },
    "publishedAt": "2025-05-26T12:40:22.000Z",
    "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
    "summary": "Classifier-Free Guidance (CFG) significantly enhances controllability in\ngenerative models by interpolating conditional and unconditional predictions.\nHowever, standard CFG often employs a static unconditional input, which can be\nsuboptimal for iterative generation processes where model uncertainty varies\ndynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel\nmethod that tailors the unconditional input by leveraging the model's\ninstantaneous predictive confidence. At each step of an iterative (masked)\ndiffusion language model, A-CFG identifies tokens in the currently generated\nsequence for which the model exhibits low confidence. These tokens are\ntemporarily re-masked to create a dynamic, localized unconditional input. This\nfocuses CFG's corrective influence precisely on areas of ambiguity, leading to\nmore effective guidance. We integrate A-CFG into a state-of-the-art masked\ndiffusion language model and demonstrate its efficacy. Experiments on diverse\nlanguage generation benchmarks show that A-CFG yields substantial improvements\nover standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work\nhighlights the benefit of dynamically adapting guidance mechanisms to model\nuncertainty in iterative generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20199.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64245f2c089d5fae56b4549a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
      "fullname": "Pengxiang Li",
      "name": "pengxiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19236",
      "authors": [
        {
          "_id": "68392fc4b6280677f75e6194",
          "user": {
            "_id": "5fbdf878485ef14d9a960f4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634724388998-5fbdf878485ef14d9a960f4d.jpeg",
            "isPro": false,
            "fullname": "Qian Cao",
            "user": "Aman",
            "type": "user"
          },
          "name": "Qian Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-30T06:53:24.101Z",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6195",
          "name": "Xiting Wang",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6196",
          "name": "Yuzhuo Yuan",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6197",
          "name": "Yahui Liu",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6198",
          "name": "Fang Luo",
          "hidden": false
        },
        {
          "_id": "68392fc4b6280677f75e6199",
          "name": "Ruihua Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T17:25:23.000Z",
      "submittedOnDailyAt": "2025-05-30T02:42:27.745Z",
      "title": "多様な領域でのテキスト創造性評価：データセットと大規模言語モデル評価者",
      "submittedOnDailyBy": {
        "_id": "5fbdf878485ef14d9a960f4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634724388998-5fbdf878485ef14d9a960f4d.jpeg",
        "isPro": false,
        "fullname": "Qian Cao",
        "user": "Aman",
        "type": "user"
      },
      "summary": "創造性評価は、大規模言語モデル（LLMs）では難しい境地です。現在の評価は、無効率で高額な人間の判断を重視していますが、機械の創造性向上につながりません。自動化の方法は、心理学テストからヒューリスティックやプロンプティングベースのアプローチまでありますが、一般化可能であり人間の判断に合わせていません。これらの問題を解決するために、この論文では、共通のコンテキストフレームで評価の一致性を改善するための新しい二つ比較フレームワークを提案します。CreataSetという大規模なデータセットを紹介します。このデータセットは、100K+の人間レベルと1M+の合成的な創造的なインストラクションレスポンスペアを含み、多様な開放ドメインタスクに広がります。CreataSetでの訓練を通じて、LLMベースの評価者CrEvalを開発します。CrEvalは、人間の判断に合わせて現在の方法よりも驚異的な優れた性能を示します。実験結果は、人間生成および合成データの両方を含めた高度な強固な評価者の訓練の不可欠な重要性を強調し、CrEvalの実用的な効用を示します。すぐに公開していくことで、進める研究のサポートを提供します。",
      "upvotes": 1,
      "discussionId": "68392fc5b6280677f75e61e6",
      "projectPage": "https://creval-creative-evaluation.github.io/",
      "ai_summary": "A novel pairwise-comparison framework using CreataSet dataset trains CrEval, an LLM-based evaluator that significantly improves the assessment of textual creativity aligned with human judgments.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "automated methods",
        "heuristic-based approaches",
        "prompting-based approaches",
        "pairwise-comparison framework",
        "shared contextual instructions",
        "CreataSet",
        "synthetic creative instruction-response pairs",
        "open-domain tasks",
        "human-level instructions",
        "CrEval",
        "human judgments",
        "robust evaluators",
        "creativity enhancement"
      ]
    },
    "publishedAt": "2025-05-25T13:25:23.000Z",
    "title": "Evaluating Text Creativity across Diverse Domains: A Dataset and Large\n  Language Model Evaluator",
    "summary": "Creativity evaluation remains a challenging frontier for large language\nmodels (LLMs). Current evaluations heavily rely on inefficient and costly human\njudgments, hindering progress in enhancing machine creativity. While automated\nmethods exist, ranging from psychological testing to heuristic- or\nprompting-based approaches, they often lack generalizability or alignment with\nhuman judgment. To address these issues, in this paper, we propose a novel\npairwise-comparison framework for assessing textual creativity, leveraging\nshared contextual instructions to improve evaluation consistency. We introduce\nCreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic\ncreative instruction-response pairs spanning diverse open-domain tasks. Through\ntraining on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval\ndemonstrates remarkable superiority over existing methods in alignment with\nhuman judgments. Experimental results underscore the indispensable significance\nof integrating both human-generated and synthetic data in training highly\nrobust evaluators, and showcase the practical utility of CrEval in boosting the\ncreativity of LLMs. We will release all data, code, and models publicly soon to\nsupport further research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19236.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fbdf878485ef14d9a960f4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634724388998-5fbdf878485ef14d9a960f4d.jpeg",
      "fullname": "Qian Cao",
      "name": "Aman",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  }
]