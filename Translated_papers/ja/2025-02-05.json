[
  {
    "paper": {
      "id": "2502.01362",
      "authors": [
        {
          "_id": "67a2ad6ac7caec9bf5a45e61",
          "name": "Nikita Gushchin",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e62",
          "name": "David Li",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e63",
          "name": "Daniil Selikhanovych",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e64",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e65",
          "name": "Dmitry Baranchuk",
          "hidden": false
        },
        {
          "_id": "67a2ad6ac7caec9bf5a45e66",
          "name": "Alexander Korotin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T13:56:03.000Z",
      "title": "逆ブリッジマッチングディスタイル",
      "summary": "学習ディフュージョンブリッジモデルは簡単ですが、それを高速化して実用的にするのは芸術的です。ディフュージョンブリッジモデル（DBMs）は、画像への画像転写の適用において有望なディフュージョンモデルの拡張です。しかし、その他の現代的なディフュージョンモデルやフローモデルと同様に、DBMsは推論速度が遅くなる問題を見落としています。これを解決するために、我々は逆ブリッジマッチングの基礎に立った新しいディスティルメソッドを提案し、実際に問題を解決するための計算可能なオブジェクトを計算します。以前開発されたDBMディスティルメソッドと異なり、提案された方法は条件付きおよび非条件付きの両方のDBMsをディスティルでき、一ステップのジェネレーターでディスティルし、そのみ破壊された画像を使用して訓練することができます。条件付きおよび非条件付きのブリッジマッチングの両方を対象に広範囲の設定で評価し、超解像、JPEGリフターム、スケッチオブジェントへの画像転写その他のタスクについて、我々のディスティルメソッドがDBMsの推論速度を4倍から100倍に加速し、ティーチャーモデルの生成品質を超えることができることを示します。",
      "upvotes": 16,
      "discussionId": "67a2ad70c7caec9bf5a45fb0"
    },
    "publishedAt": "2025-02-05T03:01:40.464Z",
    "title": "Inverse Bridge Matching Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01362.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "672503c59f68afdd63cc81a2",
      "avatarUrl": "/avatars/91207207b56a1fc2b4a8197b1ab3a7f9.svg",
      "fullname": "Nikita Gushchin",
      "name": "ngushchin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01718",
      "authors": [
        {
          "_id": "67a2d995c97974764a8c294c",
          "name": "Huaye Zeng",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294d",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294e",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c294f",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c2950",
          "name": "Xiaotong Chen",
          "hidden": false
        },
        {
          "_id": "67a2d995c97974764a8c2951",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:46:04.000Z",
      "title": "ACECODER: 自動テストケース合成によるランダムフォレストでのコーディングの優勝",
      "summary": "最近のコードモデルの進展は、主に観学微調（SFT）によって駆動されていますが、強化学習（RL）の潜力は、コード領域での信頼性のある報酬データやモデルの欠如により、大きく探索されていません。本論文では、この挑戦を解決するために、自動化された大規模なテストケース合成を活用してコードモデルの訓練を強化します。特に、現有のコードデータから詳細な（問題、テストケース）ペアを生成するパイプラインを設計します。これらのテストケースを使用して、サンプリングされたプログラムのパス率に基づいて、ブラディーリーディー損失を使用して報酬モデルを訓練します。これは、Llama-3.1-8B-Insで平均10点、Qwen2.5-Coder-7B-Insで5点の改善を示し、32回のベストの32サンプリングで7BモデルがDeepSeek-V2.5の236Bモデルと同等になることを示します。また、報酬モデルとテストケースパス報酬を用いて強化学習を行い、HumanEval、MBPP、BigCodeBench、LiveCodeBench（V4）で一致的な改善を収めました。特に、R1スタイルの訓練をみたし、Qwen2.5-Coder-baseから直接始め、HumanEval-plusで25％以上、MBPP-plusで6％の改善を示します。私たちは、この結果から、強化学習がコードモデルにおける巨大な潜力を示していることを信じています。",
      "upvotes": 12,
      "discussionId": "67a2d996c97974764a8c29a1"
    },
    "publishedAt": "2025-02-04T22:23:07.858Z",
    "title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01718.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5946
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.02492",
      "authors": [
        {
          "_id": "67a2ec904ea0e3138ac966f2",
          "user": {
            "_id": "6181c72cdcc1df2c9de8a4d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
            "isPro": false,
            "fullname": "Hila Chefer",
            "user": "Hila",
            "type": "user"
          },
          "name": "Hila Chefer",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-05T04:44:03.218Z",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f3",
          "name": "Uriel Singer",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f4",
          "name": "Amit Zohar",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f5",
          "name": "Yuval Kirstain",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f6",
          "name": "Adam Polyak",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f7",
          "name": "Yaniv Taigman",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f8",
          "name": "Lior Wolf",
          "hidden": false
        },
        {
          "_id": "67a2ec904ea0e3138ac966f9",
          "name": "Shelly Sheynin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T17:07:10.000Z",
      "title": "VideoJAM: ビデオモデルでの機能性を向上させる共通の外見-挙動表現",
      "summary": "最近の過去による巨大な進歩にもかかわらず、生成ビデオモデルは実世界的な動き、動力学、および物理を捉えることに難しい。私たちは、この制限は、伝統的なピクセル再構成の目標によって起こることを示し、これは外観の忠実性に向けて動きの一貫性を負けるようにモデルをバイアスしていることを示しています。これを解決するために、私たちはVideoJAMという新しいフレームワークを紹介し、ビデオジェネレーターに効果的な動きの先驅を与えるよう、モデルが共通の外観-動き表現を学習するように励まします。VideoJAMは2つの補間するユニットからなります。訓練中、目標を拡張し、生成されたピクセルとその対応する動きを予測することによって、一つの学習された表現からできるものです。推論中、私たちはInner-Guidanceという機構を導入し、モデルの自らの進化する動き予測を動的なガイドライン信号として利用して、一貫性のある動きに向けて生成を制御するものです。特に、私たちのフレームワークは、最小限の変更を必要としないように、訓練データまたはモデルのスケーリングによる変更を必要としないことを特徴として、動きの一貫性について最先端の性能を達成し、高度な競争力のプロプライエモリモデルを超えることを証明し、同時に生成物の視覚質量を向上させます。これらの発見は、外観と動きは補間的であり、エフェクティブに統合されることで、ビデオ生成の視覚質量と一貫性を両方とも向上させることを強調しています。プロジェクトウェブサイト：https://hila-chefer.github.io/videojam-paper.github.io/",
      "upvotes": 11,
      "discussionId": "67a2ec934ea0e3138ac9678e"
    },
    "publishedAt": "2025-02-04T23:46:17.626Z",
    "title": "VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6181c72cdcc1df2c9de8a4d8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
      "fullname": "Hila Chefer",
      "name": "Hila",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.02584",
      "authors": [
        {
          "_id": "67a2d59fd5ad3369a66ff394",
          "name": "Zongyu Lin",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff395",
          "name": "Yao Tang",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff396",
          "name": "Xingcheng Yao",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff397",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff398",
          "name": "Ziniu Hu",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff399",
          "name": "Yizhou Sun",
          "hidden": false
        },
        {
          "_id": "67a2d59fd5ad3369a66ff39a",
          "name": "Kai-Wei Chang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T18:58:31.000Z",
      "title": "QLASS: Q-Guided Stepwise Searchによる言語アウトプット推論の向上",
      "summary": "言語アガントは複雑な相互作用タスクに対する有望な解決策となっており、成功の鍵の一つはアガントワークフローの軌道上の報酬モデルである。これは学習または推論中に有効な指導を提供します。しかし、中間的な相互作用の訳注が不足しているため、多くの既存の研究は結果報酬モデルを使用して、全体の軌道を構成する政策を最適化しています。これは最適な政策を得られなく、全体の性能を妨げることがあります。これを解決するために、QLASS（Qガイドされた言語アガントステップワイズサーチ）を提案し、開放フレームワークの言語アガントに対してステップごとにQ値を推定して自動的に訳注を生成することで、中間的な指導を提供します。理由の木を挙げ、プロセス報酬モデリングを行うことで、QLASSは各ステップに効果的な中間的な指導を提供します。ステップごとの指導をもとに、QLASSは長期の価値によるより良い適応性を賦与し、複雑な相互作用アガントタスクのモデル推論中での性能向上を実現します。特に、半分近くの訳注データを使用しても強い性能を維持し、限られたスーパーバイオンに対応する効率性を示します。また、定性的な分析を通じて、QLASSがより有効な決策を促進することを実験的に示します。コードとデータを公開します。",
      "upvotes": 7,
      "discussionId": "67a2d5a0d5ad3369a66ff3d4"
    },
    "publishedAt": "2025-02-04T22:08:25.652Z",
    "title": "QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4670a51d5df8c2d92fce",
      "avatarUrl": "/avatars/c52d7150b4de6a2eb2d83b345d35cbc2.svg",
      "fullname": "Da Yin",
      "name": "DaYin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01941",
      "authors": [
        {
          "_id": "67a2e2a02dd2adbc88755a47",
          "name": "Xiang Liu",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a48",
          "name": "Zhenheng Tang",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a49",
          "name": "Hong Chen",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4a",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4b",
          "name": "Zeyu Li",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4c",
          "name": "Xiuze Zhou",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4d",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4e",
          "name": "Xuming Hu",
          "hidden": false
        },
        {
          "_id": "67a2e2a02dd2adbc88755a4f",
          "name": "Xiaowen Chu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T02:23:06.000Z",
      "title": "LLMはKVキャッシュ圧縮の下で基本的な能力を維持できるか？",
      "summary": "この論文は、大規模言語モデル（LLMs）における未調査された課題を調査しています：KVキャッシュの圧縮方法がLLMsの基本的な能力に与える影響。既存の方法は、長文脈ベンチマークで驚異的な圧縮比を達成しますが、それら対核心モデル能力に与える影響はまだ調査不足です。私たちは、世界知識、常識推理、算術推理、コード生成、安全性、長文脈の理解と生成を範囲に広めた多様なタスクでのプライマリーなKVキャッシュ圧縮方法を評価する詳細な実験結果を提供します。分析により、KVキャッシュ圧縮方法はタスクによって特に性能低下を示します。算術推理タスクは、激しい圧縮に特に敏感で、17.4%-43.3%の性能低下を示します。特に、DeepSeek R1 Distillモデルは、指示チューニングモデルに比べて圧縮の耐性が高く、9.67%-25.53%の性能低下を示します。私たちは、注意パターンとクロスタスクの圧縮性能を評価し、新しい圧縮アプローチShotKVを提案します。ShotKVは、prefillと解碼フェーズを単独で処理し、ショットレベルの意味的なコネクティビティを維持することで、激しい圧縮比の下で長文脈生成タスクで9%-18%の性能向上を達成します。",
      "upvotes": 6,
      "discussionId": "67a2e2a22dd2adbc88755ab4"
    },
    "publishedAt": "2025-02-04T23:04:25.888Z",
    "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63024676056ec3a2a8714b24/XcgjmhpXd3dH6LnFZGupJ.png",
      "https://cdn-uploads.huggingface.co/production/uploads/63024676056ec3a2a8714b24/hxWz1iVOUcE76E_K5z-B0.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.02508",
      "authors": [
        {
          "_id": "67a2d1f9bc9d072d9459e857",
          "user": {
            "_id": "6553c985a7aded0380b5f928",
            "avatarUrl": "/avatars/36109d6f536d2b34d98822b88eac9608.svg",
            "isPro": false,
            "fullname": "Maohao Shen",
            "user": "maohaos2",
            "type": "user"
          },
          "name": "Maohao Shen",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-05T03:00:33.470Z",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e858",
          "name": "Guangtao Zeng",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e859",
          "name": "Zhenting Qi",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85a",
          "name": "Zhang-Wei Hong",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85b",
          "name": "Zhenfang Chen",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85c",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85d",
          "name": "Gregory Wornell",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85e",
          "name": "Subhro Das",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e85f",
          "name": "David Cox",
          "hidden": false
        },
        {
          "_id": "67a2d1f9bc9d072d9459e860",
          "name": "Chuang Gan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-04T17:26:58.000Z",
      "title": "サトリ: 行動の連鎖での思考を用いた強化学習がLLMを強化する\n  自動帰納的検索による推理",
      "summary": "大語言モデル（LLMs）は、多様な領域で驚異的な理由論能力を示しています。最近の研究により、検証時の計算量の増加がLLMsの理由論能力を向上させることが見られています。これは通常、推論時に外部のLLMバリデーターによる広範囲的なサンプリングを含むもので、これにより2つのプレイヤーシステムが形成されます。外部のガイドを受けても、このシステムの効果性は、単一のLLMが複雑なタスクを解決できることを示しています。そこで、私たちは新しい研究問題を提案します：単一のLLMの理由論能力を根本的に向上させるために、探索能力を内部化できるかどうか？この研究は、自動協調的な探索（つまり、自己反省と新しい戦略の自己探索を含む拡張された理由論プロセス）を焦点にした後学習LLMsに焦点を当てて、正交方向を検討しています。これを達成するために、Chain-of-Action-Thought（COAT）理由論と2段階の学習パラダイムを提案しています：1）COAT理由論のフォーマットを内部化する小規模のフォーマットチューニングステージと、2）大規模な自己改善ステージでの強化学習を活用しています。私たちのアプローチは、開放ソースモデルとデータによって訓練された7B LLMである「Satori」を結果として得ました。拡張的な実験評価により、Satoriは数学的な理由論ベンチマークで最先端の性能を達成し、外側領域タスクへの強い拡張性を示しています。コード、データ、モデルは完全に開放ソースになります。",
      "upvotes": 5,
      "discussionId": "67a2d1fcbc9d072d9459e91b"
    },
    "publishedAt": "2025-02-04T21:55:09.693Z",
    "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.02508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60ad0de755f970745d4ec28d",
      "avatarUrl": "/avatars/b0de0222b8ed5fdac8dc7cb0336d2ec7.svg",
      "fullname": "GtZeng",
      "name": "chaoscodes",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.01720",
      "authors": [
        {
          "_id": "67a2fddb4044bf1c86f765a3",
          "name": "Nupur Kumari",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a4",
          "name": "Xi Yin",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a5",
          "name": "Jun-Yan Zhu",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a6",
          "name": "Ishan Misra",
          "hidden": false
        },
        {
          "_id": "67a2fddb4044bf1c86f765a7",
          "name": "Samaneh Azadi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T18:59:41.000Z",
      "title": "マルチイメージの合成データを生成して、テキストから画像のカスタマイズを実現する",
      "summary": "テキストから画像モデルのカスタマイズでユーザーがカスタム概念を挿入し、見たことのない設定でその概念を生成できるようになることができます。現在の方法は、コスト高いテスト時の最適化を依存しているか、または、単一画像のトレーニングデータセットでエンコーダーをトレーニングしているが、多画像のスーパビューラーを含まないため、画像の質が悪くなることがあります。私たちは両方の制限を解決するために簡単なアプローチを提案します。まず、既存のテキストから画像モデルと3Dデータセットを利用して、同じ物体の様々な照明、背景、および姿勢の画像を含む高品質のSynthetic Customization Dataset (SynCD)を作成します。次に、共有アテンション機構に基づく新しいエンコーダーアーキテクチャを提案し、入力画像からの細かい視覚詳細をより良く統合することを目指します。最後に、推論時のオーバーエクスポーション問題を軽減するために、テキストと画像のガイドベクトルを正規化する新しい推論手法を提案します。拡張した実験を通じて、私たちのモデルは、提案されたエンコーダーと推論アルゴリズムを用いた合成データセットでトレーニングされ、標準的なカスタマイズベンチマーク上で現在のトーンフリー方法を上回ることを示します。",
      "upvotes": 2,
      "discussionId": "67a2fde34044bf1c86f767ba"
    },
    "publishedAt": "2025-02-05T00:59:11.275Z",
    "title": "Generating Multi-Image Synthetic Data for Text-to-Image Customization",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.01720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f6a894c3372328414c7021",
      "avatarUrl": "/avatars/e8b10912355712f38f10805c31bea962.svg",
      "fullname": "Nupur Kumari",
      "name": "nupurkmr9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]