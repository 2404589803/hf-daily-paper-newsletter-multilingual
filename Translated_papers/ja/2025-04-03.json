[
  {
    "paper": {
      "id": "2504.00999",
      "authors": [
        {
          "_id": "67ecc3973d267d266649e075",
          "user": {
            "_id": "640f7083208821a59b74c757",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678735253848-640f7083208821a59b74c757.jpeg",
            "isPro": false,
            "fullname": "Siyuan Li",
            "user": "Lupin1998",
            "type": "user"
          },
          "name": "Siyuan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T08:28:38.819Z",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e076",
          "user": {
            "_id": "671b4781d2f774c5ec9ebd62",
            "avatarUrl": "/avatars/b4f1cbaa6e092eda005f81f199a35e19.svg",
            "isPro": false,
            "fullname": "Luyuan Zhang",
            "user": "LuyuanZhang01",
            "type": "user"
          },
          "name": "Luyuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T08:28:41.242Z",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e077",
          "user": {
            "_id": "6594d390674349122ce6f368",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6594d390674349122ce6f368/luDBiSMX_9l8QEpAQu3HJ.jpeg",
            "isPro": false,
            "fullname": "Zedong Wang",
            "user": "ZedongWangAI",
            "type": "user"
          },
          "name": "Zedong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T07:52:39.150Z",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e078",
          "user": {
            "_id": "670880950e79a8b46f7ff9dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
            "isPro": false,
            "fullname": "Juanxi Tian",
            "user": "Juanxi",
            "type": "user"
          },
          "name": "Juanxi Tian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T08:28:43.436Z",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e079",
          "name": "Cheng Tan",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07a",
          "name": "Zicheng Liu",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07b",
          "name": "Chang Yu",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07c",
          "name": "Qingsong Xie",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07d",
          "name": "Haonan Lu",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07e",
          "name": "Haoqian Wang",
          "hidden": false
        },
        {
          "_id": "67ecc3973d267d266649e07f",
          "name": "Zhen Lei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T17:39:19.000Z",
      "submittedOnDailyAt": "2025-04-03T00:15:32.614Z",
      "title": "MergeVQ: 画像生成と表現の統合フレームワークでは、分離されたトークンの結合と数値化を用います。",
      "submittedOnDailyBy": {
        "_id": "670880950e79a8b46f7ff9dd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
        "isPro": false,
        "fullname": "Juanxi Tian",
        "user": "Juanxi",
        "type": "user"
      },
      "summary": "マスクされた画像モデリング（MIM）とベクトルディスクリミネーション（VQ）を組み合わせた方法は、自動認識学習の予ち練習と画像生成において大きな成功を収めています。しかし、現在の多くの方法は、生成品質と表現学習と効率の間の調整を解決することが難しいです。このパラダイムの限界を超えるために、MergeVQを提案します。MergeVQは、ベクトルディスクリミネーションベースの生成モデルにトークンメリングテクニックを組み込み、画像生成と視覚的表現学習の間の隙間を一つのアーキテクチャでバランスを取ります。予ち練習期間、MergeVQは、エンコーダの自己注意ブロック後にトークンメリングモジュールを用いて、潜在空間からトップ-kのセマンティクスを分離し、Look-up Free Quantization（LFQ）と全球的なアラインメントを行い、デコーダでクロス注意を用いてそれらの細かい詳細を復元します。第二段階の生成においては、MergeARを導入し、KVキャッシュ圧縮を行い、効率的なラスター順番予測を実現します。ImageNet上での拡大的な実験は、MergeVQはAR生成モデルであり、視覚的表現学習と画像生成の両方で優れた性能を収め、トークン効率と推論スピードのフィナライトを維持していることを証明します。コードとモデルは、https://apexgen-x.github.io/MergeVQ で公開されます。",
      "upvotes": 52,
      "discussionId": "67ecc3993d267d266649e10c",
      "projectPage": "https://apexgen-x.github.io/MergeVQ/",
      "githubRepo": "https://github.com/ApexGen-X/MergeVQ",
      "ai_keywords": [
        "Masked Image Modeling (MIM)",
        "Vector Quantization (VQ)",
        "shared latent space",
        "generation quality",
        "representation learning",
        "token merging",
        "generative models",
        "token merge module",
        "self-attention blocks",
        "encoder",
        "Look-up Free Quantization (LFQ)",
        "global alignment",
        "cross-attention",
        "decoder",
        "reconstruction",
        "MergeAR",
        "KV Cache compression",
        "raster-order prediction",
        "AR generative model",
        "ImageNet",
        "token efficiency",
        "inference speed"
      ]
    },
    "publishedAt": "2025-04-01T13:39:19.000Z",
    "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
    "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00999.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "670880950e79a8b46f7ff9dd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg",
      "fullname": "Juanxi Tian",
      "name": "Juanxi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00883",
      "authors": [
        {
          "_id": "67edf28e042e8ba3e95d1960",
          "name": "Zhenyi Liao",
          "hidden": false
        },
        {
          "_id": "67edf28e042e8ba3e95d1961",
          "name": "Qingsong Xie",
          "hidden": false
        },
        {
          "_id": "67edf28e042e8ba3e95d1962",
          "name": "Yanhao Zhang",
          "hidden": false
        },
        {
          "_id": "67edf28e042e8ba3e95d1963",
          "name": "Zijian Kong",
          "hidden": false
        },
        {
          "_id": "67edf28e042e8ba3e95d1964",
          "name": "Haonan Lu",
          "hidden": false
        },
        {
          "_id": "67edf28e042e8ba3e95d1965",
          "name": "Zhenyu Yang",
          "hidden": false
        },
        {
          "_id": "67edf28e042e8ba3e95d1966",
          "user": {
            "_id": "64bba541da140e461924dfed",
            "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
            "isPro": false,
            "fullname": "zhijie deng",
            "user": "zhijie3",
            "type": "user"
          },
          "name": "Zhijie Deng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T07:52:14.204Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T15:11:11.000Z",
      "submittedOnDailyAt": "2025-04-03T01:03:18.798Z",
      "title": "R1-Zero-Like Trainingを用いた視覚スペクトラル論理の向上",
      "submittedOnDailyBy": {
        "_id": "64bba541da140e461924dfed",
        "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
        "isPro": false,
        "fullname": "zhijie deng",
        "user": "zhijie3",
        "type": "user"
      },
      "summary": "多モデル大語言モデル（MLLMs）の理由能力を向上させるために注目が集まっています。物理的な領域で機能するAIアガントの基礎となる映画ベースの視覚空間知能（VSI）は、MLLMsの最も重要な理由能力の一つです。本稿では、R1-Zeroベースのトレーニングを用いて、MLLMsの視覚空間理由能力を向上させる最初の詳細な研究を行います。技術的には、最初に、小さいか中間のQwen2-VLモデルの視覚空間理由能力がChain of Thought（CoT）プロンプトで動くことができないことを認識しました。次に、GRPOトレーニングを用いて視覚空間理由能力を向上させ、DeepSeek-R1-Zeroによって調整されたVSI-100kデータセットを使用しました。調査中、GRPOのKLペナルティ（小さな値を持つものも）を維持する必要性を認識しました。120GPU時間で、Qwen2-VL-2Bから微調節されたvsGRPO-2Bモデルは、基礎モデルより12.1%の優位を示し、GPT-4oを超えます。また、Qwen2-VL-7Bから微調節されたvsGRPO-7Bモデルは、最良の開放ソースモデルLLaVA-NeXT-Video-72Bと同等の性能を達成します。また、vsGRPOとサブジェクト微調節、直接の好み最適化ベースラインの比較を行い、強い性能優位を見出しました。コードとデータセットは、このままです。",
      "upvotes": 40,
      "discussionId": "67edf28f042e8ba3e95d1a60",
      "githubRepo": "https://github.com/zhijie-group/R1-Zero-VSI",
      "ai_keywords": [
        "multi-modal large language models (MLLMs)",
        "video-based visual-spatial intelligence (VSI)",
        "Chain of Thought (CoT)",
        "GRPO training",
        "VSI-100k dataset",
        "DeepSeek-R1-Zero",
        "KL penalty",
        "vsGRPO-2B model",
        "Qwen2-VL-2B",
        "vsGRPO-7B model",
        "Qwen2-VL-7B",
        "LLaVA-NeXT-Video-72B",
        "supervised fine-tuning",
        "direct preference optimization"
      ]
    },
    "publishedAt": "2025-04-01T11:11:11.000Z",
    "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
    "summary": "Increasing attention has been placed on improving the reasoning capacities of\nmulti-modal large language models (MLLMs). As the cornerstone for AI agents\nthat function in the physical realm, video-based visual-spatial intelligence\n(VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This\nwork conducts a first, in-depth study on improving the visual-spatial reasoning\nof MLLMs via R1-Zero-like training. Technically, we first identify that the\nvisual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models\ncannot be activated via Chain of Thought (CoT) prompts. We then incorporate\nGRPO training for improved visual-spatial reasoning, using the carefully\ncurated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation,\nwe identify the necessity to keep the KL penalty (even with a small value) in\nGRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from\nQwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o.\nMoreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves\nperformance comparable to that of the best open-source model\nLLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning\nand direct preference optimization baselines and observe strong performance\nsuperiority. The code and dataset will be available soon.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00883.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64bba541da140e461924dfed",
      "avatarUrl": "/avatars/367993765b0ca3734b2b100db33ed787.svg",
      "fullname": "zhijie deng",
      "name": "zhijie3",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.01014",
      "authors": [
        {
          "_id": "67eca389e14049f5ff064ea6",
          "user": {
            "_id": "6506b77a773ceaa8d52ecea1",
            "avatarUrl": "/avatars/0e769a0795063e1491c44760a4a83097.svg",
            "isPro": false,
            "fullname": "CJH",
            "user": "Howe666",
            "type": "user"
          },
          "name": "Junhao Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:27.228Z",
          "hidden": false
        },
        {
          "_id": "67eca389e14049f5ff064ea7",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "67eca389e14049f5ff064ea8",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "67eca389e14049f5ff064ea9",
          "name": "Jing Liao",
          "hidden": false
        },
        {
          "_id": "67eca389e14049f5ff064eaa",
          "name": "Ying Shan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T17:57:18.000Z",
      "submittedOnDailyAt": "2025-04-03T01:15:35.152Z",
      "title": "アニメゲーマー：無限アニメライフシミュレーションと次のゲーム状態予測",
      "submittedOnDailyBy": {
        "_id": "6506b77a773ceaa8d52ecea1",
        "avatarUrl": "/avatars/0e769a0795063e1491c44760a4a83097.svg",
        "isPro": false,
        "fullname": "CJH",
        "user": "Howe666",
        "type": "user"
      },
      "summary": "最近の画像とビデオ合成の進展は、生成ゲームに新たな可能性を開きました。特に興味深いアプリケーションは、アニメ映画のキャラクターを相互作用可能な、実際に動かせる存在に変換することです。これにより、プレイヤーは、自分の好きなキャラクターの姿をして、言語指示でライフシミュレーションを通じて動的なアニメ世界に浸れることができます。このようなゲームは、予定された境界と固定したゲームプレイルールを除外し、ゲームワールドとの開放的な言語での相互作用を通じて、永遠に変化するストーリーラインと環境を体験できるため、無限ゲームとして定義されます。最近、無限アニメライフシミュレーションの先進的なアプローチは、大規模言語モデル（LLMs）を用いて、多ターンのテキストダイアログを画像生成の言語指示に翻訳することで実現しています。しかし、これは歴史的な可視的なコンテキストを無視し、不確実なゲームプレイにつながります。また、静的な画像だけを生成し、楽しいゲーム体験に必要な動的性質を載せることができません。本稿では、Multimodal Large Language Models（MLLMs）を基盤に構築したアニメゲーマーを提案します。アニメゲーマーは、ゲーム状態の生成において、人物の動きを描く動的なアニメショットと、人物状態の更新を含むものを生成することで、ゲーム状態を生成します。アニメゲーマーでは、新しいアクションに関する多タイプ表現を導入し、ビデオディフューションモデルを用いて高品質なビデオクリップに解釈できます。歴史的なアニメショットの表現をコンテキストとして、次の表現を予測することで、アニメゲーマーは、コンテキストの一致と満足度のある動的性質を持つゲームを生成することができます。自動化メトリックと人間評価を用いた拡大的な評価により、アニメゲーマーは、現在の方法と比較して、ゲーム体験の多方面で上位を輝くことが示されます。コードとチェックポイントは、https://github.com/TencentARC/AnimeGamer から利用可能です。",
      "upvotes": 22,
      "discussionId": "67eca39ce14049f5ff06535b",
      "projectPage": "https://howe125.github.io/AnimeGamer.github.io/",
      "githubRepo": "https://github.com/TencentARC/AnimeGamer",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "video diffusion model",
        "action-aware multimodal representations",
        "automated metrics",
        "human evaluations"
      ]
    },
    "publishedAt": "2025-04-01T13:57:18.000Z",
    "title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction",
    "summary": "Recent advancements in image and video synthesis have opened up new promise\nin generative games. One particularly intriguing application is transforming\ncharacters from anime films into interactive, playable entities. This allows\nplayers to immerse themselves in the dynamic anime world as their favorite\ncharacters for life simulation through language instructions. Such games are\ndefined as infinite game since they eliminate predetermined boundaries and\nfixed gameplay rules, where players can interact with the game world through\nopen-ended language and experience ever-evolving storylines and environments.\nRecently, a pioneering approach for infinite anime life simulation employs\nlarge language models (LLMs) to translate multi-turn text dialogues into\nlanguage instructions for image generation. However, it neglects historical\nvisual context, leading to inconsistent gameplay. Furthermore, it only\ngenerates static images, failing to incorporate the dynamics necessary for an\nengaging gaming experience. In this work, we propose AnimeGamer, which is built\nupon Multimodal Large Language Models (MLLMs) to generate each game state,\nincluding dynamic animation shots that depict character movements and updates\nto character states, as illustrated in Figure 1. We introduce novel\naction-aware multimodal representations to represent animation shots, which can\nbe decoded into high-quality video clips using a video diffusion model. By\ntaking historical animation shot representations as context and predicting\nsubsequent representations, AnimeGamer can generate games with contextual\nconsistency and satisfactory dynamics. Extensive evaluations using both\nautomated metrics and human evaluations demonstrate that AnimeGamer outperforms\nexisting methods in various aspects of the gaming experience. Codes and\ncheckpoints are available at https://github.com/TencentARC/AnimeGamer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01014.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6506b77a773ceaa8d52ecea1",
      "avatarUrl": "/avatars/0e769a0795063e1491c44760a4a83097.svg",
      "fullname": "CJH",
      "name": "Howe666",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20783",
      "authors": [
        {
          "_id": "67e97f581cb6fc648f642a05",
          "user": {
            "_id": "65f5392c68b8e0cb3c9977a2",
            "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
            "isPro": false,
            "fullname": "Zichen",
            "user": "lkevinzc",
            "type": "user"
          },
          "name": "Zichen Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T08:29:10.035Z",
          "hidden": false
        },
        {
          "_id": "67e97f581cb6fc648f642a06",
          "user": {
            "_id": "64e416dc54e18f390ef79ba4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5n01J00ZaVRrebsON8iYA.jpeg",
            "isPro": true,
            "fullname": "Changyu Chen",
            "user": "Cameron-Chen",
            "type": "user"
          },
          "name": "Changyu Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T08:29:07.988Z",
          "hidden": false
        },
        {
          "_id": "67e97f581cb6fc648f642a07",
          "name": "Wenjun Li",
          "hidden": false
        },
        {
          "_id": "67e97f581cb6fc648f642a08",
          "user": {
            "_id": "63885f1d0bebb233d8ad6e5b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669881620925-noauth.jpeg",
            "isPro": false,
            "fullname": "Penghui Qi",
            "user": "QPHutu",
            "type": "user"
          },
          "name": "Penghui Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T08:29:05.240Z",
          "hidden": false
        },
        {
          "_id": "67e97f581cb6fc648f642a09",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "67e97f581cb6fc648f642a0a",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "67e97f581cb6fc648f642a0b",
          "name": "Wee Sun Lee",
          "hidden": false
        },
        {
          "_id": "67e97f581cb6fc648f642a0c",
          "name": "Min Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:59:14.000Z",
      "submittedOnDailyAt": "2025-04-03T03:47:54.547Z",
      "title": "R1-Zero-Like Trainingの理解：批判的視点",
      "submittedOnDailyBy": {
        "_id": "65f5392c68b8e0cb3c9977a2",
        "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
        "isPro": false,
        "fullname": "Zichen",
        "user": "lkevinzc",
        "type": "user"
      },
      "summary": "DeepSeek-R1-Zeroは、スケールアップされた強化学習（RL）がLLMの説明能力を直接的に向上させることができることを示しました。本研究では、R1-Zero-likeのトレーニングを批判的に見直し、基礎モデルとRLの2つの核心的な成分を分析しています。基礎モデルの幅広い範囲を調査し、DeepSeek-V3-Baseを含むことで、予習学習の特徴がRLの性能にどのような影響を与えるかを理解しています。分析により、DeepSeek-V3-Baseは「Ahaモーメント」を示し、Qwen2.5の基礎モデルはプロンプトテンプレートを必要としないでも強い説明能力を示し、予習学習バイアスがある可能性が示唆されています。また、Group Relative Policy Optimization（GRPO）における最適化バイアスを認識し、不正な出力の長さを人工的に増やすことを見出しました。これに対して、Dr. GRPOという無バイアスの最適化手法を導入し、トークンの効率を向上させながら説明能力を維持することを目指しています。これらの洞察を活用し、7Bの基礎モデルを使用してAIME 2024で43.3%の精度を達成する最小限のR1-Zeroのレシピを提案し、新たな最先端として立ち上がりました。コードは、https://github.com/sail-sg/understand-r1-zeroから利用できます。",
      "upvotes": 20,
      "discussionId": "67e97f591cb6fc648f642a38",
      "githubRepo": "https://github.com/sail-sg/understand-r1-zero",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "reasoning capabilities",
        "LLMs",
        "base models",
        "DeepSeek-V3-Base",
        "pretraining characteristics",
        "Qwen2.5",
        "prompt templates",
        "pretraining biases",
        "Group Relative Policy Optimization (GRPO)",
        "optimization bias",
        "response length",
        "Dr. GRPO",
        "token efficiency",
        "minimalist R1-Zero recipe",
        "AIME 2024",
        "7B base model"
      ]
    },
    "publishedAt": "2025-03-26T13:59:14.000Z",
    "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
    "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20783.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f5392c68b8e0cb3c9977a2",
      "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
      "fullname": "Zichen",
      "name": "lkevinzc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.01956",
      "authors": [
        {
          "_id": "67ee01265839c8a023344aee",
          "user": {
            "_id": "65c38f6c137aba2aee524989",
            "avatarUrl": "/avatars/a93e29f55876df3e65e2532972e057e4.svg",
            "isPro": false,
            "fullname": "Hanyang Wang",
            "user": "hanyang-21",
            "type": "user"
          },
          "name": "Hanyang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T07:52:06.284Z",
          "hidden": false
        },
        {
          "_id": "67ee01265839c8a023344aef",
          "name": "Fangfu Liu",
          "hidden": false
        },
        {
          "_id": "67ee01265839c8a023344af0",
          "name": "Jiawei Chi",
          "hidden": false
        },
        {
          "_id": "67ee01265839c8a023344af1",
          "name": "Yueqi Duan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65c38f6c137aba2aee524989/JKAKb_7rnf6eZT56AF6aM.mp4"
      ],
      "publishedAt": "2025-04-02T17:59:21.000Z",
      "submittedOnDailyAt": "2025-04-03T02:07:36.716Z",
      "title": "VideoScene: 一歩で3Dスキームを生成するビデオディフュージョンモデルの結晶化",
      "submittedOnDailyBy": {
        "_id": "65c38f6c137aba2aee524989",
        "avatarUrl": "/avatars/a93e29f55876df3e65e2532972e057e4.svg",
        "isPro": false,
        "fullname": "Hanyang Wang",
        "user": "hanyang-21",
        "type": "user"
      },
      "summary": "稀疏ビューから3Dスケーンを復元するは、その固有の不備な問題により難しい任務です。傳統的な方法は、この問題を軽減するために特製の解決策（例：ジェネラリケーション調整または前進的な確率的モデル）を開発しましたが、入力ビュー間の最小重なりと視覚情報の不足により性能低下を受けます。幸い、最近のビデオ生成モデルは、3D構造を持つ説得的なビデオクリップを生成することができることでこの課題を解決する可能性があります。大きな事前学習ビデオディフュージョンモデルをもちましたプロダクティブな研究は、ビデオ生成先驚きの可能性を評価し、稀疏ビューから3Dスケーンを生成することを試みました。驚異的な改善を見せるにもかかわらず、それらは推論時間の長さと3D制約の欠落により制限され、効率や再構築アーティファクトが実世界のジェネリック構造に合わないことにより不適切な結果を生み出します。この論文では、VideoSceneを提案し、ビデオディフュージョンモデルを1ステップで3Dスケーンを生成することを目指し、ビデオから3Dへの隙間を結ぶ効率的かつ有効なツールを構築することを目指しています。特に、3Dに関する知識を持つランプフローの蒸し潰し戦略を設計し、推論中の最適なランプ時間ステップを適応的に決定するための動的なデノイズポリシーネットワークを訓練します。拡張した実験は、以前のビデオディフュージョンモデルよりも速くて優れた3Dスケーン生成結果を実現することを示し、将来のビデオから3Dへのアプリケーションの効率的なツールとしての可能性を明らかにしています。プロジェクトページ：https://hanyang-21.github.io/VideoScene",
      "upvotes": 19,
      "discussionId": "67ee012a5839c8a023344bdb",
      "projectPage": "https://hanyang-21.github.io/VideoScene",
      "githubRepo": "https://github.com/hanyang-21/VideoScene",
      "ai_keywords": [
        "video generative models",
        "video diffusion models",
        "3D scenes",
        "sparse views",
        "geometry regularization",
        "feed-forward model",
        "video generative prior",
        "inference time",
        "3D constraint",
        "reconstruction artifacts",
        "VideoScene",
        "3D-aware leap flow distillation",
        "dynamic denoising policy network"
      ]
    },
    "publishedAt": "2025-04-02T13:59:21.000Z",
    "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step",
    "summary": "Recovering 3D scenes from sparse views is a challenging task due to its\ninherent ill-posed problem. Conventional methods have developed specialized\nsolutions (e.g., geometry regularization or feed-forward deterministic model)\nto mitigate the issue. However, they still suffer from performance degradation\nby minimal overlap across input views with insufficient visual information.\nFortunately, recent video generative models show promise in addressing this\nchallenge as they are capable of generating video clips with plausible 3D\nstructures. Powered by large pretrained video diffusion models, some pioneering\nresearch start to explore the potential of video generative prior and create 3D\nscenes from sparse views. Despite impressive improvements, they are limited by\nslow inference time and the lack of 3D constraint, leading to inefficiencies\nand reconstruction artifacts that do not align with real-world geometry\nstructure. In this paper, we propose VideoScene to distill the video diffusion\nmodel to generate 3D scenes in one step, aiming to build an efficient and\neffective tool to bridge the gap from video to 3D. Specifically, we design a\n3D-aware leap flow distillation strategy to leap over time-consuming redundant\ninformation and train a dynamic denoising policy network to adaptively\ndetermine the optimal leap timestep during inference. Extensive experiments\ndemonstrate that our VideoScene achieves faster and superior 3D scene\ngeneration results than previous video diffusion models, highlighting its\npotential as an efficient tool for future video to 3D applications. Project\nPage: https://hanyang-21.github.io/VideoScene",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65c38f6c137aba2aee524989/JKAKb_7rnf6eZT56AF6aM.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01956.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c38f6c137aba2aee524989",
      "avatarUrl": "/avatars/a93e29f55876df3e65e2532972e057e4.svg",
      "fullname": "Hanyang Wang",
      "name": "hanyang-21",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.01724",
      "authors": [
        {
          "_id": "67edf7b6d277de0ec2aa5b6b",
          "name": "Yuxuan Luo",
          "hidden": false
        },
        {
          "_id": "67edf7b6d277de0ec2aa5b6c",
          "name": "Zhengkun Rong",
          "hidden": false
        },
        {
          "_id": "67edf7b6d277de0ec2aa5b6d",
          "name": "Lizhen Wang",
          "hidden": false
        },
        {
          "_id": "67edf7b6d277de0ec2aa5b6e",
          "name": "Longhao Zhang",
          "hidden": false
        },
        {
          "_id": "67edf7b6d277de0ec2aa5b6f",
          "name": "Tianshu Hu",
          "hidden": false
        },
        {
          "_id": "67edf7b6d277de0ec2aa5b70",
          "name": "Yongming Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T13:30:32.000Z",
      "submittedOnDailyAt": "2025-04-03T01:22:04.548Z",
      "title": "ドリーマエター-M1: ハリウッドグイドをもっての全体的的、表現的で強固な人間イメージアニメーション",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "最近の画像ベースの人間アニメーション方法は、写実的な体と顔の動きの合成を実現していますが、細かい全体的な制御可能性、多スケール適応性、長期的な時系列の一致性において重要な欠点があり、これらは表現力と強固性の低下につながっています。私たちは、これらの制限を克服するために、ダイフォーゼンドライバー（DiT）に基づくフレームワーク、DreamActor-M1を提案します。これは、間接的な顔の表現、3D頭部球体、3D体の骨格を統合した組み合わせされた制御信号を用いて、顔の表情と体の動きの強固な制御を実現し、表現的で同一性を保つアニメーションを生成します。スケール適応性において、ポートレートから全身のビューまで様々な体の姿勢と画像スケールを処理するために、変化する解像度とスケールのデータを用いた進歩的な訓練戦略を実施します。外観の制御において、連続するフレームからの動きパターンと補間的な視覚的リファレンスを統合し、複雑な動作中の見えない領域でも長期的な時系列の一致性を確保します。実験は、私たちの方法が最先端の作品を上回り、ポートレート、上半身、全身の生成に表現的な結果を提供し、強固な長期的な一致性を実現していることを示しています。プロジェクトページ：https://grisoon.github.io/DreamActor-M1/。",
      "upvotes": 16,
      "discussionId": "67edf7bcd277de0ec2aa5d7b",
      "ai_keywords": [
        "diffusion transformer (DiT)",
        "hybrid guidance",
        "implicit facial representations",
        "3D head spheres",
        "3D body skeletons",
        "facial expressions",
        "body movements",
        "expressive animations",
        "identity-preserving animations",
        "progressive training strategy",
        "varying resolutions",
        "varying scales",
        "motion patterns",
        "sequential frames",
        "visual references",
        "long-term temporal coherence",
        "long-term consistency",
        "expressive results",
        "upper-body generation",
        "full-body generation"
      ]
    },
    "publishedAt": "2025-04-02T09:30:32.000Z",
    "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation\n  with Hybrid Guidance",
    "summary": "While recent image-based human animation methods achieve realistic body and\nfacial motion synthesis, critical gaps remain in fine-grained holistic\ncontrollability, multi-scale adaptability, and long-term temporal coherence,\nwhich leads to their lower expressiveness and robustness. We propose a\ndiffusion transformer (DiT) based framework, DreamActor-M1, with hybrid\nguidance to overcome these limitations. For motion guidance, our hybrid control\nsignals that integrate implicit facial representations, 3D head spheres, and 3D\nbody skeletons achieve robust control of facial expressions and body movements,\nwhile producing expressive and identity-preserving animations. For scale\nadaptation, to handle various body poses and image scales ranging from\nportraits to full-body views, we employ a progressive training strategy using\ndata with varying resolutions and scales. For appearance guidance, we integrate\nmotion patterns from sequential frames with complementary visual references,\nensuring long-term temporal coherence for unseen regions during complex\nmovements. Experiments demonstrate that our method outperforms the\nstate-of-the-art works, delivering expressive results for portraits,\nupper-body, and full-body generation with robust long-term consistency. Project\nPage: https://grisoon.github.io/DreamActor-M1/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01724.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6570
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01848",
      "authors": [
        {
          "_id": "67edf3d579018bf61e050435",
          "name": "Giulio Starace",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e050436",
          "name": "Oliver Jaffe",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e050437",
          "name": "Dane Sherburn",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e050438",
          "name": "James Aung",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e050439",
          "name": "Jun Shern Chan",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e05043a",
          "name": "Leon Maksin",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e05043b",
          "name": "Rachel Dias",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e05043c",
          "name": "Evan Mays",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e05043d",
          "name": "Benjamin Kinsella",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e05043e",
          "name": "Wyatt Thompson",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e05043f",
          "name": "Johannes Heidecke",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e050440",
          "name": "Amelia Glaese",
          "hidden": false
        },
        {
          "_id": "67edf3d579018bf61e050441",
          "name": "Tejal Patwardhan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T15:55:24.000Z",
      "submittedOnDailyAt": "2025-04-03T01:05:22.442Z",
      "title": "PaperBench: AIの研究を再現する能力の評価",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "PaperBenchは、AIアガントが最先端のAI研究を再現する能力を評価するベンチマークです。アガントは、2024年ICMLのスポットライトとオーラル論文をショートカットから再現する必要があります。これには、論文の貢献を理解し、コードベースを開発し、実験を成功して行うことが含まれます。対象的な評価を行うために、各再現タスクを明確な評価基準で小さなサブタスクに分けるレビューガイドを開発しました。総計では、PaperBenchは8,316個の個別評価可能なタスクを含みます。レビューガイドは、各ICML論文の著者と共同に開発され、正確性とリアリティを確保しています。スケーラブルな評価を可能にするために、LLMベースの判定者を開発し、判定者の性能を評価するために別々のベンチマークを作成しました。PaperBench上で数々の先端モデルを評価し、最良の実績を示したアガントは、Claude 3.5 Sonnet (New)と開源スキームを使用したもので、平均再現スコアが21.0%でした。最後に、最高のMLプロフェッショナルを招き、PaperBenchの一部を試みることで、モデルはまだ人間の基準を超えていないことを確認しました。私たちは、https://github.com/openai/preparedness{開源コード}を公開し、AIアガントのAI工学能力を理解するための将来の研究を促進することを目的としています。",
      "upvotes": 15,
      "discussionId": "67edf3d679018bf61e0504c0",
      "ai_keywords": [
        "anLM-based judge",
        "replication attempts"
      ]
    },
    "publishedAt": "2025-04-02T11:55:24.000Z",
    "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
    "summary": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\nhttps://github.com/openai/preparedness{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01848.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6570
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00824",
      "authors": [
        {
          "_id": "67ede79d21d7e74ee3e2832a",
          "name": "Yubo Wang",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832b",
          "name": "Xueguang Ma",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832c",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832d",
          "name": "Huaye Zeng",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832e",
          "name": "Zhiheng Lyu",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e2832f",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e28330",
          "name": "Benjamin Schneider",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e28331",
          "name": "Yi Lu",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e28332",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "67ede79d21d7e74ee3e28333",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/uqsPar9J0O8bRmITeMkzM.png"
      ],
      "publishedAt": "2025-04-01T14:12:14.000Z",
      "submittedOnDailyAt": "2025-04-03T00:13:20.491Z",
      "title": "ScholarCopilot: 学術書籍の書き上げをさらに進めるための大規模言語モデルの訓練と正確な引用",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "学術書面作成には、コホエンスなテキスト生成と適切な文献引用が必要です。過去のRetrieval-Augmented Generation (RAG)システムは一般的なテキスト生成の事実的な正確性を大幅に向上させましたが、その効果が学術的な専門的な書面作成に適切に利用できるようには限られています。本研究では、ScholarCopilotという一ノイドフレームワークを紹介します。これは、既存の大規模な言語モデルを強化して、正確でコンテキストに適切な引用を含む専門的な学術論文を生成することを目的としています。ScholarCopilotは、学術的な参照をドキュメントから検索するために、生成テキスト中の[RET]トークンを生成し、その表現を用いて適切な引用を検索します。検索された参照はモデルに入力され、生成プロセスを増強します。生成タスクと引用タスクを一つのフレームワークで共に最適化し、効率を向上させます。arXivから500Kの論文で訓練されたモデルは、評価データセットでのトップ1検索正確率は40.1%で、E5-Mistral-7B-Instruct (15.0%)とBM25 (9.8%)よりも優れています。1,000の学術書面サンプルデータセットでは、ScholarCopilotは生成質量の評価(関連性、コホエンス、学術的厳密性、完全性、創新性)で16.2/25です。10倍以上のパラメーターを持つモデルよりも上回ります。Qwen-2.5-72B-Instruct (15.8/25)。人間の評価も、引用の再現性、書面の効率、全体のユーザー経験においてScholarCopilotの優れた性能を確認し、我々のアプローチの効果を確認しました。",
      "upvotes": 15,
      "discussionId": "67ede79e21d7e74ee3e2838c",
      "githubRepo": "https://github.com/TIGER-AI-Lab/ScholarCopilot",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "ScholarCopilot",
        "large language models",
        "retrieval token [RET]",
        "scholarly references",
        "top-1 retrieval accuracy",
        "arXiv",
        "generation quality",
        "relevance",
        "coherence",
        "academic rigor",
        "completeness",
        "innovation",
        "citation recall",
        "writing efficiency",
        "user experience"
      ]
    },
    "publishedAt": "2025-04-01T10:12:14.000Z",
    "title": "ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations",
    "summary": "Academic writing requires both coherent text generation and precise citation\nof relevant literature. Although recent Retrieval-Augmented Generation (RAG)\nsystems have significantly improved factual accuracy in general-purpose text\ngeneration, their capacity to adequately support professional academic writing\nremains limited. In this work, we introduce ScholarCopilot, a unified framework\ndesigned to enhance existing large language models for generating professional\nacademic articles with accurate and contextually relevant citations.\nScholarCopilot dynamically determines when to retrieve scholarly references by\ngenerating a retrieval token [RET], and then utilizes its representation to\nlook up relevant citations from a database. The retrieved references are fed\ninto the model to augment the generation process. We jointly optimize both the\ngeneration and citation tasks within a single framework to increase efficiency.\nTrained on 500K papers from arXiv, our model achieves a top-1 retrieval\naccuracy of 40.1% on our evaluation dataset, outperforming baselines such as\nE5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic\nwriting samples, ScholarCopilot scores 16.2/25 in generation quality (measured\nacross relevance, coherence, academic rigor, completeness, and innovation),\nsurpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct\n(15.8/25). Human studies also confirm ScholarCopilot's superior performance in\ncitation recall, writing efficiency, and overall user experience, confirming\nthe effectiveness of our approach.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/uqsPar9J0O8bRmITeMkzM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00824.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01934",
      "authors": [
        {
          "_id": "67edfe07f5d1509d1a990178",
          "name": "Runhui Huang",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a990179",
          "name": "Chunwei Wang",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a99017a",
          "name": "Junwei Yang",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a99017b",
          "name": "Guansong Lu",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a99017c",
          "name": "Yunlong Yuan",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a99017d",
          "name": "Jianhua Han",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a99017e",
          "name": "Lu Hou",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a99017f",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a990180",
          "name": "Lanqing Hong",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a990181",
          "name": "Hengshuang Zhao",
          "hidden": false
        },
        {
          "_id": "67edfe07f5d1509d1a990182",
          "name": "Hang Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T17:45:00.000Z",
      "submittedOnDailyAt": "2025-04-03T01:58:02.658Z",
      "title": "ILLUME+: 双重ビジュアルトークナリゼーションとディフュージョンリファインメントをもつ統一ユニット MLLM を照明する",
      "submittedOnDailyBy": {
        "_id": "630f0542cc8ed75decb03b68",
        "avatarUrl": "/avatars/f76c3603b2700591f33a8a931f7ca664.svg",
        "isPro": false,
        "fullname": "huangrh9",
        "user": "huangrh9",
        "type": "user"
      },
      "summary": "ILLUME+は、双重ビジュアルトークナイザルとディフュージョンデコーダーを使用して、深いセマンティック理解と高品質画像生成の両方を改善します。現在の統一モデルは、理解、生成、編集の3つの基本的な能力を同時に統一モデルで扱うことが難しかった。ChameleonやEMU3のようなモデルはVQGANを使用して画像の離散化を実現していますが、深いセマンティックインタラクションの不足により、LLaVAよりも視覚理解タスクに落ち着かないことがあります。これを軽減するために、LaViTとILLUMEはセマンティックエンコーダーを使用してトークナイザルを実現していますが、画像編集においてテクスチャ保存が不良であるため、問題があります。一方、Janusシリーズは入力と出力の画像表現を分離して、間接結合された画像-テキストの理解と生成を流れ通して処理する能力を限定しています。相反的に、ILLUME+は、フィンエグリードテクスチャとテキスト対応されたセマンティックを保つ同時に、多様性理解と生成を可能にする粗至微画像表現戦略を導入しています。また、ディフュージョンモデルを画像デターマイカナイザーとして使用して生成品質の向上と効率的な超解像処理を実現しています。ILLUME+は、統一的なMLLM内で継続的な入力と離散的な出力シナリオを採用し、ビジョントークナイザー、MLLM、ディフュージョンデコーダーの適応的な訓練プロセスを採用しています。この設計は、多様なタスクで柔軟かつ効率的なコンテキスト関係の画像編集と生成を可能にします。ILLUME+ (3B)は、現在の統一モデルと専門モデルとの間で多様性理解、生成、編集ベンチマークで競争的な性能を示しています。強い性能を持っているILLUME+は、将来の多様性アプリケーションのスケーラブルで機能広範囲な基盤を提供しています。プロジェクトページ：https://illume-unified-mllm.github.io/",
      "upvotes": 12,
      "discussionId": "67edfe09f5d1509d1a990214",
      "projectPage": "https://illume-unified-mllm.github.io/",
      "githubRepo": "https://github.com/illume-unified-mllm/ILLUME_plus",
      "ai_keywords": [
        "dual visual tokenization",
        "diffusion decoder",
        "deep semantic understanding",
        "high-fidelity image generation",
        "VQGAN",
        "LaViT",
        "semantic encoders",
        "DualViTok",
        "texture preservation",
        "multimodal understanding",
        "continuous-input, discrete-output scheme",
        "MLLM",
        "progressive training procedure",
        "dynamic resolution",
        "context-aware image editing"
      ]
    },
    "publishedAt": "2025-04-02T13:45:00.000Z",
    "title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and\n  Diffusion Refinement",
    "summary": "We present ILLUME+ that leverages dual visual tokenization and a diffusion\ndecoder to improve both deep semantic understanding and high-fidelity image\ngeneration. Existing unified models have struggled to simultaneously handle the\nthree fundamental capabilities in a unified model: understanding, generation,\nand editing. Models like Chameleon and EMU3 utilize VQGAN for image\ndiscretization, due to the lack of deep semantic interaction, they lag behind\nspecialist models like LLaVA in visual understanding tasks. To mitigate this,\nLaViT and ILLUME employ semantic encoders for tokenization, but they struggle\nwith image editing due to poor texture preservation. Meanwhile, Janus series\ndecouples the input and output image representation, limiting their abilities\nto seamlessly handle interleaved image-text understanding and generation. In\ncontrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which\npreserves both fine-grained textures and text-aligned semantics while enabling\na coarse-to-fine image representation strategy for multimodal understanding and\ngeneration. Additionally, we employ a diffusion model as the image detokenizer\nfor enhanced generation quality and efficient super-resolution. ILLUME+ follows\na continuous-input, discrete-output scheme within the unified MLLM and adopts a\nprogressive training procedure that supports dynamic resolution across the\nvision tokenizer, MLLM, and diffusion decoder. This design allows for flexible\nand efficient context-aware image editing and generation across diverse tasks.\nILLUME+ (3B) exhibits competitive performance against existing unified MLLMs\nand specialized models across multimodal understanding, generation, and editing\nbenchmarks. With its strong performance, ILLUME+ provides a scalable and\nversatile foundation for future multimodal applications. Project Page:\nhttps://illume-unified-mllm.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01934.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "630f0542cc8ed75decb03b68",
      "avatarUrl": "/avatars/f76c3603b2700591f33a8a931f7ca664.svg",
      "fullname": "huangrh9",
      "name": "huangrh9",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01204",
      "authors": [
        {
          "_id": "67edf4bf5e87fcaa485a0ad9",
          "name": "Xuan Li",
          "hidden": false
        },
        {
          "_id": "67edf4bf5e87fcaa485a0ada",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "67edf4bf5e87fcaa485a0adb",
          "name": "Tsung-Yi Lin",
          "hidden": false
        },
        {
          "_id": "67edf4bf5e87fcaa485a0adc",
          "name": "Yongxin Chen",
          "hidden": false
        },
        {
          "_id": "67edf4bf5e87fcaa485a0add",
          "name": "Chenfanfu Jiang",
          "hidden": false
        },
        {
          "_id": "67edf4bf5e87fcaa485a0ade",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "67edf4bf5e87fcaa485a0adf",
          "name": "Donglai Xiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T21:37:57.000Z",
      "submittedOnDailyAt": "2025-04-03T01:09:40.312Z",
      "title": "アーチュレーテッド・キネマティクス・ディスティルデーションよりビデオ・ディフュージョン・モデルから",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Articulated Kinematics Distillation (AKD) を紹介します。AKD は、スケルトンベースのアニメーションと現代の生成モデルの強みを統合して、高品質なキャラクターアニメーションを生成するフレームワークです。AKD は、リグされた 3D アセットに対してスケルトンベースの表現を使用し、関節レベルの制御を焦点として Degrees of Freedom (DoFs) を大幅に削減し、効率的な、一貫した動作合成を可能にします。Score Distillation Sampling (SDS) と予え学習されたビデオディフュージョンモデルを使用して、AKD は複雑な、アーチュレートされた動作を結構的性質を維持しながら結構化を克服します。このアプローチは、物理ベースのシミュレーションと自然に相容し、物理的に可能な相互作用を確認できます。実験は、AKD は現在のテキストから 4D の生成において既存のワークと比較して、上位の 3D 一致性と動作質量を達成します。プロジェクトページは、https://research.nvidia.com/labs/dir/akd/ にあります。",
      "upvotes": 11,
      "discussionId": "67edf4c65e87fcaa485a0cb7",
      "ai_keywords": [
        "skeleton-based representation",
        "Degrees of Freedom (DoFs)",
        "joint-level control",
        "Score Distillation Sampling (SDS)",
        "video diffusion models",
        "articulated motions",
        "structural integrity",
        "physics-based simulation",
        "text-to-4D generation"
      ]
    },
    "publishedAt": "2025-04-01T17:37:57.000Z",
    "title": "Articulated Kinematics Distillation from Video Diffusion Models",
    "summary": "We present Articulated Kinematics Distillation (AKD), a framework for\ngenerating high-fidelity character animations by merging the strengths of\nskeleton-based animation and modern generative models. AKD uses a\nskeleton-based representation for rigged 3D assets, drastically reducing the\nDegrees of Freedom (DoFs) by focusing on joint-level control, which allows for\nefficient, consistent motion synthesis. Through Score Distillation Sampling\n(SDS) with pre-trained video diffusion models, AKD distills complex,\narticulated motions while maintaining structural integrity, overcoming\nchallenges faced by 4D neural deformation fields in preserving shape\nconsistency. This approach is naturally compatible with physics-based\nsimulation, ensuring physically plausible interactions. Experiments show that\nAKD achieves superior 3D consistency and motion quality compared with existing\nworks on text-to-4D generation. Project page:\nhttps://research.nvidia.com/labs/dir/akd/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01204.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6570
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01308",
      "authors": [
        {
          "_id": "67ede544ed9c94861b82b29f",
          "user": {
            "_id": "64060b49a577649430bf6974",
            "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
            "isPro": false,
            "fullname": "Jiawei Wang",
            "user": "Jarvis1111",
            "type": "user"
          },
          "name": "Jiawei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T07:52:27.815Z",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a0",
          "name": "Yushen Zuo",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a1",
          "user": {
            "_id": "64756323d815855e4ef945a0",
            "avatarUrl": "/avatars/29f5150805dafce2b3f9da441c8be988.svg",
            "isPro": false,
            "fullname": "Chai",
            "user": "AllenChai",
            "type": "user"
          },
          "name": "Yuanjun Chai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T07:52:23.962Z",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a2",
          "name": "Zhendong Liu",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a3",
          "user": {
            "_id": "6528ce81598467feb33d992d",
            "avatarUrl": "/avatars/e98a7bf16e6fd5118e861d562f93bb9b.svg",
            "isPro": false,
            "fullname": "Yicheng Fu",
            "user": "sofyc",
            "type": "user"
          },
          "name": "Yichen Fu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-03T01:32:57.886Z",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a4",
          "name": "Yichun Feng",
          "hidden": false
        },
        {
          "_id": "67ede544ed9c94861b82b2a5",
          "name": "Kin-man Lam",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-02T02:35:19.000Z",
      "submittedOnDailyAt": "2025-04-03T00:10:56.307Z",
      "title": "グイオーシャンノイズに対するパーバランスバードアタックの脆弱性によるビジョン・ラングハウスモデルのセキュリティ保護",
      "submittedOnDailyBy": {
        "_id": "64060b49a577649430bf6974",
        "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
        "isPro": false,
        "fullname": "Jiawei Wang",
        "user": "Jarvis1111",
        "type": "user"
      },
      "summary": "ビジョン・ラングワードモデル（VLMs）は、ビジョン情報を統合して大規模言語モデル（LLMs）の機能を拡張しますが、噪音や破損画像を処理する際には、ジャイルブレイク攻撃に脆弱です。現在のVLMsは、これらの攻撃を回避するために訓練時に安全対策を採用していますが、噪音付きビジョン入力に関連する脆弱性は見落とされています。本稿では、噪音付き訓練の欠陥が重要な安全ギャップを原因としていることを明らかにします：多くのVLMsは、ガウスノイズなどの簡単な変形に脆弱です。この挑戦に対して、Robust-VLGuardを提案します。これは、対応したかつ不対応した画像・テキストペアを含む多様性の安全データセットで、噪音付きの微調節を行い、攻撃成功率を減少しながらVLMsの機能を保つものです。より強力な最適化基づいた可視化変形攻撃に対しては、DiffPure-VLMを提案します。これは、拡散モデルを利用して、敵意的な変形をガウスノイズのようなノイズに変換し、噪音付きの安全ファイナルチューニングで防御できるものです。実験結果によると、拡散モデルの分布移動性は、微調節されたVLMsと非常に一致し、変形の強度により変化する対抗的な変形を显著に軽減できます。データセットとコードは、https://github.com/JarvisUSTC/DiffPure-RobustVLM から利用できます。",
      "upvotes": 10,
      "discussionId": "67ede549ed9c94861b82b433",
      "githubRepo": "https://github.com/JarvisUSTC/DiffPure-RobustVLM",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "Large Language Models (LLMs)",
        "noise-augmented training",
        "Gaussian noise",
        "Robust-VLGuard",
        "multimodal safety dataset",
        "aligned / misaligned image-text pairs",
        "noise-augmented fine-tuning",
        "diffusion models",
        "DiffPure-VLM",
        "diffusion model",
        "distribution-shifting property",
        "adversarial perturbations"
      ]
    },
    "publishedAt": "2025-04-01T22:35:19.000Z",
    "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks",
    "summary": "Vision-Language Models (VLMs) extend the capabilities of Large Language\nModels (LLMs) by incorporating visual information, yet they remain vulnerable\nto jailbreak attacks, especially when processing noisy or corrupted images.\nAlthough existing VLMs adopt security measures during training to mitigate such\nattacks, vulnerabilities associated with noise-augmented visual inputs are\noverlooked. In this work, we identify that missing noise-augmented training\ncauses critical security gaps: many VLMs are susceptible to even simple\nperturbations such as Gaussian noise. To address this challenge, we propose\nRobust-VLGuard, a multimodal safety dataset with aligned / misaligned\nimage-text pairs, combined with noise-augmented fine-tuning that reduces attack\nsuccess rates while preserving functionality of VLM. For stronger\noptimization-based visual perturbation attacks, we propose DiffPure-VLM,\nleveraging diffusion models to convert adversarial perturbations into\nGaussian-like noise, which can be defended by VLMs with noise-augmented safety\nfine-tuning. Experimental results demonstrate that the distribution-shifting\nproperty of diffusion model aligns well with our fine-tuned VLMs, significantly\nmitigating adversarial perturbations across varying intensities. The dataset\nand code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01308.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64060b49a577649430bf6974",
      "avatarUrl": "/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg",
      "fullname": "Jiawei Wang",
      "name": "Jarvis1111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2405.20216",
      "authors": [
        {
          "_id": "666e9b96bc840e67481f20f3",
          "user": {
            "_id": "6662b3ee280fb71780b85ef8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/wDr9CDlL40_gp-NBRm7gB.png",
            "isPro": false,
            "fullname": "Sanghyeon Na",
            "user": "sanghyeonna",
            "type": "user"
          },
          "name": "Sanghyeon Na",
          "status": "claimed_verified",
          "statusLastChangedAt": "2024-07-02T11:59:46.873Z",
          "hidden": false
        },
        {
          "_id": "666e9b96bc840e67481f20f4",
          "name": "Yonggyu Kim",
          "hidden": false
        },
        {
          "_id": "666e9b96bc840e67481f20f5",
          "name": "Hyunjoon Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2024-05-30T16:18:05.000Z",
      "submittedOnDailyAt": "2025-04-03T06:19:33.352Z",
      "title": "Boost Your Own Human Image Generation Model via Direct Preference Optimization with AI Feedback",
      "submittedOnDailyBy": {
        "_id": "63a369d98c0c89dcae3b8329",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png",
        "isPro": false,
        "fullname": "Adina Yakefu",
        "user": "AdinaY",
        "type": "user"
      },
      "summary": "高品質の人間画像をテキストから画像に変換（T2I）の方法を用いて生成することは重要であるが、難しい課題です。一般的な画像生成と異なり、人間画像の合成は、人間の姿勢、解剖学、テキストプライムとの一致に関する厳格な基準を満たす必要があり、実写的な結果を達成することが特に難しい。最近、拡散モデルに基づくT2I生成の進歩は希望のあることになっていますが、人間特有の好みに合わせることでも難しい問題が残っています。本論文では、Direct Preference Optimization（DPO）を特に人間画像生成に適した新しいアプローチを紹介します。具体的には、DPOデータセットの構築に効率的な方法を提案し、高額な人間のフィードバックを必要とさせないようにします。また、DPOのトレーニングプロセスを効率的にするために、フィードバックの最小化と画像の忠実度の向上を図る改良された損失関数を提案します。我々の方法は、人間画像の生成に対して、個別化されたテキストから画像生成にも実用性と効果性を示します。詳細な評価を通じて、我々のアプローチが人間画像生成の状態を大幅に進め、自然な解剖学、姿勢、テキスト画像の一致において上位の結果を達成していることを示します。",
      "upvotes": 8,
      "discussionId": "666e9b9cbc840e67481f2329",
      "ai_keywords": [
        "diffusion models",
        "Direct Preference Optimization (DPO)",
        "DPO dataset",
        "specialized DPO dataset",
        "modified loss function",
        "artifacts",
        "image fidelity",
        "personalized text-to-image generation",
        "natural anatomies",
        "poses",
        "text-image alignment"
      ]
    },
    "publishedAt": "2024-05-30T12:18:05.000Z",
    "title": "Boost Your Own Human Image Generation Model via Direct Preference\n  Optimization with AI Feedback",
    "summary": "The generation of high-quality human images through text-to-image (T2I)\nmethods is a significant yet challenging task. Distinct from general image\ngeneration, human image synthesis must satisfy stringent criteria related to\nhuman pose, anatomy, and alignment with textual prompts, making it particularly\ndifficult to achieve realistic results. Recent advancements in T2I generation\nbased on diffusion models have shown promise, yet challenges remain in meeting\nhuman-specific preferences. In this paper, we introduce a novel approach\ntailored specifically for human image generation utilizing Direct Preference\nOptimization (DPO). Specifically, we introduce an efficient method for\nconstructing a specialized DPO dataset for training human image generation\nmodels without the need for costly human feedback. We also propose a modified\nloss function that enhances the DPO training process by minimizing artifacts\nand improving image fidelity. Our method demonstrates its versatility and\neffectiveness in generating human images, including personalized text-to-image\ngeneration. Through comprehensive evaluations, we show that our approach\nsignificantly advances the state of human image generation, achieving superior\nresults in terms of natural anatomies, poses, and text-image alignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.20216.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 526
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23135",
      "authors": [
        {
          "_id": "67eb3d2110032c28d1ea109f",
          "user": {
            "_id": "628ece6054698ce61d1e7be3",
            "avatarUrl": "/avatars/c6ab33843fd0d8ef003650c1094214c0.svg",
            "isPro": false,
            "fullname": "Ao Wang",
            "user": "jameslahm",
            "type": "user"
          },
          "name": "Ao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T16:11:10.373Z",
          "hidden": false
        },
        {
          "_id": "67eb3d2110032c28d1ea10a0",
          "name": "Hui Chen",
          "hidden": false
        },
        {
          "_id": "67eb3d2110032c28d1ea10a1",
          "name": "Zijia Lin",
          "hidden": false
        },
        {
          "_id": "67eb3d2110032c28d1ea10a2",
          "name": "Jungong Han",
          "hidden": false
        },
        {
          "_id": "67eb3d2110032c28d1ea10a3",
          "name": "Guiguang Ding",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/628ece6054698ce61d1e7be3/5E8ul0VN002EtCsoCp3mt.png"
      ],
      "publishedAt": "2025-03-29T16:00:54.000Z",
      "submittedOnDailyAt": "2025-04-03T00:26:03.944Z",
      "title": "LSNet: シー リグハード、フォカス サイボード",
      "submittedOnDailyBy": {
        "_id": "628ece6054698ce61d1e7be3",
        "avatarUrl": "/avatars/c6ab33843fd0d8ef003650c1094214c0.svg",
        "isPro": false,
        "fullname": "Ao Wang",
        "user": "jameslahm",
        "type": "user"
      },
      "summary": "ビジョンネットワークの設計、特に畳み込みニューラルネットワークとビジョントランスフォーマーは、コンピュータビジョンの分野において大幅に進展しています。しかし、それらの複雑な計算は実用的な機能の実装に課題をもたらし、特にリアルタイムアプリケーションでは厳しいチャレンジを呈しています。この問題に対処するために、研究者は様々な軽量および効率的なネットワークの設計を検討しています。しかし、現在の軽量モデルは主に自己アテンション機構と畳み込みを利用してトークンの混合を行います。この依存関係は、軽量ネットワークの認識とアグラゲーションプロセスにおいて効果と効率の制限をもたらし、限定的な計算バッジ内で性能と効率のバランスを保つには妨げています。この論文では、効率的な人間の視覚システムにおける動的なヒューマノイド視覚能力をモデルとして、「大きく見、小さく焦点を当て」という戦略を提案しています。LS（Large-Small）畳み込みを導入し、大キャンネルの認識と小キャンネルのアグラゲーションを組み合わせています。これは、広範囲の認識情報を効率的に捉え、動的で複雑な可視的表現において精確な特徴量のアグラゲーションを実現することができ、これにより可視的情報の効果的な処理を可能にします。LS畳み込みに基づいて、LSNet（Large-Small Network）という新しい軽量モデルの家族を提案します。拡張した実験は、LSNetは現在の軽量ネットワークと比較して多様なビジョンタスクで優れた性能と効率を収めていることを示しています。コードとモデルは、https://github.com/jameslahm/lsnet から利用可能です。",
      "upvotes": 3,
      "discussionId": "67eb3d2310032c28d1ea1108",
      "projectPage": "https://github.com/THU-MIG/lsnet",
      "githubRepo": "https://github.com/THU-MIG/lsnet",
      "ai_keywords": [
        "Convolutional Neural Networks",
        "Vision Transformers",
        "lightweight and efficient network designs",
        "self-attention mechanisms",
        "token mixing",
        "small-kernel aggregation",
        "dynamic heteroscale vision ability",
        "human vision system",
        "``See Large, Focus Small'' strategy",
        "LS (\\textbf{L}arge-\\textbf{S}mall) convolution",
        "large-kernel perception",
        "precise feature aggregation",
        "visual representations",
        "efficient processing of visual information",
        "LSNet",
        "superior performance",
        "efficiency"
      ]
    },
    "publishedAt": "2025-03-29T12:00:54.000Z",
    "title": "LSNet: See Large, Focus Small",
    "summary": "Vision network designs, including Convolutional Neural Networks and Vision\nTransformers, have significantly advanced the field of computer vision. Yet,\ntheir complex computations pose challenges for practical deployments,\nparticularly in real-time applications. To tackle this issue, researchers have\nexplored various lightweight and efficient network designs. However, existing\nlightweight models predominantly leverage self-attention mechanisms and\nconvolutions for token mixing. This dependence brings limitations in\neffectiveness and efficiency in the perception and aggregation processes of\nlightweight networks, hindering the balance between performance and efficiency\nunder limited computational budgets. In this paper, we draw inspiration from\nthe dynamic heteroscale vision ability inherent in the efficient human vision\nsystem and propose a ``See Large, Focus Small'' strategy for lightweight vision\nnetwork design. We introduce LS (Large-Small) convolution,\nwhich combines large-kernel perception and small-kernel aggregation. It can\nefficiently capture a wide range of perceptual information and achieve precise\nfeature aggregation for dynamic and complex visual representations, thus\nenabling proficient processing of visual information. Based on LS convolution,\nwe present LSNet, a new family of lightweight models. Extensive experiments\ndemonstrate that LSNet achieves superior performance and efficiency over\nexisting lightweight networks in various vision tasks. Codes and models are\navailable at https://github.com/jameslahm/lsnet.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/628ece6054698ce61d1e7be3/5E8ul0VN002EtCsoCp3mt.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23135.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "628ece6054698ce61d1e7be3",
      "avatarUrl": "/avatars/c6ab33843fd0d8ef003650c1094214c0.svg",
      "fullname": "Ao Wang",
      "name": "jameslahm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00406",
      "authors": [
        {
          "_id": "67ee3e63e7defc1b8655c8f6",
          "name": "Jiuzhou Han",
          "hidden": false
        },
        {
          "_id": "67ee3e63e7defc1b8655c8f7",
          "name": "Wray Buntine",
          "hidden": false
        },
        {
          "_id": "67ee3e63e7defc1b8655c8f8",
          "name": "Ehsan Shareghi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T04:05:03.000Z",
      "submittedOnDailyAt": "2025-04-03T06:26:28.255Z",
      "title": "VerifiAgent: 言語モデル計算の統一バリデーションアガント",
      "submittedOnDailyBy": {
        "_id": "63b0e5a7f2eb87a4d695398a",
        "avatarUrl": "/avatars/3a03fbb3edbb4aad848cd63c0bce6853.svg",
        "isPro": false,
        "fullname": "Jiuzhou Han",
        "user": "Jiuzhouh",
        "type": "user"
      },
      "summary": "大語言モデルは驚異的な理由論能力を示しますが、通常は不信頼または不正確な回答を生成します。現在の検証方法は通常モデル専用かドメイン制限されており、計算資源の大量の必要性と多様な理由論タスクのスケーラビリティの欠如を含みます。これらの制限を解決するために、私たちはVerifiAgentを提案します。VerifiAgentは、モデルの完全性と一貫性を評価するメタ検証と、理由論の種類に基づいて適切な検証ツールを自動的に選択するツールベースの適応検証の2つのレベルを統合した一連の検証アガントです。この適応的なアプローチは、異なる検証シナリオでのエフカイジャクションと強固性を確保します。実験結果から、VerifiAgentはすべての理由論タスクで基準検証方法（例：演繹検証、後退検証）を上回ります。また、検証結果からのフィードバックを活用することで理由論の精度を進一步に向上させることができます。VerifiAgentは、数学的な理由論領域の既存のプロセス報酬モデルと比較して、生成されたサンプルとコストが少なくてもより良い結果を実現することができます。コードは、https://github.com/Jiuzhouh/VerifiAgentにアクセスできます。",
      "upvotes": 1,
      "discussionId": "67ee3e64e7defc1b8655c93b",
      "githubRepo": "https://github.com/Jiuzhouh/VerifiAgent",
      "ai_keywords": [
        "reasoning capabilities",
        "verification methods",
        "unified verification agent",
        "meta-verification",
        "completeness",
        "consistency",
        "tool-based adaptive verification",
        "verification tools",
        "mathematical reasoning",
        "logical reasoning",
        "commonsense reasoning",
        "adaptive approach",
        "verification scenarios",
        "baseline verification methods",
        "deductive verifier",
        "backward verifier",
        "reasoning accuracy",
        "feedback",
        "inference scaling",
        "generated samples",
        "process reward models"
      ]
    },
    "publishedAt": "2025-04-01T00:05:03.000Z",
    "title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning",
    "summary": "Large language models demonstrate remarkable reasoning capabilities but often\nproduce unreliable or incorrect responses. Existing verification methods are\ntypically model-specific or domain-restricted, requiring significant\ncomputational resources and lacking scalability across diverse reasoning tasks.\nTo address these limitations, we propose VerifiAgent, a unified verification\nagent that integrates two levels of verification: meta-verification, which\nassesses completeness and consistency in model responses, and tool-based\nadaptive verification, where VerifiAgent autonomously selects appropriate\nverification tools based on the reasoning type, including mathematical,\nlogical, or commonsense reasoning. This adaptive approach ensures both\nefficiency and robustness across different verification scenarios. Experimental\nresults show that VerifiAgent outperforms baseline verification methods (e.g.,\ndeductive verifier, backward verifier) among all reasoning tasks. Additionally,\nit can further enhance reasoning accuracy by leveraging feedback from\nverification results. VerifiAgent can also be effectively applied to inference\nscaling, achieving better results with fewer generated samples and costs\ncompared to existing process reward models in the mathematical reasoning\ndomain. Code is available at https://github.com/Jiuzhouh/VerifiAgent",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00406.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b0e5a7f2eb87a4d695398a",
      "avatarUrl": "/avatars/3a03fbb3edbb4aad848cd63c0bce6853.svg",
      "fullname": "Jiuzhou Han",
      "name": "Jiuzhouh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.18817",
      "authors": [
        {
          "_id": "67ede492bdd88c72dc99fbd7",
          "user": {
            "_id": "654b4c9cfabd2cc66874806c",
            "avatarUrl": "/avatars/c48b423d3657ee27eed0d3d83738ec2e.svg",
            "isPro": false,
            "fullname": "jeonghyeon kim",
            "user": "mawjdgus",
            "type": "user"
          },
          "name": "Jeonghyeon Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-03T07:52:31.201Z",
          "hidden": false
        },
        {
          "_id": "67ede492bdd88c72dc99fbd8",
          "name": "Sangheum Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T16:00:21.000Z",
      "submittedOnDailyAt": "2025-04-03T06:37:34.505Z",
      "title": "多モーダル表現のクロスモーダルアラインメントによる拡張オウィド検出",
      "submittedOnDailyBy": {
        "_id": "654b4c9cfabd2cc66874806c",
        "avatarUrl": "/avatars/c48b423d3657ee27eed0d3d83738ec2e.svg",
        "isPro": false,
        "fullname": "jeonghyeon kim",
        "user": "mawjdgus",
        "type": "user"
      },
      "summary": "先週の研究では、分布外検出（OoDD）については主にシングルモーダリティモデルを中心にしていました。最近、CLIPなどの大規模な予っちゅうビジョン・ラベルモデルの登場に伴い、ゼロショットとプロンプト学習のステラテジーを用いたモバイドル表現を利用したOoDD手法が出現しました。しかし、これらの手法は通常、学習済み重みを固定するか、それを一部だけ調整することを通じて行われ、これはダウンストリームデータセットに対しては最適ではありません。本論文では、モバイドルの調整（MMFT）が顕著なOoDD性能を実現できることを主張します。最近の研究では、OoDDに関する調整方法の影響を示しているが、性能向上の大幅な可能性が残っています。な\\\"ive fine-tuning方法の限界を調査し、その為に、調整方法が学習済み知識を完全に活用できない理由を明らかにします。実験的な分析により、この問題は分布内（ID）埋め込みのモーデル間の間違いに起因することがあることが示唆されます。これを解決するために、IDデータの画像とテキスト埋め込み間の距離を正規化することで、クロスモードアライメントを強化する訓練目標を提案します。この調整は、異なるモード（テキストと画像）からの類似な意味をより近いように超球面表現空間でアライメントし、プレトレーンドテキスト情報をより良く利用することを促進します。提案された正規化は、超球面上のエネルギーベースモデルの最大尤度推定に対応していることを理論的に示します。ImageNet-1k OoDベンチマークデータセットを用いて、我々の方法は、プレトレーンド知識を利用する後処理OoDD手法（例：NegLabel）を組み合わせて、現在の手法を大幅に超え、最先端のOoDD性能と高いID精度を実現します。",
      "upvotes": 1,
      "discussionId": "67ede493bdd88c72dc99fc2d",
      "githubRepo": "https://github.com/ma-kjh/CMA-OoDD",
      "ai_keywords": [
        "out-of-distribution detection (OoDD)",
        "single-modality models",
        "large-scale pretrained vision-language models",
        "CLIP",
        "zero-shot learning",
        "prompt learning",
        "multi-modal fine-tuning (MMFT)",
        "downstream datasets",
        "fine-tuning methods",
        "modality gap",
        "in-distribution (ID) embeddings",
        "cross-modal alignment",
        "regularization",
        "image and text embeddings",
        "hyperspherical representation space",
        "energy-based model",
        "NegLabel",
        "post-hoc OoDD approaches",
        "ImageNet-1k",
        "state-of-the-art OoDD performance",
        "ID accuracy"
      ]
    },
    "publishedAt": "2025-03-24T12:00:21.000Z",
    "title": "Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal\n  Representations",
    "summary": "Prior research on out-of-distribution detection (OoDD) has primarily focused\non single-modality models. Recently, with the advent of large-scale pretrained\nvision-language models such as CLIP, OoDD methods utilizing such multi-modal\nrepresentations through zero-shot and prompt learning strategies have emerged.\nHowever, these methods typically involve either freezing the pretrained weights\nor only partially tuning them, which can be suboptimal for downstream datasets.\nIn this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve\nnotable OoDD performance. Despite some recent works demonstrating the impact of\nfine-tuning methods for OoDD, there remains significant potential for\nperformance improvement. We investigate the limitation of na\\\"ive fine-tuning\nmethods, examining why they fail to fully leverage the pretrained knowledge.\nOur empirical analysis suggests that this issue could stem from the modality\ngap within in-distribution (ID) embeddings. To address this, we propose a\ntraining objective that enhances cross-modal alignment by regularizing the\ndistances between image and text embeddings of ID data. This adjustment helps\nin better utilizing pretrained textual information by aligning similar\nsemantics from different modalities (i.e., text and image) more closely in the\nhyperspherical representation space. We theoretically demonstrate that the\nproposed regularization corresponds to the maximum likelihood estimation of an\nenergy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark\ndatasets, we show that our method, combined with post-hoc OoDD approaches\nleveraging pretrained knowledge (e.g., NegLabel), significantly outperforms\nexisting methods, achieving state-of-the-art OoDD performance and leading ID\naccuracy.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18817.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654b4c9cfabd2cc66874806c",
      "avatarUrl": "/avatars/c48b423d3657ee27eed0d3d83738ec2e.svg",
      "fullname": "jeonghyeon kim",
      "name": "mawjdgus",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]