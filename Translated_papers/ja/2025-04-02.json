[
  {
    "paper": {
      "id": "2503.24379",
      "authors": [
        {
          "_id": "67ec0a7262144ec35d0e571d",
          "user": {
            "_id": "64c139d867eff857ea51caa8",
            "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
            "isPro": false,
            "fullname": "Shengqiong Wu",
            "user": "ChocoWu",
            "type": "user"
          },
          "name": "Shengqiong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:23:35.392Z",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e571e",
          "user": {
            "_id": "6360d9f0472131c3bc4f61df",
            "avatarUrl": "/avatars/c5d884e5ef19b781e3405aba6dd68ca8.svg",
            "isPro": false,
            "fullname": "WeicaiYe",
            "user": "WeicaiYe",
            "type": "user"
          },
          "name": "Weicai Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:36:51.426Z",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e571f",
          "name": "Jiahao Wang",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5720",
          "name": "Quande Liu",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5721",
          "user": {
            "_id": "60e272ca6c78a8c122b12127",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
            "isPro": false,
            "fullname": "Xintao Wang",
            "user": "Xintao",
            "type": "user"
          },
          "name": "Xintao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:38:20.890Z",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5722",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5723",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:38:50.116Z",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5724",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5725",
          "user": {
            "_id": "67eaa070b9fa8908e151fd7d",
            "avatarUrl": "/avatars/1fe2fd678d2e71099a83a9bcb9ab517e.svg",
            "isPro": false,
            "fullname": "shuicheng yan",
            "user": "shuicheng",
            "type": "user"
          },
          "name": "Shuicheng Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:39:18.884Z",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5726",
          "user": {
            "_id": "647773a1168cb428e00e9a8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
            "isPro": false,
            "fullname": "Hao Fei",
            "user": "scofield7419",
            "type": "user"
          },
          "name": "Hao Fei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:23:38.087Z",
          "hidden": false
        },
        {
          "_id": "67ec0a7262144ec35d0e5727",
          "user": {
            "_id": "6570ae84c4993b8fb96f41a8",
            "avatarUrl": "/avatars/21f7d79d46ac4df0ecff8eca7678b33f.svg",
            "isPro": false,
            "fullname": "Tat-Seng Chua",
            "user": "chuats",
            "type": "user"
          },
          "name": "Tat-Seng Chua",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:39:25.537Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:59:01.000Z",
      "submittedOnDailyAt": "2025-04-02T01:29:41.083Z",
      "title": "Any2Caption: 条件の読み取りを行い、制御可能なビデオのキャプション生成",
      "submittedOnDailyBy": {
        "_id": "64c139d867eff857ea51caa8",
        "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
        "isPro": false,
        "fullname": "Shengqiong Wu",
        "user": "ChocoWu",
        "type": "user"
      },
      "summary": "現在のビデオ生成コミュニティでの正確なユーザーの意図解釈のボトルネックに対処するために、私たちはAny2Captionを紹介します。これは、どの条件でも制御可能なビデオ生成の新しいフレームワークです。その核心的な概念は、ビデオ合成ステップから異なる条件解釈ステップを分離することです。現代の多モーダル大語言モデル（MLLMs）を活用して、Any2Captionはテキスト、画像、ビデオ、および特別なカテガー（エリア、動き、カメラの姿勢など）を複雑な入力として受け取り、ビデオジェネレーターにより良いガイドを提供するための密度の高い構造化されたサンプリングを生成します。また、Any2CapInsという大規模なデータセットを紹介します。これは337Kのインスタンスと407Kの条件を持ち、どの条件からサンプリングへの指示調整に使用できるものです。詳細な評価は、現在のビデオ生成モデルの様々な面での制御可能性とビデオの質の向上について示します。プロジェクトページは、https://sqwu.top/Any2Cap/ です。",
      "upvotes": 39,
      "discussionId": "67ec0a7562144ec35d0e57fc",
      "projectPage": "https://sqwu.top/Any2Cap/",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "dense, structured captions",
        "region",
        "motion",
        "camera poses",
        "any-condition-to-caption instruction tuning"
      ]
    },
    "publishedAt": "2025-03-31T13:59:01.000Z",
    "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video\n  Generation",
    "summary": "To address the bottleneck of accurate user intent interpretation within the\ncurrent video generation community, we present Any2Caption, a novel framework\nfor controllable video generation under any condition. The key idea is to\ndecouple various condition interpretation steps from the video synthesis step.\nBy leveraging modern multimodal large language models (MLLMs), Any2Caption\ninterprets diverse inputs--text, images, videos, and specialized cues such as\nregion, motion, and camera poses--into dense, structured captions that offer\nbackbone video generators with better guidance. We also introduce Any2CapIns, a\nlarge-scale dataset with 337K instances and 407K conditions for\nany-condition-to-caption instruction tuning. Comprehensive evaluations\ndemonstrate significant improvements of our system in controllability and video\nquality across various aspects of existing video generation models. Project\nPage: https://sqwu.top/Any2Cap/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24379.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "64c139d867eff857ea51caa8",
      "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
      "fullname": "Shengqiong Wu",
      "name": "ChocoWu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24376",
      "authors": [
        {
          "_id": "67eca2b8351721d62aa537df",
          "user": {
            "_id": "60d045c4778bafd0fbcfa3f5",
            "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
            "isPro": false,
            "fullname": "Yi Chen",
            "user": "ChenYi99",
            "type": "user"
          },
          "name": "Yi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:30.143Z",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e0",
          "user": {
            "_id": "6455cc8f654d8bccae50e4d4",
            "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
            "isPro": false,
            "fullname": "Yuying Ge",
            "user": "tttoaster",
            "type": "user"
          },
          "name": "Yuying Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:40:05.288Z",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e1",
          "user": {
            "_id": "62e0f1314db2175cd270ad08",
            "avatarUrl": "/avatars/1d3d6af6c63557f4abf0484e028fa942.svg",
            "isPro": false,
            "fullname": "Rui Wang",
            "user": "ruiwang",
            "type": "user"
          },
          "name": "Rui Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:40:12.320Z",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e2",
          "user": {
            "_id": "640e9762b03f4cd29f58d982",
            "avatarUrl": "/avatars/81da37d628163fe3e094b247c7c3a3b5.svg",
            "isPro": false,
            "fullname": "Yixiao Ge",
            "user": "yxgeee",
            "type": "user"
          },
          "name": "Yixiao Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:40:19.876Z",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e3",
          "name": "Lu Qiu",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e4",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:40:26.551Z",
          "hidden": false
        },
        {
          "_id": "67eca2b8351721d62aa537e5",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:40:32.831Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:55:23.000Z",
      "submittedOnDailyAt": "2025-04-02T01:06:48.435Z",
      "title": "強化学習効果を調査して映画理解における影響を明らかにする：\nSEED-Bench-R1からのアインサイエンス",
      "submittedOnDailyBy": {
        "_id": "60d045c4778bafd0fbcfa3f5",
        "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
        "isPro": false,
        "fullname": "Yi Chen",
        "user": "ChenYi99",
        "type": "user"
      },
      "summary": "最近のChain of Thought (COT) 生成の進展は、Large Language Models (LLMs) の論理能力を大幅に向上させ、強化学習 (RL) が後ハイパートレーニング手法として効果的であることが明らかになりました。Multimodal Large Language Models (MLLMs) はこの論理ポテンシャルを継承しているが、視覚的認識と論理的論理が必要となるタスクには調査が不足していました。これに対して、SEED-Bench-R1 を紹介します。SEED-Bench-R1 は、MLLM の後ハイパートレーニング手法をシステマティックに評価するために設計されています。これは、複雑な実世界的映像や日常的な計画タスクを含む、複数選択問題の形式で、高度な視覚的認識と論理的論理が必要となるものです。SEED-Bench-R1 は、汎化能力を評価するために、分布内、環境間、環境タスク間の三段階ヒューリスティックを用いて、大規模な訓練データセットと容易に確認可能な正解を満たすものです。Qwen2-VL-Instruct-7B を基礎モデルとして、RL と超監督学習 (SFT) を比較し、RL のデータエフィシェンスと分布内と分布外タスクの優れた性能を示し、LongVideoBench のような一般的な映像理解ベンチマークで SFT を上回ることも示しました。詳細な分析から、RL は視覚的認識を強化したが、論理的な論理鍵が少なくなることが見られました。不確かな論理と視覚的キープを見落としたようなキー限界が識別され、基礎モデルの論理、報酬モデリング、および噪音信号に対する RL の強固性についての将来の改善を提案しました。",
      "upvotes": 21,
      "discussionId": "67eca2b9351721d62aa53822",
      "githubRepo": "https://github.com/TencentARC/SEED-Bench-R1",
      "ai_keywords": [
        "Chain of Thought (COT)",
        "Large Language Models (LLMs)",
        "reinforcement learning (RL)",
        "Multimodal Large Language Models (MLLMs)",
        "SEED-Bench-R1",
        "video understanding",
        "multiple-choice questions",
        "in-distribution",
        "cross-environment",
        "cross-environment-task scenarios",
        "Qwen2-VL-Instruct-7B",
        "supervised fine-tuning (SFT)",
        "LongVideoBench",
        "visual perception",
        "reasoning chains",
        "inconsistent reasoning",
        "reward modeling"
      ]
    },
    "publishedAt": "2025-03-31T13:55:23.000Z",
    "title": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1",
    "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d045c4778bafd0fbcfa3f5",
      "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
      "fullname": "Yi Chen",
      "name": "ChenYi99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23145",
      "authors": [
        {
          "_id": "67eb710f1f8a09c48b2e3ba1",
          "user": {
            "_id": "674286496efe2b931f7ce354",
            "avatarUrl": "/avatars/8f920618b777dbff5f2a117ebd9e9caa.svg",
            "isPro": false,
            "fullname": "Anjiang Wei",
            "user": "anjiangwei",
            "type": "user"
          },
          "name": "Anjiang Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:42.109Z",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba2",
          "user": {
            "_id": "65e7bb35e5e78134ab049942",
            "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
            "isPro": false,
            "fullname": "Tarun Suresh",
            "user": "tarsur909",
            "type": "user"
          },
          "name": "Tarun Suresh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:23:41.048Z",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba3",
          "name": "Jiannan Cao",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba4",
          "name": "Naveen Kannan",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba5",
          "name": "Yuheng Wu",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba6",
          "user": {
            "_id": "65de7628deee79773f0f46f6",
            "avatarUrl": "/avatars/6c509dbe96e47b47271eb74178c1c9ba.svg",
            "isPro": false,
            "fullname": "Kai Yan",
            "user": "kaiyan289",
            "type": "user"
          },
          "name": "Kai Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:43:08.150Z",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba7",
          "name": "Thiago S. F. X. Teixeira",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba8",
          "name": "Ke Wang",
          "hidden": false
        },
        {
          "_id": "67eb710f1f8a09c48b2e3ba9",
          "name": "Alex Aiken",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-29T16:50:39.000Z",
      "submittedOnDailyAt": "2025-04-02T04:28:51.051Z",
      "title": "CodeARC: インデュシャンプログラム合成のためのLLMアガントの論理能力のベンチマーク",
      "submittedOnDailyBy": {
        "_id": "65e7bb35e5e78134ab049942",
        "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
        "isPro": false,
        "fullname": "Tarun Suresh",
        "user": "tarsur909",
        "type": "user"
      },
      "summary": "Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning.",
      "upvotes": 20,
      "discussionId": "67eb71101f8a09c48b2e3bee",
      "ai_keywords": [
        "inductive program synthesis",
        "programming by example",
        "function synthesis",
        "input-output examples",
        "large language model agents",
        "natural language",
        "evaluation protocols",
        "feedback mechanism",
        "real-world scenarios",
        "reverse engineering",
        "CodeARC",
        "Code Abstraction and Reasoning Challenge",
        "hidden target function",
        "querying",
        "candidate functions",
        "differential testing oracle",
        "interactive setting",
        "function calls",
        "self-correction",
        "large-scale benchmark",
        "general-purpose inductive program synthesis",
        "fine-tuning",
        "LLaMA-3.1-8B-Instruct",
        "synthesis traces",
        "relative performance gain"
      ]
    },
    "publishedAt": "2025-03-29T12:50:39.000Z",
    "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive\n  Program Synthesis",
    "summary": "Inductive program synthesis, or programming by example, requires synthesizing\nfunctions from input-output examples that generalize to unseen inputs. While\nlarge language model agents have shown promise in programming tasks guided by\nnatural language, their ability to perform inductive program synthesis is\nunderexplored. Existing evaluation protocols rely on static sets of examples\nand held-out tests, offering no feedback when synthesized functions are\nincorrect and failing to reflect real-world scenarios such as reverse\nengineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge,\na new evaluation framework where agents interact with a hidden target function\nby querying it with new inputs, synthesizing candidate functions, and\niteratively refining their solutions using a differential testing oracle. This\ninteractive setting encourages agents to perform function calls and\nself-correction based on feedback. We construct the first large-scale benchmark\nfor general-purpose inductive program synthesis, featuring 1114 functions.\nAmong 18 models evaluated, o3-mini performs best with a success rate of 52.7%,\nhighlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on\ncurated synthesis traces yields up to a 31% relative performance gain. CodeARC\nprovides a more realistic and challenging testbed for evaluating LLM-based\nprogram synthesis and inductive reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23145.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e7bb35e5e78134ab049942",
      "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
      "fullname": "Tarun Suresh",
      "name": "tarsur909",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00050",
      "authors": [
        {
          "_id": "67ec9bf3e58745dc7d652587",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d652588",
          "user": {
            "_id": "64351475901c5734bcb64248",
            "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
            "isPro": false,
            "fullname": "Zhiyuan Hu",
            "user": "zhiyuanhucs",
            "type": "user"
          },
          "name": "Zhiyuan Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:44:43.089Z",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d652589",
          "user": {
            "_id": "6730a1fed66bf1b6378cd451",
            "avatarUrl": "/avatars/5ec9b7313213a951b7c325d35ca26692.svg",
            "isPro": false,
            "fullname": "qy",
            "user": "qingyunzou",
            "type": "user"
          },
          "name": "Qingyun Zou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:44:49.663Z",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d65258a",
          "name": "Jiaying Wu",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d65258b",
          "name": "Qian Wang",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d65258c",
          "user": {
            "_id": "651d8032c50012d33e914f2f",
            "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
            "isPro": false,
            "fullname": "Bryan Hooi",
            "user": "bhooi",
            "type": "user"
          },
          "name": "Bryan Hooi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:45:24.844Z",
          "hidden": false
        },
        {
          "_id": "67ec9bf3e58745dc7d65258d",
          "name": "Bingsheng He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T02:18:51.000Z",
      "submittedOnDailyAt": "2025-04-02T04:52:20.239Z",
      "title": "ジャッジLRM: 大論理モデルをジャッジとして",
      "submittedOnDailyBy": {
        "_id": "64351475901c5734bcb64248",
        "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
        "isPro": false,
        "fullname": "Zhiyuan Hu",
        "user": "zhiyuanhucs",
        "type": "user"
      },
      "summary": "LLMの昇興は、人間の記録に比べてスケーラブルな評価手段としての機能を提供していますが、現在のSupervised Fine-Tuning (SFT) の判定者ニーズにおいて複雑な理由論を求める領域では、その効果は限られています。本稿では、LLMの判定者が実際に複雑な理由論能力を得ることができるかどうかを調査しています。評価タスクの理由論要求を詳細に分析し、SFTの性能の向上と理由論要求の高いサンプルの割合との負の相関を明らかにし、このような場合でのSFTの限界を示しています。これに対して、JudgeLRMという判定者向けのLLMの家族を導入し、判定者の知識を基にした報酬を与える強化学習(RL)を用いて訓練しています。JudgeLRMモデルは、SFT調整されたモデルや最先端の理由論モデルをもっとも上位に出色しています。特に、JudgeLRM-3BはGPT-4を超え、JudgeLRM-7BはDeepSeek-R1に対してF1スコアで2.79%より上位に出色し、複雑な理由論が必要な判定者タスクで特に優れています。",
      "upvotes": 18,
      "discussionId": "67ec9bf4e58745dc7d6525c1",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Supervised Fine-Tuning (SFT)",
        "reasoning capabilities",
        "reinforcement learning (RL)",
        "judge-wise, outcome-driven rewards",
        "JudgeLRM",
        "GPT-4",
        "DeepSeek-R1"
      ]
    },
    "publishedAt": "2025-03-30T22:18:51.000Z",
    "title": "JudgeLRM: Large Reasoning Models as a Judge",
    "summary": "The rise of Large Language Models (LLMs) as evaluators offers a scalable\nalternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for\njudges approaches often fall short in domains requiring complex reasoning. In\nthis work, we investigate whether LLM judges truly benefit from enhanced\nreasoning capabilities. Through a detailed analysis of reasoning requirements\nacross evaluation tasks, we reveal a negative correlation between SFT\nperformance gains and the proportion of reasoning-demanding samples -\nhighlighting the limitations of SFT in such scenarios. To address this, we\nintroduce JudgeLRM, a family of judgment-oriented LLMs trained using\nreinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM\nmodels consistently outperform both SFT-tuned and state-of-the-art reasoning\nmodels. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms\nDeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks\nrequiring deep reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00050.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64351475901c5734bcb64248",
      "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
      "fullname": "Zhiyuan Hu",
      "name": "zhiyuanhucs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.01016",
      "authors": [
        {
          "_id": "67ec958ebb1d6dd924f94a31",
          "user": {
            "_id": "65f8e4778dc7bb5b4db97f92",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/gBmQdovmANmjV72k3gaW8.png",
            "isPro": false,
            "fullname": "Tian-Xing Xu",
            "user": "slothfulxtx",
            "type": "user"
          },
          "name": "Tian-Xing Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:51:06.226Z",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a32",
          "user": {
            "_id": "64c0953a8137192a1e2474dc",
            "avatarUrl": "/avatars/546405a7eaf2f60ad108ceaa0dda7d08.svg",
            "isPro": false,
            "fullname": "xiangjun gao",
            "user": "xiangjun0211",
            "type": "user"
          },
          "name": "Xiangjun Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:51:14.978Z",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a33",
          "user": {
            "_id": "657a7458afbb0117ba15c59f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
            "isPro": false,
            "fullname": "Wenbo Hu",
            "user": "wbhu-tc",
            "type": "user"
          },
          "name": "Wenbo Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:52.313Z",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a34",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a35",
          "name": "Song-Hai Zhang",
          "hidden": false
        },
        {
          "_id": "67ec958ebb1d6dd924f94a36",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:51:45.869Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/6He3mQcB_AO1G_Sq8xYc0.mp4"
      ],
      "publishedAt": "2025-04-01T17:58:03.000Z",
      "submittedOnDailyAt": "2025-04-02T00:15:17.585Z",
      "title": "GeometryCrafter: 開放世界ビデオの一貫性のない幾何学推定におけるDiffusion Priors",
      "submittedOnDailyBy": {
        "_id": "657a7458afbb0117ba15c59f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
        "isPro": false,
        "fullname": "Wenbo Hu",
        "user": "wbhu-tc",
        "type": "user"
      },
      "summary": "雖然映像の深さ推定に関する驚人的な進歩がありますが、現在の方法は、アフィン不変な予測を通じて幾何学的な忠実性を達成するには固有の制限があり、再構築や他のメトリックに基づく次期タスクの適用範囲が限られています。私たちは、GeometryCrafterという新しいフレームワークを提案します。これは、開放ワールドの映像から時系列的な一致性を持つ高品質な点マップシーケンスを復元し、正確な3D/4D再構築、カメラパラメータ推定や他の深さベースのアプリケーションを可能にします。私たちのアプローチの核心は、点マップバイナリアモータイカル（VAE）です。これは、映像の潜在分布に関係なく潜在空間を学習し、点マップの効果的なエンコーディングとデコーディングを行うことができます。VAEを活用して、点マップシーケンスの分布を入力映像に基づいてモデル化するための映像ディフュージョンモデルを訓練します。多様なデータセット上での拡張評価により、GeometryCrafterは最先端の3D精度、時系列的一致性、および一般化能力を達成しています。",
      "upvotes": 12,
      "discussionId": "67ec9593bb1d6dd924f94b3e",
      "projectPage": "https://geometrycrafter.github.io/",
      "githubRepo": "https://github.com/TencentARC/GeometryCrafter",
      "ai_keywords": [
        "GeometryCrafter",
        "point map Variational Autoencoder (VAE)",
        "latent space",
        "video latent distributions",
        "point map encoding",
        "point map decoding",
        "video diffusion model",
        "point map sequences",
        "3D accuracy",
        "temporal consistency",
        "generalization capability"
      ]
    },
    "publishedAt": "2025-04-01T13:58:03.000Z",
    "title": "GeometryCrafter: Consistent Geometry Estimation for Open-world Videos\n  with Diffusion Priors",
    "summary": "Despite remarkable advancements in video depth estimation, existing methods\nexhibit inherent limitations in achieving geometric fidelity through the\naffine-invariant predictions, limiting their applicability in reconstruction\nand other metrically grounded downstream tasks. We propose GeometryCrafter, a\nnovel framework that recovers high-fidelity point map sequences with temporal\ncoherence from open-world videos, enabling accurate 3D/4D reconstruction,\ncamera parameter estimation, and other depth-based applications. At the core of\nour approach lies a point map Variational Autoencoder (VAE) that learns a\nlatent space agnostic to video latent distributions for effective point map\nencoding and decoding. Leveraging the VAE, we train a video diffusion model to\nmodel the distribution of point map sequences conditioned on the input videos.\nExtensive evaluations on diverse datasets demonstrate that GeometryCrafter\nachieves state-of-the-art 3D accuracy, temporal consistency, and generalization\ncapability.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/6He3mQcB_AO1G_Sq8xYc0.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01016.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a7458afbb0117ba15c59f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
      "fullname": "Wenbo Hu",
      "name": "wbhu-tc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00906",
      "authors": [
        {
          "_id": "67ec9340913c072638c16bbf",
          "user": {
            "_id": "627837664f2afdc41bbd622a",
            "avatarUrl": "/avatars/b7bfc4fb77830bba71839c04a4aeea64.svg",
            "isPro": false,
            "fullname": "Saaket Agashe",
            "user": "saa1605",
            "type": "user"
          },
          "name": "Saaket Agashe",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:49:55.477Z",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc0",
          "name": "Kyle Wong",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc1",
          "name": "Vincent Tu",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc2",
          "user": {
            "_id": "65edff6233c279253952e0bd",
            "avatarUrl": "/avatars/3130f6f9873ae0a943631e56f6d8d341.svg",
            "isPro": false,
            "fullname": "Jiachen Yang",
            "user": "jc-y42",
            "type": "user"
          },
          "name": "Jiachen Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:46:55.173Z",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc3",
          "name": "Ang Li",
          "hidden": false
        },
        {
          "_id": "67ec9340913c072638c16bc4",
          "user": {
            "_id": "64679a226192d39142245e5e",
            "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
            "isPro": false,
            "fullname": "Xin Eric Wang",
            "user": "xw-eric",
            "type": "user"
          },
          "name": "Xin Eric Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:46:30.920Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T15:40:27.000Z",
      "submittedOnDailyAt": "2025-04-02T00:02:12.706Z",
      "title": "Agent S2: コンピューター用アグェントの構成論的な一般的者と専門家のフレームワーク",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "コンピュータ使用アガントは、コンピュータとモバイルデバイスのグラフィックユーザインターフェース（GUI）を直接インタラクティングすることで、ユーザーのクエリの開放的なスペースを完了することで人間の生産性を大幅に向上させることができることを目指しています。しかし、現在のアガントは、GUI要素の不精確な基礎化、長期視野のタスク計画の難関、多様な認知タスクを担う単一の一般主義モデルに依存した性能バックロックなど、重要な課題を見守っています。このため、我々は、認知責任を様々な一般主義モデルと専門家モデルの間で委託する新しい組み合わせフレームワーク「Agent S2」を紹介します。我々は、精確なGUIロケーションを達成するために新しい「Grounding Mixture」手法を提案し、変化する観測に対応して多様な時間スケールで動的にアクションプランを補正する「Proactive Hierarchical Planning」を導入します。評価結果によると、Agent S2は3つの著しいコンピュータ使用ベンチマークで新しい最先端（SOTA）の性能を奪いました。特に、OSWorld 15ステップと50ステップ評価では、Claude Computer UseとUI-TARSの先駆けアガントを超え、18.9%と32.7%の相対的な向上を収めました。また、Agent S2は他のオペレーティングシステムとアプリケーションにも効果的に拡張でき、WindowsAgentArenaで52.8%とAndroidWorldで16.52%の前回の最善方法を超えました。コードは、https://github.com/simular-ai/Agent-Sから利用可能です。",
      "upvotes": 12,
      "discussionId": "67ec9343913c072638c16c5f",
      "projectPage": "https://www.simular.ai/articles/agent-s2-technical-review",
      "githubRepo": "https://github.com/simular-ai/Agent-S",
      "ai_keywords": [
        "Mixture-of-Grounding",
        "Proactive Hierarchical Planning",
        "compositional framework",
        "GUI localization",
        "action plans",
        "state-of-the-art (SOTA) performance",
        "OSWorld",
        "WindowsAgentArena",
        "AndroidWorld"
      ]
    },
    "publishedAt": "2025-04-01T11:40:27.000Z",
    "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer\n  Use Agents",
    "summary": "Computer use agents automate digital tasks by directly interacting with\ngraphical user interfaces (GUIs) on computers and mobile devices, offering\nsignificant potential to enhance human productivity by completing an open-ended\nspace of user queries. However, current agents face significant challenges:\nimprecise grounding of GUI elements, difficulties with long-horizon task\nplanning, and performance bottlenecks from relying on single generalist models\nfor diverse cognitive tasks. To this end, we introduce Agent S2, a novel\ncompositional framework that delegates cognitive responsibilities across\nvarious generalist and specialist models. We propose a novel\nMixture-of-Grounding technique to achieve precise GUI localization and\nintroduce Proactive Hierarchical Planning, dynamically refining action plans at\nmultiple temporal scales in response to evolving observations. Evaluations\ndemonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance\non three prominent computer use benchmarks. Specifically, Agent S2 achieves\n18.9% and 32.7% relative improvements over leading baseline agents such as\nClaude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation.\nMoreover, Agent S2 generalizes effectively to other operating systems and\napplications, surpassing previous best methods by 52.8% on WindowsAgentArena\nand by 16.52% on AndroidWorld relatively. Code available at\nhttps://github.com/simular-ai/Agent-S.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00906.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00810",
      "authors": [
        {
          "_id": "67eca0c4cfce948cbbbcff9a",
          "user": {
            "_id": "646f3443c261dc413383b8a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3443c261dc413383b8a4/hEJd8wLyR5HTdMzApaloN.png",
            "isPro": false,
            "fullname": "Zhaojian Yu",
            "user": "zjy2001",
            "type": "user"
          },
          "name": "Zhaojian Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:38.129Z",
          "hidden": false
        },
        {
          "_id": "67eca0c4cfce948cbbbcff9b",
          "user": {
            "_id": "650ed552dc509ae7d7bb1ccc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650ed552dc509ae7d7bb1ccc/CPR6gaIjPfnBGfr-D_rpO.jpeg",
            "isPro": false,
            "fullname": "Yinghao Wu",
            "user": "yh1567",
            "type": "user"
          },
          "name": "Yinghao Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:50:27.577Z",
          "hidden": false
        },
        {
          "_id": "67eca0c4cfce948cbbbcff9c",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:34.689Z",
          "hidden": false
        },
        {
          "_id": "67eca0c4cfce948cbbbcff9d",
          "user": {
            "_id": "5f5ba21188f57f65f951f255",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1599840760465-noauth.png",
            "isPro": false,
            "fullname": "Arman Cohan",
            "user": "armanc",
            "type": "user"
          },
          "name": "Arman Cohan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:50:33.530Z",
          "hidden": false
        },
        {
          "_id": "67eca0c4cfce948cbbbcff9e",
          "name": "Xiao-Ping Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T14:01:50.000Z",
      "submittedOnDailyAt": "2025-04-02T01:59:40.340Z",
      "title": "Z1: テスト時に効率的なスケーリングを行うコード",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、テスト時の計算スケーリングを通じて複雑な問題解決に優れた性能を達成できますが、これには長いコンテキストと多くの理由トークンコストが伴うことが多いです。本論文では、LLMsにおけるコード関連の理由トラジェクトを用いた効率的なテスト時スケーリング方法を提案します。これにより、理由トークンを減らしながら性能を維持することができます。まず、簡単かつ複雑なコーディング問題とその短いおよび長い解決トラジェクトを組み合わせたコレクションデータセットZ1-Code-Reasoning-107Kを作成します。次に、理由トークンの過度考えコストを軽減するために新しいShifted Thinking Windowを提案します。これにより、コンテキスト制限タグ（例：<think> ... </think>）を削除し、理由トークンを制限します。長いおよび短いトラジェクトデータで訓練され、Shifted Thinking Windowを機能させたモデルZ1-7Bは、問題の複雑さによって理由レベルを調整する能力を示し、理由タスクの適切なスケーリングを示します。R1-Distill-Qwen-7Bの性能を追い越すことができ、平均思考トークンの30%を使用します。特に、コードトラジェクトだけで微調節されたZ1-7Bは、広範囲の理由タスクに対する拡散性（GPQA Diamondで47.5%）を示します。効率的な理由発見の分析も、将来の研究に有益なコンテンツを提供します。",
      "upvotes": 12,
      "discussionId": "67eca0c6cfce948cbbbd0042",
      "githubRepo": "https://github.com/efficientscaling/Z1",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "test-time computing scaling",
        "thinking tokens",
        "Z1-Code-Reasoning-107K",
        "solution trajectories",
        "Shifted Thinking Window",
        "context-delimiting tags",
        "efficient test-time scaling",
        "R1-Distill-Qwen-7B",
        "GPQA Diamond",
        "efficient reasoning elicitation"
      ]
    },
    "publishedAt": "2025-04-01T10:01:50.000Z",
    "title": "Z1: Efficient Test-time Scaling with Code",
    "summary": "Large Language Models (LLMs) can achieve enhanced complex problem-solving\nthrough test-time computing scaling, yet this often entails longer contexts and\nnumerous reasoning token costs. In this paper, we propose an efficient\ntest-time scaling method that trains LLMs on code-related reasoning\ntrajectories, facilitating their reduction of excess thinking tokens while\nmaintaining performance. First, we create Z1-Code-Reasoning-107K, a curated\ndataset of simple and complex coding problems paired with their short and long\nsolution trajectories. Second, we present a novel Shifted Thinking Window to\nmitigate overthinking overhead by removing context-delimiting tags (e.g.,\n<think>. . . </think>) and capping reasoning tokens. Trained with long and\nshort trajectory data and equipped with Shifted Thinking Window, our model,\nZ1-7B, demonstrates the ability to adjust its reasoning level as the complexity\nof problems and exhibits efficient test-time scaling across different reasoning\ntasks that matches R1-Distill-Qwen-7B performance with about 30% of its average\nthinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B\ndemonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond).\nOur analysis of efficient reasoning elicitation also provides valuable insights\nfor future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00810.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6566
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24377",
      "authors": [
        {
          "_id": "67eca439a62c82ed64354e36",
          "user": {
            "_id": "67298b338c66e235932ca088",
            "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
            "isPro": false,
            "fullname": "WANG Rui",
            "user": "Ray121381",
            "type": "user"
          },
          "name": "Rui Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:52:11.104Z",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e37",
          "user": {
            "_id": "65f906e5c3dbdcae83ff7aac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f906e5c3dbdcae83ff7aac/mdjiVkLDJgJcGLwv0rMe4.jpeg",
            "isPro": false,
            "fullname": "Hongru Wang",
            "user": "Merlin-Hongru",
            "type": "user"
          },
          "name": "Hongru Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:52:28.243Z",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e38",
          "user": {
            "_id": "66ab39295558689cb8676559",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/La5iNOHELvZ2peByiakqe.png",
            "isPro": false,
            "fullname": "XUE Boyang",
            "user": "BeyondHsueh",
            "type": "user"
          },
          "name": "Boyang Xue",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:52:39.163Z",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e39",
          "user": {
            "_id": "64a3d40815655921915b8ce2",
            "avatarUrl": "/avatars/6b6b550d96be4a6473e2ccf74df438f7.svg",
            "isPro": false,
            "fullname": "Jianhuipang",
            "user": "pangjh3",
            "type": "user"
          },
          "name": "Jianhui Pang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:52:45.274Z",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3a",
          "user": {
            "_id": "654ce87af0b05673196a9f45",
            "avatarUrl": "/avatars/7b9c854eb98e487e3057479b1c7860ac.svg",
            "isPro": false,
            "fullname": "Shudong Liu",
            "user": "Sudanl",
            "type": "user"
          },
          "name": "Shudong Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:52:51.029Z",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3b",
          "user": {
            "_id": "609fee945ad152dff7bb3b77",
            "avatarUrl": "/avatars/3694f5affe50ace501df7191e1b952d4.svg",
            "isPro": false,
            "fullname": "Yichen",
            "user": "Yichen",
            "type": "user"
          },
          "name": "Yi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:52:58.486Z",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3c",
          "name": "Jiahao Qiu",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3d",
          "user": {
            "_id": "648bd523805e4bcc541ec320",
            "avatarUrl": "/avatars/443ca0c7cbeda3a08eb4af6a0e2da8bc.svg",
            "isPro": false,
            "fullname": "Derek Wong",
            "user": "derekfw",
            "type": "user"
          },
          "name": "Derek Fai Wong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:53:28.254Z",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3e",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "67eca439a62c82ed64354e3f",
          "name": "Kam-Fai Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:58:07.000Z",
      "submittedOnDailyAt": "2025-04-02T01:17:01.812Z",
      "title": "Reasoning経済を駆使する：大規模言語モデルの効率的な理由論の調査",
      "submittedOnDailyBy": {
        "_id": "67298b338c66e235932ca088",
        "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
        "isPro": false,
        "fullname": "WANG Rui",
        "user": "Ray121381",
        "type": "user"
      },
      "summary": "最新の大規模言語モデル（LLMs）の進展は、複雑な理由論理任務の実行能力を大幅に向上させ、直感的な思いやり（System 1）から深い理由論理（System 2）に転換しました。System 2の理由論理は、タスクの正確性を向上させることができますが、その遅い思考の性質と無効か無駄な理由論理行為により、計算費用が大幅に課金されます。一方で、System 1の理由論理は計算的に効率的ですが、最適な性能を実現しません。このような転換に伴い、性能（利益）と計算費用（バジュー）のトレードオフを調和する重要性があり、理由論理の経済性という概念が生まれました。この調査では、LLMsのトレーニング後とテスト時の推論ステージにおける理由論理の経済性について、理由論理の無効性の原因、異なる理由論理パターンの行為分析、理由論理の経済性を達成する潜在的な解決策について、詳細な分析を提供します。行動可能な洞察を提供し、開放的な課題を明らかにし、LLMsの理由論理の経済性を向上させるための戦略についてもっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとももっとも",
      "upvotes": 10,
      "discussionId": "67eca43aa62c82ed64354e82",
      "githubRepo": "https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers"
    },
    "publishedAt": "2025-03-31T13:58:07.000Z",
    "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models",
    "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24377.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67298b338c66e235932ca088",
      "avatarUrl": "/avatars/912990b3b8b2e67663fc395c73287593.svg",
      "fullname": "WANG Rui",
      "name": "Ray121381",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.01019",
      "authors": [
        {
          "_id": "67ecd893553eb1c01a311882",
          "user": {
            "_id": "62ace9bc717ee4c12b72e275",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ace9bc717ee4c12b72e275/VfR-7R3H-5FV0OILa0XzL.jpeg",
            "isPro": false,
            "fullname": "Pablo Ruiz-Ponce",
            "user": "pabloruizponce",
            "type": "user"
          },
          "name": "Pablo Ruiz-Ponce",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:21:16.654Z",
          "hidden": false
        },
        {
          "_id": "67ecd893553eb1c01a311883",
          "user": {
            "_id": "64fac0f3b961d0d12c756b59",
            "avatarUrl": "/avatars/27cc7dbad927df818ba2f91a5b6942f9.svg",
            "isPro": false,
            "fullname": "German Barquero",
            "user": "Germs96",
            "type": "user"
          },
          "name": "German Barquero",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:53:49.237Z",
          "hidden": false
        },
        {
          "_id": "67ecd893553eb1c01a311884",
          "name": "Cristina Palmero",
          "hidden": false
        },
        {
          "_id": "67ecd893553eb1c01a311885",
          "name": "Sergio Escalera",
          "hidden": false
        },
        {
          "_id": "67ecd893553eb1c01a311886",
          "name": "José García-Rodríguez",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62ace9bc717ee4c12b72e275/zNX1xkmtw7ZpRcK_qxAHv.mp4"
      ],
      "publishedAt": "2025-04-01T17:59:44.000Z",
      "submittedOnDailyAt": "2025-04-02T05:06:30.372Z",
      "title": "MixerMDM: 人間の動きを学習可能に構成するDiffusionモデル",
      "submittedOnDailyBy": {
        "_id": "62ace9bc717ee4c12b72e275",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ace9bc717ee4c12b72e275/VfR-7R3H-5FV0OILa0XzL.jpeg",
        "isPro": false,
        "fullname": "Pablo Ruiz-Ponce",
        "user": "pabloruizponce",
        "type": "user"
      },
      "summary": "テキスト説明によってガイドされた人間の動きの生成は、高品質の動きとその対応する条件のペアを含むデータセットの必要性によって難しい。より細かな制御を目指すにせよ、この難易度は増加する。そのため、先行研究は、異なる条件のデータセットで事前学習された数々の動きのディフフェレンシャルモデルを組み合わせる方法を提案し、複数の条件での制御を可能にした。しかし、提案された統合戦略は、最適な組み合わせ方法が各事前学習された生成モデルの特徴や特異なテキスト説明に依存することを考慮していないことを見出す。このコンテキストで、我々は、最初の学習可能なモデル組み合わせ手法であるMixerMDMを介して、テキスト説明による人間の動きのディフフェレンシャルモデルを組み合わせる方法を導入します。先行のアプローチと異なり、MixerMDMは対抗的な方法で学習された動きのディフフェレンシャルプロセスを組み合わせるダイナミックな戦略を提供し、生成を駆動する条件のセットによって各モデルのディフフェレンシャルプロセスを組み合わせることを学習します。MixerMDMを用いて、単人および複数人の動きのディフフェレンシャルモデルを組み合わせることで、それぞれの人の動きの動態における細かな制御を実現し、全体の相互作用についても同様の制御を可能にします。また、我々は、このタスクで初めて、混合生成された動きとその条件のアライメントとMixerMDMが動きを混合する過程での適応性を評価する新しい評価手法を提案します。",
      "upvotes": 9,
      "discussionId": "67ecd894553eb1c01a3118df",
      "projectPage": "https://www.pabloruizponce.com/papers/MixerMDM",
      "githubRepo": "https://github.com/pabloruizponce/MixerMDM",
      "ai_keywords": [
        "motion diffusion models",
        "text-conditioned",
        "learnable model composition",
        "dynamic mixing strategy",
        "adversarial fashion",
        "denoising process",
        "fine-grained control",
        "single-person",
        "multi-person",
        "interaction",
        "evaluation technique",
        "alignment between generated motions and conditions"
      ]
    },
    "publishedAt": "2025-04-01T13:59:44.000Z",
    "title": "MixerMDM: Learnable Composition of Human Motion Diffusion Models",
    "summary": "Generating human motion guided by conditions such as textual descriptions is\nchallenging due to the need for datasets with pairs of high-quality motion and\ntheir corresponding conditions. The difficulty increases when aiming for finer\ncontrol in the generation. To that end, prior works have proposed to combine\nseveral motion diffusion models pre-trained on datasets with different types of\nconditions, thus allowing control with multiple conditions. However, the\nproposed merging strategies overlook that the optimal way to combine the\ngeneration processes might depend on the particularities of each pre-trained\ngenerative model and also the specific textual descriptions. In this context,\nwe introduce MixerMDM, the first learnable model composition technique for\ncombining pre-trained text-conditioned human motion diffusion models. Unlike\nprevious approaches, MixerMDM provides a dynamic mixing strategy that is\ntrained in an adversarial fashion to learn to combine the denoising process of\neach model depending on the set of conditions driving the generation. By using\nMixerMDM to combine single- and multi-person motion diffusion models, we\nachieve fine-grained control on the dynamics of every person individually, and\nalso on the overall interaction. Furthermore, we propose a new evaluation\ntechnique that, for the first time in this task, measures the interaction and\nindividual quality by computing the alignment between the mixed generated\nmotions and their conditions as well as the capabilities of MixerMDM to adapt\nthe mixing throughout the denoising process depending on the motions to mix.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62ace9bc717ee4c12b72e275/zNX1xkmtw7ZpRcK_qxAHv.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01019.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ace9bc717ee4c12b72e275",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ace9bc717ee4c12b72e275/VfR-7R3H-5FV0OILa0XzL.jpeg",
      "fullname": "Pablo Ruiz-Ponce",
      "name": "pabloruizponce",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00595",
      "authors": [
        {
          "_id": "67ecaf516560da48c5c34106",
          "user": {
            "_id": "63d34004b734eaa4d4faeccf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
            "isPro": false,
            "fullname": "Weizhi Wang",
            "user": "weizhiwang",
            "type": "user"
          },
          "name": "Weizhi Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:20.594Z",
          "hidden": false
        },
        {
          "_id": "67ecaf516560da48c5c34107",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "67ecaf516560da48c5c34108",
          "user": {
            "_id": "67cd0b291580ba5d5ee65ffd",
            "avatarUrl": "/avatars/9584a55473868e5ca1fa09b1536ca546.svg",
            "isPro": false,
            "fullname": "yanglinjie",
            "user": "yanglj55",
            "type": "user"
          },
          "name": "Linjie Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:55:04.382Z",
          "hidden": false
        },
        {
          "_id": "67ecaf516560da48c5c34109",
          "name": "Heng Wang",
          "hidden": false
        },
        {
          "_id": "67ecaf516560da48c5c3410a",
          "user": {
            "_id": "65cd4785c40ab294321d610e",
            "avatarUrl": "/avatars/3f0053aa2b3d90a10b60ab24cf575fd5.svg",
            "isPro": false,
            "fullname": "Xifeng Yan",
            "user": "windwest",
            "type": "user"
          },
          "name": "Xifeng Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-02T09:54:38.935Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T09:54:00.000Z",
      "submittedOnDailyAt": "2025-04-02T02:00:49.375Z",
      "title": "Open-Qwen2VL: 学術資源上の完全モノモダルLLMの計算効率的事前学習",
      "submittedOnDailyBy": {
        "_id": "63d34004b734eaa4d4faeccf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
        "isPro": false,
        "fullname": "Weizhi Wang",
        "user": "weizhiwang",
        "type": "user"
      },
      "summary": "最先進の多モーダルLLMの予め学習の再現は、プロセスの各ステップで高品質データフィルタリング、多モーダルデータの混合戦略、シーケンスパッキング技術、トレーニングフレームワークによる壁障がある。ここでは、Open-Qwen2VLを紹介します。これは、29M画像-テキストペアを使用して、2Bパラメータの完全オープンソースMultimodal Large Language Modelを効率的に予め学習したものです。我々のアプローチは、低から高い動的な画像解像度と多モーダルシーケンスパッキングを用いて、予め学習の効率を大幅に向上させます。トレーニングデータセットは、MLLMベースのフィルタリング手法（例：MLM-Filter）と伝統的なCLIPベースのフィルタリング方法を両方用いて、データの品質とトレーニングの効率を大幅に向上させました。Open-Qwen2VLの予め学習は、UCSBで8xA100-40G GPUを使用して、5Bパッキングされた多モーダルトークンで行われました。これは、Qwen2-VLの1.4T多モーダル予め学習トークンの0.36%です。最終的に、指示に基づいたトレーニングを行ったOpen-Qwen2VLは、MMBench、SEEDBench、MMstar、MathVistaの多モーダルベンチマークで、部分オープンの最先進のMLLM Qwen2-VL-2Bを超えました。我々は、計算効率的でデータ効率的なトレーニング詳細、データフィルタリング方法、シーケンスパッキングスクリプト、WebDataset形式の予め学習データ、FSDPベースのトレーニングコードベース、モデルの開発に使用された全ての予め学習データと指導された補習データ、ベースモデルと指示に基づいたトレーニングモデルチェックポイントを全てオープンソース化します。我々は、多モーダルLLMの「完全オープン」を再定義し、以下の3つの内容の完全リリースを意味します：1) トレーニングコードベース、2) 詳細なデータフィルタリング技術、3) モデルの開発に使用された全ての予め学習データと指導された補習データ。",
      "upvotes": 9,
      "discussionId": "67ecaf546560da48c5c341dc",
      "projectPage": "https://victorwz.github.io/Open-Qwen2VL/",
      "githubRepo": "https://github.com/Victorwz/Open-Qwen2VL",
      "ai_keywords": [
        "multimodal LLM",
        "pre-training",
        "high-quality data filtering",
        "multimodal data mixture strategies",
        "sequence packing techniques",
        "training frameworks",
        "Open-Qwen2VL",
        "fully open-source",
        "2B-parameter",
        "image-text pairs",
        "A100-40G GPU hours",
        "low-to-high dynamic image resolution",
        "multimodal sequence packing",
        "data quality",
        "academic level 8xA100-40G GPUs",
        "UCSB",
        "packed multimodal tokens",
        "Qwen2-VL",
        "instruction-tuned",
        "MMBench",
        "SEEDBench",
        "MMstar",
        "MathVista",
        "compute-efficient",
        "FSDP-based training",
        "WebDataset format",
        "pre-training data",
        "training codebase",
        "data filtering techniques",
        "sequence packing scripts",
        "pre-training data in WebDataset format",
        "FSDP-based training codebase",
        "base and instruction"
      ]
    },
    "publishedAt": "2025-04-01T05:54:00.000Z",
    "title": "Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal\n  LLMs on Academic Resources",
    "summary": "The reproduction of state-of-the-art multimodal LLM pre-training faces\nbarriers at every stage of the pipeline, including high-quality data filtering,\nmultimodal data mixture strategies, sequence packing techniques, and training\nframeworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter\nMultimodal Large Language Model pre-trained efficiently on 29M image-text pairs\nusing only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic\nimage resolution and multimodal sequence packing to significantly enhance\npre-training efficiency. The training dataset was carefully curated using both\nMLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based\nfiltering methods, substantially improving data quality and training\nefficiency. The Open-Qwen2VL pre-training is conducted on academic level\n8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\\% of 1.4T\nmultimodal pre-training tokens of Qwen2-VL. The final instruction-tuned\nOpen-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on\nvarious multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista,\nindicating the remarkable training efficiency of Open-Qwen2VL. We open-source\nall aspects of our work, including compute-efficient and data-efficient\ntraining details, data filtering methods, sequence packing scripts,\npre-training data in WebDataset format, FSDP-based training codebase, and both\nbase and instruction-tuned model checkpoints. We redefine \"fully open\" for\nmultimodal LLMs as the complete release of: 1) the training codebase, 2)\ndetailed data filtering techniques, and 3) all pre-training and supervised\nfine-tuning data used to develop the model.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00595.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "63d34004b734eaa4d4faeccf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/aH7F-xn4PMQjHHcvL-0vs.jpeg",
      "fullname": "Weizhi Wang",
      "name": "weizhiwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.22952",
      "authors": [
        {
          "_id": "67eb58e5969b278277adb831",
          "user": {
            "_id": "60b9e6837946aff342f734ae",
            "avatarUrl": "/avatars/a711a6aa35757dfd7b78b26098a964fc.svg",
            "isPro": false,
            "fullname": "Yuxuan Wang",
            "user": "ColorfulAI",
            "type": "user"
          },
          "name": "Yuxuan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T09:42:25.151Z",
          "hidden": false
        },
        {
          "_id": "67eb58e5969b278277adb832",
          "name": "Yueqian Wang",
          "hidden": false
        },
        {
          "_id": "67eb58e5969b278277adb833",
          "name": "Bo Chen",
          "hidden": false
        },
        {
          "_id": "67eb58e5969b278277adb834",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "67eb58e5969b278277adb835",
          "name": "Dongyan Zhao",
          "hidden": false
        },
        {
          "_id": "67eb58e5969b278277adb836",
          "user": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "isPro": false,
            "fullname": "Zilong Zheng",
            "user": "zlzheng",
            "type": "user"
          },
          "name": "Zilong Zheng",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T03:09:26.871Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-29T02:46:58.000Z",
      "submittedOnDailyAt": "2025-04-02T07:32:05.470Z",
      "title": "OmniMMI: 流れる映像コンテキストでの詳細な多モードインタラクションベンチマーク",
      "submittedOnDailyBy": {
        "_id": "60b9e6837946aff342f734ae",
        "avatarUrl": "/avatars/a711a6aa35757dfd7b78b26098a964fc.svg",
        "isPro": false,
        "fullname": "Yuxuan Wang",
        "user": "ColorfulAI",
        "type": "user"
      },
      "summary": "多模言語モデル（MLLMs）の急速な進歩が促進したOmni言語モデルの開発は、多様なモードのデータの流れを処理し、主動的に応答することを目的としています。その可能性はあるが、流れ映像コンテキストでの実世界の相互作用能力の評価は、大きな課題です。本論文では、OmniLLMsの流れ映像コンテキストに適した詳細な多様交互作用ベンチマーク、OmniMMIを紹介します。OmniMMIは1,121以上の映像と2,290以上の質問を含み、流れ映像理解と主動的な理由論の2つの重要なけど調査不足している課題を解決する6つの異なるサブタスクを挙げます。また、新しいフレームワーク、Multi-modal Multiplexing Modeling（M4）を提案し、推論効率的な流れモデルを可能にし、視聴しながら生成できるものを設計します。",
      "upvotes": 9,
      "discussionId": "67eb58e6969b278277adb887",
      "ai_keywords": [
        "multi-modal language models",
        "GPT-4o",
        "Omni language models",
        "streaming video contexts",
        "OmniMMI",
        "multi-modal interaction benchmark",
        "streaming video understanding",
        "proactive reasoning",
        "Multi-modal Multiplexing Modeling",
        "inference-efficient streaming model"
      ]
    },
    "publishedAt": "2025-03-28T22:46:58.000Z",
    "title": "OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming\n  Video Contexts",
    "summary": "The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has\npropelled the development of Omni language models, designed to process and\nproactively respond to continuous streams of multi-modal data. Despite their\npotential, evaluating their real-world interactive capabilities in streaming\nvideo contexts remains a formidable challenge. In this work, we introduce\nOmniMMI, a comprehensive multi-modal interaction benchmark tailored for\nOmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and\n2,290 questions, addressing two critical yet underexplored challenges in\nexisting video benchmarks: streaming video understanding and proactive\nreasoning, across six distinct subtasks. Moreover, we propose a novel\nframework, Multi-modal Multiplexing Modeling (M4), designed to enable an\ninference-efficient streaming model that can see, listen while generating.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.22952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60b9e6837946aff342f734ae",
      "avatarUrl": "/avatars/a711a6aa35757dfd7b78b26098a964fc.svg",
      "fullname": "Yuxuan Wang",
      "name": "ColorfulAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.00509",
      "authors": [
        {
          "_id": "67ecae49a34baf018ca3c4cd",
          "user": {
            "_id": "65de7628deee79773f0f46f6",
            "avatarUrl": "/avatars/6c509dbe96e47b47271eb74178c1c9ba.svg",
            "isPro": false,
            "fullname": "Kai Yan",
            "user": "kaiyan289",
            "type": "user"
          },
          "name": "Kai Yan",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-02T03:26:02.443Z",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4ce",
          "user": {
            "_id": "642957104e073875f6a5ddd0",
            "avatarUrl": "/avatars/32b706d35c5ff52932c5029b94caa7b9.svg",
            "isPro": false,
            "fullname": "Yufei Xu",
            "user": "yfxu",
            "type": "user"
          },
          "name": "Yufei Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:23.307Z",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4cf",
          "name": "Zhengyin Du",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4d0",
          "name": "Xuesong Yao",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4d1",
          "name": "Zheyu Wang",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4d2",
          "name": "Xiaowen Guo",
          "hidden": false
        },
        {
          "_id": "67ecae49a34baf018ca3c4d3",
          "name": "Jiecao Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T07:57:58.000Z",
      "submittedOnDailyAt": "2025-04-02T01:56:35.333Z",
      "title": "レシピオーデロジン：どのように先進的な言語モデルが小学校レベルの理由問題で失敗するか？",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "最近数年、LLMベンチマークの難易度が幼稚園レベルから先進的な問題に急速に上昇し、研究者たちには、私たちは人間の知能を超えることにちょっとだけ離れているという奇跡を織り出した。しかし、LLMの驚異的な論理能力は、人間の標準での真の知能か、または、テキストデータの学習中に見た解決策を再現しているかどうかであるかどうかであるか。この問題を研究するために、私たちはRoR-Benchという新しい、多タイプベンチマークを提案し、このベンチマークで実験的な分析を行います。その結果として、すべての先進的なLLMが非常に厳しい再現的な行動を示し、条件の一言を変更すると、OpenAI-o1やDeepSeek-R1のようなトップモデルは幼稚園レベルの算術と論理問題の性能が60%損失します。このファインドは、LLMコミュニティに警告を呼びかけ、先進的なLLMの真の知能レベルを再評価する必要を迫ります。",
      "upvotes": 7,
      "discussionId": "67ecae4aa34baf018ca3c506",
      "projectPage": "https://team.doubao.com/zh/publication/recitation-over-reasoning-how-cutting-edge-language-models-can-fail-on-elementary-school-level-reasoning-problems?view_from=homepage_recommend",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "RoR-Bench",
        "multi-modal benchmark",
        "recitation behavior",
        "elementary school-level arithmetic",
        "reasoning problems",
        "OpenAI-o1",
        "DeepSeek-R1"
      ]
    },
    "publishedAt": "2025-04-01T03:57:58.000Z",
    "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on\n  Elementary School-Level Reasoning Problems?",
    "summary": "The rapid escalation from elementary school-level to frontier problems of the\ndifficulty for LLM benchmarks in recent years have weaved a miracle for\nresearchers that we are only inches away from surpassing human intelligence.\nHowever, is the LLMs' remarkable reasoning ability indeed comes from true\nintelligence by human standards, or are they simply reciting solutions\nwitnessed during training at an Internet level? To study this problem, we\npropose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's\nrecitation behavior when asked simple reasoning problems but with conditions\nsubtly shifted, and conduct empirical analysis on our benchmark. Surprisingly,\nwe found existing cutting-edge LLMs unanimously exhibits extremely severe\nrecitation behavior; by changing one phrase in the condition, top models such\nas OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary\nschool-level arithmetic and reasoning problems. Such findings are a wake-up\ncall to the LLM community that compels us to re-evaluate the true intelligence\nlevel of cutting-edge LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00509.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6566
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.01005",
      "authors": [
        {
          "_id": "67ec95ea3d267d26663ea34b",
          "name": "Nishad Singhi",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea34c",
          "name": "Hritik Bansal",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea34d",
          "name": "Arian Hosseini",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea34e",
          "name": "Aditya Grover",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea34f",
          "name": "Kai-Wei Chang",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea350",
          "name": "Marcus Rohrbach",
          "hidden": false
        },
        {
          "_id": "67ec95ea3d267d26663ea351",
          "name": "Anna Rohrbach",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T17:41:57.000Z",
      "submittedOnDailyAt": "2025-04-02T00:12:49.083Z",
      "title": "どのような場合で解決し、どのような場合で確認するか：最適な計算の問題解決と生成的な確認について",
      "submittedOnDailyBy": {
        "_id": "61c5c25705aa54027c52f7b3",
        "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
        "isPro": false,
        "fullname": "Hritik Bansal",
        "user": "hbXNov",
        "type": "user"
      },
      "summary": "スケーリングテストタイムコンピュートは、大規模言語モデル（LLMs）の説明能力を向上させるための重要な戦略として現れてきました、特に数理問題解決のような複雑なタスクにおいて。従来のアプローチでは、Self-Consistency（SC）が問題に対して複数の解を生成し、多数決で最もよく出る回答を選択することで、最適な解を選択することができました。もう一つの一般的な方法は、各解を報酬モデル（バリデータ）でスコアを設定し、最も良い解を選択することです。最近、Generative Reward Models（GenRM）の進展は、バリデーションを次のトークン予測タスクとして再構成し、推論時コンピュートのスケーリングを新たな軸に沿って行うことができるようになりました。特に、GenRMは各解を評価するために複数のバリデーションのチャイムストークを生成します。推論バッジが限られた状況下では、これは基本的なトレードオフを引き起こすことになります：解のスケーリングにバッジを使うか、解の生成を減らし、バリデーションにコンピュートを割り当てるかの選択を求めます。これに対して、SCとGenRMを固定した推論バッジの下で比較しました。興味深いなことに、SCは多様なモデルとデータセットで実用的な推論バッジのほとんど全ての場合に、GenRMよりもコンピュート効率的でした。例えば、GenRMは8倍の推論コンピュートを使用してSCと同じのような性能を達成し、それに比べてもっと多くのコンピュートを必要とします。また、GenRMパラダイムの推論スケーリングの法則を見出し、コンピュート最適化の推論は解の生成のスケーリングよりもバリデーションの数のスケーリングを激しく行うことが有利であることが明らかになりました。我々の研究は、解の生成とバリデーションのバランスを調整することでテストタイムスケーリングを最適化するための実用的なガイドラインを提供します。コードは、https://github.com/nishadsinghi/sc-genrm-scaling に公開されています。",
      "upvotes": 6,
      "discussionId": "67ec95eb3d267d26663ea38b",
      "ai_keywords": [
        "Large language models (LLMs)",
        "Mathematical problem-solving",
        "Self-Consistency (SC)",
        "Reward model (verifier)",
        "Generative Reward Models (GenRM)",
        "Next-token prediction task",
        "Chains-of-thought",
        "Inference budget",
        "Compute-efficient",
        "Inference scaling laws",
        "Compute-optimal inference"
      ]
    },
    "publishedAt": "2025-04-01T13:41:57.000Z",
    "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and\n  Generative Verification for LLM Reasoning",
    "summary": "Scaling test-time compute has emerged as a key strategy for enhancing the\nreasoning capabilities of large language models (LLMs), particularly in tasks\nlike mathematical problem-solving. A traditional approach, Self-Consistency\n(SC), generates multiple solutions to a problem and selects the most common\nanswer via majority voting. Another common method involves scoring each\nsolution with a reward model (verifier) and choosing the best one. Recent\nadvancements in Generative Reward Models (GenRM) reframe verification as a\nnext-token prediction task, enabling inference-time scaling along a new axis.\nSpecifically, GenRM generates multiple verification chains-of-thought to score\neach solution. Under a limited inference budget, this introduces a fundamental\ntrade-off: should you spend the budget on scaling solutions via SC or generate\nfewer solutions and allocate compute to verification via GenRM? To address\nthis, we evaluate GenRM against SC under a fixed inference budget.\nInterestingly, we find that SC is more compute-efficient than GenRM for most\npractical inference budgets across diverse models and datasets. For instance,\nGenRM first matches SC after consuming up to 8x the inference compute and\nrequires significantly more compute to outperform it. Furthermore, we derive\ninference scaling laws for the GenRM paradigm, revealing that compute-optimal\ninference favors scaling solution generation more aggressively than scaling the\nnumber of verifications. Our work provides practical guidance on optimizing\ntest-time scaling by balancing solution generation and verification. The code\nis available at https://github.com/nishadsinghi/sc-genrm-scaling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01005.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "61c5c25705aa54027c52f7b3",
      "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
      "fullname": "Hritik Bansal",
      "name": "hbXNov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00557",
      "authors": [
        {
          "_id": "67eca9a501a4d1c29e4e70f1",
          "name": "Jewon Lee",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f2",
          "name": "Ki-Ung Song",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f3",
          "name": "Seungmin Yang",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f4",
          "name": "Donguk Lim",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f5",
          "name": "Jaeyeon Kim",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f6",
          "name": "Wooksu Shin",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f7",
          "name": "Bo-Kyeong Kim",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f8",
          "name": "Yong Jae Lee",
          "hidden": false
        },
        {
          "_id": "67eca9a501a4d1c29e4e70f9",
          "name": "Tae-Ho Kim",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/61f44bab7eba274ea80b74ce/Tk5AAk1Qg7HtPWEYdavBk.png"
      ],
      "publishedAt": "2025-04-01T09:10:32.000Z",
      "submittedOnDailyAt": "2025-04-02T01:41:42.016Z",
      "title": "Efficient LLaMA-3.2-Vision を実現するために、クロスアテンデッドビジュアルフィーチャーを削減する方法",
      "submittedOnDailyBy": {
        "_id": "61f44bab7eba274ea80b74ce",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61f44bab7eba274ea80b74ce/BRbKX1jephdZ7D44FATl4.jpeg",
        "isPro": false,
        "fullname": "Hyoung-Kyu Song",
        "user": "deepkyu",
        "type": "user"
      },
      "summary": "画像特徴量の拡大による推論コストの低減を図るために、可視トークン削減が大規模な視覚言語モデル（LVLMs）における推論コストを下げる。これは、自動注意だけのLVLMsでトークンを削減する相関研究と異なり、我々の研究は特にクロス注意ベースのモデルを対象にして優れた性能を実現している。我々は、クロス注意層の画像トークンのキーバリュー（KV）キャッシュサイズが自動注意層のテキストトークンより大幅に大きく、計算ボトルネックとして重要な問題を形成していることを見出した。この問題を軽減するために、我々はクロス注意マップのスパースな性質を利用して、過剩な視覚特徴量を選択的に削減する。我々の「トリミングラマ」は、追加のトレーニングが必要とされないように、KVキャッシュの要求を効果的に減らす。50%の視覚特徴量を削減することにより、我々のモデルは推論ラテンシーとメモリ使用量を減少しながらベンチマークの準同じ性能を実現できる。",
      "upvotes": 5,
      "discussionId": "67eca9a601a4d1c29e4e7131",
      "ai_keywords": [
        "visual token reduction",
        "inference costs",
        "image features",
        "large vision-language models (LVLMs)",
        "self-attention-only LVLMs",
        "cross-attention-based models",
        "key-value (KV) cache size",
        "self-attention layers",
        "cross-attention layers",
        "sparse nature",
        "cross-attention maps",
        "redundant visual features",
        "Trimmed Llama",
        "KV cache demands",
        "inference latency",
        "memory usage",
        "benchmark parity"
      ]
    },
    "publishedAt": "2025-04-01T05:10:32.000Z",
    "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
    "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/61f44bab7eba274ea80b74ce/Tk5AAk1Qg7HtPWEYdavBk.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00557.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61f44bab7eba274ea80b74ce",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61f44bab7eba274ea80b74ce/BRbKX1jephdZ7D44FATl4.jpeg",
      "fullname": "Hyoung-Kyu Song",
      "name": "deepkyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00294",
      "authors": [
        {
          "_id": "67ecadb199892db0bda56561",
          "name": "Vidhisha Balachandran",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56562",
          "name": "Jingya Chen",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56563",
          "name": "Lingjiao Chen",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56564",
          "name": "Shivam Garg",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56565",
          "name": "Neel Joshi",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56566",
          "name": "Yash Lara",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56567",
          "name": "John Langford",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56568",
          "name": "Besmira Nushi",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda56569",
          "name": "Vibhav Vineet",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda5656a",
          "name": "Yue Wu",
          "hidden": false
        },
        {
          "_id": "67ecadb199892db0bda5656b",
          "name": "Safoora Yousefi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T23:40:28.000Z",
      "submittedOnDailyAt": "2025-04-02T01:53:41.171Z",
      "title": "推論時のスケーリングにおける複雑なタスク：現在の状況と次のステップ",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "推理時のスケーリングは、ステップごとの問題解決をベースにした複雑な問題において、大規模な言語モデル（LLMs）の推理能力を向上させることができます。長くする生成フラッシュペープは、数学タスクに対して効果的であることは証明されていますが、このアプローチの広範囲的な影響は他のタスクに対しては明確ではありません。本研究では、9つの最先端のモデルと8つの艱難なタスク（数学やSTEMの理由論、日曜表計画、NP-hard問題、ナビゲーション、空間の理由論）において、スケーリング方法の利益と制限を調査します。単なるモデル呼び出しの繰り返しや、フィードバックを含む順次なものでの評価プロトコルを用いて、伝統的なモデル（例：GPT-4o）と推論時のスケーリングに対して調整されたモデル（例：o1）を比較します。これらの評価は、各モデルの性能の下限と上限を近似し、将来の性能向上の可能性を示します。これらの評価は、強化された訓練や多モデル推論システムを通じて実現できる性能向上の可能性を示します。厳密な実験的な分析により、推論時のスケーリングの優位はタスクにより異なり、問題の複雑さが増加するにつれて減少することがわかります。また、単にモデルが生成するトークンを増やすだけでは、これらの難しいセンターでの精度の向上は確実に得られないこともわかります。伝統的なモデルを用いた複数の独立な実験の結果から、ポートフォートバリデーターを使用した場合、あるタスクでは、今日最先端の理由論モデルの平均性能に近い性能を達成することができます。しかし、他のタスクでは、非常に高いスケーリングレジムでも顕著な性能間隔が残っています。興味深いことに、すべてのモデルは、ポートフォートバリデーターを使用した場合や強いフィードバックを受けた場合には、顕著な効果を示し、将来の改善のためには充足な可能性があることが示唆されています。",
      "upvotes": 4,
      "discussionId": "67ecadb299892db0bda565aa",
      "ai_keywords": [
        "inference-time scaling",
        "reasoning capabilities",
        "large language models (LLMs)",
        "step-by-step problem solving",
        "generated scratchpads",
        "state-of-the-art models",
        "math and STEM reasoning",
        "calendar planning",
        "NP-hard problems",
        "navigation",
        "spatial reasoning",
        "conventional models",
        "fine-tuned models",
        "model calls",
        "performance bounds",
        "multi-model inference systems",
        "empirical analysis",
        "token usage",
        "accuracy",
        "perfect verifiers",
        "performance gap"
      ]
    },
    "publishedAt": "2025-03-31T19:40:28.000Z",
    "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies\n  Ahead",
    "summary": "Inference-time scaling can enhance the reasoning capabilities of large\nlanguage models (LLMs) on complex problems that benefit from step-by-step\nproblem solving. Although lengthening generated scratchpads has proven\neffective for mathematical tasks, the broader impact of this approach on other\ntasks remains less clear. In this work, we investigate the benefits and\nlimitations of scaling methods across nine state-of-the-art models and eight\nchallenging tasks, including math and STEM reasoning, calendar planning,\nNP-hard problems, navigation, and spatial reasoning. We compare conventional\nmodels (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g.,\no1) through evaluation protocols that involve repeated model calls, either\nindependently or sequentially with feedback. These evaluations approximate\nlower and upper performance bounds and potential for future performance\nimprovements for each model, whether through enhanced training or multi-model\ninference systems. Our extensive empirical analysis reveals that the advantages\nof inference-time scaling vary across tasks and diminish as problem complexity\nincreases. In addition, simply using more tokens does not necessarily translate\nto higher accuracy in these challenging regimes. Results from multiple\nindependent runs with conventional models using perfect verifiers show that,\nfor some tasks, these models can achieve performance close to the average\nperformance of today's most advanced reasoning models. However, for other\ntasks, a significant performance gap remains, even in very high scaling\nregimes. Encouragingly, all models demonstrate significant gains when inference\nis further scaled with perfect verifiers or strong feedback, suggesting ample\npotential for future improvements.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00294.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6566
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23361",
      "authors": [
        {
          "_id": "67eb57a56522661171fb4725",
          "user": {
            "_id": "64d660308ebc40443813f014",
            "avatarUrl": "/avatars/516bb2d2383be99794e366dfb41636b6.svg",
            "isPro": false,
            "fullname": "Linxin Song",
            "user": "linxinso",
            "type": "user"
          },
          "name": "Linxin Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:23:45.180Z",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb4726",
          "name": "Xuwei Ding",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb4727",
          "name": "Jieyu Zhang",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb4728",
          "user": {
            "_id": "62e1b3cb3eb0730f621a83f6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg",
            "isPro": false,
            "fullname": "Taiwei Shi",
            "user": "MaksimSTW",
            "type": "user"
          },
          "name": "Taiwei Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:48.349Z",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb4729",
          "name": "Ryotaro Shimizu",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb472a",
          "name": "Rahul Gupta",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb472b",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb472c",
          "name": "Jian Kang",
          "hidden": false
        },
        {
          "_id": "67eb57a56522661171fb472d",
          "name": "Jieyu Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T08:33:56.000Z",
      "submittedOnDailyAt": "2025-04-02T00:04:15.369Z",
      "title": "マスターキャップの知識不足を語言モデルで発見する",
      "submittedOnDailyBy": {
        "_id": "62e1b3cb3eb0730f621a83f6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg",
        "isPro": false,
        "fullname": "Taiwei Shi",
        "user": "MaksimSTW",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、誇りのある言語能力を持っていますが、事実的な知識を忠実に保持することが難しく、ハロケーションと不信頼な出力を生み出します。LLMsの知識の欠陥を理解するために、全サイズの知識ベースに対して厳密な評価を行うことは、特にクローズドウェイトモデルに対して計算的に難しいです。私たちは、厳格なクエリバケットの下でクローズドウェイトLLMsの知識の欠陥（エラー）を発見するためのスケーラブルで効率的なフレームワーク「stochastic error ascent (SEA)」を提案します。SEAは、すべての知識候補を無駄に調査するよりも、ロジック的に類似性を用いて以前に見た失敗によって新しい高エラー候補を発見することをシステマ的に構築します。SEAは、ドキュメントと段落レベルでのヒューリスティックな検索を行い、エラーの伝播をモデル化し、システム的な失敗モードを特定するために関係ディレクショナルな有向なグラフを構築します。実験的には、SEAはAutomated Capability Discoveryより40.7倍、AutoBencherより26.7%より多くの知識エラーを発見し、エラーごとのコストを599倍と9倍に減少させます。人間評価は、生成された質の高い質問を確認し、ablationとconvergence分析はSEAの各成分の貢献を証明します。発見されたエラーの進める分析は、LLMファミリー間の関連ファイルと再現する欠陥を示し、将来のLLM開発でより良いデータカバージョンとターゲット付きの微調校の必要性を強調します。",
      "upvotes": 4,
      "discussionId": "67eb57a66522661171fb476a",
      "githubRepo": "https://github.com/uscnlp-lime/SEA",
      "ai_keywords": [
        "stochastic error ascent (SEA)",
        "knowledge deficiencies (errors)",
        "closed-weight LLMs",
        "stochastic optimization process",
        "semantic similarity",
        "hierarchical retrieval",
        "document level",
        "paragraph level",
        "relation directed acyclic graph (DAG)",
        "error propagation",
        "systematic failure modes",
        "Automated Capability Discovery",
        "AutoBencher",
        "cost-per-error",
        "human evaluation",
        "ablation analysis",
        "convergence analysis",
        "correlated failure patterns",
        "LLM families",
        "data coverage",
        "targeted fine-tuning"
      ]
    },
    "publishedAt": "2025-03-30T04:33:56.000Z",
    "title": "Discovering Knowledge Deficiencies of Language Models on Massive\n  Knowledge Base",
    "summary": "Large language models (LLMs) possess impressive linguistic capabilities but\noften fail to faithfully retain factual knowledge, leading to hallucinations\nand unreliable outputs. Understanding LLMs' knowledge deficiencies by\nexhaustively evaluating against full-scale knowledge bases is computationally\nprohibitive, especially for closed-weight models. We propose stochastic error\nascent (SEA), a scalable and efficient framework for discovering knowledge\ndeficiencies (errors) in closed-weight LLMs under a strict query budget. Rather\nthan naively probing all knowledge candidates, SEA formulates error discovery\nas a stochastic optimization process: it iteratively retrieves new high-error\ncandidates by leveraging the semantic similarity to previously observed\nfailures. To further enhance search efficiency and coverage, SEA employs\nhierarchical retrieval across document and paragraph levels, and constructs a\nrelation directed acyclic graph to model error propagation and identify\nsystematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors\nthan Automated Capability Discovery and 26.7% more than AutoBencher, while\nreducing the cost-per-error by 599x and 9x, respectively. Human evaluation\nconfirms the high quality of generated questions, while ablation and\nconvergence analyses validate the contribution of each component in SEA.\nFurther analysis on the discovered errors reveals correlated failure patterns\nacross LLM families and recurring deficits, highlighting the need for better\ndata coverage and targeted fine-tuning in future LLM development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23361.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e1b3cb3eb0730f621a83f6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg",
      "fullname": "Taiwei Shi",
      "name": "MaksimSTW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.01017",
      "authors": [
        {
          "_id": "67ecebc51f669fb5591616cd",
          "name": "David Fan",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616ce",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616cf",
          "name": "Jiachen Zhu",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d0",
          "name": "Koustuv Sinha",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d1",
          "name": "Zhuang Liu",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d2",
          "name": "Xinlei Chen",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d3",
          "name": "Michael Rabbat",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d4",
          "name": "Nicolas Ballas",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d5",
          "name": "Yann LeCun",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d6",
          "name": "Amir Bar",
          "hidden": false
        },
        {
          "_id": "67ecebc51f669fb5591616d7",
          "name": "Saining Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T17:59:15.000Z",
      "submittedOnDailyAt": "2025-04-02T06:19:35.566Z",
      "title": "スケーリングフレームワークラングジャイアント語言無し可視表現学習",
      "submittedOnDailyBy": {
        "_id": "6374cbb7255276f3a22b4b35",
        "avatarUrl": "/avatars/7cf1bbb83447441e5fa2e1e4fcf7617b.svg",
        "isPro": true,
        "fullname": "Peter Tong",
        "user": "tsbpp",
        "type": "user"
      },
      "summary": "Visual Self-Supervised Learning (SSL)は、Visual Question Answering (VQA)などの多モデル設定では、Contrastive Language-Image Pretraining (CLIP)よりも性能が低い状況にある。この多モデル間の間違いは、言語のサブジェクトによる意味を引き出したものであることが多くの場合であるが、Visual SSLとCLIPモデルは異なるデータによって訓練されていることを踏まえている。本論文では、「Visual SSLはCLIPよりも性能が低いのは、言語のサブジェクトの欠如か、訓練データの違いからであるか」という問題を調べる。Visual SSLとCLIPモデルを同じMetaCLIPデータによって訓練し、VQAを多様性のテストベッドとして利用することで、この問題を調べる。この制御された設定では、Visual SSLモデルはデータとモデルの容量においてCLIPモデルよりもスケーリングが良く、7Bパラメータまでスケールアップした後も性能が減衰しない。このため、Visual SSLモデルはVQAと古典的な視覚ベンチマークでCLIPレベルの性能を達成することを観察する。これらの発見は、言語サブジェクトによる視覚予備学習と比べて、純粋なVisual SSLがスケールでも同程度の性能を達成できることを示し、視覚コーナーカスティングの表現学習の新たな機会を開拓する。",
      "upvotes": 3,
      "discussionId": "67ecebc61f669fb559161742",
      "ai_keywords": [
        "Visual Self-Supervised Learning (SSL)",
        "Contrastive Language-Image Pretraining (CLIP)",
        "Visual Question Answering (VQA)",
        "MetaCLIP data",
        "vision encoders",
        "data capacity",
        "CLIP-level performance",
        "vision-centric representation learning"
      ]
    },
    "publishedAt": "2025-04-01T13:59:15.000Z",
    "title": "Scaling Language-Free Visual Representation Learning",
    "summary": "Visual Self-Supervised Learning (SSL) currently underperforms Contrastive\nLanguage-Image Pretraining (CLIP) in multimodal settings such as Visual\nQuestion Answering (VQA). This multimodal gap is often attributed to the\nsemantics introduced by language supervision, even though visual SSL and CLIP\nmodels are often trained on different data. In this work, we ask the question:\n\"Do visual self-supervised approaches lag behind CLIP due to the lack of\nlanguage supervision, or differences in the training data?\" We study this\nquestion by training both visual SSL and CLIP models on the same MetaCLIP data,\nand leveraging VQA as a diverse testbed for vision encoders. In this controlled\nsetup, visual SSL models scale better than CLIP models in terms of data and\nmodel capacity, and visual SSL performance does not saturate even after scaling\nup to 7B parameters. Consequently, we observe visual SSL methods achieve\nCLIP-level performance on a wide range of VQA and classic vision benchmarks.\nThese findings demonstrate that pure visual SSL can match language-supervised\nvisual pretraining at scale, opening new opportunities for vision-centric\nrepresentation learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.01017.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6374cbb7255276f3a22b4b35",
      "avatarUrl": "/avatars/7cf1bbb83447441e5fa2e1e4fcf7617b.svg",
      "fullname": "Peter Tong",
      "name": "tsbpp",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00927",
      "authors": [
        {
          "_id": "67ecc23b6a2bc6abdc2e9183",
          "name": "Olga Golovneva",
          "hidden": false
        },
        {
          "_id": "67ecc23b6a2bc6abdc2e9184",
          "name": "Tianlu Wang",
          "hidden": false
        },
        {
          "_id": "67ecc23b6a2bc6abdc2e9185",
          "name": "Jason Weston",
          "hidden": false
        },
        {
          "_id": "67ecc23b6a2bc6abdc2e9186",
          "name": "Sainbayar Sukhbaatar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T15:59:32.000Z",
      "submittedOnDailyAt": "2025-04-02T03:21:25.809Z",
      "title": "Multi-Token Attention を日本語に翻訳します。\n\n**Multi-Token Attention**\n\nこれは、多タグ関注（Multi-Token Attention）という機械学習や自然言語処理における重要な概念です。このアプローチは、通常の単一タグ関注（Single-Token Attention）に比べて、文脈をより複雑なように捉えることができます。特に、長文や複雑な文脈を処理する場合には、このアプローチが有用です。",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Soft attentionは、LLMsが与えられたコンテキスト内の関連部分を特定するための重要な機能です。しかし、個々のattention weightsは、そのひとつのqueryとkey token vectorの類似性によって決定されます。この「単一トークンattention」は、コンテキスト全体から関連部分を区別するために使用される情報量を制限しています。この問題に対処するために、私たちは、LLMsが同時に複数のqueryとkey vectorによってattention weightsを条件化する新しいattention方法、Multi-Token Attention (MTA)を提案します。これは、queries、keysとheadsにコンバージョン操作を適用し、近くのqueryとkeyがそれぞれのattention weightsを影響するようにして実現されます。このように、私たちの方法は、一つのvectorの容量を超える豊富な、より複雑な情報を使用して関連コンテキストを特定することができます。詳細な評価を通じて、私たちは、MTAが様々な人気ベンチマークで機能を向上させることを示します。特に、標準的な言語モデリングタスクでTransformerの基準モデルよりも優れて、長いコンテキスト内の情報検索を必要とするタスクでも、私たちの方法の豊富な情報を活用することが特に有利です。",
      "upvotes": 3,
      "discussionId": "67ecc23c6a2bc6abdc2e91c2",
      "ai_keywords": [
        "Soft attention",
        "LLMs (Large Language Models)",
        "single token attention",
        "Multi-Token Attention (MTA)",
        "convolution operations",
        "queries",
        "keys",
        "heads",
        "attention weights"
      ]
    },
    "publishedAt": "2025-04-01T11:59:32.000Z",
    "title": "Multi-Token Attention",
    "summary": "Soft attention is a critical mechanism powering LLMs to locate relevant parts\nwithin a given context. However, individual attention weights are determined by\nthe similarity of only a single query and key token vector. This \"single token\nattention\" bottlenecks the amount of information used in distinguishing a\nrelevant part from the rest of the context. To address this issue, we propose a\nnew attention method, Multi-Token Attention (MTA), which allows LLMs to\ncondition their attention weights on multiple query and key vectors\nsimultaneously. This is achieved by applying convolution operations over\nqueries, keys and heads, allowing nearby queries and keys to affect each\nother's attention weights for more precise attention. As a result, our method\ncan locate relevant context using richer, more nuanced information that can\nexceed a single vector's capacity. Through extensive evaluations, we\ndemonstrate that MTA achieves enhanced performance on a range of popular\nbenchmarks. Notably, it outperforms Transformer baseline models on standard\nlanguage modeling tasks, and on tasks that require searching for information\nwithin long contexts, where our method's ability to leverage richer information\nproves particularly beneficial.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00927.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6566
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00869",
      "authors": [
        {
          "_id": "67ecb60276900f68cd1df503",
          "name": "Xiaoke Huang",
          "hidden": false
        },
        {
          "_id": "67ecb60276900f68cd1df504",
          "name": "Juncheng Wu",
          "hidden": false
        },
        {
          "_id": "67ecb60276900f68cd1df505",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "67ecb60276900f68cd1df506",
          "name": "Xianfeng Tang",
          "hidden": false
        },
        {
          "_id": "67ecb60276900f68cd1df507",
          "name": "Yuyin Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T14:57:43.000Z",
      "submittedOnDailyAt": "2025-04-02T02:30:28.553Z",
      "title": "m1: 医療診断のテスト時スケーリングの潛力を大規模言語モデルによって解放する",
      "submittedOnDailyBy": {
        "_id": "63318b2349a9563915469f3b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg",
        "isPro": true,
        "fullname": "Xiaoke Huang",
        "user": "xk-huang",
        "type": "user"
      },
      "summary": "検証時スケーリングは、大規模な言語モデルの論理能力を向上させるために強力な技術として現れた。しかし、その医学論理における効果は不明確であり、医学領域は数学タスクと比べて知識表現と判断過程に構造的に異なるためである。本論文では、検証時スケーリングの最初の詳細な調査を提供し、m1という簡単で効果的なアプローチを提案し、推論時にモデルの医学論理能力を向上させる。多様な医学タスクの評価により、検証時スケーリングは医学論理を一貫して向上させ、10Bパラメータ未満の軽量調節されたモデルが新しい最先端性能を収め、我々の32Bモデルは先週の70Bスケールの医学LLMとの比較で優れていることを示した。しかし、最適な論理トークンバジェットは約4Kに近いものであり、これよりも多くの場合は誤った判断を原因として性能が低下することが見られる。反復的なプロンプトを通じた検証時計算を延長するバジェットフォーシングは、モデルが答えをチェックすることを促すが、全体の医学QA性能を向上させることは非必然的であり、その場合はそれほど正しい回答を導くことはなく、先ほど正しい回答に誤りを引き込むこともある。個別の分析では、不十分な医学知識が性能の進歩を妨げる主要なボトルネックとして識別され、データサイズの拡大、データ品質の向上、モデルキャパシティの拡大は医学知識の基礎を強化し、特に難しい医学ベンチマークでは小さなモデルがサチュレーションを達成することができ、性能の進歩が継続されることが示された。これらの発見は、LLMの医学と数学の論理の基本的な違いを明らかにし、医学知識の豊富化が、理由の深さの増加だけでなく、検証時スケーリングの効果を実現するために不可欠であることを強調している。",
      "upvotes": 3,
      "discussionId": "67ecb60376900f68cd1df550",
      "githubRepo": "https://github.com/UCSC-VLAA/m1",
      "ai_keywords": [
        "test-time scaling",
        "large language models",
        "medical reasoning",
        "knowledge representation",
        "decision-making processes",
        "lightweight fine-tuned models",
        "state-of-the-art performance",
        "reasoning token budget",
        "budget forcing",
        "iterative prompts",
        "medical QA performance",
        "medical knowledge",
        "data scale",
        "data quality",
        "model capacity",
        "medical knowledge grounding",
        "challenging medical benchmarks",
        "reasoning depth"
      ]
    },
    "publishedAt": "2025-04-01T10:57:43.000Z",
    "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning\n  with Large Language Models",
    "summary": "Test-time scaling has emerged as a powerful technique for enhancing the\nreasoning capabilities of large language models. However, its effectiveness in\nmedical reasoning remains uncertain, as the medical domain fundamentally\ndiffers from mathematical tasks in terms of knowledge representation and\ndecision-making processes. In this paper, we provide the first comprehensive\ninvestigation of test-time scaling for medical reasoning and present m1, a\nsimple yet effective approach that increases a model's medical reasoning\ncapability at inference. Our evaluation across diverse medical tasks\ndemonstrates that test-time scaling consistently enhances medical reasoning,\nenabling lightweight fine-tuned models under 10B parameters to establish new\nstate-of-the-art performance, while our 32B model rivals previous 70B-scale\nmedical LLMs. However, we identify an optimal reasoning token budget of\napproximately 4K, beyond which performance may degrade due to overthinking.\nBudget forcing, which extends test-time computation through iterative prompts,\nhelps models double-check answers but does not necessarily improve the overall\nmedical QA performance and, in some cases, even introduces errors into\npreviously correct responses. Our case-by-case analysis identifies insufficient\nmedical knowledge as a key bottleneck that prevents further performance gains\nthrough test-time scaling. We find that increasing data scale, improving data\nquality, and expanding model capacity consistently enhance medical knowledge\ngrounding, enabling continued performance improvements, particularly on\nchallenging medical benchmarks where smaller models reach saturation. These\nfindings underscore fundamental differences between medical and mathematical\nreasoning in LLMs, highlighting that enriched medical knowledge, other than\nincreased reasoning depth alone, is essential for realizing the benefits of\ntest-time scaling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00869.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63318b2349a9563915469f3b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg",
      "fullname": "Xiaoke Huang",
      "name": "xk-huang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23434",
      "authors": [
        {
          "_id": "67ecf47dd0f4d6684e0fb7e4",
          "name": "Yucheng Shi",
          "hidden": false
        },
        {
          "_id": "67ecf47dd0f4d6684e0fb7e5",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "67ecf47dd0f4d6684e0fb7e6",
          "name": "Wenlin Yao",
          "hidden": false
        },
        {
          "_id": "67ecf47dd0f4d6684e0fb7e7",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "67ecf47dd0f4d6684e0fb7e8",
          "name": "Ninghao Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T13:26:00.000Z",
      "submittedOnDailyAt": "2025-04-02T06:59:17.200Z",
      "title": "デプロイドガイドバイダーズに向けたチャレンジ：サーチ",
      "submittedOnDailyBy": {
        "_id": "64beb6b6140491ca9f803ebf",
        "avatarUrl": "/avatars/0daa2e813a13668b8b708cd8c12763d9.svg",
        "isPro": false,
        "fullname": "Yucheng SHi",
        "user": "YuchengShi",
        "type": "user"
      },
      "summary": "GUI アガント、大規模な基礎モデルをもって力が入っているものは、デジタルインターフェースと相互作用することができ、ウェブ自動化、モバイルナビゲーション、ソフトウェアテストなど様々なアプリケーションを可能にします。しかし、その拡大した自動性により、セキュリティ、プライバシー、安全性についての重要な懸念が生じてきました。この調査は、GUI アガントの信頼性を構成する5つの重要な次元について検討しています：セキュリティの脆弱性、動的な環境での信頼性、透明性と説明性、倫理的な考慮、評価手法。また、対抗的攻撃に脆弱なこと、連鎖的失敗モード、実際的な評価ベンチマークの欠如などの大きな課題を明らかにしています。これらの問題は、実世界的な機能の妨げ、そしてタスクの成功よりも広範囲に及ぶ全面的な対策の必要性を求めています。GUI アガントが普及することにより、強固な安全規格と責任的な開発プラクティスの構築が重要です。この調査は、信頼性のある GUI アガントの進歩を促進するための基盤を提供し、将来の研究によりシステム的な理解を深めることを目指しています。",
      "upvotes": 3,
      "discussionId": "67ecf480d0f4d6684e0fb855",
      "githubRepo": "https://github.com/sycny/Awesome-Trustworthy-GUI-Agents",
      "ai_keywords": [
        "foundation models",
        "GUI agents",
        "web automation",
        "mobile navigation",
        "software testing",
        "security vulnerabilities",
        "reliability in dynamic environments",
        "transparency and explainability",
        "ethical considerations",
        "evaluation methodologies",
        "adversarial attacks",
        "cascading failure modes",
        "sequential decision-making",
        "realistic evaluation benchmarks"
      ]
    },
    "publishedAt": "2025-03-30T09:26:00.000Z",
    "title": "Towards Trustworthy GUI Agents: A Survey",
    "summary": "GUI agents, powered by large foundation models, can interact with digital\ninterfaces, enabling various applications in web automation, mobile navigation,\nand software testing. However, their increasing autonomy has raised critical\nconcerns about their security, privacy, and safety. This survey examines the\ntrustworthiness of GUI agents in five critical dimensions: security\nvulnerabilities, reliability in dynamic environments, transparency and\nexplainability, ethical considerations, and evaluation methodologies. We also\nidentify major challenges such as vulnerability to adversarial attacks,\ncascading failure modes in sequential decision-making, and a lack of realistic\nevaluation benchmarks. These issues not only hinder real-world deployment but\nalso call for comprehensive mitigation strategies beyond task success. As GUI\nagents become more widespread, establishing robust safety standards and\nresponsible development practices is essential. This survey provides a\nfoundation for advancing trustworthy GUI agents through systematic\nunderstanding and future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23434.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64beb6b6140491ca9f803ebf",
      "avatarUrl": "/avatars/0daa2e813a13668b8b708cd8c12763d9.svg",
      "fullname": "Yucheng SHi",
      "name": "YuchengShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00698",
      "authors": [
        {
          "_id": "67ecc2eb077a9bc63e7ba1a2",
          "name": "Team Cohere",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a3",
          "name": "Aakanksha",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a4",
          "name": "Arash Ahmadian",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a5",
          "name": "Marwan Ahmed",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a6",
          "name": "Jay Alammar",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a7",
          "name": "Yazeed Alnumay",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a8",
          "name": "Sophia Althammer",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1a9",
          "name": "Arkady Arkhangorodsky",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1aa",
          "name": "Viraat Aryabumi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ab",
          "name": "Dennis Aumiller",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ac",
          "name": "Raphaël Avalos",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ad",
          "name": "Zahara Aviv",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ae",
          "name": "Sammie Bae",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1af",
          "name": "Saurabh Baji",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b0",
          "name": "Alexandre Barbet",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b1",
          "name": "Max Bartolo",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b2",
          "name": "Björn Bebensee",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b3",
          "name": "Neeral Beladia",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b4",
          "name": "Walter Beller-Morales",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b5",
          "name": "Alexandre Bérard",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b6",
          "name": "Andrew Berneshawi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b7",
          "name": "Anna Bialas",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b8",
          "name": "Phil Blunsom",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1b9",
          "name": "Matt Bobkin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ba",
          "name": "Adi Bongale",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1bb",
          "name": "Sam Braun",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1bc",
          "name": "Maxime Brunet",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1bd",
          "name": "Samuel Cahyawijaya",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1be",
          "name": "David Cairuz",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1bf",
          "name": "Jon Ander Campos",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c0",
          "name": "Cassie Cao",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c1",
          "name": "Kris Cao",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c2",
          "name": "Roman Castagné",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c3",
          "name": "Julián Cendrero",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c4",
          "name": "Leila Chan Currie",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c5",
          "name": "Yash Chandak",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c6",
          "name": "Diane Chang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c7",
          "name": "Giannis Chatziveroglou",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c8",
          "name": "Hongyu Chen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1c9",
          "name": "Claire Cheng",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ca",
          "name": "Alexis Chevalier",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1cb",
          "name": "Justin T. Chiu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1cc",
          "name": "Eugene Cho",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1cd",
          "name": "Eugene Choi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ce",
          "name": "Eujeong Choi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1cf",
          "name": "Tim Chung",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d0",
          "name": "Volkan Cirik",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d1",
          "name": "Ana Cismaru",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d2",
          "name": "Pierre Clavier",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d3",
          "name": "Henry Conklin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d4",
          "name": "Lucas Crawhall-Stein",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d5",
          "name": "Devon Crouse",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d6",
          "name": "Andres Felipe Cruz-Salinas",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d7",
          "name": "Ben Cyrus",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d8",
          "name": "Daniel D'souza",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1d9",
          "name": "Hugo Dalla-Torre",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1da",
          "name": "John Dang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1db",
          "name": "William Darling",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1dc",
          "name": "Omar Darwiche Domingues",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1dd",
          "name": "Saurabh Dash",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1de",
          "name": "Antoine Debugne",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1df",
          "name": "Théo Dehaze",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e0",
          "name": "Shaan Desai",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e1",
          "name": "Joan Devassy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e2",
          "name": "Rishit Dholakia",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e3",
          "name": "Kyle Duffy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e4",
          "name": "Ali Edalati",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e5",
          "name": "Ace Eldeib",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e6",
          "name": "Abdullah Elkady",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e7",
          "name": "Sarah Elsharkawy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e8",
          "name": "Irem Ergün",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1e9",
          "name": "Beyza Ermis",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ea",
          "name": "Marzieh Fadaee",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1eb",
          "name": "Boyu Fan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ec",
          "name": "Lucas Fayoux",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ed",
          "name": "Yannis Flet-Berliac",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ee",
          "name": "Nick Frosst",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ef",
          "name": "Matthias Gallé",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f0",
          "name": "Wojciech Galuba",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f1",
          "name": "Utsav Garg",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f2",
          "name": "Matthieu Geist",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f3",
          "name": "Mohammad Gheshlaghi Azar",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f4",
          "name": "Seraphina Goldfarb-Tarrant",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f5",
          "name": "Tomas Goldsack",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f6",
          "name": "Aidan Gomez",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f7",
          "name": "Victor Machado Gonzaga",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f8",
          "name": "Nithya Govindarajan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1f9",
          "name": "Manoj Govindassamy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fa",
          "name": "Nathan Grinsztajn",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fb",
          "name": "Nikolas Gritsch",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fc",
          "name": "Patrick Gu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fd",
          "name": "Shangmin Guo",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1fe",
          "name": "Kilian Haefeli",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba1ff",
          "name": "Rod Hajjar",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba200",
          "name": "Tim Hawes",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba201",
          "name": "Jingyi He",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba202",
          "name": "Sebastian Hofstätter",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba203",
          "name": "Sungjin Hong",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba204",
          "name": "Sara Hooker",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba205",
          "name": "Tom Hosking",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba206",
          "name": "Stephanie Howe",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba207",
          "name": "Eric Hu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba208",
          "name": "Renjie Huang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba209",
          "name": "Hemant Jain",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20a",
          "name": "Ritika Jain",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20b",
          "name": "Nick Jakobi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20c",
          "name": "Madeline Jenkins",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20d",
          "name": "JJ Jordan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20e",
          "name": "Dhruti Joshi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba20f",
          "name": "Jason Jung",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba210",
          "name": "Trushant Kalyanpur",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba211",
          "name": "Siddhartha Rao Kamalakara",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba212",
          "name": "Julia Kedrzycki",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba213",
          "name": "Gokce Keskin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba214",
          "name": "Edward Kim",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba215",
          "name": "Joon Kim",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba216",
          "name": "Wei-Yin Ko",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba217",
          "name": "Tom Kocmi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba218",
          "name": "Michael Kozakov",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba219",
          "name": "Wojciech Kryściński",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21a",
          "name": "Arnav Kumar Jain",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21b",
          "name": "Komal Kumar Teru",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21c",
          "name": "Sander Land",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21d",
          "name": "Michael Lasby",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21e",
          "name": "Olivia Lasche",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba21f",
          "name": "Justin Lee",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba220",
          "name": "Patrick Lewis",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba221",
          "name": "Jeffrey Li",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba222",
          "name": "Jonathan Li",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba223",
          "name": "Hangyu Lin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba224",
          "name": "Acyr Locatelli",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba225",
          "name": "Kevin Luong",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba226",
          "name": "Raymond Ma",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba227",
          "name": "Lukas Mach",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba228",
          "name": "Marina Machado",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba229",
          "name": "Joanne Magbitang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22a",
          "name": "Brenda Malacara Lopez",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22b",
          "name": "Aryan Mann",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22c",
          "name": "Kelly Marchisio",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22d",
          "name": "Olivia Markham",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22e",
          "name": "Alexandre Matton",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba22f",
          "name": "Alex McKinney",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba230",
          "name": "Dominic McLoughlin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba231",
          "name": "Jozef Mokry",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba232",
          "name": "Adrien Morisot",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba233",
          "name": "Autumn Moulder",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba234",
          "name": "Harry Moynehan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba235",
          "name": "Maximilian Mozes",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba236",
          "name": "Vivek Muppalla",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba237",
          "name": "Lidiya Murakhovska",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba238",
          "name": "Hemangani Nagarajan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba239",
          "name": "Alekhya Nandula",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23a",
          "name": "Hisham Nasir",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23b",
          "name": "Shauna Nehra",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23c",
          "name": "Josh Netto-Rosen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23d",
          "name": "Daniel Ohashi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23e",
          "name": "James Owers-Bardsley",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba23f",
          "name": "Jason Ozuzu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba240",
          "name": "Dennis Padilla",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba241",
          "name": "Gloria Park",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba242",
          "name": "Sam Passaglia",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba243",
          "name": "Jeremy Pekmez",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba244",
          "name": "Laura Penstone",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba245",
          "name": "Aleksandra Piktus",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba246",
          "name": "Case Ploeg",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba247",
          "name": "Andrew Poulton",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba248",
          "name": "Youran Qi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba249",
          "name": "Shubha Raghvendra",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24a",
          "name": "Miguel Ramos",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24b",
          "name": "Ekagra Ranjan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24c",
          "name": "Pierre Richemond",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24d",
          "name": "Cécile Robert-Michon",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24e",
          "name": "Aurélien Rodriguez",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba24f",
          "name": "Sudip Roy",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba250",
          "name": "Laura Ruis",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba251",
          "name": "Louise Rust",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba252",
          "name": "Anubhav Sachan",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba253",
          "name": "Alejandro Salamanca",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba254",
          "name": "Kailash Karthik Saravanakumar",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba255",
          "name": "Isha Satyakam",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba256",
          "name": "Alice Schoenauer Sebag",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba257",
          "name": "Priyanka Sen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba258",
          "name": "Sholeh Sepehri",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba259",
          "name": "Preethi Seshadri",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25a",
          "name": "Ye Shen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25b",
          "name": "Tom Sherborne",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25c",
          "name": "Sylvie Chang Shi",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25d",
          "name": "Sanal Shivaprasad",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25e",
          "name": "Vladyslav Shmyhlo",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba25f",
          "name": "Anirudh Shrinivason",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba260",
          "name": "Inna Shteinbuk",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba261",
          "name": "Amir Shukayev",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba262",
          "name": "Mathieu Simard",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba263",
          "name": "Ella Snyder",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba264",
          "name": "Ava Spataru",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba265",
          "name": "Victoria Spooner",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba266",
          "name": "Trisha Starostina",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba267",
          "name": "Florian Strub",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba268",
          "name": "Yixuan Su",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba269",
          "name": "Jimin Sun",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26a",
          "name": "Dwarak Talupuru",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26b",
          "name": "Eugene Tarassov",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26c",
          "name": "Elena Tommasone",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26d",
          "name": "Jennifer Tracey",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26e",
          "name": "Billy Trend",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba26f",
          "name": "Evren Tumer",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba270",
          "name": "Ahmet Üstün",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba271",
          "name": "Bharat Venkitesh",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba272",
          "name": "David Venuto",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba273",
          "name": "Pat Verga",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba274",
          "name": "Maxime Voisin",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba275",
          "name": "Alex Wang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba276",
          "name": "Donglu Wang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba277",
          "name": "Shijian Wang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba278",
          "name": "Edmond Wen",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba279",
          "name": "Naomi White",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27a",
          "name": "Jesse Willman",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27b",
          "name": "Marysia Winkels",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27c",
          "name": "Chen Xia",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27d",
          "name": "Jessica Xie",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27e",
          "name": "Minjie Xu",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba27f",
          "name": "Bowen Yang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba280",
          "name": "Tan Yi-Chern",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba281",
          "name": "Ivan Zhang",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba282",
          "name": "Zhenyu Zhao",
          "hidden": false
        },
        {
          "_id": "67ecc2eb077a9bc63e7ba283",
          "name": "Zhoujie Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-01T12:08:07.000Z",
      "submittedOnDailyAt": "2025-04-02T03:24:20.179Z",
      "title": "Command A: 企業用の大規模言語モデル",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "このレポートでは、Command Aの開発を説明します。Command Aは、実世界的企業ケースに優れるために特化された強力な大規模言語モデルです。Command Aは、エージェント最適化されたモデルで、23言語のビジネスに対応し、新しいハイブリッドアーキテクチャを持ち、効率と最高レベルの性能をバランスに持つものです。その能力は、複雑なビジネスプロセスを自動化するための最先端のレビュアリングアウガーデジュース（RAG）機能とツール使用により実現されます。これらの能力は、分散的なトレーニングアプローチ、オートフィナルティングアルゴリズムとモデルマージテクニックを含むもので実現されます。また、Command R7Bの結果も含みます。Command R7Bは、Command Aと能力とアーキテクチャの類似性を持ちます。両モデルの重みは、研究のためにリリースされています。この技術レポートでは、元のトレーニングパイプラインを詳細に説明し、企業に関連するタスクと公開ベンチマークでの様々な評価結果を提供し、より良い性能と効率を示しています。",
      "upvotes": 2,
      "discussionId": "67ecc2ec077a9bc63e7ba2bd",
      "ai_keywords": [
        "agent-optimised",
        "multilingual-capable",
        "hybrid architecture",
        "Retrieval Augmented Generation (RAG)",
        "grounding",
        "tool use",
        "decentralised training",
        "self-refinement algorithms",
        "model merging techniques"
      ]
    },
    "publishedAt": "2025-04-01T08:08:07.000Z",
    "title": "Command A: An Enterprise-Ready Large Language Model",
    "summary": "In this report we describe the development of Command A, a powerful large\nlanguage model purpose-built to excel at real-world enterprise use cases.\nCommand A is an agent-optimised and multilingual-capable model, with support\nfor 23 languages of global business, and a novel hybrid architecture balancing\nefficiency with top of the range performance. It offers best-in-class Retrieval\nAugmented Generation (RAG) capabilities with grounding and tool use to automate\nsophisticated business processes. These abilities are achieved through a\ndecentralised training approach, including self-refinement algorithms and model\nmerging techniques. We also include results for Command R7B which shares\ncapability and architectural similarities to Command A. Weights for both models\nhave been released for research purposes. This technical report details our\noriginal training pipeline and presents an extensive evaluation of our models\nacross a suite of enterprise-relevant tasks and public benchmarks,\ndemonstrating excellent performance and efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00698.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6566
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.00072",
      "authors": [
        {
          "_id": "67eccbca1006da75eca94d24",
          "name": "Lucas Ventura",
          "hidden": false
        },
        {
          "_id": "67eccbca1006da75eca94d25",
          "name": "Antoine Yang",
          "hidden": false
        },
        {
          "_id": "67eccbca1006da75eca94d26",
          "name": "Cordelia Schmid",
          "hidden": false
        },
        {
          "_id": "67eccbca1006da75eca94d27",
          "name": "Gül Varol",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:41:29.000Z",
      "submittedOnDailyAt": "2025-04-02T04:02:28.798Z",
      "title": "Chapter-Llama: 長時間ビデオの効率的な章分割におけるLLM",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "ビデオチャプターリングの任務を扱い、長いビデオタイムラインを意味的単位に分割し、対応するチャプタータイトルを生成する。これは相対的に調査が少ないが、自動チャプターリングは長ビデオでの効率的なナビゲーションと内容検索において潜力がある。本論文では、'Chapter-Llama'フレームワークを用いて、テキスト領域で効率的に問題を解決し、1時間のビデオに強力なチャプターリング性能を達成した。特に、大規模な予っちゃった言語モデル（LLM）を利用し、ビデオフレームを説明するキャプションとその時間スタンプを入力している。全フレームを厳密にキャプションすることでは効率が低いため、ビデオトランスクリプトの内容に基づく軽量ビデオフレーム選択戦略を提案し、実験的に顕著な優勢を示した。LLMを学習させて、チャプターの境界の時間スタンプと自由形式のチャプタータイトルを出力することを目指している。この簡単で強力なアプローチは、1時間のビデオを1つのforward passで処理することができる。我々の結果は、最近のVidChapters-7Mベンチマークでの状態の最先端に対して大幅な向上（例えば、F1スコア45.3 vs 26.7）を示した。プロジェクトページでコードとモデルを公開し、進める研究のためにさらに研究を促進する。",
      "upvotes": 2,
      "discussionId": "67eccbce1006da75eca94e68",
      "ai_keywords": [
        "large language model (LLM)",
        "context window",
        "speech transcripts",
        "captions",
        "timestamps",
        "frame selection strategy",
        "chapter boundaries",
        "F1 score"
      ]
    },
    "publishedAt": "2025-03-31T13:41:29.000Z",
    "title": "Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs",
    "summary": "We address the task of video chaptering, i.e., partitioning a long video\ntimeline into semantic units and generating corresponding chapter titles. While\nrelatively underexplored, automatic chaptering has the potential to enable\nefficient navigation and content retrieval in long-form videos. In this paper,\nwe achieve strong chaptering performance on hour-long videos by efficiently\naddressing the problem in the text domain with our 'Chapter-Llama' framework.\nSpecifically, we leverage a pretrained large language model (LLM) with large\ncontext window, and feed as input (i) speech transcripts and (ii) captions\ndescribing video frames, along with their respective timestamps. Given the\ninefficiency of exhaustively captioning all frames, we propose a lightweight\nspeech-guided frame selection strategy based on speech transcript content, and\nexperimentally demonstrate remarkable advantages. We train the LLM to output\ntimestamps for the chapter boundaries, as well as free-form chapter titles.\nThis simple yet powerful approach scales to processing one-hour long videos in\na single forward pass. Our results demonstrate substantial improvements (e.g.,\n45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M\nbenchmark. To promote further research, we release our code and models at our\nproject page.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.00072.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6566
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23733",
      "authors": [
        {
          "_id": "67ec99788088196efd062021",
          "name": "Yiyang Du",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062022",
          "name": "Xiaochen Wang",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062023",
          "user": {
            "_id": "642086ed290342c5df85662d",
            "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
            "isPro": false,
            "fullname": "Chi Chen",
            "user": "carboncoo",
            "type": "user"
          },
          "name": "Chi Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:22:49.411Z",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062024",
          "name": "Jiabo Ye",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062025",
          "name": "Yiru Wang",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062026",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062027",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062028",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd062029",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd06202a",
          "name": "Zhifang Sui",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd06202b",
          "name": "Maosong Sun",
          "hidden": false
        },
        {
          "_id": "67ec99788088196efd06202c",
          "name": "Yang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T05:13:02.000Z",
      "submittedOnDailyAt": "2025-04-02T00:32:49.651Z",
      "title": "AdaMMS: ホモジュール多モデルに対するモデル統合と無サブバイバーコエフセントリティ最適化",
      "submittedOnDailyBy": {
        "_id": "642086ed290342c5df85662d",
        "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
        "isPro": false,
        "fullname": "Chi Chen",
        "user": "carboncoo",
        "type": "user"
      },
      "summary": "最近、モデル統合手法は、多くの大規模言語モデル（LLMs）からの多様なタスクの能力を統合する強力な強さを示しています。前のモデル統合手法は主に構造が同じなホモジネースモデルを統合することを焦点としており、固有のヘテロジネイスの性質を持つ多モディアル大規模言語モデル（MLLMs）に対しては挑戦を受けます。この構文では、ヘテロジネイスのMLLMsに適した新しいモデル統合手法AdaMMSを提案します。我々の方法は、3ステップで挑戦を解決します：マッピング、統合、探索。特に、まず、異なる構造のMLLMにモデル統合を適用するためのマッピング関数を設計します。次に、ヘテロジネイスのMLLMのパラメータ空間の不均衡を主動的に調整するために線形インタープローテーションを適用します。最後に、ハイパーパラメータ探索ステップでは、ラベル付けデータを使用しないモデル統合のための無サバイジェンスハイパーパラメータ選択方法を提案します。最初のヘテロジネイスのMLLMを統合することが可能なモデル統合手法であるため、様々なモデル組み合わせに対しての拡張的な実験では、AdaMMSは様々な視覚言語ベンチマーク上で以前のモデル統合手法よりも上位に評価されました。",
      "upvotes": 2,
      "discussionId": "67ec99798088196efd062081",
      "ai_keywords": [
        "model merging",
        "Large Language Models (LLMs)",
        "Multimodal Large Language Models (MLLMs)",
        "heterogeneous property",
        "model architecture",
        "parameter space",
        "AdaMMS",
        "mapping function",
        "linear interpolation",
        "hyper-parameter searching",
        "unsupervised hyper-parameter selection",
        "vision-language benchmarks"
      ]
    },
    "publishedAt": "2025-03-31T01:13:02.000Z",
    "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models\n  with Unsupervised Coefficient Optimization",
    "summary": "Recently, model merging methods have demonstrated powerful strengths in\ncombining abilities on various tasks from multiple Large Language Models\n(LLMs). While previous model merging methods mainly focus on merging\nhomogeneous models with identical architecture, they meet challenges when\ndealing with Multimodal Large Language Models (MLLMs) with inherent\nheterogeneous property, including differences in model architecture and the\nasymmetry in the parameter space. In this work, we propose AdaMMS, a novel\nmodel merging method tailored for heterogeneous MLLMs. Our method tackles the\nchallenges in three steps: mapping, merging and searching. Specifically, we\nfirst design mapping function between models to apply model merging on MLLMs\nwith different architecture. Then we apply linear interpolation on model\nweights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in\nthe hyper-parameter searching step, we propose an unsupervised hyper-parameter\nselection method for model merging. As the first model merging method capable\nof merging heterogeneous MLLMs without labeled data, extensive experiments on\nvarious model combinations demonstrated that AdaMMS outperforms previous model\nmerging methods on various vision-language benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23733.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642086ed290342c5df85662d",
      "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
      "fullname": "Chi Chen",
      "name": "carboncoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.21860",
      "authors": [
        {
          "_id": "67ec97f65d9c75ff46de2974",
          "name": "Kailin Li",
          "hidden": false
        },
        {
          "_id": "67ec97f65d9c75ff46de2975",
          "name": "Puhao Li",
          "hidden": false
        },
        {
          "_id": "67ec97f65d9c75ff46de2976",
          "name": "Tengyu Liu",
          "hidden": false
        },
        {
          "_id": "67ec97f65d9c75ff46de2977",
          "name": "Yuyang Li",
          "hidden": false
        },
        {
          "_id": "67ec97f65d9c75ff46de2978",
          "name": "Siyuan Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-27T17:50:30.000Z",
      "submittedOnDailyAt": "2025-04-02T00:21:41.585Z",
      "title": "ManipTrans: 残差学習による効率的な左右手共動操作のマニピュレーション転送",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "人間の手は相互作用に中心的な役割を果たし、そのためにディケッタブルなロボットの操作において研究が増加しています。データ駆動の具象化AIアルゴリズムは、実際の強化学習または実世界のテレオプレーで得ることが難しいですが、人間のような精密な大規模な操作列を要求します。これを解決するために、私たちは、シミュレーションで人間の二手のスキルをディケッタブルなロボットの手に効率的に転移するための新しい2段階の方法ManipTransを紹介します。ManipTransは、手の動きを模倣する一般的なトラジェクトイマイターを事前学習し、相互作用の制約の下で特定の残差モジュールを微調節し、複雑な二手の仕事の効率的な学習と正確な実行を可能にします。実験は、ManipTransは成功率、忠実度、および効率において最先端の方法を超えることを示します。ManipTransを活用し、私たちは、複数の手のオブジェクトデータセットをロボットの手に転移し、DexManipNetを作成します。DexManipNetは、前に試されていなかったタスクのように、ペンのカップのマスターやボトルのウィルドリングなどを扱う大型データセットです。DexManipNetは3.3Kのロボットの操作のエピソードを含み、簡単に拡張可能で、ディケッタブルな手のポリシーの訓練を促進し、実世界の実装を可能にします。",
      "upvotes": 2,
      "discussionId": "67ec97f85d9c75ff46de29f0",
      "projectPage": "https://maniptrans.github.io/",
      "ai_keywords": [
        "dexterous robotic manipulation",
        "data-driven embodied AI",
        "trajectory imitator",
        "bimanual skills",
        "interaction constraints",
        "fine-tuning",
        "parameter space",
        "DexManipNet",
        "pen capping",
        "bottle unscrewing",
        "episodes",
        "policy training"
      ]
    },
    "publishedAt": "2025-03-27T13:50:30.000Z",
    "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via\n  Residual Learning",
    "summary": "Human hands play a central role in interacting, motivating increasing\nresearch in dexterous robotic manipulation. Data-driven embodied AI algorithms\ndemand precise, large-scale, human-like manipulation sequences, which are\nchallenging to obtain with conventional reinforcement learning or real-world\nteleoperation. To address this, we introduce ManipTrans, a novel two-stage\nmethod for efficiently transferring human bimanual skills to dexterous robotic\nhands in simulation. ManipTrans first pre-trains a generalist trajectory\nimitator to mimic hand motion, then fine-tunes a specific residual module under\ninteraction constraints, enabling efficient learning and accurate execution of\ncomplex bimanual tasks. Experiments show that ManipTrans surpasses\nstate-of-the-art methods in success rate, fidelity, and efficiency. Leveraging\nManipTrans, we transfer multiple hand-object datasets to robotic hands,\ncreating DexManipNet, a large-scale dataset featuring previously unexplored\ntasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K\nepisodes of robotic manipulation and is easily extensible, facilitating further\npolicy training for dexterous hands and enabling real-world deployments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21860.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6566
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.24210",
      "authors": [
        {
          "_id": "67ece4b12511c2aab09b84ca",
          "name": "Seungjun Lee",
          "hidden": false
        },
        {
          "_id": "67ece4b12511c2aab09b84cb",
          "name": "Gim Hee Lee",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/662f5b1e5aeedf55d209741a/CYuJcfz3_px75zb9IqBTP.png"
      ],
      "publishedAt": "2025-03-31T15:27:07.000Z",
      "submittedOnDailyAt": "2025-04-02T05:49:14.777Z",
      "title": "DiET-GS: 拡散先頭とイベントストリームを助ける3D移動のデフォーメーションデブリューリング 3D Gaussian Splatting",
      "submittedOnDailyBy": {
        "_id": "662f5b1e5aeedf55d209741a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662f5b1e5aeedf55d209741a/YdsbnRJPj_6PRfWbCNWfX.jpeg",
        "isPro": false,
        "fullname": "Seungjun Lee",
        "user": "onandon",
        "type": "user"
      },
      "summary": "ブラーミュービウス画像から鋭い3D表現を再構築するのは、コンピュータビジョンの長年の問題です。最近の研究は、イベントベースカメラを使って、動きブラーからの高品質な新視点合成を強化し、高動態範囲と微秒レベルの時間分解能から利点を得ています。しかし、それらは、色の不正な復元や細かな詳細の失われにより、視覚的な質が最適値を達成しません。本論文では、DiET-GS（Diffusion Prior and Event Stream-Assisted Motion Deblurring 3DGS）を紹介します。我々のフレームワークは、ブラーなしイベントストリームとディフュージョン先頭を両方とも2段階の訓練戦略によって効果的に活用します。特に、我々は、イベントの双重積分を用いて3DGSを制約する新しいフレームワークを提案し、正確な色と明確な詳細を実現します。また、我々は、ディフュージョン先頭を利用してエッジの詳細をさらに強化する簡単な技術を提案します。合成データと実世界データの両方での定性的・定量的な結果は、我々のDiET-GSが既存の基準と比較して新視点の質が大幅に向上していることを示しています。プロジェクトページはhttps://diet-gs.github.ioです。",
      "upvotes": 1,
      "discussionId": "67ece4b62511c2aab09b8660",
      "projectPage": "https://diet-gs.github.io",
      "githubRepo": "https://github.com/DiET-GS/DiET-GS",
      "ai_keywords": [
        "diffusion prior",
        "event stream",
        "motion deblurring",
        "3DGS (3D Geometry Synthesis)",
        "blur-free event streams",
        "event double integral",
        "edge details",
        "novel view synthesis"
      ]
    },
    "publishedAt": "2025-03-31T11:27:07.000Z",
    "title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D\n  Gaussian Splatting",
    "summary": "Reconstructing sharp 3D representations from blurry multi-view images are\nlong-standing problem in computer vision. Recent works attempt to enhance\nhigh-quality novel view synthesis from the motion blur by leveraging\nevent-based cameras, benefiting from high dynamic range and microsecond\ntemporal resolution. However, they often reach sub-optimal visual quality in\neither restoring inaccurate color or losing fine-grained details. In this\npaper, we present DiET-GS, a diffusion prior and event stream-assisted motion\ndeblurring 3DGS. Our framework effectively leverages both blur-free event\nstreams and diffusion prior in a two-stage training strategy. Specifically, we\nintroduce the novel framework to constraint 3DGS with event double integral,\nachieving both accurate color and well-defined details. Additionally, we\npropose a simple technique to leverage diffusion prior to further enhance the\nedge details. Qualitative and quantitative results on both synthetic and\nreal-world data demonstrate that our DiET-GS is capable of producing\nsignificantly better quality of novel views compared to the existing baselines.\nOur project page is https://diet-gs.github.io",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/662f5b1e5aeedf55d209741a/CYuJcfz3_px75zb9IqBTP.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24210.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662f5b1e5aeedf55d209741a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662f5b1e5aeedf55d209741a/YdsbnRJPj_6PRfWbCNWfX.jpeg",
      "fullname": "Seungjun Lee",
      "name": "onandon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]