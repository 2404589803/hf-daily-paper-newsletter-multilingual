[
  {
    "paper": {
      "id": "2507.09477",
      "authors": [
        {
          "_id": "68787030001546c83aa4f9ae",
          "name": "Yangning Li",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9af",
          "user": {
            "_id": "6667e801fd95ddf66cac84ff",
            "avatarUrl": "/avatars/b75f6253160c81f558e6b40ed2ca1755.svg",
            "isPro": false,
            "fullname": "Weizhi Zhang",
            "user": "WZDavid",
            "type": "user"
          },
          "name": "Weizhi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:48.379Z",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b0",
          "name": "Yuyao Yang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b1",
          "name": "Wei-Chieh Huang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b2",
          "name": "Yaozu Wu",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b3",
          "name": "Junyu Luo",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b4",
          "name": "Yuanchen Bei",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b5",
          "user": {
            "_id": "633f112013e836a0fc4fa567",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665077519962-noauth.jpeg",
            "isPro": false,
            "fullname": "Henry Peng Zou",
            "user": "TreeForest",
            "type": "user"
          },
          "name": "Henry Peng Zou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:46.466Z",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b6",
          "name": "Xiao Luo",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b7",
          "name": "Yusheng Zhao",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b8",
          "name": "Chunkit Chan",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9b9",
          "name": "Yankai Chen",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9ba",
          "name": "Zhongfen Deng",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bb",
          "name": "Yinghui Li",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bc",
          "name": "Hai-Tao Zheng",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bd",
          "name": "Dongyuan Li",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9be",
          "name": "Renhe Jiang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9bf",
          "name": "Ming Zhang",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9c0",
          "name": "Yangqiu Song",
          "hidden": false
        },
        {
          "_id": "68787030001546c83aa4f9c1",
          "name": "Philip S. Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-13T03:29:41.000Z",
      "submittedOnDailyAt": "2025-07-17T02:27:52.541Z",
      "title": "エージェント型RAGと深い理由論を実現するためのシステムについてのシンプレックス：LLM内のRAG-理由論システムの概観",
      "submittedOnDailyBy": {
        "_id": "6667e801fd95ddf66cac84ff",
        "avatarUrl": "/avatars/b75f6253160c81f558e6b40ed2ca1755.svg",
        "isPro": false,
        "fullname": "Weizhi Zhang",
        "user": "WZDavid",
        "type": "user"
      },
      "summary": "レビュアルアウゲーション（RAG）は、外部知識を注入して大規模言語モデル（LLMs）の事実性を向上させるが、多段階推論の問題に対しては欠点があります。反対に、単に理由取りに向けたアプローチは、ハロカノイゼーションや事実の不正な基礎を与えることもあります。本調査は、一つの理由取り・レビュアルの視点で両方のトレンドを合成しています。まず、進捗の理由取りがRAGの各ステージを最適化する方法を地図化します（Reasoning-Enhanced RAG）。次に、異なるタイプのレビュアル知識が欠損の前提を補完し、複雑な推論のコンテキストを拡張する方法を示します（RAG-Enhanced Reasoning）。最後に、レビュアルと理由取りの協調化が進むシンプレックスフレームワークを特別に強調し、知識密集型ベンチマークで最先端の性能を達成するために、探索と理由取りを交互に繰り返す（agentic）LLMsを中心にしています。方法、データセット、開放された課題を分類し、RAG-Reasoningシステムの深さや効果性、多モデル慣れ性、信頼性、人間中心性に向けた研究のポートを設定します。このコレクションは、https://github.com/DavidZWZ/Awesome-RAG-Reasoningにアクセスできます。",
      "upvotes": 35,
      "discussionId": "68787031001546c83aa4f9c2",
      "githubRepo": "https://github.com/DavidZWZ/Awesome-RAG-Reasoning",
      "ai_summary": "This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.",
      "ai_keywords": [
        "Retrieval-Augmented Generation",
        "Large Language Models",
        "Reasoning-Enhanced RAG",
        "RAG-Enhanced Reasoning",
        "Synergized RAG-Reasoning",
        "knowledge-intensive benchmarks",
        "multimodally-adaptive",
        "trustworthy",
        "human-centric"
      ],
      "githubStars": 46
    },
    "publishedAt": "2025-07-12T23:29:41.000Z",
    "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs",
    "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09477.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6667e801fd95ddf66cac84ff",
      "avatarUrl": "/avatars/b75f6253160c81f558e6b40ed2ca1755.svg",
      "fullname": "Weizhi Zhang",
      "name": "WZDavid",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12465",
      "authors": [
        {
          "_id": "6878635e001546c83aa4f979",
          "user": {
            "_id": "65af6f6b52e1b2aae437af2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
            "isPro": false,
            "fullname": "Ziang Cao",
            "user": "Caoza",
            "type": "user"
          },
          "name": "Ziang Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:44.605Z",
          "hidden": false
        },
        {
          "_id": "6878635e001546c83aa4f97a",
          "user": {
            "_id": "62fc8cf7ee999004b5a8b982",
            "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
            "isPro": false,
            "fullname": "Zhaoxi Chen",
            "user": "FrozenBurning",
            "type": "user"
          },
          "name": "Zhaoxi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:22:11.615Z",
          "hidden": false
        },
        {
          "_id": "6878635e001546c83aa4f97b",
          "name": "Linag Pan",
          "hidden": false
        },
        {
          "_id": "6878635e001546c83aa4f97c",
          "user": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "isPro": false,
            "fullname": "Ziwei Liu",
            "user": "liuziwei7",
            "type": "user"
          },
          "name": "Ziwei Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:21:56.595Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65af6f6b52e1b2aae437af2e/nrL7wGaZ1Z5FZWuu0GTbg.mp4"
      ],
      "publishedAt": "2025-07-16T17:59:35.000Z",
      "submittedOnDailyAt": "2025-07-17T01:26:11.001Z",
      "title": "PhysX: 物理ベースド 3D アセット生成",
      "submittedOnDailyBy": {
        "_id": "65af6f6b52e1b2aae437af2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
        "isPro": false,
        "fullname": "Ziang Cao",
        "user": "Caoza",
        "type": "user"
      },
      "summary": "3Dモデリングは、ビュータルから物理的に移動しています。現在の3D生成は、ギオメトリーとテクスチャを主に強調し、物理的に基づいたモデリングを飛ばしています。その結果、3D生成モデルの急速な進歩に伴い、合成された3Dアセットは、豊富な重要な物理的な属性を飛ばし、物理的領域のシミュレーションや具象化AIの実世界の応用を妨げています。この挑戦を初めて解決するために、私たちはPhysX、物理的に基づいた3Dアセット生成の終始一致したパラダイムを提案します。1) 物理的なアノテーションが欠けている3Dデータセットの重要な間違いを経って、PhysXNet、物理的に基づいた5つの基礎的な次元でシステマチックにアノテーションされた最初の3Dデータセットを提出します。特に、視覚言語モデルに基づくスケーラブルな人間のループアノテーションパイプラインを開発し、プロシーズブルなアセットを効率的に作成します。2) また、PhysXGen、物理的に基づいた画像から3Dアセット生成のフィードフォワードフレームワークを提案し、予った物理的知識を事前学習された3D構造空間に注入します。特に、PhysXGenは3D構造と物理的属性の潜在的な相関を明確にモデル化し、合理的な物理的予測を持つ3Dアセットを生成し、ネイティブなギオメトリー品質を保つことを目指します。拡大的な実験は、私たちのフレームワークの上位の性能と期待的な一般化能力を証明します。すべてのコード、データ、モデルは、生成的物理的AIの将来の研究を促進することを目的に公開します。",
      "upvotes": 19,
      "discussionId": "6878635e001546c83aa4f97d",
      "projectPage": "https://physx-3d.github.io/",
      "githubRepo": "https://github.com/ziangcao0312/PhysX",
      "ai_summary": "PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.",
      "ai_keywords": [
        "physics-grounded 3D asset generation",
        "PhysXNet",
        "physics-annotated 3D datasets",
        "vision-language models",
        "human-in-the-loop annotation pipeline",
        "PhysXGen",
        "feed-forward framework",
        "dual-branch architecture",
        "latent correlations",
        "physical predictions",
        "geometry quality"
      ],
      "githubStars": 20
    },
    "publishedAt": "2025-07-16T13:59:35.000Z",
    "title": "PhysX: Physical-Grounded 3D Asset Generation",
    "summary": "3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose PhysX, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose PhysXGen, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65af6f6b52e1b2aae437af2e/nrL7wGaZ1Z5FZWuu0GTbg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12465.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65af6f6b52e1b2aae437af2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
      "fullname": "Ziang Cao",
      "name": "Caoza",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.11949",
      "authors": [
        {
          "_id": "687872c3001546c83aa4f9cf",
          "user": {
            "_id": "683d94e5ba11bab2cc848aab",
            "avatarUrl": "/avatars/4a1915ad48c78b3a71733b3282f2f93c.svg",
            "isPro": false,
            "fullname": "Shuyang Xu",
            "user": "JimSYXu",
            "type": "user"
          },
          "name": "Shuyang Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:22:22.253Z",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d0",
          "user": {
            "_id": "645223fb01d7bd9555ea399a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
            "isPro": false,
            "fullname": "Zhiyang Dou",
            "user": "frankzydou",
            "type": "user"
          },
          "name": "Zhiyang Dou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:59.825Z",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d1",
          "name": "Mingyi Shi",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d2",
          "name": "Liang Pan",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d3",
          "name": "Leo Ho",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d4",
          "name": "Jingbo Wang",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d5",
          "name": "Yuan Liu",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d6",
          "name": "Cheng Lin",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d7",
          "name": "Yuexin Ma",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d8",
          "name": "Wenping Wang",
          "hidden": false
        },
        {
          "_id": "687872c3001546c83aa4f9d9",
          "name": "Taku Komura",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645223fb01d7bd9555ea399a/KoPBJkGwyft7hfVmM8ikZ.mp4"
      ],
      "publishedAt": "2025-07-16T06:33:11.000Z",
      "submittedOnDailyAt": "2025-07-17T02:48:15.112Z",
      "title": "MOSPA: 空間音声による人間の動作生成",
      "submittedOnDailyBy": {
        "_id": "645223fb01d7bd9555ea399a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
        "isPro": false,
        "fullname": "Zhiyang Dou",
        "user": "frankzydou",
        "type": "user"
      },
      "summary": "虚擬人間が多様な聴覚刺激に対して動的で写実的に応答することを実現することは、人物動画において重要な課題であり、観測認知モデリングと動作合成の統合が求められています。この課題は重要であるが、ほとんど調査されていません。これまでの研究は主に話し言葉、音声、音楽などのモデルを機能と動作に対応させることを中心にしていました。まだ、これらのモデルは空間音声信号に含まれる空間特徴量の影響を考慮していません。この隙を埋め、空間音声による高品質な人物動作のモデリングを可能にしようとして、我々は最初の詳細な空間音声駆動人物動作（SAM）データセットを紹介します。このデータセットは多様で高品質な空間音声と動作データを含みます。ベンチマークのために、我々は簡単で効果的な拡散基礎の生成フレームワークを開発し、SPAtial Audioによる人物MOtion生成を行うことをMOSPAという名前で示します。このフレームワークは体動と空間音声の関係を信頼的に捉えるために、効果的な融合機構を用いています。トレーニング後、MOSPAは空間音声の入力に対して多様な実写的な人物動作を生成できます。我々のデータセットを詳細に調査し、ベンチマークのために拡散的な実験を行いました。このタスクで我々の方法は最先端の性能を達成しました。我々のモデルとデータセットは受け入れられたら公開されます。詳細はサブプロジェクトのビデオに参照してください。",
      "upvotes": 10,
      "discussionId": "687872c9001546c83aa4f9da",
      "ai_summary": "A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.",
      "ai_keywords": [
        "diffusion-based generative framework",
        "spatial audio",
        "human motion",
        "SAM dataset",
        "MOSPA",
        "spatial features",
        "perceptual modeling",
        "motion synthesis"
      ]
    },
    "publishedAt": "2025-07-16T02:33:11.000Z",
    "title": "MOSPA: Human Motion Generation Driven by Spatial Audio",
    "summary": "Enabling virtual humans to dynamically and realistically respond to diverse\nauditory stimuli remains a key challenge in character animation, demanding the\nintegration of perceptual modeling and motion synthesis. Despite its\nsignificance, this task remains largely unexplored. Most previous works have\nprimarily focused on mapping modalities like speech, audio, and music to\ngenerate human motion. As of yet, these models typically overlook the impact of\nspatial features encoded in spatial audio signals on human motion. To bridge\nthis gap and enable high-quality modeling of human movements in response to\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\nmotion data. For benchmarking, we develop a simple yet effective\ndiffusion-based generative framework for human MOtion generation driven by\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\nbody motion and spatial audio through an effective fusion mechanism. Once\ntrained, MOSPA could generate diverse realistic human motions conditioned on\nvarying spatial audio inputs. We perform a thorough investigation of the\nproposed dataset and conduct extensive experiments for benchmarking, where our\nmethod achieves state-of-the-art performance on this task. Our model and\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\nvideo for more details.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645223fb01d7bd9555ea399a/KoPBJkGwyft7hfVmM8ikZ.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645223fb01d7bd9555ea399a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
      "fullname": "Zhiyang Dou",
      "name": "frankzydou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12463",
      "authors": [
        {
          "_id": "68788789001546c83aa4f9e4",
          "name": "Renjie Li",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e5",
          "user": {
            "_id": "6825fc0e58cf56d164cb339d",
            "avatarUrl": "/avatars/4ade4a5bbb0a805a92a83bfb233f805f.svg",
            "isPro": false,
            "fullname": "Ruijie Ye",
            "user": "jerryye0110",
            "type": "user"
          },
          "name": "Ruijie Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:17:53.937Z",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e6",
          "name": "Mingyang Wu",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e7",
          "name": "Hao Frank Yang",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e8",
          "user": {
            "_id": "63b354bb7091e602f1a0e2e8",
            "avatarUrl": "/avatars/a388d93c0af2f57eadb6fa60d6789041.svg",
            "isPro": false,
            "fullname": "wayne",
            "user": "waynefan",
            "type": "user"
          },
          "name": "Zhiwen Fan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:23:18.102Z",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9e9",
          "name": "Hezhen Hu",
          "hidden": false
        },
        {
          "_id": "68788789001546c83aa4f9ea",
          "user": {
            "_id": "62548d5fef3debb2ddf91217",
            "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
            "isPro": false,
            "fullname": "Zhengzhong Tu",
            "user": "vztu",
            "type": "user"
          },
          "name": "Zhengzhong Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:23:16.047Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ARblfCBCEmNaS2zzbTJLm.jpeg"
      ],
      "publishedAt": "2025-07-16T17:59:30.000Z",
      "submittedOnDailyAt": "2025-07-17T03:48:41.381Z",
      "title": "MMHU: 大規模多モデルベンチマークで人間の行動を理解する",
      "submittedOnDailyBy": {
        "_id": "62548d5fef3debb2ddf91217",
        "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
        "isPro": false,
        "fullname": "Zhengzhong Tu",
        "user": "vztu",
        "type": "user"
      },
      "summary": "人間は交通機関システムの重要な構成要素であり、その行動を理解することが安全運転システムの開発において重要である。過去の進歩は、人間の行動のさまざまな面での研究を行っており、動き、軌跡、そして意向などを検討しているが、自動運転での人間の行動理解を評価するための一様的なベンチマークはまだ存在しない。本研究では、MMHUという大規模な人間の行動分析ベンチマークを提案し、豊富な注釈を扱うことで、人間の動きと軌跡、人間の動きの説明、人間の意向、駆転転安全に関連する重要な行動ラベルなどを含む。データセットは、Waymoの既存の運転データセット、YouTubeからの「in-the-wild」ビデオ、そして自己集計データからの57kの人間の動きクリップと1.73Mのフレームを構成している。人間がロープ内での注釈パイプラインを開発し、豊富な行動キャプションを生成することで、データセットの詳細な分析と複数のタスクのベンチマークを提供し、動き予測から動き生成まで、人間の行動に関する質問回答への幅広い評価システムを提供します。プロジェクトページ：https://MMHU-Benchmark.github.io.",
      "upvotes": 9,
      "discussionId": "6878878a001546c83aa4f9eb",
      "projectPage": "https://mmhu-benchmark.github.io/",
      "ai_summary": "A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.",
      "ai_keywords": [
        "human behavior analysis",
        "motion prediction",
        "motion generation",
        "human behavior question answering",
        "human-in-the-loop annotation",
        "Waymo",
        "YouTube",
        "self-collected data"
      ]
    },
    "publishedAt": "2025-07-16T13:59:30.000Z",
    "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\n  Understanding",
    "summary": "Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behaviorx2014such as motion, trajectories, and\nintentionx2014a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose MMHU, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasksx2014ranging from motion prediction to motion\ngeneration and human behavior question answeringx2014thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ARblfCBCEmNaS2zzbTJLm.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12463.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62548d5fef3debb2ddf91217",
      "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
      "fullname": "Zhengzhong Tu",
      "name": "vztu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12415",
      "authors": [
        {
          "_id": "68788b9b001546c83aa4f9ed",
          "name": "Xinyi He",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9ee",
          "user": {
            "_id": "612ee6a7b960e78c6d2319d4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "isPro": false,
            "fullname": "Qian Liu",
            "user": "SivilTaram",
            "type": "user"
          },
          "name": "Qian Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:17:46.375Z",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9ef",
          "user": {
            "_id": "61711f02e0b1ddb56eb9b526",
            "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
            "isPro": true,
            "fullname": "Mingzhe Du",
            "user": "Elfsong",
            "type": "user"
          },
          "name": "Mingzhe Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T09:08:30.259Z",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f0",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f1",
          "name": "Zhijie Fan",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f2",
          "name": "Yiming Huang",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f3",
          "name": "Zejian Yuan",
          "hidden": false
        },
        {
          "_id": "68788b9b001546c83aa4f9f4",
          "name": "Zejun Ma",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/_XkK-c8Xm-G1Ui5AwtGdu.png"
      ],
      "publishedAt": "2025-07-16T17:05:17.000Z",
      "submittedOnDailyAt": "2025-07-17T04:10:30.408Z",
      "title": "SWE-Perf: 言語モデルが実世界的リポジトリでコードの性能を最適化できるか？",
      "submittedOnDailyBy": {
        "_id": "612ee6a7b960e78c6d2319d4",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
        "isPro": false,
        "fullname": "Qian Liu",
        "user": "SivilTaram",
        "type": "user"
      },
      "summary": "コードの効率化は、実世界的なソフトウェア開発では最高の重要性を持ち、生産レベルのシステムではもっとも重要です。ラージュ言語モデル（LLMs）は、コード生成とバグ修正において驚異的な能力を示しましたが、リポジトリレベルでのコード効率化における優れた能力は、主に探索されていません。この空白を埋めるために、私たちはSWE-Perfを紹介します。これは、実際のリポジトリコンテキストでのコード効率化タスクに対するLLMsのシステマティックな評価を行うために特に設計された最初のベンチマークです。SWE-Perfは、プロパーGitHubリポジトリからの性能向上のプルリクエストから精選された140サインも含むものです。各ベンチマークインスタンスには、関連するコードベース、目標関数、性能関連テスト、エクスペリエンスのポーターズのパッチ、実行可能な環境が含まれています。ファイルレベルとリポジトリレベルのアプローチ（例：AgentlessとOpenHands）を拡張的に評価することで、現在のLLMsとエクスペリエンスレベルの最適化性能の間の大きな能力間違いを明らかにし、この新興分野における重要な研究機会を強調します。",
      "upvotes": 8,
      "discussionId": "68788b9b001546c83aa4f9f5",
      "projectPage": "https://swe-perf.github.io/",
      "githubRepo": "https://github.com/SWE-Perf/SWE-Perf",
      "ai_summary": "SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.",
      "ai_keywords": [
        "Large Language Models",
        "code performance optimization",
        "benchmark",
        "performance-improving pull requests",
        "codebase",
        "target functions",
        "performance-related tests",
        "expert-authored patches",
        "executable environments",
        "Agentless",
        "OpenHands"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-07-16T13:05:17.000Z",
    "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?",
    "summary": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/_XkK-c8Xm-G1Ui5AwtGdu.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "612ee6a7b960e78c6d2319d4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
      "fullname": "Qian Liu",
      "name": "SivilTaram",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 85
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.11527",
      "authors": [
        {
          "_id": "68785ee1001546c83aa4f967",
          "user": {
            "_id": "65c950ebd908bd52a4477116",
            "avatarUrl": "/avatars/bc6ba0dd2903c7bea37f7b9c40857718.svg",
            "isPro": false,
            "fullname": "Yinsheng Li",
            "user": "Eason666",
            "type": "user"
          },
          "name": "Yinsheng Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:42.470Z",
          "hidden": false
        },
        {
          "_id": "68785ee1001546c83aa4f968",
          "user": {
            "_id": "643ba2f725681c3afaa8f05e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
            "isPro": false,
            "fullname": "Zhen Dong",
            "user": "zhendongucb",
            "type": "user"
          },
          "name": "Zhen Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:22:37.102Z",
          "hidden": false
        },
        {
          "_id": "68785ee1001546c83aa4f969",
          "name": "Yi Shao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643ba2f725681c3afaa8f05e/2T1jxRrhqMGVLESbEDwJT.png"
      ],
      "publishedAt": "2025-07-15T17:56:04.000Z",
      "submittedOnDailyAt": "2025-07-17T01:30:58.515Z",
      "title": "DrafterBench: 土木工学の任務自動化における大規模言語モデルのベンチマーク",
      "submittedOnDailyBy": {
        "_id": "643ba2f725681c3afaa8f05e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
        "isPro": false,
        "fullname": "Zhen Dong",
        "user": "zhendongucb",
        "type": "user"
      },
      "summary": "大型言語モデル（LLM）アガントは、実世界的問題を解決するための大きなポテンシャルを示し、産業でのタスク自動化の解決策としての可能性を示しています。しかし、工業的な視点からアガントの自動化をシステマティックに評価するためには、より多くのベンチマークが必要です。そこで、我々は、土木工学の表現タスクである技術的な描き出の修正のコンテキストでのLLMアガントの詳細な評価を目的としてDrafterBenchを提案します。DrafterBenchは、実世界的な描き出ファイルからの12種類のタスクをまとめ、46つのカスタマイズされた関数/ツールと1920つのタスクを含みます。DrafterBenchは、AIアガントの複雑な長文脈の指示を理解する能力、先行知識を活用すること、動的な指示品質に適応するための潜在的なポリシー認識を活用した厳密なテストベンチマークです。ツールキットは、構造化されたデータの理解、関数実行、指示従行、批判的な理由の異なる能力を全面的に評価します。DrafterBenchは、タスクの正確性と誤りの統計を詳細に分析し、エージェントの能力に深い視点を提供し、工学アプリケーションでLLMの統合の改善ターゲットを特定することを目的としています。我々のベンチマークは、https://github.com/Eason-Li-AIS/DrafterBenchにアクセスでき、テストセットはhttps://huggingface.co/datasets/Eason666/DrafterBenchにアップロードされています。",
      "upvotes": 8,
      "discussionId": "68785ee1001546c83aa4f96a",
      "githubRepo": "https://github.com/Eason-Li-AIS/DrafterBench",
      "ai_summary": "DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.",
      "ai_keywords": [
        "Large Language Model (LLM)",
        "technical drawing revision",
        "structured data comprehension",
        "function execution",
        "instruction following",
        "critical reasoning",
        "benchmark",
        "open-source",
        "implicit policy awareness"
      ],
      "githubStars": 29
    },
    "publishedAt": "2025-07-15T13:56:04.000Z",
    "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering",
    "summary": "Large Language Model (LLM) agents have shown great potential for solving\nreal-world problems and promise to be a solution for tasks automation in\nindustry. However, more benchmarks are needed to systematically evaluate\nautomation agents from an industrial perspective, for example, in Civil\nEngineering. Therefore, we propose DrafterBench for the comprehensive\nevaluation of LLM agents in the context of technical drawing revision, a\nrepresentation task in civil engineering. DrafterBench contains twelve types of\ntasks summarized from real-world drawing files, with 46 customized\nfunctions/tools and 1920 tasks in total. DrafterBench is an open-source\nbenchmark to rigorously test AI agents' proficiency in interpreting intricate\nand long-context instructions, leveraging prior knowledge, and adapting to\ndynamic instruction quality via implicit policy awareness. The toolkit\ncomprehensively assesses distinct capabilities in structured data\ncomprehension, function execution, instruction following, and critical\nreasoning. DrafterBench offers detailed analysis of task accuracy and error\nstatistics, aiming to provide deeper insight into agent capabilities and\nidentify improvement targets for integrating LLMs in engineering applications.\nOur benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,\nwith the test set hosted at\nhttps://huggingface.co/datasets/Eason666/DrafterBench.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643ba2f725681c3afaa8f05e/2T1jxRrhqMGVLESbEDwJT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643ba2f725681c3afaa8f05e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
      "fullname": "Zhen Dong",
      "name": "zhendongucb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.02857",
      "authors": [
        {
          "_id": "68788cd9001546c83aa4f9f7",
          "user": {
            "_id": "66aef8691dd7d0a8c6584724",
            "avatarUrl": "/avatars/df9c2a56f3d0746cf64a330137a105b4.svg",
            "isPro": false,
            "fullname": "Ziye Li",
            "user": "TribeRinb",
            "type": "user"
          },
          "name": "Ziye Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:48:49.437Z",
          "hidden": false
        },
        {
          "_id": "68788cd9001546c83aa4f9f8",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "68788cd9001546c83aa4f9f9",
          "user": {
            "_id": "6335c7fa2db86a181cca723f",
            "avatarUrl": "/avatars/13f04af01914f8473f9939f49b4eecd4.svg",
            "isPro": false,
            "fullname": "XC",
            "user": "XinchengShuai",
            "type": "user"
          },
          "name": "Xincheng Shuai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:48:34.400Z",
          "hidden": false
        },
        {
          "_id": "68788cd9001546c83aa4f9fa",
          "user": {
            "_id": "67ff29ecbf6889a333c69c7a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
            "isPro": false,
            "fullname": "Henghui Ding",
            "user": "HenghuiDing",
            "type": "user"
          },
          "name": "Henghui Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:48:26.454Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T17:59:02.000Z",
      "submittedOnDailyAt": "2025-07-17T04:11:55.501Z",
      "title": "AnyI2V: 条件付き画像を動作制御でアニメーションする",
      "submittedOnDailyBy": {
        "_id": "67ff29ecbf6889a333c69c7a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
        "isPro": false,
        "fullname": "Henghui Ding",
        "user": "HenghuiDing",
        "type": "user"
      },
      "summary": "最近の映像生成の進歩、特にディフュージョンモデルにおける進歩は、文字から映像（T2V）と画像から映像（I2V）の合成における顕著な進歩を駆動しています。しかし、動的な動き信号と柔軟な空間制約の有効な統合には課題が残っています。現在のT2V手法は通常、テキストプラントに依存し、生成内容の空間配置の精密な制御を欠くことがあります。対して、I2V手法は実写画像に依存し、合成内容の編集可能性が制限されています。しかし、一部の手法はControlNetを採用して画像ベースの条件付けを導入していますが、通常は明確な動き制御を欠け、計算量の負荷を重ねています。これらの制限に対して、我々は、ユーザー定義された動きトラジェクトで条件付け画像を動画化するためのトレーニング無制限フレームワークAnyI2Vを提案しています。AnyI2Vは、ControlNetがサポートしないデータタイプのようにメッシュや点センターなどのより広範囲のモデルタイプを条件付け画像としてサポートし、より柔軟かつ多様な動画生成を可能にします。また、ミックスデータの条件付け入力をサポートし、LoRAとテキストプラントを通じてステールト転移と編集を可能にします。拡張された実験により、提案されたAnyI2Vは、空間と動きの制御を可能にした新しい視点を提供し、優れた性能を収めています。コードは、https://henghuiding.com/AnyI2V/ に提供されています。",
      "upvotes": 3,
      "discussionId": "68788cda001546c83aa4f9fb",
      "projectPage": "https://henghuiding.com/AnyI2V/",
      "githubRepo": "https://github.com/FudanCVL/AnyI2V",
      "ai_summary": "AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.",
      "ai_keywords": [
        "diffusion models",
        "text-to-video",
        "image-to-video",
        "ControlNet",
        "motion trajectories",
        "conditional images",
        "meshes",
        "point clouds",
        "mixed conditional inputs",
        "style transfer",
        "LoRA",
        "text prompts"
      ],
      "githubStars": 83
    },
    "publishedAt": "2025-07-03T13:59:02.000Z",
    "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
    "summary": "Recent advancements in video generation, particularly in diffusion models,\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\nsynthesis. However, challenges remain in effectively integrating dynamic motion\nsignals and flexible spatial constraints. Existing T2V methods typically rely\non text prompts, which inherently lack precise control over the spatial layout\nof generated content. In contrast, I2V methods are limited by their dependence\non real images, which restricts the editability of the synthesized content.\nAlthough some methods incorporate ControlNet to introduce image-based\nconditioning, they often lack explicit motion control and require\ncomputationally expensive training. To address these limitations, we propose\nAnyI2V, a training-free framework that animates any conditional images with\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\nas the conditional image, including data types such as meshes and point clouds\nthat are not supported by ControlNet, enabling more flexible and versatile\nvideo generation. Additionally, it supports mixed conditional inputs and\nenables style transfer and editing via LoRA and text prompts. Extensive\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\nand provides a new perspective in spatial- and motion-controlled video\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02857.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ff29ecbf6889a333c69c7a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
      "fullname": "Henghui Ding",
      "name": "HenghuiDing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.12462",
      "authors": [
        {
          "_id": "68785eb6001546c83aa4f95b",
          "name": "Yuxi Xiao",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f95c",
          "user": {
            "_id": "649bf403fd9cea8366d669ad",
            "avatarUrl": "/avatars/27bd8ca9a948ec38fee950b64f669ce3.svg",
            "isPro": false,
            "fullname": "Jianyuan Wang",
            "user": "JianyuanWang",
            "type": "user"
          },
          "name": "Jianyuan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:37:32.434Z",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f95d",
          "user": {
            "_id": "6485ce5ec7f19728a49df17a",
            "avatarUrl": "/avatars/e83966e6906c1d0f151300981e30f85a.svg",
            "isPro": true,
            "fullname": "Nan",
            "user": "cherubicxn",
            "type": "user"
          },
          "name": "Nan Xue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:15:41.525Z",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f95e",
          "user": {
            "_id": "6393a73584c565d2c3416cb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6393a73584c565d2c3416cb9/OGtX-i-_MLg5--qA054ti.jpeg",
            "isPro": true,
            "fullname": "Nikita Karaev",
            "user": "nikkar",
            "type": "user"
          },
          "name": "Nikita Karaev",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:40:30.784Z",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f95f",
          "name": "Yuri Makarov",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f960",
          "user": {
            "_id": "647b5fef6a79fbf5e996c47c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647b5fef6a79fbf5e996c47c/IkSMnDsCY_CyEFCiMDuxe.jpeg",
            "isPro": false,
            "fullname": "Bingyi Kang",
            "user": "bykang",
            "type": "user"
          },
          "name": "Bingyi Kang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:40:39.614Z",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f961",
          "name": "Xing Zhu",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f962",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f963",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "68785eb6001546c83aa4f964",
          "name": "Xiaowei Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-16T17:59:03.000Z",
      "submittedOnDailyAt": "2025-07-17T06:55:04.439Z",
      "title": "スペクトルトラッカーV2: 3次元点追踪を簡単にするソフトウェア",
      "submittedOnDailyBy": {
        "_id": "6688a8f30bf195d6e53ac28d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6688a8f30bf195d6e53ac28d/5izbVmdCWWwA1wBjcxZPB.jpeg",
        "isPro": true,
        "fullname": "Yuxi Xiao",
        "user": "Yuxihenry",
        "type": "user"
      },
      "summary": "SpatialTrackerV2を紹介します。これは、モノカメラビデオ用の3次元点追跡手法です。現成的なコンポーネントをもとに構築されたモジュレーションプイルスによる3次元追跡よりも進歩し、点追跡、モノカメラの深さ、カメラの姿勢推定の間の内在的な連結を高性能でフィードフォワード的な3次元点追跡器にまとめました。世界空間の3次元移動をスケーンジェネリック、カメラの自動移動、ピクセルごとの物達の移動に分解し、完全に微分可能でエンドツーエンドアーキテクチャを採用し、合成シーケンス、姿勢付きRGB-Dビデオ、無ラベルのフリーランドのファイルを含む幅広いデータセットでスケーラブルなトレーニングが可能になります。このような異質なデータから構造と移動を共に学習することで、SpatialTrackerV2は既存の3次元追跡手法を30%以上の効果的さに超え、先進的な動的な3次元再構成アプローチの精度を追い越し、50倍速く動作します。",
      "upvotes": 2,
      "discussionId": "68785eb7001546c83aa4f965",
      "projectPage": "https://spatialtracker.github.io",
      "githubRepo": "https://github.com/henry123-boy/SpaTrackerV2",
      "ai_summary": "SpatialTrackerV2 is a feed-forward 3D point tracking method for monocular videos that integrates point tracking, monocular depth, and camera pose estimation into a unified, end-to-end architecture, achieving high performance and speed.",
      "ai_keywords": [
        "feed-forward",
        "3D point tracking",
        "monocular videos",
        "intrinsic connections",
        "monocular depth",
        "camera pose estimation",
        "fully differentiable",
        "end-to-end architecture",
        "scene geometry",
        "camera ego-motion",
        "pixel-wise object motion",
        "synthetic sequences",
        "posed RGB-D videos",
        "unlabeled in-the-wild footage",
        "dynamic 3D reconstruction"
      ],
      "githubStars": 467
    },
    "publishedAt": "2025-07-16T13:59:03.000Z",
    "title": "SpatialTrackerV2: 3D Point Tracking Made Easy",
    "summary": "We present SpatialTrackerV2, a feed-forward 3D point tracking method for\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\ncomponents for 3D tracking, our approach unifies the intrinsic connections\nbetween point tracking, monocular depth, and camera pose estimation into a\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\nwith a fully differentiable and end-to-end architecture, allowing scalable\ntraining across a wide range of datasets, including synthetic sequences, posed\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\ndynamic 3D reconstruction approaches while running 50times faster.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12462.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6688a8f30bf195d6e53ac28d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6688a8f30bf195d6e53ac28d/5izbVmdCWWwA1wBjcxZPB.jpeg",
      "fullname": "Yuxi Xiao",
      "name": "Yuxihenry",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.09025",
      "authors": [
        {
          "_id": "68786e45001546c83aa4f9a0",
          "name": "Chien Van Nguyen",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a1",
          "name": "Ruiyi Zhang",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a2",
          "user": {
            "_id": "652767bfbdcf00b9b9ac9a74",
            "avatarUrl": "/avatars/2cc8e9167562f364f0c25410f13a9d62.svg",
            "isPro": false,
            "fullname": "Hanieh Deilamsalehy",
            "user": "haniehds",
            "type": "user"
          },
          "name": "Hanieh Deilamsalehy",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:36:52.637Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a3",
          "name": "Puneet Mathur",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a4",
          "name": "Viet Dac Lai",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a5",
          "name": "Haoliang Wang",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a6",
          "user": {
            "_id": "66d32a678819c81cce2052f4",
            "avatarUrl": "/avatars/3a5b40ef9e9e73512743756d1c24ab6c.svg",
            "isPro": false,
            "fullname": "Jayakumar Subramanian",
            "user": "jasubram",
            "type": "user"
          },
          "name": "Jayakumar Subramanian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:37:03.196Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a7",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a8",
          "user": {
            "_id": "67f1035155fe17a33dc71f23",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ohlRrzUI8VwvyYYxFXJuY.png",
            "isPro": false,
            "fullname": "Trung Bui",
            "user": "TrungBui1111",
            "type": "user"
          },
          "name": "Trung Bui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:37:13.349Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9a9",
          "user": {
            "_id": "675346f0ab1d47a36ca60b89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mqWUUxAuFn3DsgIqoF_ah.png",
            "isPro": false,
            "fullname": "Nikos Vlassis",
            "user": "Nikosapa",
            "type": "user"
          },
          "name": "Nikos Vlassis",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:36:17.107Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9aa",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:14:23.916Z",
          "hidden": false
        },
        {
          "_id": "68786e45001546c83aa4f9ab",
          "user": {
            "_id": "64804fad8c6a3b8f11f73912",
            "avatarUrl": "/avatars/61e37a91d4bba35fda9bf52aadd87745.svg",
            "isPro": false,
            "fullname": "Thien Huu Nguyen",
            "user": "anoperson",
            "type": "user"
          },
          "name": "Thien Huu Nguyen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:36:11.546Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-11T21:19:18.000Z",
      "submittedOnDailyAt": "2025-07-17T02:00:25.332Z",
      "title": "Lizard: 大規模言語モデルの効率的な線形化フレームワーク",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "ライターは、無限コンテキスト生成のための柔軟な、次平方構造のTransformerベースの大脳言語モデル（LLMs）を変換する線形化フレームワークです。TransformerベースのLLMsは、コンテキスト長の増加に伴い、ソフトマックスアテンションの二次元複雑性と増加するキー・値（KV）キャッシュによってメモリと計算バックロックが生じます。ライターは、ソフトマックスアテンションを近似しながら出力質量を保持した次平方アテンション機構を導入してこれらの制限を解決します。前の線形化方法と違い、固定モデル構造による制限があったため、ゲーティング機構を含めないことが多いでしたが、ライターは最近の最先端の線形モデルによるゲーティングモジュールをモデル化し、適応的なメモリ制御を可能にし、定数メモリ推論をサポートし、強い長さ一般化を提供し、モデル設計の柔軟性を増強します。ライターは、ゲーティングラインアテンションを用いたグローバルコンテキストコンパクティングとメタメモリによるスライディングウィンドウアテンションを組み合わせ、長距離依存関係と細かい局所的相互作用を同時に捉えるハイブリッド機構を形成します。また、ハードウェアによるアルゴリズムを導入し、モデルの訓練速度を加速します。拡張された実験は、ライターは標準言語モデリングタスクでティーチャーモデルの性能を近似無失的に復元し、前の線形化方法よりも显著に優れていることを示します。5ショットMMLUベンチマークでは、ライターは先駆者のモデルより18点以上の改善を示し、連想記憶タスクにおいても顕著な改善を示します。",
      "upvotes": 2,
      "discussionId": "68786e45001546c83aa4f9ac",
      "ai_summary": "Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.",
      "ai_keywords": [
        "Transformer-based LLMs",
        "subquadratic architectures",
        "softmax attention",
        "key-value (KV) cache",
        "subquadratic attention mechanism",
        "gating module",
        "adaptive memory control",
        "constant-memory inference",
        "length generalization",
        "gated linear attention",
        "sliding window attention",
        "meta memory",
        "hardware-aware algorithm",
        "MMLU benchmark",
        "associative recall tasks"
      ]
    },
    "publishedAt": "2025-07-11T17:19:18.000Z",
    "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
    "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09025.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.07451",
      "authors": [
        {
          "_id": "68747204257d4f04353702de",
          "user": {
            "_id": "6474b290d815855e4ef59b05",
            "avatarUrl": "/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg",
            "isPro": false,
            "fullname": "Hongzhi Zhang",
            "user": "hongzhizhang",
            "type": "user"
          },
          "name": "Hongzhi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-17T08:21:31.114Z",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702df",
          "name": "Jia Fu",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e0",
          "name": "Jingyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e1",
          "name": "Kai Fu",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e2",
          "name": "Qi Wang",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e3",
          "user": {
            "_id": "67c5945da1661d5fa6f29adb",
            "avatarUrl": "/avatars/62561f3875c0c251cae949acc38d72dc.svg",
            "isPro": false,
            "fullname": "Fuzheng Zhang",
            "user": "Edrex",
            "type": "user"
          },
          "name": "Fuzheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:35:13.823Z",
          "hidden": false
        },
        {
          "_id": "68747204257d4f04353702e4",
          "user": {
            "_id": "67c6c570cf87e2d2ebfc81aa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c6c570cf87e2d2ebfc81aa/7qAstZtIT86Uwrz3u_anv.jpeg",
            "isPro": false,
            "fullname": "Guorui Zhou",
            "user": "GuoruiZhou",
            "type": "user"
          },
          "name": "Guorui Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-17T09:34:55.007Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-10T05:58:55.000Z",
      "submittedOnDailyAt": "2025-07-17T07:36:19.087Z",
      "title": "RLEP: 経験再利用を用いた強化学習によるLLMの論理",
      "submittedOnDailyBy": {
        "_id": "6474b290d815855e4ef59b05",
        "avatarUrl": "/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg",
        "isPro": false,
        "fullname": "Hongzhi Zhang",
        "user": "hongzhizhang",
        "type": "user"
      },
      "summary": "強化学習（RL）を大型言語モデルに適用することは、エネルギーの大費用なことである：トレーニングは安定しないことがあり、政策は学習前の重みから徐々に漂流してしまうこともある。私たちは、経験再現を基にした強化学習（RLEP）を紹介します。RLEPは、最初に確認されたタロックを収集し、その後、後続トレーニング中に再現する2段階フレームワークです。更新ステップごとに、政策は新しく生成されたロールアウトとこれらの再現された成功を混ぜたミニバッチによって最適化されます。高品質の例を再現することで、RLEPはモデルから無駄な探索から引き出し、有望な理由のパスに集中し、ファスターな収束と最終的に強力な性能を提供します。Qwen2.5-Math-7Bベースモデルに対して、RLEPは基準の最高精度を達成することができ、大幅に少ない更新でも達成し、AIME-2024の精度を38.2%から39.9%、AIME-2025の精度を19.8%から22.3%、AMC-2023の精度を77.0%から82.2%に改善します。私たちのコード、データセット、チェックポイントは、https://github.com/Kwai-Klear/RLEPで公開しており、再現性と進める研究を促進することを目的としています。",
      "upvotes": 1,
      "discussionId": "68747204257d4f04353702e5",
      "ai_summary": "RLEP, a reinforcement learning framework with experience replay, enhances large language model training by focusing on high-quality examples, leading to faster convergence and improved performance on math-related benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "experience replay",
        "trajectories",
        "rollouts",
        "policy optimization",
        "convergence",
        "performance",
        "Qwen2.5-Math-7B",
        "AIME-2024",
        "AIME-2025",
        "AMC-2023"
      ]
    },
    "publishedAt": "2025-07-10T01:58:55.000Z",
    "title": "RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning",
    "summary": "Reinforcement learning (RL) for large language models is an energy-intensive\nendeavor: training can be unstable, and the policy may gradually drift away\nfrom its pretrained weights. We present RLEP\\, -- \\,Reinforcement\nLearning with Experience rePlay\\, -- \\,a two-phase framework that first\ncollects verified trajectories and then replays them during subsequent\ntraining. At every update step, the policy is optimized on mini-batches that\nblend newly generated rollouts with these replayed successes. By replaying\nhigh-quality examples, RLEP steers the model away from fruitless exploration,\nfocuses learning on promising reasoning paths, and delivers both faster\nconvergence and stronger final performance. On the Qwen2.5-Math-7B base model,\nRLEP reaches baseline peak accuracy with substantially fewer updates and\nultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,\non AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our\ncode, datasets, and checkpoints are publicly available at\nhttps://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07451.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6474b290d815855e4ef59b05",
      "avatarUrl": "/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg",
      "fullname": "Hongzhi Zhang",
      "name": "hongzhizhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05065",
      "authors": [
        {
          "_id": "68776e57ff8f47a7f86442bd",
          "user": {
            "_id": "669e707ec517d804cfce91c5",
            "avatarUrl": "/avatars/fc18e64e100cbf28763db04b44242747.svg",
            "isPro": false,
            "fullname": "Corrado Rainone",
            "user": "crainone",
            "type": "user"
          },
          "name": "Corrado Rainone",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-16T15:05:55.722Z",
          "hidden": false
        },
        {
          "_id": "68776e57ff8f47a7f86442be",
          "name": "Tim Bakker",
          "hidden": false
        },
        {
          "_id": "68776e57ff8f47a7f86442bf",
          "name": "Roland Memisevic",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T14:49:18.000Z",
      "submittedOnDailyAt": "2025-07-17T06:25:52.334Z",
      "title": "思考をツールの使用に置き換えることで、小規模な言語モデルでの理由論を可能にする",
      "submittedOnDailyBy": {
        "_id": "669e707ec517d804cfce91c5",
        "avatarUrl": "/avatars/fc18e64e100cbf28763db04b44242747.svg",
        "isPro": false,
        "fullname": "Corrado Rainone",
        "user": "crainone",
        "type": "user"
      },
      "summary": "最近の進展は、推論時と学習時の計算量の拡大に基づく新しい機械学習パラダイムを確立しました。その研究の流れでは、合成的な例示に対するSupervised Fine-Tuning (SFT) と可証明的報酬を持つReinforcement Learning (RLVR) の組み合わせを用いて、大規模な言語モデルを学習させ、推論時に自然言語で表される「思い」の形で額外の計算量を費用することを目指しています。この論文では、これらのトークンを状態保持ツールとの多ターンインタラクショントレースとしてフォーマットすることを提案します。各ターンでは、新しいツールの状態がモデルのコンテキストに追加され、モデルの仕事は、ユーザー定義のDSLをもとにトークンを生成してツールを制御することを目的とします。Pythonコードの修正問題にこのアプローチをベンチマークし、この制限された設定では、経験のサンプリングが速く、報酬信号が密になり、さらにモデルのサイズが3Bパラメータまで学習することができることを示します。",
      "upvotes": 1,
      "discussionId": "68776e58ff8f47a7f86442c0",
      "ai_summary": "A new approach formats tokens as a multi-turn interaction trace with a stateful tool for training Large Language Models, enabling faster sampling and denser reward signals for tasks like repairing Python code.",
      "ai_keywords": [
        "Supervised Fine-Tuning",
        "Reinforcement Learning with Verifiable Rewards",
        "Large Language Models",
        "multi-turn interaction trace",
        "stateful tool",
        "custom DSL"
      ]
    },
    "publishedAt": "2025-07-07T10:49:18.000Z",
    "title": "Replacing thinking with tool usage enables reasoning in small language\n  models",
    "summary": "Recent advances have established a new machine learning paradigm based on\nscaling up compute at inference time as well as at training time. In that line\nof work, a combination of Supervised Fine-Tuning (SFT) on synthetic\ndemonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is\nused for training Large Language Models to expend extra compute during\ninference in the form of \"thoughts\" expressed in natural language. In this\npaper, we propose to instead format these tokens as a multi-turn interaction\ntrace with a stateful tool. At each turn, the new state of the tool is appended\nto the context of the model, whose job is to generate the tokens necessary to\ncontrol the tool via a custom DSL. We benchmark this approach on the problem of\nrepairing malfunctioning Python code, and show that this constrained setup\nallows for faster sampling of experience and a denser reward signal, allowing\neven models of size up to 3B parameters to learn how to proficiently expend\nadditional compute on the task.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05065.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669e707ec517d804cfce91c5",
      "avatarUrl": "/avatars/fc18e64e100cbf28763db04b44242747.svg",
      "fullname": "Corrado Rainone",
      "name": "crainone",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]