[
  {
    "paper": {
      "id": "2504.13835",
      "authors": [
        {
          "_id": "6805b38355d3c792e1a9d0dd",
          "name": "Yicheng Chen",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0de",
          "name": "Yining Li",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0df",
          "name": "Kai Hu",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0e0",
          "name": "Zerun Ma",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0e1",
          "name": "Haochen Ye",
          "hidden": false
        },
        {
          "_id": "6805b38355d3c792e1a9d0e2",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T17:59:46.000Z",
      "submittedOnDailyAt": "2025-04-21T01:39:08.191Z",
      "title": "MIG: セマンティックスペースで情報収益を最大化するための自動的なデータ選択による指遺学調整",
      "submittedOnDailyBy": {
        "_id": "649988726677f66c2b486392",
        "avatarUrl": "/avatars/b649a77370660e129726e29504daba34.svg",
        "isPro": false,
        "fullname": "Yining Li",
        "user": "ly015",
        "type": "user"
      },
      "summary": "データの品質と多様性は、有効なインストラクションチューニングデータセットの構築において重要な要素です。オープンソースインストラクションチューニングデータセットの利用が増加していることに伴い、大量のデータから高品質と多様性を持つサブセットを自動的に選択することが有利です。現在の方法は通常インスタンスの品質を優先し、ヒューリスティックルールを使用して多様性を維持しますが、このような方法はデータセット全体の一貫した視点を持たないため、最適な結果を得ることが難しいことがあります。また、ヒューリスティックルールは通常埋め込み空間内の距離やクラスタリングを焦点にしているため、複雑なインストラクションの意図を正確に検出することができません。このギャップを埋めるために、データセットの情報量を定量化するための統一的な方法を提案します。この方法はラベルグラフを構築して記号的な空間をモデル化し、グラフ内の情報分布に基づいて多様性を定量化します。このような評価に基づいて、データサンプリング方法を効率的に導入し、イテレーティブにデータサンプルを選択し、記号的な空間で情報ゲインを最大化することを目指します。多様なデータセットと基盤モデルに対する実験は、この方法が最先端の方法を上回ることを示しています。特に、MIGでサンプリングされた5%のTulu3データを用いて微調節されたモデルは、全データセットで訓練された公式のSFTモデルと比較的性能を示し、AlpacaEvalで+5.73%とWildbenchで+6.89%の改善を収めました。",
      "upvotes": 26,
      "discussionId": "6805b38555d3c792e1a9d155",
      "projectPage": "https://yichengchen24.github.io/projects/mig",
      "githubRepo": "https://github.com/yichengchen24/MIG",
      "ai_keywords": [
        "label graph",
        "semantic space",
        "information content",
        "Maximize the Information Gain (MIG)"
      ]
    },
    "publishedAt": "2025-04-18T13:59:46.000Z",
    "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space",
    "summary": "Data quality and diversity are key to the construction of effective\ninstruction-tuning datasets. % With the increasing availability of open-source\ninstruction-tuning datasets, it is advantageous to automatically select\nhigh-quality and diverse subsets from a vast amount of data. % Existing methods\ntypically prioritize instance quality and use heuristic rules to maintain\ndiversity. % However, this absence of a comprehensive view of the entire\ncollection often leads to suboptimal results. % Moreover, heuristic rules\ngenerally focus on distance or clustering within the embedding space, which\nfails to accurately capture the intent of complex instructions in the semantic\nspace. % To bridge this gap, we propose a unified method for quantifying the\ninformation content of datasets. This method models the semantic space by\nconstructing a label graph and quantifies diversity based on the distribution\nof information within the graph. % Based on such a measurement, we further\nintroduce an efficient sampling method that selects data samples iteratively to\nMaximize the Information Gain (MIG) in semantic\nspace. % Experiments on various datasets and base models demonstrate that MIG\nconsistently outperforms state-of-the-art methods. % Notably, the model\nfine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance\nto the official SFT model trained on the full dataset, with improvements of\n+5.73\\% on AlpacaEval and +6.89\\% on Wildbench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13835.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649988726677f66c2b486392",
      "avatarUrl": "/avatars/b649a77370660e129726e29504daba34.svg",
      "fullname": "Yining Li",
      "name": "ly015",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13837",
      "authors": [
        {
          "_id": "6805b9ec7c5fa8020f595642",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595643",
          "name": "Zhiqi Chen",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595644",
          "name": "Rui Lu",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595645",
          "name": "Andrew Zhao",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595646",
          "name": "Zhaokai Wang",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595647",
          "name": "Yang Yue",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595648",
          "name": "Shiji Song",
          "hidden": false
        },
        {
          "_id": "6805b9ec7c5fa8020f595649",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/2KWQqFdVDUCAu-kSu87fa.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/rF4cAa6DAI3EaejDV_dgG.mp4"
      ],
      "publishedAt": "2025-04-18T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-21T01:54:36.096Z",
      "title": "強化学習は、基礎モデルを超えてLLMの理由論能力を奨励しますか？",
      "submittedOnDailyBy": {
        "_id": "649d475111592b1a765ac1a3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
        "isPro": false,
        "fullname": "Yang Yue",
        "user": "Yang130",
        "type": "user"
      },
      "summary": "RLVR（Verifiable Rewards）は、最近LLMの理由能力を向上させるために顕著な成功を示し、特に数学およびプログラミングタスクにおいても優れた性能を示している。一般的には、RLVRがLLMが継続的に自己改善することを可能にし、対応する基礎モデルよりも新しい理由能力を獲得することを想定されている。しかし、本研究では、この仮定を批判的に再検討し、大きなk値を用いたpass@kメトリックを評価し、様々なモデルフamiliesとベンチマークにおけるモデルの理由能力の境界を調べている。驚くべきに、RLは新しい理由パターンを根本的に引き出さないことがわかった。RLを用いた訓練されたモデルは、小さなk値（例えばk=1）で基礎モデルよりも優れた性能を示すが、大きなk値では、基礎モデルが比較的またはより高いpass@kスコアを達成することが見られる。RLを用いた訓練されたモデルが生成する理由パスは、基礎モデルのサンプリング分布に既に含まれていることを示し、これらのモデルが獲得した理由能力は基礎モデルですでにあるものであることを示している。進みに、RL訓練は、報酬を与えやすいパスにモデルの出力分布をバイアスさせ、正しい応答をより効率的にサンプリングすることで性能を向上させることがわかったが、これは基礎モデルと比較して理由能力の境界が狭まることも見られる。RLVRを用いた可視化理由タスクにおいても類似の結果が得られた。また、ディスティルショップは、RLVRと違い、新しい知識をモデルに紹介することができることが見られた。これらの発見は、RLVRがLLMの理由能力を向上させるための重要な制限を示し、理由LLMのRL訓練の影響と、より良いパラダイムの必要性を根本的に再考する必要を求めることを示している。プロジェクトページ：https://limit-of-RLVR.github.io",
      "upvotes": 24,
      "discussionId": "6805b9ed7c5fa8020f59568c",
      "projectPage": "https://limit-of-rlvr.github.io/",
      "githubRepo": "https://github.com/LeapLabTHU/limit-of-RLVR",
      "ai_keywords": [
        "Reinforcement Learning",
        "Verifiable Rewards",
        "RLVR",
        "LLMs (Large Language Models)",
        "reasoning capabilities",
        "mathematics",
        "programming tasks",
        "pass@\\textit{k}",
        "benchmark",
        "RL-trained models",
        "base models",
        "reasoning paths",
        "sampling distribution",
        "performance",
        "biasing",
        "output distribution",
        "visual reasoning tasks",
        "distillation"
      ]
    },
    "publishedAt": "2025-04-18T13:59:56.000Z",
    "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@k metric with large values of k to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does not, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of k (\\eg, k=1), base models can achieve a comparable or\neven higher pass@k score compared to their RL counterparts at large k\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/2KWQqFdVDUCAu-kSu87fa.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/649d475111592b1a765ac1a3/rF4cAa6DAI3EaejDV_dgG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13837.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "649d475111592b1a765ac1a3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649d475111592b1a765ac1a3/rjORJjErJq-mthghan08U.jpeg",
      "fullname": "Yang Yue",
      "name": "Yang130",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11833",
      "authors": [
        {
          "_id": "6805bb01747a412bca737b53",
          "name": "Changjiang Gao",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b54",
          "name": "Xu Huang",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b55",
          "name": "Wenhao Zhu",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b56",
          "name": "Shujian Huang",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b57",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "6805bb01747a412bca737b58",
          "name": "Fei Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T07:45:10.000Z",
      "submittedOnDailyAt": "2025-04-21T01:57:09.327Z",
      "title": "思考多语言是否能增强大型语言模型的推理能力？",
      "submittedOnDailyBy": {
        "_id": "65fed45b08d35929362dd651",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
        "isPro": false,
        "fullname": "FeiYuan",
        "user": "FeYuan",
        "type": "user"
      },
      "summary": "前の研究によると、大規模言語モデルは显著な「英語の偏った」という現象を示し、英語でタスクを提供する場合にはより良い性能を示すことが多い。興味深いながら、私たちは、特定の他の言語を用いた論理タスクでより良い性能を得ることができることを観察しましたが、この現象はまだ調査不足のままです。本論文では、論理タスクでの多言語利用の上限を評価し、英語だけの論理に比べて、多言語論理は約10 Acc@kポイント程度の効果的な増進と強固な（翻訳品質や言語選択の変化に対する耐性）上限を高めることを示します。また、上限の理由とその達成における課題を分析し、普通の答え選択方法がこの上限を達成できないことを明らかにします。これらのヒントは、LLMの多言語論理の全ての可能性を活用するための未来の研究のために役立つことを期待します。",
      "upvotes": 14,
      "discussionId": "6805bb02747a412bca737b7e",
      "githubRepo": "https://github.com/CONE-MT/multilingual_reasoning"
    },
    "publishedAt": "2025-04-16T03:45:10.000Z",
    "title": "Could Thinking Multilingually Empower LLM Reasoning?",
    "summary": "Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@k points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11833.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fed45b08d35929362dd651",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fed45b08d35929362dd651/KLMxsyRN6_HhCZP1iDw6K.png",
      "fullname": "FeiYuan",
      "name": "FeYuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13072",
      "authors": [
        {
          "_id": "6805bfc5e332a61dd90160b0",
          "name": "Wenqi Dong",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b1",
          "name": "Bangbang Yang",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b2",
          "name": "Zesong Yang",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b3",
          "name": "Yuan Li",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b4",
          "name": "Tao Hu",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b5",
          "name": "Hujun Bao",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b6",
          "name": "Yuewen Ma",
          "hidden": false
        },
        {
          "_id": "6805bfc5e332a61dd90160b7",
          "name": "Zhaopeng Cui",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63d748ff6f49aa82306b7e48/DNTOfy5oOkjTJwLBeGOUH.qt"
      ],
      "publishedAt": "2025-04-17T16:33:39.000Z",
      "submittedOnDailyAt": "2025-04-21T03:26:56.809Z",
      "title": "HiScene: エキゾチック3Dスキームの作成における等角ビュー生成による階層的なスキーム作成",
      "submittedOnDailyBy": {
        "_id": "63d748ff6f49aa82306b7e48",
        "avatarUrl": "/avatars/9f0b8b8a09b14d76e52ed1bd312e6b63.svg",
        "isPro": false,
        "fullname": "BB Yang",
        "user": "ybbbbt",
        "type": "user"
      },
      "summary": "シーンレベルの3D生成は、マルチメディアとコンピューターグラフィックでの重要な境界面であり、現在の手法は、対象の物体カテゴリーの制限やインタラクティブアプリケーションの編集柔軟性に欠点を持っています。本論文では、2D画像生成と3D物体生成の間のギャップをカットする新しい分階段的フレームワーク「HiScene」を紹介します。このフレームワークは、構成的な識別と美術的なシーン内容を持つ高精度のシーンを生成します。私たちの主なアインサインは、等距変換ビューでのシーンを分階段的な「物体」として扱うことです。これにより、部屋はさらに分解可能な複雑な物体として扱え、さらに操作可能な項目に分解されます。この分階段的なアプローチにより、3D内容を2D表現と一致させながら構成的構造を維持することができます。各分解されたインスタンスの完全性と空間的なアラインメントを確保するために、遮蔽や陰影の処理を効果的に行うビデオディフュージョンベースのアモデル完成技術を開発し、シーン内の空間的な一貫性を確保するために形状先驅注入を導入します。実験結果によると、私たちの方法は、インタラクティブアプリケーションに適した自然な物体配置と完全な物体インスタンスを生成し、物理的な合理性とユーザーの入力とのアラインメントを維持します。",
      "upvotes": 5,
      "discussionId": "6805bfc9e332a61dd901618b",
      "ai_keywords": [
        "hierarchical framework",
        "2D image generation",
        "3D object generation",
        "video-diffusion-based amodal completion",
        "occlusions",
        "shadows",
        "shape prior injection",
        "spatial coherence",
        "natural object arrangements",
        "complete object instances",
        "interactive applications",
        "physical plausibility",
        "user inputs"
      ]
    },
    "publishedAt": "2025-04-17T12:33:39.000Z",
    "title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation",
    "summary": "Scene-level 3D generation represents a critical frontier in multimedia and\ncomputer graphics, yet existing approaches either suffer from limited object\ncategories or lack editing flexibility for interactive applications. In this\npaper, we present HiScene, a novel hierarchical framework that bridges the gap\nbetween 2D image generation and 3D object generation and delivers high-fidelity\nscenes with compositional identities and aesthetic scene content. Our key\ninsight is treating scenes as hierarchical \"objects\" under isometric views,\nwhere a room functions as a complex object that can be further decomposed into\nmanipulatable items. This hierarchical approach enables us to generate 3D\ncontent that aligns with 2D representations while maintaining compositional\nstructure. To ensure completeness and spatial alignment of each decomposed\ninstance, we develop a video-diffusion-based amodal completion technique that\neffectively handles occlusions and shadows between objects, and introduce shape\nprior injection to ensure spatial coherence within the scene. Experimental\nresults demonstrate that our method produces more natural object arrangements\nand complete object instances suitable for interactive applications, while\nmaintaining physical plausibility and alignment with user inputs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63d748ff6f49aa82306b7e48/DNTOfy5oOkjTJwLBeGOUH.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13072.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d748ff6f49aa82306b7e48",
      "avatarUrl": "/avatars/9f0b8b8a09b14d76e52ed1bd312e6b63.svg",
      "fullname": "BB Yang",
      "name": "ybbbbt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13157",
      "authors": [
        {
          "_id": "6804392129303a3402c4f38e",
          "user": {
            "_id": "631bfb21f6bc4be4a6592afc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
            "isPro": false,
            "fullname": "Khiem Vuong",
            "user": "kvuong2711",
            "type": "user"
          },
          "name": "Khiem Vuong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-20T15:01:35.224Z",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f38f",
          "name": "Anurag Ghosh",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f390",
          "name": "Deva Ramanan",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f391",
          "name": "Srinivasa Narasimhan",
          "hidden": false
        },
        {
          "_id": "6804392129303a3402c4f392",
          "name": "Shubham Tulsiani",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/x2dR6H7Gl0l8qbXY9YxGP.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/Kf60HymR1YXGVqoSmQNYb.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/O0wHR9zr6E56IE7sjs0UI.jpeg"
      ],
      "publishedAt": "2025-04-17T17:57:05.000Z",
      "submittedOnDailyAt": "2025-04-21T01:28:10.375Z",
      "title": "AerialMegaDepth: 空中-地面再構造と視点合成の学習",
      "submittedOnDailyBy": {
        "_id": "631bfb21f6bc4be4a6592afc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
        "isPro": false,
        "fullname": "Khiem Vuong",
        "user": "kvuong2711",
        "type": "user"
      },
      "summary": "グリードと空気からの写真からの幾何的再構成の任務を調査します。現在の最先端的学習ベースのアプローチは、空気からの写真とグリードからの写真の極端な視点の変化を扱えません。私たちの仮説は、高品質の共変記録された空気からのグリードのデータセットがないことがこの失敗の主な理由です。このデータは、どのようにも構築するのが難しいため、スケーラブルに構築することが難しいです。この挑戦を克服するために、私たちは、3D都市ワークスファイアからのフォームバイトデータ（例：Google Earth）と、実際のグリードレベルのコムフォードサンプリングデータ（例：MegaDepth）を組み合わせたスケーラブルなフレームワークを提案します。フォームバイトデータは広範囲の空気からの視点をシミュレートし、実際のコムフォードサンプリングデータはマップベースのフォームバイトデータが十分な詳細を持たない場合のグリードレベルの写真の視覚的な忠実度を向上させ、実写画像とフォームバイトデータの領域間のギャップを閉じます。このハイブリッドデータセットを使用して、私たちは数々の最先端のアルゴリズムを調整し、実世界的な、ゼロショットの空気からのグリードのタスクで顕著な向上を達成します。例えば、ベースラインDUSt3Rは、カメラ回転誤差が5度以内での空気からのグリードのペアを5%未満で定位するが、私たちのデータでの調整で精度が近似56%に達し、大視点変化を扱う際の主要な失敗点を解決します。カメラの計測とスケーナの再構成を除き、私たちのデータも、難しい空気からのグリードのスケーナの合成のような下流タスクの性能を向上させ、実世界的なアプリケーションでの我々のアプローチの実用的な価値を示します。",
      "upvotes": 4,
      "discussionId": "6804392329303a3402c4f3e8",
      "projectPage": "https://aerial-megadepth.github.io/",
      "githubRepo": "https://github.com/kvuong2711/aerial-megadepth",
      "ai_keywords": [
        "geometric reconstruction",
        "learning-based approaches",
        "extreme viewpoint variation",
        "co-registered",
        "aerial-ground datasets",
        "pseudo-synthetic renderings",
        "3D city-wide meshes",
        "crowd-sourced images",
        "visual fidelity",
        "domain gap",
        "mesh-based renderings",
        "fine-tuning",
        "DUSt3R",
        "camera rotation error",
        "scene reconstruction",
        "novel-view synthesis",
        "downstream tasks"
      ]
    },
    "publishedAt": "2025-04-17T13:57:05.000Z",
    "title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View\n  Synthesis",
    "summary": "We explore the task of geometric reconstruction of images captured from a\nmixture of ground and aerial views. Current state-of-the-art learning-based\napproaches fail to handle the extreme viewpoint variation between aerial-ground\nimage pairs. Our hypothesis is that the lack of high-quality, co-registered\naerial-ground datasets for training is a key reason for this failure. Such data\nis difficult to assemble precisely because it is difficult to reconstruct in a\nscalable way. To overcome this challenge, we propose a scalable framework\ncombining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google\nEarth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The\npseudo-synthetic data simulates a wide range of aerial viewpoints, while the\nreal, crowd-sourced images help improve visual fidelity for ground-level images\nwhere mesh-based renderings lack sufficient detail, effectively bridging the\ndomain gap between real images and pseudo-synthetic renderings. Using this\nhybrid dataset, we fine-tune several state-of-the-art algorithms and achieve\nsignificant improvements on real-world, zero-shot aerial-ground tasks. For\nexample, we observe that baseline DUSt3R localizes fewer than 5% of\naerial-ground pairs within 5 degrees of camera rotation error, while\nfine-tuning with our data raises accuracy to nearly 56%, addressing a major\nfailure point in handling large viewpoint changes. Beyond camera estimation and\nscene reconstruction, our dataset also improves performance on downstream tasks\nlike novel-view synthesis in challenging aerial-ground scenarios, demonstrating\nthe practical value of our approach in real-world applications.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/x2dR6H7Gl0l8qbXY9YxGP.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/Kf60HymR1YXGVqoSmQNYb.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/631bfb21f6bc4be4a6592afc/O0wHR9zr6E56IE7sjs0UI.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13157.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631bfb21f6bc4be4a6592afc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631bfb21f6bc4be4a6592afc/FRgc7nwHylQZ9QURrr88y.jpeg",
      "fullname": "Khiem Vuong",
      "name": "kvuong2711",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11544",
      "authors": [
        {
          "_id": "6804ca9fd8538baa1c39ca93",
          "name": "Tianyang Xu",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca94",
          "name": "Haojie Zheng",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca95",
          "name": "Chengze Li",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca96",
          "name": "Haoxiang Chen",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca97",
          "name": "Yixin Liu",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca98",
          "name": "Ruoxi Chen",
          "hidden": false
        },
        {
          "_id": "6804ca9fd8538baa1c39ca99",
          "name": "Lichao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T18:24:00.000Z",
      "submittedOnDailyAt": "2025-04-21T01:38:46.380Z",
      "title": "NodeRAG: ホモジュラスノードを持つグラフベースのRAGの構造化",
      "submittedOnDailyBy": {
        "_id": "6610fb736504d9bed5890d58",
        "avatarUrl": "/avatars/832b186fc51c639f1709025d442b3f4b.svg",
        "isPro": false,
        "fullname": "Tianyang Xu",
        "user": "TerryXu666",
        "type": "user"
      },
      "summary": "レタイブレーディングアウガーエンジャン（RAG）は、大規模な言語モデルが外部および個人的なコーパスをアクセスできるようにし、特定の領域で事実的に一致した回答を提供できるようにする。コーパスの内在的な構造を利用することで、グラフベースのRAG手法は、グラフインデックスを構築し、グラフの構造的な性質を活用することでこのプロセスを進める。しかし、現在のグラフベースのRAGアプローチは、グラフ構造の設計を優先しません。不十分に設計されたグラフは、多様なグラフアルゴリズムの無難な統合を妨げ、ワークフローの不連続性と性能の低下を招きます。グラフの潜力をさらに引き出すために、ノードRAGというグラフ中心的なフレームワークを提案します。このフレームワークは、異なるグラフ構造を導入し、グラフベースのマシン学をRAGワークフローに無難で全体的に統合することを可能にします。LLMの能力と密接に合わせることで、このフレームワークは、一貫したであるような効率的な端末から端末までのプロセスを確保します。詳細な実験を通じて、ノードRAGは、前の方法と比較して、インデックス時間、クエリ時間、ストレージエフフィシェンス、そして多段階ベンチマークと開放的なヘッドオンエンド評価での優れた質問回答性能によって、性能の優れた側面を示します。私たちのGitHubリポジトリは、https://github.com/Terry-Xu-666/NodeRAGにあります。",
      "upvotes": 4,
      "discussionId": "6804caa0d8538baa1c39cac2",
      "projectPage": "https://terry-xu-666.github.io/NodeRAG_web/",
      "githubRepo": "https://github.com/Terry-Xu-666/NodeRAG",
      "ai_keywords": [
        "Retrieval-augmented generation (RAG)",
        "external and private corpus",
        "factually consistent responses",
        "knowledge graph index",
        "graph-based RAG methods",
        "heterogeneous graph structures",
        "seamless and holistic integration",
        "end-to-end process",
        "question-answering performance",
        "multi-hop benchmarks",
        "open-ended head-to-head evaluations",
        "retrieval tokens"
      ]
    },
    "publishedAt": "2025-04-15T14:24:00.000Z",
    "title": "NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes",
    "summary": "Retrieval-augmented generation (RAG) empowers large language models to access\nexternal and private corpus, enabling factually consistent responses in\nspecific domains. By exploiting the inherent structure of the corpus,\ngraph-based RAG methods further enrich this process by building a knowledge\ngraph index and leveraging the structural nature of graphs. However, current\ngraph-based RAG approaches seldom prioritize the design of graph structures.\nInadequately designed graph not only impede the seamless integration of diverse\ngraph algorithms but also result in workflow inconsistencies and degraded\nperformance. To further unleash the potential of graph for RAG, we propose\nNodeRAG, a graph-centric framework introducing heterogeneous graph structures\nthat enable the seamless and holistic integration of graph-based methodologies\ninto the RAG workflow. By aligning closely with the capabilities of LLMs, this\nframework ensures a fully cohesive and efficient end-to-end process. Through\nextensive experiments, we demonstrate that NodeRAG exhibits performance\nadvantages over previous methods, including GraphRAG and LightRAG, not only in\nindexing time, query time, and storage efficiency but also in delivering\nsuperior question-answering performance on multi-hop benchmarks and open-ended\nhead-to-head evaluations with minimal retrieval tokens. Our GitHub repository\ncould be seen at https://github.com/Terry-Xu-666/NodeRAG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11544.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6610fb736504d9bed5890d58",
      "avatarUrl": "/avatars/832b186fc51c639f1709025d442b3f4b.svg",
      "fullname": "Tianyang Xu",
      "name": "TerryXu666",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.09621",
      "authors": [
        {
          "_id": "6800ef5509eaa9d1d87a6eaf",
          "name": "Jiuchen Chen",
          "hidden": false
        },
        {
          "_id": "6800ef5509eaa9d1d87a6eb0",
          "user": {
            "_id": "6672c01fa6eb488f049ecb80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
            "isPro": false,
            "fullname": "Xinyu Yan",
            "user": "fengyanzi",
            "type": "user"
          },
          "name": "Xinyu Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-19T15:17:09.181Z",
          "hidden": false
        },
        {
          "_id": "6800ef5509eaa9d1d87a6eb1",
          "name": "Qizhi Xu",
          "hidden": false
        },
        {
          "_id": "6800ef5509eaa9d1d87a6eb2",
          "name": "Kaiqi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T15:41:25.000Z",
      "submittedOnDailyAt": "2025-04-21T05:45:47.747Z",
      "title": "トークナイズ画像パチス：大画像の有効な濁り除去のグローバルコンテキスト融合",
      "submittedOnDailyBy": {
        "_id": "6672c01fa6eb488f049ecb80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
        "isPro": false,
        "fullname": "Xinyu Yan",
        "user": "fengyanzi",
        "type": "user"
      },
      "summary": "グローバルのコンテキスト情報と地域的な詳細特徴は、霧除去タスクには不可欠です。深層学習モデルは小さい、低解像度の画像に優れていますが、大きい、高解像度の画像に対してはGPUメモリの制限により困難を見せます。補償として、それほどは画像スライシングまたはダウンサンプリングを選ぶことになります。その一方では、前者はグローバル情報を減少させ、後者は高周波数の詳細を捨てます。これらの課題を解決するために、私たちはDehazeXLという霧除去方法を提案します。これは、グローバルコンテキストと地域的な特徴抽出をよりよく調和させ、主流のGPUハードウェア上で大きな画像の端末から端末までのモデリングを可能にします。また、霧除去タスクの特性に合わせた視覚的な属性付与方法を設計し、グローバルコンテキストの利用の効率を評価するためにも役立ちます。最後に、大きな画像の霧除去に関するベンチマークデータセットの欠如を認識し、8KDehazeという超高解像度の霧除去データセットを開発しました。これは10000ペアの清かったと霧のある遠隔観測画像を含み、各画像のサイズは8192×8192ピクセルです。拡大した実験は、DehazeXLは21GBのメモリで最大10240×10240ピクセルの画像を推論でき、すべての評価された方法の中で最先端の結果を実現します。ソースコードと実験データセットは、https://github.com/CastleChen339/DehazeXLから利用可能です。",
      "upvotes": 4,
      "discussionId": "6800ef5709eaa9d1d87a6f76",
      "projectPage": "https://castlechen339.github.io/DehazeXL.github.io/",
      "githubRepo": "https://github.com/CastleChen339/DehazeXL",
      "ai_keywords": [
        "haze removal",
        "image slicing",
        "downsampling",
        "DehazeXL",
        "global context",
        "local feature extraction",
        "end-to-end modeling",
        "visual attribution",
        "8KDehaze",
        "ultra-high-resolution haze removal dataset",
        "remote sensing images"
      ]
    },
    "publishedAt": "2025-04-13T11:41:25.000Z",
    "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal\n  in Large Images",
    "summary": "Global contextual information and local detail features are essential for\nhaze removal tasks. Deep learning models perform well on small, low-resolution\nimages, but they encounter difficulties with large, high-resolution ones due to\nGPU memory limitations. As a compromise, they often resort to image slicing or\ndownsampling. The former diminishes global information, while the latter\ndiscards high-frequency details. To address these challenges, we propose\nDehazeXL, a haze removal method that effectively balances global context and\nlocal feature extraction, enabling end-to-end modeling of large images on\nmainstream GPU hardware. Additionally, to evaluate the efficiency of global\ncontext utilization in haze removal performance, we design a visual attribution\nmethod tailored to the characteristics of haze removal tasks. Finally,\nrecognizing the lack of benchmark datasets for haze removal in large images, we\nhave developed an ultra-high-resolution haze removal dataset (8KDehaze) to\nsupport model training and testing. It includes 10000 pairs of clear and hazy\nremote sensing images, each sized at 8192 times 8192 pixels. Extensive\nexperiments demonstrate that DehazeXL can infer images up to 10240 times\n10240 pixels with only 21 GB of memory, achieving state-of-the-art results\namong all evaluated methods. The source code and experimental dataset are\navailable at https://github.com/CastleChen339/DehazeXL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6672c01fa6eb488f049ecb80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6672c01fa6eb488f049ecb80/f6SetkETOWgyXy1KmBPhK.jpeg",
      "fullname": "Xinyu Yan",
      "name": "fengyanzi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13626",
      "authors": [
        {
          "_id": "6805fa66fddd500b98039425",
          "name": "Yule Liu",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b98039426",
          "name": "Jingyi Zheng",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b98039427",
          "name": "Zhen Sun",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b98039428",
          "name": "Zifan Peng",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b98039429",
          "name": "Wenhan Dong",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b9803942a",
          "name": "Zeyang Sha",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b9803942b",
          "name": "Shiwen Cui",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b9803942c",
          "name": "Weiqiang Wang",
          "hidden": false
        },
        {
          "_id": "6805fa66fddd500b9803942d",
          "name": "Xinlei He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T11:07:19.000Z",
      "submittedOnDailyAt": "2025-04-21T06:29:28.134Z",
      "title": "サインインして詳細な回答を受け取る",
      "submittedOnDailyBy": {
        "_id": "63da3d7ae697e5898cb86854",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675246771355-noauth.jpeg",
        "isPro": false,
        "fullname": "Talha Rüzgar Akkuş",
        "user": "Q-bert",
        "type": "user"
      },
      "summary": "最近の大規模認知モデル（LRMs）の進展は、テスト時の計算量をスケーリングして多様なタスクの認知能力を向上させる効果性を示しています。しかし、LRMsは通常「過度認知」問題に苦しむことがあり、モデルが限られた性能効果を得るには非常に多項な認知ステップを生成することがあります。現在の研究は、過度認知を軽減するために微調節を依存していますが、これは追加データ、非常规の訓練設定、安全性の調整へのリスク、および汎化能力の低下を伴うことがあります。\n\n実験的な分析を通じて、LRMの行動の重要な特徴を明らかにしたことで、小さなモデルが生成した外部のCoTsを思い出のトークン（<think>と</think>）の間に置くことで、モデルを複数の思い出を生成することを抑えることができることを明らかにしました。このエイリアスに基づき、ThoughtManiという簡単で効率的なパイプラインを提案し、LRMsが不必要な中間ステップをスキップして計算コストを大幅に減少させることを許可します。実験を幅広く実施し、ThoughtManiの効用と効率を証明しました。例えば、LiveBench/Codeデータセットに対してQwQ-32Bに応用した場合、ThoughtManiは元の性能を保ったまま出力トークン数を約30%減少させ、CoTジェネレーターからのオーバーヘッドが少なくなっています。また、ThoughtManiは安全性の調整を平均で10%向上させることがわかりました。モデルのビジネスポートは通常、異なるサイズのモデルを同時に提供しているため、ThoughtManiは実世界的なアプリケーションに対してより効率的かつアクセス可能なLRMsを構築するための効果的な方法を提供します。",
      "upvotes": 3,
      "discussionId": "6805fa67fddd500b98039461",
      "ai_keywords": [
        "large reasoning models (LRMs)",
        "overthinking problems",
        "fine-tuning",
        "thinking token",
        "external CoTs (Chain of Thought)",
        "ThoughtMani",
        "unnecessary intermediate steps",
        "computational costs",
        "LiveBench/Code dataset",
        "output token counts",
        "safety alignment"
      ]
    },
    "publishedAt": "2025-04-18T07:07:19.000Z",
    "title": "Thought Manipulation: External Thought Can Be Efficient for Large\n  Reasoning Models",
    "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities in multiple tasks. However, LRMs typically suffer from\n\"overthinking\" problems, where models generate significantly redundant\nreasoning steps while bringing limited performance gains. Existing work relies\non fine-tuning to mitigate overthinking, which requires additional data,\nunconventional training setups, risky safety misalignment, and poor\ngeneralization.\n  Through empirical analysis, we reveal an important characteristic of LRM\nbehaviors that placing external CoTs generated by smaller models between the\nthinking token (<think> and </think>) can effectively\nmanipulate the model to generate fewer thoughts. Building on these insights, we\npropose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass\nunnecessary intermediate steps and reduce computational costs significantly. We\nconduct extensive experiments to validate the utility and efficiency of\nThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code\ndataset, ThoughtMani keeps the original performance and reduces output token\ncounts by approximately 30%, with little overhead from the CoT generator.\nFurthermore, we find that ThoughtMani enhances safety alignment by an average\nof 10%. Since model vendors typically serve models of different sizes\nsimultaneously, ThoughtMani provides an effective way to construct more\nefficient and accessible LRMs for real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13626.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63da3d7ae697e5898cb86854",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675246771355-noauth.jpeg",
      "fullname": "Talha Rüzgar Akkuş",
      "name": "Q-bert",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 89
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.13173",
      "authors": [
        {
          "_id": "6805c4dab15a57fcb59b6f08",
          "name": "Ali Behrouz",
          "hidden": false
        },
        {
          "_id": "6805c4dab15a57fcb59b6f09",
          "name": "Meisam Razaviyayn",
          "hidden": false
        },
        {
          "_id": "6805c4dab15a57fcb59b6f0a",
          "name": "Peilin Zhong",
          "hidden": false
        },
        {
          "_id": "6805c4dab15a57fcb59b6f0b",
          "name": "Vahab Mirrokni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-17T17:59:33.000Z",
      "submittedOnDailyAt": "2025-04-21T02:39:28.607Z",
      "title": "全てが繋がっている：テストタイムメモリゼーション、注意バイアス、保存、オンライン最適化の旅程",
      "submittedOnDailyBy": {
        "_id": "65cccd5134a5d74cbaa9446c",
        "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
        "isPro": false,
        "fullname": "Ali Behrouz",
        "user": "AliBehrouz",
        "type": "user"
      },
      "summary": "設計効率の高いおよび効果的なアーキテクチャルバックボーンは、基盤モデルの能力を向上させるための研究の核心であった。人間の注意バイアスの現象をヒントに、特定のイベントや刺激を優先する自然的な傾向に類似するように、Transformers、Titans、そして現代の線形再帰的ニューラルネットワークを再概念化し、内部のオブジェクティブとして知識バイアスと呼ばれるものを使用して鍵と値のマッピングを学習する連想記憶モジュールとして扱う。驚きのほど、現在の多数のシーケンスモデルは、(1) dot-product 類似性、または (2) L2 回帰オブジェクティブを知識バイアスとして利用していることを見出した。これらのオブジェクティブを超えて、知識バイアスの選択肢とその効果的な近似を提供し、トレーニングプロセスの安定化を図る。そして、現代の深層学習アーキテクチャーでの忘れ機構を種のリテンション正規化として再詮釈し、シーケンスモデルに新たな忘れゲートを提供する。これらの見解に基づき、Mirasという一般的なフレームワークを提案し、(i) 連想記憶アーキテクチャ、(ii) 知識バイアスオブジェクティブ、(iii) リテンションゲート、(iv) 記憶学習アルゴリズムの4つの選択肢を基に深層学習アーキテクチャーを設計する。Moneta、Yaad、Memoraという3つの新しいシーケンスモデルを提案し、現在の線形RNNの能力を超えながら、高速な並列化可能なトレーニングプロセスを維持する。実験は、Mirasの異なる設計選択により、強度の異なるモデルを得ることを示し、例えば、言語モデリング、常識推理、そして記憶強調タスクなどの特別なタスクで、Transformersや現代の線形再帰的モデルを超える例外的な性能を収めるようなものがあることを示した。",
      "upvotes": 3,
      "discussionId": "6805c4dbb15a57fcb59b6f3d",
      "ai_keywords": [
        "Transformers",
        "Titans",
        "linear recurrent neural networks",
        "associative memory modules",
        "attentional bias",
        "dot-product similarity",
        "L2 regression",
        "retention regularization",
        "forget gates",
        "Miras",
        "Moneta",
        "Yaad",
        "Memora",
        "parallelizable training process",
        "language modeling",
        "commonsense reasoning",
        "recall intensive tasks"
      ]
    },
    "publishedAt": "2025-04-17T13:59:33.000Z",
    "title": "It's All Connected: A Journey Through Test-Time Memorization,\n  Attentional Bias, Retention, and Online Optimization",
    "summary": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13173.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cccd5134a5d74cbaa9446c",
      "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg",
      "fullname": "Ali Behrouz",
      "name": "AliBehrouz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]