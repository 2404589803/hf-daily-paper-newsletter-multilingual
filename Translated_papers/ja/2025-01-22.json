[
  {
    "paper": {
      "id": "2501.12380",
      "authors": [
        {
          "_id": "67906f432565fc5140d72dc3",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dc4",
          "name": "Lujing Xie",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dc5",
          "name": "Haowei Zhang",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dc6",
          "name": "Guo Gan",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dc7",
          "name": "Yitao Long",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dc8",
          "name": "Zhiyuan Hu",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dc9",
          "name": "Tongyan Hu",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dca",
          "name": "Weiyuan Chen",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dcb",
          "name": "Chuhan Li",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dcc",
          "name": "Junyang Song",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dcd",
          "name": "Zhijian Xu",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dce",
          "name": "Chengye Wang",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dcf",
          "name": "Weifeng Pan",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dd0",
          "name": "Ziyao Shangguan",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dd1",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dd2",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dd3",
          "name": "Yixin Liu",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dd4",
          "name": "Chen Zhao",
          "hidden": false
        },
        {
          "_id": "67906f432565fc5140d72dd5",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-21T18:56:18.000Z",
      "title": "MMVU: 専門家レベルの多様な学問領域のビデオ理解の評価",
      "summary": "MMVUは、ビデオ理解の基盤モデルを評価するための詳細な専門家レベルの多専門分野ベンチマークです。MMVUは、科学、医療、人文学・社会科学、工学の4つの核心分野における27つの主題を掲載した3,000件の専門家注記の質問を含みます。先行ベンチマークと比較して、MMVUは3つの主な進歩を特徴としています。1つ目は、モデルが専門分野の知識を適用し、専門家レベルの理由を持って専門分野のビデオを分析することを課題にし、現在のビデオベンチマークで通常評価される基本的な視覚認識を超えています。2つ目は、各例はプロフェッショナルから始めて注記されています。データの品質管理を厳格に実施し、データセットの高い品質を確保しています。最後に、各例は専門家の注記された理由と関連する分野の知識を追加されて、詳細な分析を促進します。MMVU上で32つの先鋒の多モデルを評価しました。最新のシステム2能力モデルであるo1とGemini 2.0 Flash Thinkingは、評価されたモデルの中で最高の性能を収めています。しかし、それらはまだ専門家の知識を完全に匹敵していません。詳細な誤り分析とケーススタディを通じて、将来の専門家レベルの知識密集型の専門分野のビデオ理解の進歩についての実用的なエイリアンを提供します。",
      "upvotes": 23,
      "discussionId": "67906f442565fc5140d72e4a"
    },
    "publishedAt": "2025-01-21T23:19:52.256Z",
    "title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12380.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.11425",
      "authors": [
        {
          "_id": "679080298ad1d8203a994f7f",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "679080298ad1d8203a994f80",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "679080298ad1d8203a994f81",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "679080298ad1d8203a994f82",
          "name": "Junjie Ye",
          "hidden": false
        },
        {
          "_id": "679080298ad1d8203a994f83",
          "name": "Zhengyin Du",
          "hidden": false
        },
        {
          "_id": "679080298ad1d8203a994f84",
          "name": "Jiecao Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-20T11:46:04.000Z",
      "title": "Agent-R: イテレーティブな自己学習を通じて表現する言語モデルアガントの訓練",
      "summary": "大語言モデル（LLMs）のアガントは、相互作用環境で複雑なタスクを解決するために重要な役割を果たしています。現在の研究は主に強いエクスパートからの行動クローニングを通じて性能を向上させることを焦点としていますが、これらのアプローチは実世界的なアプリケーションでは誤りを回復することができないため、失敗します。しかし、ステップレベルの評価データの収集は難しくコストが高いです。そこで、自動化して動的に自評価データセットを構築することが、アガントの知能的な能力を持つことを可能にするために重要です。本研究では、言語アガントが即時に反省することを可能にする迭り返し自学フレームワーク、Agent-Rを提案します。従来の方法と異なり、Agent-RはMCTSを利用して、誤りの軌道から正しい軌道を構築する訓練データを構築します。アガントの反省における主な課題は、軌道の終わりまで待つことではなく、時間的に修正する必要性です。これに対して、モデルガイドされた評価構成機構を導入します：アカウンターモデルは失敗した軌道の最初の誤りステップ（現在の能力の範囲内で）を識別します。そこから、その隣接する正しいパスをスプリードし、木の親ノードが同じです。この戦略は、モデルが現在のポリシーに基づいて反省を学習することを可能にし、より効率的な学習を実現します。さらに、この自動改善パラダイムのスケーラビリティを探求するために、誤り修正能力とデータセット構築の迭り返し精修を調査します。我々の調査結果は、Agent-Rがモデルの誤り回復能力を継続的に向上させ、時間的な誤り修正を可能にします。3つの相互作用環境での実験は、Agent-Rがアガントを備えることで誤りの行動を修正することを効果的に実現し、基準方法よりも上位の性能を達成しました（+5.59%）。",
      "upvotes": 13,
      "discussionId": "6790802b8ad1d8203a994fc7"
    },
    "publishedAt": "2025-01-22T00:20:57.292Z",
    "title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11425.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5732
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.12273",
      "authors": [
        {
          "_id": "67906c674932687e24e0cc08",
          "name": "Maosong Cao",
          "hidden": false
        },
        {
          "_id": "67906c674932687e24e0cc09",
          "name": "Taolin Zhang",
          "hidden": false
        },
        {
          "_id": "67906c674932687e24e0cc0a",
          "name": "Mo Li",
          "hidden": false
        },
        {
          "_id": "67906c674932687e24e0cc0b",
          "name": "Chuyu Zhang",
          "hidden": false
        },
        {
          "_id": "67906c674932687e24e0cc0c",
          "name": "Yunxin Liu",
          "hidden": false
        },
        {
          "_id": "67906c674932687e24e0cc0d",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "67906c674932687e24e0cc0e",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "67906c674932687e24e0cc0f",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-21T16:44:12.000Z",
      "title": "コンドール: 知識を駆使したデータ合成と編集によるLLMの調整向上",
      "summary": "Supervised Fine-Tuning (SFT)データの質は、Large Language Models (LLMs)の会話能力を向上させるために重要な役割を果たしています。しかし、LLMsが進歩していくに伴い、高品質の人間注釈されたSFTデータの可利用性が重要なバックロックとなり、合成データの利用によりもっと強い依存関係を持つ必要がありました。本論文では、World Knowledge TreeとSelf-Reflection Refinementを組み合わせた新しい2段階の合成データ生成フレームワークCondorを紹介します。このフレームワークは、スケール的に高品質のSFTデータを生成することを目的としています。実験結果によると、Condorで生成された20Kサンプルをベースモデルに適用した場合、極めて優れた性能を示し、同様のモデルに比べて上位に立ちます。Condorの追加の精進ステージは、LLMsの様々なスケール（最大72B）での反復的な自己改善を可能にし、我々のアプローチの効果を証明します。また、合成データのスケーリングについての調査によると、学習後の性能向上において、大きな未開発の可能性があり、将来の研究に希望のある道を開きます。",
      "upvotes": 7,
      "discussionId": "67906c684932687e24e0cc61"
    },
    "publishedAt": "2025-01-21T22:56:36.701Z",
    "title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12273.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630716d11801ecc7d2595021",
      "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
      "fullname": "Songyang Zhang",
      "name": "zsytony",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.11733",
      "authors": [
        {
          "_id": "6790791b203b95acf96ebf45",
          "user": {
            "_id": "628d7265db4cd1d1717c884f",
            "avatarUrl": "/avatars/dff2a3dd10d84b4a73fa486402de7219.svg",
            "isPro": false,
            "fullname": "Zhenhailong Wang",
            "user": "mikewang",
            "type": "user"
          },
          "name": "Zhenhailong Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-01-22T04:50:40.468Z",
          "hidden": false
        },
        {
          "_id": "6790791b203b95acf96ebf46",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "6790791b203b95acf96ebf47",
          "name": "Junyang Wang",
          "hidden": false
        },
        {
          "_id": "6790791b203b95acf96ebf48",
          "name": "Xi Zhang",
          "hidden": false
        },
        {
          "_id": "6790791b203b95acf96ebf49",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "6790791b203b95acf96ebf4a",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6790791b203b95acf96ebf4b",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6790791b203b95acf96ebf4c",
          "name": "Heng Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-20T20:35:46.000Z",
      "title": "Mobile-Agent-E: 複雑なタスク向けの自動転化モバイルアシスタント",
      "summary": "スマートフォンは現代生活に不可欠な存在となりましたが、モバイルデバイスで複雑なタスクを扱う際には、時々不満を感じることもあります。最近、大規模な多モダルモデル（LMM）に基づくモバイルアガントの進展は、モバイル環境での認識と行動を可能にしたことを示しています。しかし、現在のアプローチは以下の限界を抱えています：実世界的な人の需要を満たさない、理由論的な長期タスクを扱うことが難しい、そして先の経験から学習して改善する機構がないことです。これらの課題を克服するために、Mobile-Agent-Eという、過去の経験を通じて自己進化可能なヒューリスティックな多アガントフレームワークを紹介します。ヒューリスティックとは、高レベルの計画と低レベルの行動実行の明示的な区別を指します。このフレームワークは、複雑なタスクを次第的に分解して全体の計画を決定するManagerと、Perceptor、Operator、Action Reflector、Notetakerという4つの部下アガントからなります。Perceptorは細かい視覚認識、Operatorは即時的行動実行、Action Reflectorは誤りの確認、Notetakerは情報の集約を担当します。Mobile-Agent-Eは、TipsとShortcutsを含む新しい自己進化モジュールを採用し、これらは、先のタスクから得られた一般的な指導と教訓、特定のサブルーチンに適した再利用可能な原子操作の列です。TipsとShortcutsの採用により、性能と効率の継続的な改善が可能になります。また、Mobile-Eval-Eという新しいベンチマークも紹介し、複雑なモバイルタスクを扱う際に必要な長期タスクと多アプリの相互作用を含むものです。実験結果は、Mobile-Agent-Eは3つの基礎モデルバックボーンを通じて、前の最先端のアプローチより22%の絶対的な向上を収めたことを示しています。プロジェクトページは、https://x-plug.github.io/MobileAgentにあります。",
      "upvotes": 5,
      "discussionId": "67907920203b95acf96ec126"
    },
    "publishedAt": "2025-01-22T00:17:48.799Z",
    "title": "Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11733.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.10687",
      "authors": [
        {
          "_id": "6790856e3b0a6384a4117d0e",
          "name": "Linrui Tian",
          "hidden": false
        },
        {
          "_id": "6790856e3b0a6384a4117d0f",
          "name": "Siqi Hu",
          "hidden": false
        },
        {
          "_id": "6790856e3b0a6384a4117d10",
          "name": "Qi Wang",
          "hidden": false
        },
        {
          "_id": "6790856e3b0a6384a4117d11",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "6790856e3b0a6384a4117d12",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-18T07:51:29.000Z",
      "title": "EMO2: エンドエフェクターガイドドリーブアウディオ駆動アバタービデオ生成",
      "summary": "この論文では、高度な表情と手の手势を同時に生成できる新しい音声駆動テーブルヘッド手法を提案します。既存の手法は全身または半身の姿勢を生成することを焦点としていますが、我々は音声特徴量と全身の手势との弱い対応関係を主な制限として課題を調査し、これを解決するためにタスクを2段階的なプロセスとして再定義します。1段階目では、音声入力から直接手の姿勢を生成し、音声信号と手の動きとの強い関連性を活用します。2段階目では、生成された手の姿勢を組み込んでディフュージョンモデルを使用して動画フレームを合成し、実感的な面部表情と身体の動きを生成します。実験結果によると、提案手法はビジュアルキャラクター（CyberHost, Vlogger）といった最先端手法と比較して、視覚質量と同期精度においても優れています。この研究は、音声駆動ジェスチャー生成に新しい視点を提供し、表情的で自然なテーブルヘッドアニメーションの作成に強固なフレームワークを提供します。",
      "upvotes": 3,
      "discussionId": "679085813b0a6384a41183f1"
    },
    "publishedAt": "2025-01-22T00:49:10.316Z",
    "title": "EMO2: End-Effector Guided Audio-Driven Avatar Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10687.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65df1f1ee98700500d4c289c",
      "avatarUrl": "/avatars/be11bf61465df29ac997cc0fedad1cb9.svg",
      "fullname": "qi wang",
      "name": "lucaskingjade",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.12326",
      "authors": [
        {
          "_id": "679078f902b4d94b0f2347c1",
          "name": "Yujia Qin",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c2",
          "name": "Yining Ye",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c3",
          "name": "Junjie Fang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c4",
          "name": "Haoming Wang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c5",
          "name": "Shihao Liang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c6",
          "name": "Shizuo Tian",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c7",
          "name": "Junda Zhang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c8",
          "name": "Jiahao Li",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347c9",
          "name": "Yunxin Li",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347ca",
          "name": "Shijue Huang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347cb",
          "name": "Wanjun Zhong",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347cc",
          "name": "Kuanye Li",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347cd",
          "name": "Jiale Yang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347ce",
          "name": "Yu Miao",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347cf",
          "name": "Woyu Lin",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d0",
          "name": "Longxiang Liu",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d1",
          "name": "Xu Jiang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d2",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d3",
          "name": "Jingyu Li",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d4",
          "name": "Xiaojun Xiao",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d5",
          "name": "Kai Cai",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d6",
          "name": "Chuang Li",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d7",
          "name": "Yaowei Zheng",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d8",
          "name": "Chaolin Jin",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347d9",
          "name": "Chen Li",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347da",
          "name": "Xiao Zhou",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347db",
          "name": "Minchao Wang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347dc",
          "name": "Haoli Chen",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347dd",
          "name": "Zhaojian Li",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347de",
          "name": "Haihua Yang",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347df",
          "name": "Haifeng Liu",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347e0",
          "name": "Feng Lin",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347e1",
          "name": "Tao Peng",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347e2",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "679078f902b4d94b0f2347e3",
          "name": "Guang Shi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-21T17:48:10.000Z",
      "title": "UI-TARS: プロダクション化ガイドユィエリアインタラクションの先駆者",
      "summary": "この論文では、シーンショットをそのまま入力として扱うよりも人間のような交互作用を行うための原生GUIアガントモデル「UI-TARS」を紹介します。これは、プロフェッショナルなプロンプトとワークフローを用いた重複された商業モデル（例：GPT-4o）に依存している傍らのアガントフレームワークと比較して、これらを上回る優れた性能を示す端到端モデルです。実験は、視覚、グローディング、GUIタスク実行の評価を行う10以上のGUIアガントベンチマークでの最先端性能を達成することを示します。特に、OSWorldベンチマークでは、50ステップで24.6、15ステップで22.7のスコアを達成し、Claude（22.0と14.9）を上回ります。AndroidWorldでは、46.6のスコアを達成し、GPT-4o（34.5）を上回ります。UI-TARSは以下の数ノケイ新規性を採用しています： （1）拡張された視覚化：GUIシーンショットの大規模データセットを利用して、UI要素のコンテキストによる理解と正確なキャプチングを行います。 （2）統一されたアクションモデリング：プラットフォーム間でアクションを統一し、大規模アクショントレースを通じて正確なグローディングと交互作用を実現します。 （3）システム2の理由：多ステップ決定論において誤った理由を含めた認識を含み、タスク分解、反省思考、マイルストーン認識などの多数の理由パターンを含むことで、複数の理由を含めた認識を実現します。 （4）反省的オンライントレースを用いたイテレーティブトレーニング：データボックナーを解決するために、数百台のベーシックマシン上で自動的にコレクト、フィルタリング、反省的に改良された新しい交互作用トレースを実行します。イテレーティブトレーニングと反省調整を通じて、UI-TARSはその誤りから学ぶことを繰り返し、最小限の人間の介入を通じて予想されない状況に適応します。また、GUIアガントの進化パスを分析し、この領域の進展をガイドすることを試みています。",
      "upvotes": 3,
      "discussionId": "679078ff02b4d94b0f2348e0"
    },
    "publishedAt": "2025-01-21T23:51:53.248Z",
    "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12326.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5732
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.11223",
      "authors": [
        {
          "_id": "6790772b8d7df822f1fb4405",
          "name": "Maciej Besta",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4406",
          "name": "Julia Barth",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4407",
          "name": "Eric Schreiber",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4408",
          "name": "Ales Kubicek",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4409",
          "name": "Afonso Catarino",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb440a",
          "name": "Robert Gerstenberger",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb440b",
          "name": "Piotr Nyczyk",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb440c",
          "name": "Patrick Iff",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb440d",
          "name": "Yueling Li",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb440e",
          "name": "Sam Houliston",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb440f",
          "name": "Tomasz Sternal",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4410",
          "name": "Marcin Copik",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4411",
          "name": "Grzegorz Kwaśniewski",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4412",
          "name": "Jürgen Müller",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4413",
          "name": "Łukasz Flis",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4414",
          "name": "Hannes Eberhard",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4415",
          "name": "Hubert Niewiadomski",
          "hidden": false
        },
        {
          "_id": "6790772b8d7df822f1fb4416",
          "name": "Torsten Hoefler",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-20T02:16:19.000Z",
      "title": "Reasoning 言語モデル：プランフォルム",
      "summary": "理由言語モデル（RLMs）、もしくは大規模理由モデル（LRMs）と呼ばれるもの、OpenAIのo1とo3、DeepSeek-V3、AlibabaのQwQなどが、進捗的な理由機構を追加して大規模言語モデル（LLMs）の解決問題能力を再定義しました。しかし、高いコスト、専有性、複雑なアーキテクチャー、特に強化学習（RL）、探索ヒューリスティクス、LLMsの組み合わせによる独自な構造は、アクセス性とスケーラビリティの課題を生み出しています。これらの課題を解決するために、RLMのコンポーネントをモジュール化されたフレームワークに組み立てるための一様なプランを提案します。このプランは、すべてのRLMの調査と分析に基づいています。このプランは、多様な理由構造（鏈、木、グラフ、ネストフォーム）や理由戦略（例：モンテカルロ木探索、ビーム探索）、RLの概念（政策、価値モデルなど）、および観測計画（出力ベースとプロセスベースの観測）を含みます。また、RLMの実装を簡単にするために詳細な数学的な公式とアルゴリズムの規格を提供します。LLaMA-Berry、QwQ、Journey Learning、Graph of Thoughtsなどのシンプルな場合にどのように適用されるかを示し、プランの多様性と統一性を証明します。そして、x1と文献調査を用いて、政策モデルと価値モデルの多段階訓練や観測分布の重要性などのキーインサイドを提供します。最後に、RLMは広範囲のLLMエコシステムと統合する方法を説明します。我々の研究は、RLMの構築を解明し、進捗的な理由能力を民主化し、イノベーションを促進し、「豊かなAI」と「貧しいAI」の間の隙間を狭めるためにRLMの開発と実験の壁を下げることを目指しています。",
      "upvotes": 3,
      "discussionId": "6790772d8d7df822f1fb4493"
    },
    "publishedAt": "2025-01-21T23:42:44.747Z",
    "title": "Reasoning Language Models: A Blueprint",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11223.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5732
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.12390",
      "authors": [
        {
          "_id": "67906d622ae55818ddfd0d93",
          "name": "Chao Feng",
          "hidden": false
        },
        {
          "_id": "67906d622ae55818ddfd0d94",
          "name": "Ziyang Chen",
          "hidden": false
        },
        {
          "_id": "67906d622ae55818ddfd0d95",
          "name": "Aleksander Holynski",
          "hidden": false
        },
        {
          "_id": "67906d622ae55818ddfd0d96",
          "name": "Alexei A. Efros",
          "hidden": false
        },
        {
          "_id": "67906d622ae55818ddfd0d97",
          "name": "Andrew Owens",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-21T18:59:46.000Z",
      "title": "GPS を画像生成の制御信号としての利用",
      "summary": "GPSタグを含む写真データマテリアルは、画像生成に有用な制御信号として示唆する。GPSから画像へのモデルを訓練し、都市内の画像の微妙な変化を理解するための複雑なタスクに使用する。特に、GPSと文章の両方を条件としたディフュージョンモデルを訓練する。学習済みモデルは、異なるネイブハウス、公園、および地標の特徴的な外観を捉える画像を生成する。また、2D GPSから3Dモデルをスコアデステニションサンプリングを用いて抽出する。GPS条件を用いて、各視点からの再構成の外観を制御する。評価によると、GPS条件付きモデルは、位置に基づく画像の変化を生成することができ、GPS条件は推定される3D構造を改善することがわかる。",
      "upvotes": 3,
      "discussionId": "67906d682ae55818ddfd0f53"
    },
    "publishedAt": "2025-01-21T23:41:48.239Z",
    "title": "GPS as a Control Signal for Image Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12390.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645ab0b7c266796265baefa4",
      "avatarUrl": "/avatars/bdac661996b63c4b2a56881707afa01f.svg",
      "fullname": "Chao Feng",
      "name": "chfeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.12202",
      "authors": [
        {
          "_id": "67908409416b83605450716a",
          "name": "Zibo Zhao",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450716b",
          "name": "Zeqiang Lai",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450716c",
          "name": "Qingxiang Lin",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450716d",
          "name": "Yunfei Zhao",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450716e",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450716f",
          "name": "Shuhui Yang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507170",
          "name": "Yifei Feng",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507171",
          "name": "Mingxin Yang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507172",
          "name": "Sheng Zhang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507173",
          "name": "Xianghui Yang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507174",
          "name": "Huiwen Shi",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507175",
          "name": "Sicong Liu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507176",
          "name": "Junta Wu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507177",
          "name": "Yihang Lian",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507178",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507179",
          "name": "Ruining Tang",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450717a",
          "name": "Zebin He",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450717b",
          "name": "Xinzhou Wang",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450717c",
          "name": "Jian Liu",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450717d",
          "name": "Xuhui Zuo",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450717e",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450717f",
          "name": "Biwen Lei",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507180",
          "name": "Haohan Weng",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507181",
          "name": "Jing Xu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507182",
          "name": "Yiling Zhu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507183",
          "name": "Xinhai Liu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507184",
          "name": "Lixin Xu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507185",
          "name": "Changrong Hu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507186",
          "name": "Tianyu Huang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507187",
          "name": "Lifu Wang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507188",
          "name": "Jihong Zhang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507189",
          "name": "Meng Chen",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450718a",
          "name": "Liang Dong",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450718b",
          "name": "Yiwen Jia",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450718c",
          "name": "Yulin Cai",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450718d",
          "name": "Jiaao Yu",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450718e",
          "name": "Yixuan Tang",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450718f",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507190",
          "name": "Zheng Ye",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507191",
          "name": "Peng He",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507192",
          "name": "Runzhou Wu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507193",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507194",
          "name": "Yonghao Tan",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507195",
          "name": "Jie Xiao",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507196",
          "name": "Yangyu Tao",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507197",
          "name": "Jianchen Zhu",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507198",
          "name": "Jinbao Xue",
          "hidden": false
        },
        {
          "_id": "67908409416b836054507199",
          "name": "Kai Liu",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450719a",
          "name": "Chongqing Zhao",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450719b",
          "name": "Xinming Wu",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450719c",
          "name": "Zhichao Hu",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450719d",
          "name": "Lei Qin",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450719e",
          "name": "Jianbing Peng",
          "hidden": false
        },
        {
          "_id": "67908409416b83605450719f",
          "name": "Zhan Li",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a0",
          "name": "Minghui Chen",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a1",
          "name": "Xipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a2",
          "name": "Lin Niu",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a3",
          "name": "Paige Wang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a4",
          "name": "Yingkai Wang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a5",
          "name": "Haozhao Kuang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a6",
          "name": "Zhongyi Fan",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a7",
          "name": "Xu Zheng",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a8",
          "name": "Weihao Zhuang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071a9",
          "name": "YingPing He",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071aa",
          "name": "Tian Liu",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071ab",
          "name": "Yong Yang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071ac",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071ad",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071ae",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071af",
          "name": "Jingwei Huang",
          "hidden": false
        },
        {
          "_id": "67908409416b8360545071b0",
          "name": "Chunchao Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-21T15:16:54.000Z",
      "title": "フンユウン3D 2.0: 高解像度のテクスチャフィードドライブライブラリーの生成用の拡大ディフュージョンモデル",
      "summary": "ハニュアン3D 2.0、高度な大規模3D合成システムを紹介します。このシステムは、高解像度のテクスチャフィードドゥ3Dアセットを生成するためのものです。このシステムには、2つの基盤コンポーネントが含まれています：大規模な形状生成モデル「ハニュアン3D-DiT」と、大規模なテクスチャ合成モデル「ハニュアン3D-Paint」です。形状生成モデルは、スケーラブルなflow-based diffusion transformerにビルドされ、与えられた条件画像に合わせたようなジオメトリーを作成することを目的としています。これは、下流アプリケーションに強固な基盤を提供します。テクスチャ合成モデルは、強力なジオメトリーおよびdiffusion priorsからベースされ、生成されたもしくは手作りのメッシュに対して高解像度と豊かなテクスチャマップを生成します。また、ハニュアン3D-Studioを構築しました。これは、多様性のある、ユーザーフレンドリーな生産プラットフォームで、3Dアセットの再作成プロセスを簡単にすることを目的としています。これは、プロフェッショナルやファンがメッシュを操作やアニメーションを行うことができるようになります。モデルをシステマティックに評価し、ハニュアン3D 2.0が、ジオメトリーの詳細、条件の合わせ、テクスチャの品質などで前の最先端モデルを超えることを示しています。ハニュアン3D 2.0は、大規模な基盤生成モデルの開放ソース3Dコミュニティの欠点を埋めるために公開されています。モデルのコードと事前学習ホイールは以下のURLで利用可能です。\nhttps://github.com/Tencent/Hunyuan3D-2",
      "upvotes": 1,
      "discussionId": "6790840d416b8360545072a7"
    },
    "publishedAt": "2025-01-22T00:37:32.486Z",
    "title": "Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12202.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5732
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.10893",
      "authors": [
        {
          "_id": "67907dd5e1d8fc832b3e7b0f",
          "name": "Hongjin Su",
          "hidden": false
        },
        {
          "_id": "67907dd5e1d8fc832b3e7b10",
          "name": "Ruoxi Sun",
          "hidden": false
        },
        {
          "_id": "67907dd5e1d8fc832b3e7b11",
          "name": "Jinsung Yoon",
          "hidden": false
        },
        {
          "_id": "67907dd5e1d8fc832b3e7b12",
          "name": "Pengcheng Yin",
          "hidden": false
        },
        {
          "_id": "67907dd5e1d8fc832b3e7b13",
          "name": "Tao Yu",
          "hidden": false
        },
        {
          "_id": "67907dd5e1d8fc832b3e7b14",
          "name": "Sercan Ö. Arık",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-18T22:34:41.000Z",
      "title": "学習通り実践：実際の環境での自動調整アウトプログラムのデータ中心的フレームワーク",
      "summary": "自動語言モデル（LLMs）を基盤にした自動主義エージェントは、人間の能力を高め、メール送信からデータ分析までのデジタルタスクを支援することが可能です。現在のLLMsがこれらのタスクでの能力は、エージェントが相互作用する環境からの高品質なデータの欠落によって限定されています。私たちは、人間の注釈を必要とさせないように、給与環境に適応するLLMエージェントを構築するための「学習を通じて適応する」データセンタリックフレームワークを提案します。「学習を通じて適応する」は、文書に基づいたエージェントと環境の相互作用のトラジェクトを合成し、相互作用の歴史を要約または抽象化することでインストラクションを構築します。これを「後退構築」と呼びます。我々は、合成データの質を評価するために、学習ベースのシナリオと学習不要のin-context learning（ICL）で使用しています。ここでは、アウトロードの検索アプローチを開発し、エージェントに最適化されています。SWE-bench、WebArena、OSWorld、Spider2-Vの実際的なコーディング、ウェブ、デスクトップ環境を範囲に拡がる実験で、「学習を通じて適応する」が多様な下流エージェントタスクにおける効果を示しました。ICLの基準結果は、Claude-3.5では12.2％、Codestral-22Bでは19.5％の改善を示しました。また、後退構築の重要性を示し、学習における14.0％の改善を示しました。我々の消去研究は、ICLでの合成データの効率および我々の検索パイプラインの優越性を示しました。LLMsが実世界的な環境で実際に採用されることを期待しています。",
      "upvotes": 1,
      "discussionId": "67907dd9e1d8fc832b3e7c36"
    },
    "publishedAt": "2025-01-22T00:11:18.322Z",
    "title": "Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10893.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5732
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.11873",
      "authors": [
        {
          "_id": "679071da11a3f67d8f498649",
          "name": "Zihan Qiu",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f49864a",
          "name": "Zeyu Huang",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f49864b",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f49864c",
          "name": "Kaiyue Wen",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f49864d",
          "name": "Zekun Wang",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f49864e",
          "name": "Rui Men",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f49864f",
          "name": "Ivan Titov",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f498650",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f498651",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "679071da11a3f67d8f498652",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-21T04:04:39.000Z",
      "title": "デモンズインタイビュアル：トレーニング時に実装する負荷バランス損失について 専門的なエキスパートモデルの実装",
      "summary": "この論文は、Mixture-of-Experts (MoEs) モデルのトレーニング時に Load-balancing Loss (LBL) の実装を再調査します。特に、MoEs の LBL は、N_E\nsum_{i=1}^{N_E} f_i p_i と定義され、N_E は専門家の総数、f_i は専門家 i の選択頻度を表し、p_i は専門家 i の平均ゲーティングスコアを表します。現在の MoE トレーニングフレームワークは通常、平行トレーニング戦略を使用して、f_i と LBL はマイクロバッチ内で計算され、対応する平行グループで平均されます。本質的に、マイクロバッチは、トレーニングビリオンスケールの LLMs のトレーニング時には、非常に少ないシーケンスを含みます。そのため、マイクロバッチの LBL はほぼシーケンスレベルであり、ローターはそれぞれのシーケンス内でトークンを均等に配布するようにプレッシャーをかけられています。この厳しい制約の下で、ドメイン特化の専門家シーケンス（例：コード）からのトークンもすべての専門家に均等に配布され、専門家の特殊化を抑えます。この研究では、マイクロバッチの制約を解き、グローバルバッチを使用して LBL を計算する方法を提案します。グローバルバッチはマイクロバッチに比べて非常に多様なシーケンスを含み、これによりコーパスレベルでの負荷バランスを促進します。特に、f_i をマイクロバッチ間で同期させるための額外的なコミュニケーションステップを追加し、それを用いて LBL を計算します。MoEs ベースの LLMs のトレーニング（総パラメータ数が 42.8B まで、トークン数が 400B まで）の実験で、グローバルバッチの LBL 戦略が予測外の優れた性能向上を示しました。分析により、グローバルバッチの LBL はもっとも MoE の専門家のドメイン特殊化を大幅に向上させます。",
      "upvotes": 1,
      "discussionId": "679071db11a3f67d8f498680"
    },
    "publishedAt": "2025-01-21T23:27:52.660Z",
    "title": "Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647ccbd6e07cf9bb2d485244/ddUbQV_yVPwD6P0TSR5lu.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647ccbd6e07cf9bb2d485244/f7Q4QULppOygZlsYBUvY9.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647ccbd6e07cf9bb2d485244/9Jwx37bQkCjaWcccWbJ7b.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11873.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647ccbd6e07cf9bb2d485244",
      "avatarUrl": "/avatars/e8915abaff04f6762247e196b7cf84df.svg",
      "fullname": "Zihan Qiu",
      "name": "QwQZh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]