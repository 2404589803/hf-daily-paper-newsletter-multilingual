[
  {
    "paper": {
      "id": "2502.06807",
      "authors": [
        {
          "_id": "67ac1b080686a1e0690741ce",
          "name": "OpenAI",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d0",
          "name": "Ahmed El-Kishky",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d1",
          "name": "Alexander Wei",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d2",
          "name": "Andre Saraiva",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d3",
          "name": "Borys Minaev",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d4",
          "name": "Daniel Selsam",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d5",
          "name": "David Dohan",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d6",
          "name": "Francis Song",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d7",
          "name": "Hunter Lightman",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d8",
          "name": "Ignasi Clavera",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741d9",
          "name": "Jakub Pachocki",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741da",
          "name": "Jerry Tworek",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741db",
          "name": "Lorenz Kuhn",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741dc",
          "name": "Lukasz Kaiser",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741dd",
          "name": "Mark Chen",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741de",
          "name": "Max Schwarzer",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741df",
          "name": "Mostafa Rohaninejad",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e0",
          "name": "Nat McAleese",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e1",
          "name": "o3 contributors",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e2",
          "name": "Oleg Mürk",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e3",
          "name": "Rhythm Garg",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e4",
          "name": "Rui Shu",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e5",
          "name": "Szymon Sidor",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e6",
          "name": "Vineet Kosaraju",
          "hidden": false
        },
        {
          "_id": "67ac1b080686a1e0690741e7",
          "name": "Wenda Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-03T23:00:15.000Z",
      "title": "コンペティションプログラミングと大規模な理由論モデル",
      "summary": "We show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.",
      "upvotes": 28,
      "discussionId": "67ac1b090686a1e069074208"
    },
    "publishedAt": "2025-02-11T22:53:19.310Z",
    "title": "Competitive Programming with Large Reasoning Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07316",
      "authors": [
        {
          "_id": "67ac0ab720e98bddc5c19fed",
          "name": "Junlong Li",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19fee",
          "name": "Daya Guo",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19fef",
          "name": "Dejian Yang",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff0",
          "name": "Runxin Xu",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff1",
          "name": "Yu Wu",
          "hidden": false
        },
        {
          "_id": "67ac0ab720e98bddc5c19ff2",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T07:26:50.000Z",
      "title": "CodeI/O: コードの入力出力予測による理由のパターンの凝縮",
      "summary": "理由は、大規模言語モデルの基本的な能力です。先週の研究は、数学やコード生成の狭義スキルを向上させることを中心としていましたが、多様な理由論タスクの性能向上は、稀少で片連続な訓練データにより難しいです。この問題に対処するために、我々は、コードのコンテキストに基づく固有の理由論パターンをシステマティックに絞り込む新しいアプローチ、CodeI/Oを提案します。これは、元のコードをコード入力出力予測フォーマットに変換して行います。モデルをコードとテストケースを自然言語で与えた状態で入力/出力を予測することで、チャインオブスモール（Chain-of-Thought）の理由を学習させ、普遍的な理由論の基本要素（ロジックフロープランニング、状態空間探索、ディジション木トラバーサル、モジュラー分解）に暴露させます。これにより、コードの特定の語法を離れ、構造的な理由論をコードに関係なく保持し、手続きの厳密性を維持します。実験結果によると、CodeI/Oは記号的、科学的、ロジック、数学と数値、そして共通知識の理由論タスクで一貫した向上を示します。既存の事実をマッチした出力または予測された入力でコードを再実行することで、それぞれの予測を確認し、チャインオブスモールを複数回の修正によって進化させ、CodeI/O++を実現し、より高い性能を収めます。データとモデルは、https://github.com/hkust-nlp/CodeIO に公開されています。",
      "upvotes": 14,
      "discussionId": "67ac0ab820e98bddc5c1a039"
    },
    "publishedAt": "2025-02-11T23:00:20.080Z",
    "title": "CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07316.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "621e40ac944c7e36aaec2369",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621e40ac944c7e36aaec2369/Yj-FJRWps3rvsS_B2bnKo.jpeg",
      "fullname": "Junlong Li",
      "name": "lockon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07701",
      "authors": [
        {
          "_id": "67ac23166def89f9aae56abd",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56abe",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56abf",
          "user": {
            "_id": "66015e8aa4d296af07de538e",
            "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
            "isPro": false,
            "fullname": "Ye",
            "user": "Owen777",
            "type": "user"
          },
          "name": "Tian Ye",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:12.141Z",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac0",
          "name": "Jiantong Zhao",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac1",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac2",
          "name": "Michael Lingelbach",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac3",
          "name": "Li Yuan",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac4",
          "name": "Yonghong Tian",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac5",
          "name": "Enze Xie",
          "hidden": false
        },
        {
          "_id": "67ac23166def89f9aae56ac6",
          "name": "Daquan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T16:58:15.000Z",
      "title": "マジック1対1：1分以内に1分のビデオクリップを生成する",
      "summary": "この技術報告では、効率的なメモリ使用と推論時間を最適化したビデオ生成モデル「Magic 1-For-1 (Magic141)」を紹介します。主なアイデアは簡単です：テキストからビデオの生成を2つの異なる簡単なタスクに分解し、それらを異なる学習プロセスで実現します。テキストから画像の生成と画像からビデオの生成です。同じ最適化アルゴリズムを使用すると、画像からビデオの生成はテキストからビデオの生成よりも簡単で、収束が速いことが確認されました。また、画像からビデオの生成モデルの訓練計算コストを3つの面から減少させるための最適化トリックを調査しました。1) モデルの収束速度を加速するための多モーダル先行条件注入、2) 推論時間を加速するための対抗的なステップの学習、3) 推論時のメモリコストの最適化を行います。これらの技術を活用して、3秒以内に5秒のビデオクラップを生成できます。テスト時間のスライディングウィンドウを適用することで、1分間のビデオを1分以内に生成でき、画質と動作の視覚的な品質が大幅に向上し、平均1秒のビデオクラップの生成に1秒以内に時間を費やすことができます。これらの技術を活用して、計算コストとビデオの品質の最適な調整を見つけ、これは開放ソースの探索のガイドラインとなることを期待しています。コードとモデル重みは、https://github.com/DA-Group-PKU/Magic-1-For-1 から利用できます。",
      "upvotes": 11,
      "discussionId": "67ac23186def89f9aae56b69"
    },
    "publishedAt": "2025-02-11T23:27:13.769Z",
    "title": "Magic 1-For-1: Generating One Minute Video Clips within One Minute",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07701.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03492",
      "authors": [
        {
          "_id": "67a5a8e595df68b0a167c298",
          "user": {
            "_id": "622f103fc78da4c7ebd7c887",
            "avatarUrl": "/avatars/b0c7cd29835d92c2cd584947fcd5d520.svg",
            "isPro": false,
            "fullname": "Xie",
            "user": "Zhihui",
            "type": "user"
          },
          "name": "Zhihui Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:17:02.682Z",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c299",
          "name": "Jie chen",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29a",
          "name": "Liyu Chen",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29b",
          "name": "Weichao Mao",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29c",
          "name": "Jingjing Xu",
          "hidden": false
        },
        {
          "_id": "67a5a8e595df68b0a167c29d",
          "name": "Lingpeng Kong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-05T02:18:46.000Z",
      "title": "「Reinforcement Learningによる言語モデルの評価指導」",
      "summary": "LLMの批判と改良を学習させることは、連続的に改善可能なシステムを構築するために重要であるが、正確な判断と行動可能なアドバイスの提供能力に限られている。本研究では、コード生成のためのLLMの批判者を研究し、CTRL（Critic Training via Reinforcement Learning）フレームワークを提案します。CTRLは、人間のスーパーバイザーがない状況でも固定されたジェネレータモデルの修正性能を最大化するためのフィードバックを生成する批判者モデルを訓練するものです。我々の結果は、CTRLで訓練された批判者が基礎的なジェネレータモデルと強化したジェネレータモデルの両方でパス率を大幅に向上させ、連続的な誤りを抑えることを示します。また、これらの批判者モデルは正確な生成的報酬モデルとして機能し、試験時のスケーリングを可能にし、難しいコード生成ベンチマークでは最大106.1%の相対的な向上を達成します。",
      "upvotes": 10,
      "discussionId": "67a5a8e695df68b0a167c2c6"
    },
    "publishedAt": "2025-02-11T23:55:37.671Z",
    "title": "Teaching Language Models to Critique via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03492.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622f103fc78da4c7ebd7c887",
      "avatarUrl": "/avatars/b0c7cd29835d92c2cd584947fcd5d520.svg",
      "fullname": "Xie",
      "name": "Zhihui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.06329",
      "authors": [
        {
          "_id": "67ab4174757d2eb190af0375",
          "user": {
            "_id": "621d6f532165dc431641e438",
            "avatarUrl": "/avatars/56ccef10a8426d7160ef3586a771bd63.svg",
            "isPro": false,
            "fullname": "Kiran Kamble",
            "user": "kiranr",
            "type": "user"
          },
          "name": "Kiran Kamble",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:55.367Z",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0376",
          "name": "Melisa Russak",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0377",
          "name": "Dmytro Mozolevskyi",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0378",
          "user": {
            "_id": "6320a906a023aad6a7670e99",
            "avatarUrl": "/avatars/48071559b0c7660bf6861cfe008b3006.svg",
            "isPro": false,
            "fullname": "Muayad Sayed Ali",
            "user": "muayad",
            "type": "user"
          },
          "name": "Muayad Ali",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:53.157Z",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af0379",
          "name": "Mateusz Russak",
          "hidden": false
        },
        {
          "_id": "67ab4174757d2eb190af037a",
          "name": "Waseem AlShikh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T10:29:28.000Z",
      "title": "期待意外：FinanceのFailSafe長文脈QA",
      "summary": "ファイナンス領域のLLMベンチマーク「FailSafeQA」を提案します。これは、LLMによるクエリーアンサーシステムでの6種類の人間インターフェースインタラクションの変化に対するLLMの強固性とコンテキスト認識を測定するために設計されています。2つのケーススタディを焦点としています：クエリーファイルとコンテキストファイル。クエリーファイルの場合、元のクエリーを変形して領域専門性、完全性、語学正確性の変化を実装します。コンテキストファイルの場合、悪化した、関係ない、空のドキュメントの上書きをシミュレートします。Qwen2.5-72B-Instructを用いたLLM-as-a-Judge手法を採用し、ロバスト性、コンテキストグラウンド、コンプライアンススコアを定義し、24つのオフシャールフェルムモデルに対して計算します。結果は、あるモデルが入力の摂動を抑えるための優れた性能を示すことを示し、それらは強固な回答と幻覚を避ける能力とのバランスを保つ必要があることを示します。特に、最もコンプライアンスの高いモデルであるPalmyra-Fin-128k-Instructは、基線性能を維持するものの、17%のテストケースで強固な予測を維持することが難しかったことが調べられました。一方、最も強固なモデルであるOpenAI o3-miniは、41%のテストケースで情報を捏造しました。これらの結果は、高性能のモデルも大幅に改善の余地があることを示し、FailSafeQAは金融アプリケーションでの信頼性を最適化したLLMの開発においてツールとしての役割を示します。データセットは以下のURLで利用可能です：https://huggingface.co/datasets/Writer/FailSafeQA",
      "upvotes": 8,
      "discussionId": "67ab4175757d2eb190af03ca"
    },
    "publishedAt": "2025-02-12T02:51:41.003Z",
    "title": "Expect the Unexpected: FailSafe Long Context QA for Finance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06329.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60e61b3969bd0df25c9375da",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
      "fullname": "Melisa Russak",
      "name": "melisa",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07617",
      "authors": [
        {
          "_id": "67ac1d68c29356f92ed772c5",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c6",
          "name": "Ibrahim Alabdulmohsin",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c7",
          "name": "Daniel Salz",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c8",
          "name": "Zhe Li",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772c9",
          "name": "Keran Rong",
          "hidden": false
        },
        {
          "_id": "67ac1d68c29356f92ed772ca",
          "name": "Xiaohua Zhai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T15:05:33.000Z",
      "title": "100億データへの視覚言語モデルのスケーリング予ち学習",
      "summary": "このビジネスの潜在的な可能性を証拠的に調査します：1000億例の前所未聞のスケールでのビジョン・ラングネージュモデルの事前学習。このスケールでは、多くの一般的な西洋中心的な分類と検索ベンチマーク（例えば、COCO Captions）でモデルの性能は通常飽和します。しかし、文化多様性のタスクは、1000億例のウェブデータの幅によりより大きな効果を得ます。これは、長尾概念の幅によるものです。また、モデルの多言語性を分析し、低リソース言語でも効果を得ます。また、CLIPなどの品質フィルターを使用して予備学習データセットのサイズを削減することで、ネガティブな影響を与えることを見出しました。このデータサイズは、実際に包摂する文化多様性を持つ多言語システムの構築に重要であることを明らかにします。",
      "upvotes": 8,
      "discussionId": "67ac1d6ac29356f92ed77354"
    },
    "publishedAt": "2025-02-11T23:03:08.578Z",
    "title": "Scaling Pre-training to One Hundred Billion Data for Vision Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07617.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07374",
      "authors": [
        {
          "_id": "67ac1c6436464325ebe3c6e3",
          "name": "Dacheng Li",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e4",
          "name": "Shiyi Cao",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e5",
          "name": "Tyler Griggs",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e6",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e7",
          "name": "Xiangxi Mo",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e8",
          "name": "Shishir G. Patil",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6e9",
          "name": "Matei Zaharia",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6ea",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "67ac1c6436464325ebe3c6eb",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T08:48:48.000Z",
      "title": "LLMsは示唆から理由を理解することができ、内容ではなく、構造が重要です！",
      "summary": "大論理モデル（LRMs）は、長い連鎖オフサインソース（Long CoT）を追跡して複雑な論理問題を解決します。これには、反省、後退、自動証明などを含むものです。しかし、これらの長い連鎖オフサインソースを引き出すための訓練手法とデータ要求は、理解されていません。本研究では、大語言モデル（LLM）がデータ効率的な規範的調整（SFT）とパラメータ効率的な低レンジアダプター（LoRA）を通じて長い連鎖オフサインソース論理を有効に学習できることを見出しました。17kの長い連鎖オフサインソース訓練サンプルで、Qwen2.5-32B-Instructモデルは広範囲の数学とコーディングベンチマークにおいて显著な向上を収め、AIME 2024では56.7%（+40.0%）、LiveCodeBenchでは57.0%（+8.1%）、これらはプロプライドモデルのo1-previewモデルのスコア（44.6%と59.1%）と競合的です。より重要なことに、長い連鎖オフサインソースの構造が学習プロセスにもとって重要であり、個々の論理ステップの内容は影響を及ぼさないことを見出しました。內容に関する摂動（例えば、不正なサンプルでの訓練や論理キーワードの削除）は性能に少し影響を及ぼしません。一方、長い連鎖オフサインソースの邏輯的な一貫性を破壊する構造的変更（例えば、論理ステップのシャッフルや削除）は精度を大幅に低下させます。例えば、不正な回答を持つ長い連鎖オフサインソースサンプルでの訓練でも、完全に正確なサンプルでの訓練と比べて精度は3.2%だけ低下します。これらの見解は、LLMの論理能力を引き出す方法についての理解を深め、次世代の論理モデルの効率的な訓練のための重要な考慮点を明らかにします。これは、以前公開したSky-T1-32B-Previewモデルの学術論文です。コードはhttps://github.com/NovaSky-AI/SkyThoughtに提供されています。",
      "upvotes": 8,
      "discussionId": "67ac1c6536464325ebe3c723"
    },
    "publishedAt": "2025-02-11T22:58:37.585Z",
    "title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07374.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.03997",
      "authors": [
        {
          "_id": "67ac206214d5fe7767e7ec4e",
          "name": "Yu Yuan",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec4f",
          "user": {
            "_id": "63eb00a191a1b8ec4fbba2a9",
            "avatarUrl": "/avatars/0cc7cf9b6d05337603f700e0d592edf5.svg",
            "isPro": false,
            "fullname": "ShizhaoSun",
            "user": "ShizhaoSun",
            "type": "user"
          },
          "name": "Shizhao Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:16:14.580Z",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec50",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "67ac206214d5fe7767e7ec51",
          "name": "Jiang Bian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T11:57:14.000Z",
      "title": "CADエディター：位置指定して埋め込みのフレームワークと自動化されたトレーニングデータ合成を用いたテキストベースのCAD編集",
      "summary": "コンピューターデザイン支援（CAD）は、様々な産業で不可欠です。文脈ベースのCAD編集は、テキストベースの指示に基づいたCADモデルの変更を自動化することで、大きなポテンシャルを持っていますが、まだ見つかりませんでした。現在の方法は主に設計変化生成またはテキストベースのCAD生成に焦点を当てていますが、テキストベースの制御サポートを欠けているり、既存のCADモデルを制約として扱いません。CAD-Editorを紹介します。これは、文脈ベースのCAD編集の最初のフレームワークです。訓練に必要なデモクラットデータの厳しい対応を解決するために、自動化されたデータ合成パイプラインを提案します。このパイプラインは設計変化モデルを利用して、元のと編集されたCADモデルのペアを生成し、大規模なビジョン言語モデル（LVLMs）を使用して、その違いを編集指示として要約します。文脈ベースのCAD編集の複合的な性質を解決するために、ローケートしてフィルリングのフレームワークを提案します。これは、変更が必要な領域を特定し、適切な編集をその領域に追加することを2つの焦点付きのサブタスクに分解します。大規模な言語モデル（LLMs）は両サブタスクのベースとして役立ち、自然言語理解とCAD知識の機能を活用します。実験は、CAD-Editorは定量的および質的な両方で上位の性能を達成します。",
      "upvotes": 6,
      "discussionId": "67ac206314d5fe7767e7ec98"
    },
    "publishedAt": "2025-02-11T23:16:28.213Z",
    "title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.03997.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63eb00a191a1b8ec4fbba2a9",
      "avatarUrl": "/avatars/0cc7cf9b6d05337603f700e0d592edf5.svg",
      "fullname": "ShizhaoSun",
      "name": "ShizhaoSun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07527",
      "authors": [
        {
          "_id": "67ac1eaac61306b0ac95d2c6",
          "name": "Yingce Xia",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c7",
          "name": "Peiran Jin",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c8",
          "name": "Shufang Xie",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2c9",
          "name": "Liang He",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ca",
          "name": "Chuan Cao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cb",
          "name": "Renqian Luo",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cc",
          "name": "Guoqing Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cd",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ce",
          "name": "Zequn Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2cf",
          "name": "Yuan-Jyue Chen",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d0",
          "name": "Zekun Guo",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d1",
          "name": "Yeqi Bai",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d2",
          "name": "Pan Deng",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d3",
          "name": "Yaosen Min",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d4",
          "name": "Ziheng Lu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d5",
          "name": "Hongxia Hao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d6",
          "name": "Han Yang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d7",
          "name": "Jielan Li",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d8",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2d9",
          "name": "Jia Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2da",
          "name": "Jianwei Zhu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2db",
          "name": "Kehan Wu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2dc",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2dd",
          "name": "Kaiyuan Gao",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2de",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2df",
          "name": "Qian Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e0",
          "name": "Xixian Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e1",
          "name": "Yanting Li",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e2",
          "name": "Houtian Zhu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e3",
          "name": "Yeqing Lu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e4",
          "name": "Mingqian Ma",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e5",
          "name": "Zun Wang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e6",
          "name": "Tian Xie",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e7",
          "name": "Krzysztof Maziarz",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e8",
          "name": "Marwin Segler",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2e9",
          "name": "Zhao Yang",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ea",
          "name": "Zilong Chen",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2eb",
          "name": "Yu Shi",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ec",
          "name": "Shuxin Zheng",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ed",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ee",
          "name": "Chen Hu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2ef",
          "name": "Peggy Dai",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f0",
          "name": "Tie-Yan Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f1",
          "name": "Haiguang Liu",
          "hidden": false
        },
        {
          "_id": "67ac1eaac61306b0ac95d2f2",
          "name": "Tao Qin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T13:08:03.000Z",
      "title": "NatureLM: 自然の言語を解読して科学の発見を促進する",
      "summary": "基礎モデルは自然言語処理と人工知能に革命的な影響を与え、機械が人類の言語を理解し生成する方法を大幅に向上させました。これらの基礎モデルの成功を励まし、研究者は小分子、材料、プロティン、DNA、RNAなどの個別的科学領域における基礎モデルを開発しました。しかし、これらのモデルは通常独自に訓練され、科学領域間での統合能力を欠くことが多いです。これらの領域の中のエンティティがすべて順序表現で表現でき、それらは「自然の言語」として組み合わされることを認識し、Nature Language Model (簡略的にNatureLM) を紹介します。これは、科学発見に向けて設計された順序ベースの科学基礎モデルです。複数の科学領域からのデータで事前学習され、以下のような様々なアプリケーションを可能にします： (i) 文字指示を用いて小分子、プロティン、RNA、材料の生成と最適化；(ii) 領域間の生成/設計、そして (iii) SMILES-to-IUPAC翻訳とUSPTO-50kの逆合成において最先端の性能を達成します。NatureLMは薬物発見（ヒット生成/最適化、ADMET最適化、合成）、新規材料設計、治療プロティンまたはヌクレオチドの開発など、様々な科学タスクに対する一般的なアプローチを提供します。さまざまなサイズ（1億、8億、467億パラメータ）のNatureLMモデルを開発し、モデルサイズが増加することにより明らかに性能が向上しました。",
      "upvotes": 6,
      "discussionId": "67ac1eabc61306b0ac95d346"
    },
    "publishedAt": "2025-02-11T23:10:26.895Z",
    "title": "NatureLM: Deciphering the Language of Nature for Scientific Discovery",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.06589",
      "authors": [
        {
          "_id": "67ac1d45e6f1e95ccf6de3b7",
          "user": {
            "_id": "6471bddd609ae9f56368f132",
            "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
            "isPro": true,
            "fullname": "Yuchen Zhuang",
            "user": "yczhuang",
            "type": "user"
          },
          "name": "Yuchen Zhuang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-12T04:02:14.866Z",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3b8",
          "name": "Jingfeng Yang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3b9",
          "name": "Haoming Jiang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3ba",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bb",
          "name": "Kewei Cheng",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bc",
          "name": "Sanket Lokegaonkar",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bd",
          "name": "Yifan Gao",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3be",
          "name": "Qing Ping",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3bf",
          "name": "Tianyi Liu",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c0",
          "name": "Binxuan Huang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c1",
          "name": "Zheng Li",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c2",
          "name": "Zhengyang Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c3",
          "name": "Pei Chen",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c4",
          "name": "Ruijie Wang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c5",
          "name": "Rongzhi Zhang",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c6",
          "name": "Nasser Zalmout",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c7",
          "name": "Priyanka Nigam",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c8",
          "name": "Bing Yin",
          "hidden": false
        },
        {
          "_id": "67ac1d45e6f1e95ccf6de3c9",
          "name": "Chao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-10T15:54:34.000Z",
      "title": "ヘフェアステウス: 大語言モデルの基本的なアガント能力を継続的な予め学習によって向上させる",
      "summary": "ヘファエスチュス・フォージュ、LLMエージェントの基本的な能力を強化するための最初の大規模な予習データコーパスです。このコーパスは、API関数呼び出し、内在的な理由論と計画、環境フィードバックに対する適応性を強化することを目的としています。ヘファエスチュス・フォージュは、103B個のエージェント特有データを含み、76,537個のAPIを収録しています。これらのAPIには、ツールドキュメントが含まれており、API関数の知識を提供し、関数呼び出しのプロセスを強化しています。さらに、このコーパスによる継続的な予習を行うことで、ヘファエスチュスは小ささから中間規模のオープンソースLLMを上回り、3つのエージェントベンチマークでコマーシャルLLMと同等のレベルに達し、LLMの基本的なエージェント能力と新しいタスクまたは環境への拡張性の効果を示しています。",
      "upvotes": 6,
      "discussionId": "67ac1d46e6f1e95ccf6de419"
    },
    "publishedAt": "2025-02-11T23:04:08.153Z",
    "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.06589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6471bddd609ae9f56368f132",
      "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
      "fullname": "Yuchen Zhuang",
      "name": "yczhuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07508",
      "authors": [
        {
          "_id": "67ac2006a6b5a26040fc94f7",
          "name": "Yang Luo",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94f8",
          "name": "Xuanlei Zhao",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94f9",
          "name": "Mengzhao Chen",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fa",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fb",
          "name": "Wenqi Shao",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fc",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fd",
          "name": "Zhangyang Wang",
          "hidden": false
        },
        {
          "_id": "67ac2006a6b5a26040fc94fe",
          "name": "Yang You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T12:22:35.000Z",
      "title": "エンハンスアバイデオ：無料でより良い生成ビデオを作成する",
      "summary": "DiTベースのビデオ生成は驚異的な成果を達成しましたが、既存モデルの拡張に関する研究は相対的に見つかりにくい状態です。本研究では、DiTベースの生成ビデオの連続性と質を向上させるためのトレーニング不要のアプローチを紹介します。このアプローチは、非対角時間注意分布に基づいたフレーム間の相互関係を向上させることで核心的な概念を持ちます。このアプローチは簡単な設計を特徴とし、ほとんどのDiTベースのビデオ生成フレームワークに簡単に適用でき、リトレーニングや微調節を必要としません。各DiTベースのビデオ生成モデルにおいて、本アプローチは時間的な一貫性と視覚的な質の向上について見識のある改善を示します。私たちは、この研究は将来のビデオ生成拡張の研究に激励することを望むと思います。",
      "upvotes": 5,
      "discussionId": "67ac200ea6b5a26040fc9709"
    },
    "publishedAt": "2025-02-11T23:14:10.293Z",
    "title": "Enhance-A-Video: Better Generated Video for Free",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07508.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04223",
      "authors": [
        {
          "_id": "67ac5e0d653d273eeaf25e59",
          "name": "Ilia Karmanov",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5a",
          "user": {
            "_id": "67ac5d85a19e34140ea1013b",
            "avatarUrl": "/avatars/e5b7446787dbbd17553dc9e11b58a0b4.svg",
            "isPro": false,
            "fullname": "Amala Sanjay Deshmukh",
            "user": "amalad",
            "type": "user"
          },
          "name": "Amala Sanjay Deshmukh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:14:49.009Z",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5b",
          "name": "Lukas Voegtle",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5c",
          "name": "Philipp Fischer",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5d",
          "user": {
            "_id": "64c7a43e0d3d1b209df90b9c",
            "avatarUrl": "/avatars/1d0d2f129b799a72345b17fd5307aa5e.svg",
            "isPro": false,
            "fullname": "Kateryna Chumachenko",
            "user": "katerynaCh",
            "type": "user"
          },
          "name": "Kateryna Chumachenko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:14:47.025Z",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5e",
          "name": "Timo Roman",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e5f",
          "user": {
            "_id": "60098ca06e8ac78787773f85",
            "avatarUrl": "/avatars/be6539e5706bf07c71e553254c1751b5.svg",
            "isPro": false,
            "fullname": "Jarno Seppänen",
            "user": "jseppanen",
            "type": "user"
          },
          "name": "Jarno Seppänen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:14:51.062Z",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e60",
          "name": "Jupinder Parmar",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e61",
          "name": "Joseph Jennings",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e62",
          "name": "Andrew Tao",
          "hidden": false
        },
        {
          "_id": "67ac5e0d653d273eeaf25e63",
          "name": "Karan Sapra",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T17:07:22.000Z",
      "title": "エクレール -- 統合読み順での内容と並びの抽出\nドキュメント向け",
      "summary": "光学文字识别（OCR）テクノロジーは、ドキュメントの画像からテキストを抽出するために広く使用されており、効率的なデジタル化とデータ検索を促進します。しかし、複雑なドキュメントを対処する際には、テキストのみを抽出するだけでは十分ではありません。ドキュメントの構造を完全に理解するには、フォーマット、公式、テーブル、複数のページの複数のブロックと列の読み順、および腳注や画像のキャプチャのような要素の検出に関する意味的情報が必要です。このような詳細な理解は、検索、ドキュメントの質問回答、および大規模な言語モデル（LLMs）と視覚言語モデル（VLMs）の訓練用データのカレーティングのような下流タスクに必要です。これに対処するために、私たちは、複数のドキュメントタイプを処理するために特に設計された一般的なテキスト抽出ツールとして、\\'Eclairを紹介します。画像を与えると、\\'Eclairは、読み順に並べられたフォーマットされたテキストを抽出し、そのバウンディングボックスと対応する意味のクラスを一緒に抽出できます。これらの新しい能力を詳細に評価するために、私たちは、ドキュメントレベルのOCRと意味分類の多様な人間データベースを紹介します。\\'Eclairはこのベンチマークで最先端の精度を達成し、構成要素の主なメトリックで他の方法を上回ります。また、既存のベンチマークでも\\'Eclairを評価し、複数の評価標準での変容性と強さを示します。",
      "upvotes": 3,
      "discussionId": "67ac5e0f653d273eeaf25eea"
    },
    "publishedAt": "2025-02-12T04:25:54.558Z",
    "title": "Éclair -- Extracting Content and Layout with Integrated Reading Order for Documents",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60098ca06e8ac78787773f85/BfZ57W-gCoY32J60tx7dN.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04223.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60098ca06e8ac78787773f85",
      "avatarUrl": "/avatars/be6539e5706bf07c71e553254c1751b5.svg",
      "fullname": "Jarno Seppänen",
      "name": "jseppanen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07445",
      "authors": [
        {
          "_id": "67ac216d602eb9ca8a517be6",
          "name": "Nurit Cohen-Inger",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be7",
          "name": "Yehonatan Elisha",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be8",
          "name": "Bracha Shapira",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517be9",
          "name": "Lior Rokach",
          "hidden": false
        },
        {
          "_id": "67ac216d602eb9ca8a517bea",
          "name": "Seffi Cohen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T10:43:36.000Z",
      "title": "チャメランのようなLLMs評価を忘れてみてください",
      "summary": "大語言モデル（LLMs）は、公開ベンチマークでは高いスコアを示すことが多いが、これらの高いスコアは、真の言語理解による性能の高さを隠している可能性がある。私たちは、チャメラノンベンチマークオーバーフィット検出器（C-BOD）を導入します。C-BODは、パラメーター変換によりベンチマークのプロンプトをシステマティックに変形し、LLMsのオーバーフィットを検出するメタ評価フレームワークです。入力を再構成しながら意味的な内容とラベルを保存することで、C-BODはモデルの性能が記憶したパターンによって駆動されているかどうかを明らかにします。MMLUベンチマーク上で26つの先進LLMsを使用して評価した結果、我々の方法は、軽微な変形による平均的な性能低下率が2.15%で、26モデルのうち20モデルが統計的に有意な差異を示しました。特に、基線精度が高いモデルは、変形による性能低下が大きく、大きなLLMsは再構成に対して敏感であり、両方の場合では固定プロンプトパターンに依存している可能性が高いことが明らかになりました。対比的に、Llamaファミリーと基線精度が低いモデルは、有意な低下は見られませんでした、表面的なコードに依存している可能性が低いことを示します。また、C-BODはデータセットとモデルに依存しない設計で、訓練プインプルームに簡単に統合でき、より強固な言語理解を促進することができます。我々の発見は、ランキングスコアを超えて、LLM評価の魆動けさと拡張性を優先することをコミュニティに求めることを示します。",
      "upvotes": 3,
      "discussionId": "67ac216e602eb9ca8a517c1d"
    },
    "publishedAt": "2025-02-11T23:22:50.454Z",
    "title": "Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07445.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6731e56a07cf693a1104d2cb",
      "avatarUrl": "/avatars/46a3269a19c7e6bfb7004a5da9701459.svg",
      "fullname": "Seffi Cohen",
      "name": "seffico",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.04465",
      "authors": [
        {
          "_id": "67a953844ea315a67e02461d",
          "user": {
            "_id": "63195d0582e7eec0eac040e3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
            "isPro": false,
            "fullname": "Luca Della Libera",
            "user": "lucadellalib",
            "type": "user"
          },
          "name": "Luca Della Libera",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-11T10:03:10.257Z",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e02461e",
          "name": "Francesco Paissan",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e02461f",
          "name": "Cem Subakan",
          "hidden": false
        },
        {
          "_id": "67a953844ea315a67e024620",
          "name": "Mirco Ravanelli",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-06T19:24:50.000Z",
      "title": "FocalCodec: 焦点調制ネットワークを用いた低ビットレート音声コーディング",
      "summary": "大語言モデルは、巨大なデータセットにおける自己規範的な事前学習により、自然言語処理を革命化しました。この成功に駆り掛け、研究者は、ニューラルアウドィオコーデックを用いて、連続的な音声をトークン化して、これらの方法を音声に適用して探索しました。しかし、現在のアプローチは、高ビットレート、セマンティック情報または音響的な情報の失わせ、両方を捉えるために多コードブック設計を依存し、これは下流タスクのアーキテクチャ複雑性を増加させるなど、制限があります。これらの課題に対処するために、私たちは、フォーカルモジュレーションに基づく効率的な低ビットレートコーデックを紹介します。これは、フォーカルコーデックで、1つのビネティーコードブックを用いて、0.16から0.65 kbpsの範囲での音声を圧縮します。フォーカルコーデックは、現在の最先端技術よりも低ビットレートで、音声再合成と声の変換において競合的な性能を提供し、多言語音声とノイズエンビューでも効果的に扱います。下流タスクの評価によると、フォーカルコーデックは、十分なセマンティック情報と音響的な情報を保存し、同時に生成モデリングに適しています。デモサンプル、コードとチェックポイントは、https://lucadellalib.github.io/focalcodec-web/から利用可能です。",
      "upvotes": 2,
      "discussionId": "67a953854ea315a67e024659"
    },
    "publishedAt": "2025-02-12T01:31:44.368Z",
    "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.04465.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63195d0582e7eec0eac040e3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63195d0582e7eec0eac040e3/0tXOYkMfmv9e53zBWgqz7.png",
      "fullname": "Luca Della Libera",
      "name": "lucadellalib",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.07531",
      "authors": [
        {
          "_id": "67ac21acaa680a0f8782d273",
          "name": "Sixiao Zheng",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d274",
          "name": "Zimian Peng",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d275",
          "name": "Yanpeng Zhou",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d276",
          "name": "Yi Zhu",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d277",
          "name": "Hang Xu",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d278",
          "name": "Xiangru Huang",
          "hidden": false
        },
        {
          "_id": "67ac21acaa680a0f8782d279",
          "name": "Yanwei Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T13:11:59.000Z",
      "title": "VidCRAFT3: カメラ、物体、光源の制御を行う画像から動画の生成",
      "summary": "最近の画像から動画生成手法は、カメラの移動や物体の動きなど1つか2つの視覚的要素を制御できることを示したが、データやネットワークの効率により複数の視覚的要素を制御できるものは存在しない。本論文では、VidCRAFT3という新しいフレームワークを紹介し、カメラの移動、物体の動き、光の方向を同時に制御できる精密な画像から動画生成を可能にするものである。各視覚的要素の制御をより離れることを図るために、スペクトラルトリプルアテンショントランスフォーマーを提案し、光の方向、テキスト、画像を対称的に統合することを目的とする。多数の実世界的な動画データセットには光の方向のアノテーションがないため、高品質な合成動画データセットを構築し、VideoLightingDirection (VLD) データセットとして公開した。このデータセットは、光の方向のアノテーションと多様な外見の物体を含み、VidCRAFT3が強い光の伝達と反射の効果を有効に対応できるようにする。また、複数の視覚的要素（カメラの移動、物体の動き、光の方向）を同時にアノテーションされたトレーニングデータの必要性を除去するために、3段階のトレーニング戦略を提案した。ベンチマークデータセット上での検証は、VidCRAFT3が高品質な動画内容を生成でき、制御の粒度と視覚的な一貫性において現在の最先端の手法を超えることを示した。すべてのコードとデータは公開的に利用可能である。プロジェクトページ：https://sixiaozheng.github.io/VidCRAFT3/。",
      "upvotes": 2,
      "discussionId": "67ac21b2aa680a0f8782d3bd"
    },
    "publishedAt": "2025-02-11T23:21:13.452Z",
    "title": "VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07531.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.07776",
      "authors": [
        {
          "_id": "67ac1f7851c7f3b53ffc4def",
          "name": "Chenchen Gu",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df0",
          "name": "Xiang Lisa Li",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df1",
          "name": "Rohith Kuditipudi",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df2",
          "name": "Percy Liang",
          "hidden": false
        },
        {
          "_id": "67ac1f7851c7f3b53ffc4df3",
          "user": {
            "_id": "661595d1b3d0b21da55cde7d",
            "avatarUrl": "/avatars/ba3fa065536518637d21a5c46cee5dd1.svg",
            "isPro": false,
            "fullname": "Tatsu Hashimoto",
            "user": "thashim",
            "type": "user"
          },
          "name": "Tatsunori Hashimoto",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-12T04:11:36.912Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-11T18:58:04.000Z",
      "title": "語言モデルAPIのプロンプトキャッシュ審査",
      "summary": "プロンプトキャッシュイングが大規模な言語モデル（LLMs）では、データ依存性の時間変化を引き起こす：キャッシュされたプロンプトは非キャッシュされたプロンプトよりも高速に処理される。これらの時間違いはサイドチャネルタイミング攻撃のリスクを引き起こす。例えば、キャッシュがユーザ間で共有されている場合、攻撃者は高速なAPIレスポンスタイムからキャッシュされたプロンプトを特定し、他のユーザのプロンプトについての情報を学ぶことができる。プロンプトキャッシュイングがプライベート漏洩を原因とする可能性があるため、API提供元のキャッシュポリシーに関する透明性が重要である。このため、私たちは実世界的なLLM API提供元でのプロンプトキャッシュイングの検出を目的として、統計的なアウディトを開発し、実施した。私たちは7つのAPI提供元でユーザ間のグローバルキャッシュ共有を検出し、ユーザのプロンプトについての潜在的なプライベート漏洩を確認した。プロンプトキャッシュイングによる時間変化は、モデルアーキテクチャについての情報漏洩も引き起こす。特に、OpenAIの埋め込みモデルが以前に公開されていなかったデコーダーだけのTransformerであることを証拠した。",
      "upvotes": 2,
      "discussionId": "67ac1f7851c7f3b53ffc4e1b"
    },
    "publishedAt": "2025-02-11T23:11:49.993Z",
    "title": "Auditing Prompt Caching in Language Model APIs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.07776.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6042
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.05932",
      "authors": [
        {
          "_id": "67ac4356401012b81050022a",
          "user": {
            "_id": "67ac430c4ab9207cc227d23f",
            "avatarUrl": "/avatars/59c499cc191e28a66ae917963c28ffb3.svg",
            "isPro": false,
            "fullname": "Tenglong Liu",
            "user": "LTL07",
            "type": "user"
          },
          "name": "Tenglong Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:15:09.627Z",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022b",
          "name": "Jianxiong Li",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022c",
          "name": "Yinan Zheng",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022d",
          "name": "Haoyi Niu",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022e",
          "name": "Yixing Lan",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b81050022f",
          "name": "Xin Xu",
          "hidden": false
        },
        {
          "_id": "67ac4356401012b810500230",
          "name": "Xianyuan Zhan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-09T15:22:38.000Z",
      "title": "パラメータスペースでのスキル拡大と組み合わせ",
      "summary": "人間は、先に知っているキャパビリティを再利用して新しい課題を解決し、問題を解決する過程でスキルを開発することが得意です。このパラダイムは、自動転移エージェントの開発において、新しい課題に対して自動的に進化するシステムを開発することによって、このように広く採用されています。しかし、以前の方法は、新しいスキルを拡張する際にトレーニングの効率が限られ、先に知っているキャパビリティを完全に活用して新しいタスクの学習を支援することができませんでした。本論文では、パラメータースキル拡張と組み合わせ（PSEC）という新しいフレームワークを提案します。これは、スキルライブラリを管理可能にして、新しい課題を効率的に解決し、アガントの能力を進化させることを目的として設計されています。このライブラリは、パラメーター効率的な微調節により、スキルの基本要素を進歩的にポートフォリョンとして組み込むことができます。この構造は、LoRAモジュールを組み合わせて異なるスキルを直接的に組み合わせることを可能にし、スキル間の共有情報を活用して新しいスキルを効果的にプログラムすることを可能にします。これに基づき、コンテキストに関連付けられたモジュールを提案し、新しいタスクを共同で処理するために、動的に異なるスキルを活性化することを可能にします。多目的の組み合わせ、動的シフト、および継続的なポリシーシフトなどの多様なアプリケーションを支援することで、D4RL、DSRLベンチマーク、およびDeepMind Control Suiteでの結果から、PSECは先に知っているキャパビリティを効率的に活用して新しい課題を解決する能力を示し、スキルライブラリを拡張して能力を進化させることができることが示されます。プロジェクトウェブサイト：https://ltlhuuu.github.io/PSEC/",
      "upvotes": 0,
      "discussionId": "67ac435b401012b8105003dc"
    },
    "publishedAt": "2025-02-12T04:53:50.325Z",
    "title": "Skill Expansion and Composition in Parameter Space",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05932.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ac430c4ab9207cc227d23f",
      "avatarUrl": "/avatars/59c499cc191e28a66ae917963c28ffb3.svg",
      "fullname": "Tenglong Liu",
      "name": "LTL07",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]