[
  {
    "paper": {
      "id": "2506.18095",
      "authors": [
        {
          "_id": "685a0ac20e4ad7e2197584ea",
          "name": "Junying Chen",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584eb",
          "name": "Zhenyang Cai",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ec",
          "user": {
            "_id": "675130185d873b8ed24d964a",
            "avatarUrl": "/avatars/30ee8ce21f95423db8ced7db4df3112b.svg",
            "isPro": false,
            "fullname": "Pengcheng Chen",
            "user": "cppppppc",
            "type": "user"
          },
          "name": "Pengcheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:33:07.093Z",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ed",
          "name": "Shunian Chen",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ee",
          "name": "Ke Ji",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584ef",
          "name": "Xidong Wang",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584f0",
          "name": "Yunjin Yang",
          "hidden": false
        },
        {
          "_id": "685a0ac20e4ad7e2197584f1",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-22T16:51:09.000Z",
      "submittedOnDailyAt": "2025-06-26T02:58:42.859Z",
      "title": "ShareGPT-4o-Image: GPT-4oレベルの画像生成による多モデルのアラインメント",
      "submittedOnDailyBy": {
        "_id": "64097dd1b6a334f53e2b3e4c",
        "avatarUrl": "/avatars/18d036aab5e096054a8706bc78027126.svg",
        "isPro": false,
        "fullname": "Junying Chen",
        "user": "jymcc",
        "type": "user"
      },
      "summary": "最近の多モデル生成モデルの進歩は、写実的な、指示に対応した画像生成を可能にしましたが、GPT-4o-Imageなどのリーディングシステムは、プロプティアリティとアクセス不可になっています。これらの能力を民主化するために、ShareGPT-4o-Imageを紹介します。これは、GPT-4oの画像生成能力を使用して合成された45Kの文から画像へ、46Kの文と画像から画像へのデータの最初のデータセットです。このデータセットを活用して、Janus-4oを開発しました。これは、文から画像へ、文と画像から画像への両方の生成を可能にする多モデル大語言モデルです。Janus-4oは、前作のJanus-Proよりも文から画像への生成を大幅に向上させ、新たに文と画像から画像への生成を支援します。特に、文と画像から画像への生成のスクラッチでの優秀な性能を達成し、8A800-GPUマシンで91Kの合成サンプルと6時間の訓練を通じて実現しました。ShareGPT-4o-ImageとJanus-4oの公開により、写実的な、指示に対応した画像生成の開放的な研究を促進したいと思います。",
      "upvotes": 43,
      "discussionId": "685a0ac30e4ad7e2197584f2",
      "githubRepo": "https://github.com/FreedomIntelligence/ShareGPT-4o-Image",
      "ai_summary": "ShareGPT-4o-Image and Janus-4o enable open research in photorealistic, instruction-aligned image generation through a large dataset and multimodal model.",
      "ai_keywords": [
        "multimodal generative models",
        "text-to-image",
        "text-and-image-to-image",
        "photorealistic",
        "instruction-aligned",
        "dataset",
        "large language model",
        "synthetic samples"
      ],
      "githubStars": 63
    },
    "publishedAt": "2025-06-22T12:51:09.000Z",
    "title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image\n  Generation",
    "summary": "Recent advances in multimodal generative models have unlocked photorealistic,\ninstruction-aligned image generation, yet leading systems like GPT-4o-Image\nremain proprietary and inaccessible. To democratize these capabilities, we\npresent ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and\n46K text-and-image-to-image data, all synthesized using GPT-4o's image\ngeneration capabilities for distilling its advanced image generation abilities.\nLeveraging this dataset, we develop Janus-4o, a multimodal large language model\ncapable of both text-to-image and text-and-image-to-image generation. Janus-4o\nnot only significantly improves text-to-image generation over its predecessor,\nJanus-Pro, but also newly supports text-and-image-to-image generation. Notably,\nit achieves impressive performance in text-and-image-to-image generation from\nscratch, using only 91K synthetic samples and 6 hours of training on an 8\nA800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will\nfoster open research in photorealistic, instruction-aligned image generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18095.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64097dd1b6a334f53e2b3e4c",
      "avatarUrl": "/avatars/18d036aab5e096054a8706bc78027126.svg",
      "fullname": "Junying Chen",
      "name": "jymcc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19697",
      "authors": [
        {
          "_id": "685c1546df8a0d6c70bbf94e",
          "user": {
            "_id": "60f8435644e75317cc02ed51",
            "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
            "isPro": false,
            "fullname": "Jungwoo Park",
            "user": "affjljoo3581",
            "type": "user"
          },
          "name": "Jungwoo Park",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:32:38.469Z",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf94f",
          "name": "Taewhoo Lee",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf950",
          "name": "Chanwoong Yoon",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf951",
          "name": "Hyeon Hwang",
          "hidden": false
        },
        {
          "_id": "685c1546df8a0d6c70bbf952",
          "name": "Jaewoo Kang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T15:03:57.000Z",
      "submittedOnDailyAt": "2025-06-26T02:17:13.263Z",
      "title": "アウトライアーサフェースの予ち練習を用いた大規模な言語モデルの強固な4ビット定数化",
      "submittedOnDailyBy": {
        "_id": "60f8435644e75317cc02ed51",
        "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
        "isPro": false,
        "fullname": "Jungwoo Park",
        "user": "affjljoo3581",
        "type": "user"
      },
      "summary": "極端活性化のアウトライアーは、大規模言語モデル（LLMs）の量化性能を大きく低下させ、機器上での効率的な機能化を妨げています。チャネルごとの操作と適応的勾配スケーリングは、原因として認識されていますが、実用的な対策は難しいです。私たちは、アウトライアー形成を主動的に防ぐための実用的なガイドラインを紹介します。OSP（Outlier-Safe Pre-Training）は、3つのキーイノベーションを組み合わせて構成されています。1. モウンナーガイダンサー（Muon optimizer）は、トレーニングの効率を維持する同時に特権的な基底を削除します。2. Single-Scale RMSNormは、チャネルごとのアプローチを防ぐことを目的としています。3. 学習可能な埋め込みプロジェクションは、埋め込み行列からの活性化マグニチュードを再配分します。OSPの効果を証明するために、1万億トークンで14億パラメータのモデルをトレーニングしました。4ビットの激しい量化を行うと、OSPモデルは10ベンチマークの平均スコアが35.7となり、Adamモデルの26.5を上回ります。また、トレーニングオーバーヘッドは2%だけです。特に、OSPモデルは極端な値（1818.56）と比較して、近似したゼロエクススカートカスチム（0.04）を示します。極端活性化のアウトライアーは、LLMsに固有ではなく、トレーニング戦略の後果であることを示し、機器上での効率的なLLM機能化のための道が開かれました。ソースコードと学習済みチェックポイントは、https://github.com/dmis-lab/Outlier-Safe-Pre-Training から利用できます。",
      "upvotes": 29,
      "discussionId": "685c1546df8a0d6c70bbf953",
      "githubRepo": "https://github.com/dmis-lab/Outlier-Safe-Pre-Training",
      "ai_summary": "Outlier-Safe Pre-Training improves large language model quantization performance by preventing extreme activation outliers through innovative training techniques.",
      "ai_keywords": [
        "Muon optimizer",
        "Single-Scale RMSNorm",
        "learnable embedding projection",
        "outlier formation",
        "quantization performance",
        "LLM deployment",
        "excess kurtosis"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-06-24T11:03:57.000Z",
    "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models",
    "summary": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19697.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f8435644e75317cc02ed51",
      "avatarUrl": "/avatars/68b7fc077fe2bda6607b1c470add8140.svg",
      "fullname": "Jungwoo Park",
      "name": "affjljoo3581",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.16012",
      "authors": [
        {
          "_id": "685cf7c0696820ba1f28f2ea",
          "name": "Boyu Li",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2eb",
          "name": "Siyuan He",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ec",
          "name": "Hang Xu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ed",
          "name": "Haoqi Yuan",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ee",
          "name": "Yu Zang",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2ef",
          "name": "Liwei Hu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f0",
          "name": "Junpeng Yue",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f1",
          "name": "Zhenxiong Jiang",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f2",
          "name": "Pengbo Hu",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f3",
          "user": {
            "_id": "61e52be53d6dbb1da842316a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
            "isPro": false,
            "fullname": "Börje Karlsson",
            "user": "tellarin",
            "type": "user"
          },
          "name": "Börje F. Karlsson",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:24.327Z",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f4",
          "user": {
            "_id": "63e5e3807f9730f523655c5d",
            "avatarUrl": "/avatars/3ded710049790d025e862861039d9df2.svg",
            "isPro": false,
            "fullname": "YehuiTang",
            "user": "WizardTY",
            "type": "user"
          },
          "name": "Yehui Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:21.125Z",
          "hidden": false
        },
        {
          "_id": "685cf7c0696820ba1f28f2f5",
          "name": "Zongqing Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T04:13:36.000Z",
      "submittedOnDailyAt": "2025-06-26T06:05:04.111Z",
      "title": "DualTHOR: コンテキストに関心のある計画のためのダブルアームヒューマノイドシミュレーションプラットフォーム",
      "submittedOnDailyBy": {
        "_id": "61e52be53d6dbb1da842316a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
        "isPro": false,
        "fullname": "Börje Karlsson",
        "user": "tellarin",
        "type": "user"
      },
      "summary": "連携AIモデルが実世界のスケーナリオで複雑な相互作用タスクを行う能力を持つエボイドエージェントの開発は、連携AIにおける基本的な課題です。連携ビジョン言語モデル（VLM）の訓練において、最近のシミュレーションプラットフォームの進歩はタスクの多様性を大幅に高めましたが、ほとんどのプラットフォームは簡略化されたロボットのモーフィと低レベル実行の確率的性質を回避し、実世界のロボットへの伝達性を制限しています。これらの問題に対処するために、エボイドエージェントの複雑な双腕ヒューマノイドロボットに基づく物理ベースのシミュレーションプラットフォームDualTHORを紹介します。このシミュレータは、実世界のロボットアセット、双腕協力のタスクシート、ヒューマノイドロボット向けの逆連鎖ソルバーを含み、物理ベースの低レベル実行による潜在的な失敗を挟むコンテンジデンシー機構を導入しています。このシミュレータは、VLMの強固性と一般化能力の更なる評価を可能にします。拡散的な評価により、現在のVLMは双腕の協調性に苦戦し、コンテンジデンスを含む実写的な環境での強固性が限られていることが明らかになり、我々のシミュレータを用いて、連携タスクにより能力の高めたVLMの開発の重要性を強調します。コードは、https://github.com/ds199895/DualTHOR.gitにアクセスできます。",
      "upvotes": 16,
      "discussionId": "685cf7c1696820ba1f28f2f6",
      "ai_summary": "A simulator named DualTHOR for training dual-arm humanoid robots integrates real-world assets and physics to enhance the robustness and generalization of Vision Language Models.",
      "ai_keywords": [
        "embodied AI",
        "Vision Language Models",
        "VLMs",
        "physics-based simulation",
        "DualTHOR",
        "AI2-THOR",
        "dual-arm robots",
        "real-world robot assets",
        "task suite",
        "inverse kinematics solvers",
        "contingency mechanism",
        "physics-based low-level execution"
      ]
    },
    "publishedAt": "2025-06-19T00:13:36.000Z",
    "title": "DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware\n  Planning",
    "summary": "Developing embodied agents capable of performing complex interactive tasks in\nreal-world scenarios remains a fundamental challenge in embodied AI. Although\nrecent advances in simulation platforms have greatly enhanced task diversity to\ntrain embodied Vision Language Models (VLMs), most platforms rely on simplified\nrobot morphologies and bypass the stochastic nature of low-level execution,\nwhich limits their transferability to real-world robots. To address these\nissues, we present a physics-based simulation platform DualTHOR for complex\ndual-arm humanoid robots, built upon an extended version of AI2-THOR. Our\nsimulator includes real-world robot assets, a task suite for dual-arm\ncollaboration, and inverse kinematics solvers for humanoid robots. We also\nintroduce a contingency mechanism that incorporates potential failures through\nphysics-based low-level execution, bridging the gap to real-world scenarios.\nOur simulator enables a more comprehensive evaluation of the robustness and\ngeneralization of VLMs in household environments. Extensive evaluations reveal\nthat current VLMs struggle with dual-arm coordination and exhibit limited\nrobustness in realistic environments with contingencies, highlighting the\nimportance of using our simulator to develop more capable VLMs for embodied\ntasks. The code is available at https://github.com/ds199895/DualTHOR.git.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e52be53d6dbb1da842316a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg",
      "fullname": "Börje Karlsson",
      "name": "tellarin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 26
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18315",
      "authors": [
        {
          "_id": "685cb786696820ba1f28f286",
          "name": "Lehan He",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f287",
          "user": {
            "_id": "66c44c6826efa38bc783b07a",
            "avatarUrl": "/avatars/7a09179a2c91adc97f8db851fca37eea.svg",
            "isPro": false,
            "fullname": "Zeren Chen",
            "user": "zx55",
            "type": "user"
          },
          "name": "Zeren Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:31.152Z",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f288",
          "name": "Zhe Zhang",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f289",
          "name": "Jing Shao",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f28a",
          "name": "Xiang Gao",
          "hidden": false
        },
        {
          "_id": "685cb786696820ba1f28f28b",
          "user": {
            "_id": "65b722dbe02a17f0f8d1cc6b",
            "avatarUrl": "/avatars/65f20601ef9b8ebfdddadd737f9153d6.svg",
            "isPro": false,
            "fullname": "Lu Sheng",
            "user": "lsheng2024",
            "type": "user"
          },
          "name": "Lu Sheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:29.105Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T06:01:12.000Z",
      "submittedOnDailyAt": "2025-06-26T06:27:20.139Z",
      "title": "プロパティベーステストを用いて、LLMコード生成と検証を結びつける",
      "submittedOnDailyBy": {
        "_id": "64a96a375a69e2ca889abdff",
        "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
        "isPro": false,
        "fullname": "fanhongxing",
        "user": "fanhongxing",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）はコード生成に優れていますが、特に複雑なプログラミングタスクでは、出力を機能的に正しく確認することが長期的な課題です。テスト駆動開発（TDD）はコードの精練のパスを提供しますが、LLMsに対しては、高品質のテストケースの不足や自動テスト生成の課題により、効果が低下します。テストのバイアスや不正確な出力予測が修正プロセスを間違えることがあります。本論文では、プロパティベーステスト（PBT）を利用した新しいフレームワーク、Property-Generated Solverを紹介します。このフレームワークは、具体的な入力出力例に基づくテストを依存しないために、高レベルのプログラムのプロパティやイベントを確認することで、コードの正しさを確認します。これらのプロパティは、直接に完全なテストオーラクルを予測することではなく、定義および確認が簡単であり、テストとコードの間の「自虐の循環」を破壊します。Property-Generated Solverは、コード生成および反復的な精練を専門とするGeneratorと、PBTのライフサイクル管理およびプロパティの違反から豊富な語意的なフィードバックを提供するTesterの2つのLLMベースのアガントを使用しています。これらのアガントは、従来のTDD方法よりも大幅にパス@1の改善を実現し、23.1%から37.3%の相対的な効果を示します。このフレームワークは、プロパティベーステストをコアの検証エンジンとして、LLMsを正しく一般化可能なコードに向けるための強力的な機構を提供します。複数のコード生成ベンチマークにおける拡散的な実験結果は、Property-Generated Solverが効果的なフィードバックを提供し、Generatorの精練エフフェクトを導くことを示します。",
      "upvotes": 8,
      "discussionId": "685cb786696820ba1f28f28c",
      "githubRepo": "https://github.com/HeLeHanPrivate/PBTwithCodeGen",
      "ai_summary": "A novel framework using Property-Based Testing and collaborative LLM-based agents improves code generation correctness and generalization.",
      "ai_keywords": [
        "Large Language Models",
        "code generation",
        "Test-Driven Development",
        "Property-Based Testing",
        "PBT",
        "iterative refinement",
        "property violations",
        "pass@1 improvements"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-06-23T02:01:12.000Z",
    "title": "Use Property-Based Testing to Bridge LLM Code Generation and Validation",
    "summary": "Large Language Models (LLMs) excel at code generation, but ensuring their\noutputs to be functionally correct, especially in complex programming tasks, is\na persistent challenge. While traditional Test-Driven Development (TDD) offers\na path for code refinement, its efficacy with LLMs is often undermined by the\nscarcity of high-quality test cases or the pitfalls of automated test\ngeneration, including biased tests or inaccurate output predictions that can\nmisdirect the correction process. This paper introduces Property-Generated\nSolver, a novel framework that leverages Property-Based Testing (PBT) to\nvalidate high-level program properties or invariants, instead of relying on\nspecific input-output examples. These properties are often simpler to define\nand verify than directly predicting exhaustive test oracles, breaking the\n\"cycle of self-deception\" where tests might share flaws with the code they are\nmeant to validate. Property-Generated Solver employs two collaborative\nLLM-based agents: a Generator dedicated to code generation and iterative\nrefinement, and a Tester that manages the PBT life-cycle and formulate\nsemantically rich feedback from property violations. The resulting\ncomprehensive and actionable feedback then guides the Generator in its\nrefinement efforts. By establishing PBT as the core validation engine within\nthis iterative, closed-loop paradigm, Property-Generated Solver provides a\nrobust mechanism for steering LLMs towards more correct and generalizable code.\nExtensive experimental results on multiple code generation benchmarks\ndemonstrate that Property-Generated Solver achieves substantial pass@1\nimprovements, ranging from 23.1% to 37.3% relative gains over established TDD\nmethods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a96a375a69e2ca889abdff",
      "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
      "fullname": "fanhongxing",
      "name": "fanhongxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.18088",
      "authors": [
        {
          "_id": "685ae8e8d2ee4fac76521d03",
          "user": {
            "_id": "65b37a9b06d8b55123ef8921",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
            "isPro": false,
            "fullname": "Tianxing Chen",
            "user": "TianxingChen",
            "type": "user"
          },
          "name": "Tianxing Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T18:13:30.491Z",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d04",
          "name": "Zanxin Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d05",
          "name": "Baijun Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d06",
          "name": "Zijian Cai",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d07",
          "name": "Yibin Liu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d08",
          "name": "Qiwei Liang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d09",
          "name": "Zixuan Li",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0a",
          "name": "Xianliang Lin",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0b",
          "name": "Yiheng Ge",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0c",
          "name": "Zhenyu Gu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0d",
          "name": "Weiliang Deng",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0e",
          "name": "Yubin Guo",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d0f",
          "name": "Tian Nian",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d10",
          "name": "Xuanbing Xie",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d11",
          "name": "Qiangyu Chen",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d12",
          "name": "Kailun Su",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d13",
          "name": "Tianling Xu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d14",
          "name": "Guodong Liu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d15",
          "name": "Mengkang Hu",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d16",
          "name": "Huan-ang Gao",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d17",
          "name": "Kaixuan Wang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d18",
          "name": "Zhixuan Liang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d19",
          "name": "Yusen Qin",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1a",
          "name": "Xiaokang Yang",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1b",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "685ae8e8d2ee4fac76521d1c",
          "name": "Yao Mu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-22T16:26:53.000Z",
      "submittedOnDailyAt": "2025-06-26T06:13:35.593Z",
      "title": "RoboTwin 2.0: スケーラブルなデータ生成器と強力なドメインランダミゼーションを採用したベンチマークを挟む強固な二手ロボット操作のデザイン",
      "submittedOnDailyBy": {
        "_id": "65b37a9b06d8b55123ef8921",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
        "isPro": false,
        "fullname": "Tianxing Chen",
        "user": "TianxingChen",
        "type": "user"
      },
      "summary": "シミュレーションベースデータ合成は、実世界のロボット操作を強化するためのパワーフルパラダイムとして登場しました。しかし、現在の合成データセットは、2つの挑戦により、強固な双手操作には十分ではありません: (1) 新しいタスクに対する効率的なスケーラブルなデータ生成方法の欠如と (2) 複雑な実世界の複雑性を捉えずに簡単化されたシミュレーション環境。私たちは、RoboTwin 2.0、一つのスケーラブルなシミュレーションフレームワークを紹介します。これは、自動化された、大規模な多様性と写実的なデータの生成を可能にし、ダブルアーム操作の統一評価プロトコルを提供します。まず、RoboTwin-OD、147カテゴリの731インスタンスを含む大規模なオブジェクトライブラリを構築します。各インスタンスは、語意的ラベルと操作関係のラベルで注釈されています。この基盤により、マルチモーダル大語言モデル (MLLMs) とシミュレーションインラウンドの精確化を組み合わせた専門的なデータ合成パイプラインを開発します。シミュレーションから実世界へのタンス移行を改善するために、RoboTwin 2.0 は、クレッジ、照明、背景、テーブルの高さ、言語指示の5つの軸における構造化されたドメインランダム化を採用し、データの多様性とポリシーの強固性を向上させます。このフレームワークは、50ペアアームタスクを範囲に、5つのロボットの体像を跨ぐ範囲で実装され、100,000以上のドメインランダム化エキスパートトラジェトリーを事前的に収集します。実験結果によると、コード生成の成功率は10.9%の増加を示し、新しい実世界のスケーナーに対する一般化が改善されました。データセットにおけるVLAモデルの微調節は、未見のスケーナーの実世界タスクに対して367%の相対的な改善 (42.0% 対 9.0%) を収め、実世界のスーパーバイザーを除いて合成データにだけのトレーニングで228%の相対的な収益を収め、実世界のスーパーバイザーを除いて強い一般化を示します。データジェネレーター、ベンチマーク、データセット、コードをリリースし、強固な双手操作のスケーラブルな研究にサポートします。",
      "upvotes": 7,
      "discussionId": "685ae8e8d2ee4fac76521d1d",
      "projectPage": "https://robotwin-platform.github.io/",
      "githubRepo": "https://github.com/robotwin-Platform/RoboTwin",
      "ai_summary": "RoboTwin 2.0 is a scalable simulation framework for bimanual robotic manipulation that uses expert data synthesis and structured domain randomization to generate diverse and realistic synthetic data, improving sim-to-real transfer and generalization.",
      "ai_keywords": [
        "multimodal large language models",
        "simulation-in-the-loop",
        "structured domain randomization",
        "bimanual manipulation",
        "task-level execution code",
        "sim-to-real transfer",
        "zero-shot learning"
      ],
      "githubStars": 1129
    },
    "publishedAt": "2025-06-22T12:26:53.000Z",
    "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain\n  Randomization for Robust Bimanual Robotic Manipulation",
    "summary": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b37a9b06d8b55123ef8921",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b37a9b06d8b55123ef8921/CT5tLwezjXct1eTszA8sO.jpeg",
      "fullname": "Tianxing Chen",
      "name": "TianxingChen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18674",
      "authors": [
        {
          "_id": "685cd8fc696820ba1f28f2aa",
          "name": "Raquel Ferrando",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ab",
          "name": "Javier Conde",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ac",
          "name": "Gonzalo Martínez",
          "hidden": false
        },
        {
          "_id": "685cd8fc696820ba1f28f2ad",
          "name": "Pedro Reviriego",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T14:18:46.000Z",
      "submittedOnDailyAt": "2025-06-26T03:52:31.234Z",
      "title": "ロボットテキストモデルは、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボットテキストモデルを使用して、ロボット",
      "submittedOnDailyBy": {
        "_id": "64f31365ed48e3bb9c487d5d",
        "avatarUrl": "/avatars/979c1979eadbd4529c95b925bbb58d78.svg",
        "isPro": false,
        "fullname": "Gonzalo",
        "user": "gonzmart",
        "type": "user"
      },
      "summary": "LLMの計算コストとエネルギーコストは、モデルサイズの増大と数百万人のユーザーの大規模な採用により指数的に上昇しています。LLMの単位コストはトークンの計算です。したがって、トークナイザはモデルの効率性に重要な役割を果たし、それらは学習コーパスのテキストのトークン数を最小限に抑えることで調整されています。LLMの最も人気のあるアプリケーションの一つは、ユーザーと相互作用するチャットボットです。その鍵の観察は、そのチャットボットでは、重要なのはユーザーテキストの入力とチャットボットの回答におけるトークナイザの性能です。それは学習コーパスのテキストとは異なります。そのため、トークナイザをチャットボットの対話に最適化することにどのような潜在的な利益があるかという問題がすぐに浮かびます。本論文では、チャットボットの対話のためのトークナイザの最適化の可能性を調べるために、公開的に利用可能なチャットボットの対話コーパスを使用して、ビコラフィリーを再設計し、その性能をこの領域で評価します。結果は、対話最適化されたトークナイザがチャットボットのダイアログでのトークン数を一貫的に減少させ、これは5%から10%の範囲で意味的なエネルギー削減を実現でき、元の学習コーパスのトークナイズエフフィシーンに最小限またはそれ以上の正の影響を与えることを示しています。",
      "upvotes": 5,
      "discussionId": "685cd8fc696820ba1f28f2ae",
      "ai_summary": "Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.",
      "ai_keywords": [
        "Large Language Models",
        "token",
        "tokenizer",
        "chatbots",
        "conversation-optimized tokenizers"
      ]
    },
    "publishedAt": "2025-06-23T10:18:46.000Z",
    "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?",
    "summary": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18674.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f31365ed48e3bb9c487d5d",
      "avatarUrl": "/avatars/979c1979eadbd4529c95b925bbb58d78.svg",
      "fullname": "Gonzalo",
      "name": "gonzmart",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20544",
      "authors": [
        {
          "_id": "685cdd71696820ba1f28f2b8",
          "user": {
            "_id": "677cfa6cac2db4c2265edb26",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Kbi96ndfY-CIuJNd2TRZt.jpeg",
            "isPro": false,
            "fullname": "Ammar Khairi",
            "user": "ammar-cohere",
            "type": "user"
          },
          "name": "Ammar Khairi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:26.999Z",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2b9",
          "name": "Daniel D'souza",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2ba",
          "name": "Ye Shen",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2bb",
          "name": "Julia Kreutzer",
          "hidden": false
        },
        {
          "_id": "685cdd71696820ba1f28f2bc",
          "name": "Sara Hooker",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T15:37:53.000Z",
      "submittedOnDailyAt": "2025-06-26T04:17:49.221Z",
      "title": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs",
      "submittedOnDailyBy": {
        "_id": "6544e43b12da508864c38f96",
        "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
        "isPro": false,
        "fullname": "Julia Kreutzer",
        "user": "JuliaKreutzerCohere",
        "type": "user"
      },
      "summary": "最近の大語言モデル（LLMs）の進展は、推論時の計算量を拡大し、モデルの再学習を避けて性能を向上させることに焦点を当てています。一般的なアプローチとしては、複数の出力を並行にサンプリングし、その中から最終的な出力を選択することがあります。しかし、現在の研究は、英語と数学やコードなどの少数の領域に焦点を当てています。対照的に、私たちは開放的なタスク、正式的に可確認できるタスク、そして言語間での一般化が可能な技術に興味を持っています。この研究では、多言語、多タスクの設定での開放的な生成タスクにおける推論時の計算量の強固な拡大について調査しています。\n\n私たちの見つけたことは、温度の変化に基づくサンプリング戦略と選択戦略は、多様な領域と多様な言語設定に対応する必要があることを示しています。既存の選択方法を評価し、英語で効果的である戦略が言語間で一般化できないことを明らかにしています。私たちは、多言語および多タスクの推論シナリオに特に適した新しいサンプリングと選択戦略を提案し、それらは言語およびタスクにおいて顕著な効果を示しています。特に、我々の組み合わせたサンプリングと選択方法は、m-ArenaHard-v2.0プロンプトで我々の8Bモデルの勝率に平均+6.8の上昇を示し、ゲミニなどのプロプライモデルに対してはこれらの効果があります。より大きなスケールでは、Command-A（111Bモデル）に我々の方法を採用したものは、同じベンチマークでの勝率に+9.0の改善を示し、単一サンプルの解確認に対しても5つのサンプルで大幅な向上を示し、最小限のコストでの効果的な増加です。我々の結果は、推論時の計算量における言語およびタスクに関心を持つアプローチの必要性を強調し、代表的な言語での性能向上を民主化することを目指しています。",
      "upvotes": 4,
      "discussionId": "685cdd71696820ba1f28f2bd",
      "ai_summary": "The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.",
      "ai_keywords": [
        "sampling strategy",
        "selection strategy",
        "temperature variation",
        "open-ended generative tasks",
        "multilingual",
        "multi-task",
        "m-ArenaHard-v2.0",
        "win-rates",
        "Command-A",
        "single-sample decoding",
        "language-aware",
        "task-aware"
      ]
    },
    "publishedAt": "2025-06-25T11:37:53.000Z",
    "title": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs",
    "summary": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20544.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6544e43b12da508864c38f96",
      "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
      "fullname": "Julia Kreutzer",
      "name": "JuliaKreutzerCohere",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20495",
      "authors": [
        {
          "_id": "685d0223696820ba1f28f322",
          "name": "Haoze Wu",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f323",
          "name": "Yunzhi Yao",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f324",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f325",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "685d0223696820ba1f28f326",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T14:41:13.000Z",
      "submittedOnDailyAt": "2025-06-26T06:48:06.720Z",
      "title": "ReCode: 強化学習を用いたコードAPIの知識更新",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "大型言語モデル（LLMs）は、コード生成能力を示しますが、外部ライブラリAPIの頻繁な更新に適応すると失敗します。この重要な制限は、訓練データからの過期のAPIキャンバス知識に基づいているため、現在のドキュメントも利用できることにより、動的な環境で信頼性のあるコード生成を妨げます。この問題に対処するために、私たちは、人間のプログラマーがAPIの変更に適応するように製作したルールベースの強化学習を用いた新しいフレームワークReCodeを提案します。特に、LLMsをコードのバージョン移行に対して更新情報に基づいて訓練するために、約2,000件のデータエントリーのデータセットを構築します。次に、強化学習の報酬としてコード評価の改良された文字列類似性メトリックを介します。実験により、ReCodeは動的なAPIスケーナーでLLMsのコード生成性能を大幅に向上させ、特に見つからないCodeUpdateArenaタスクに対してもその効果が明らかに見られます。重要なことに、観覧制御の微調節に比べ、ReCodeはLLMsの一般的なコード生成能力に影響を及ぼしません。ReCodeは様々なLLMsと強化学習アルゴリズム（GRPOとDAPO）に対して適用され、すべての場合で一致した改善を収めました。特に、訓練後、Qwen2.5-Coder-7Bは同じアーキテクチャのコードインストラクション調整モデルと理由論モデルを超えました。コードはhttps://github.com/zjunlp/ReCodeにアクセスできます。",
      "upvotes": 4,
      "discussionId": "685d0224696820ba1f28f327",
      "githubRepo": "https://github.com/zjunlp/ReCode",
      "ai_summary": "ReCode, a rule-based reinforcement learning framework, enhances large language models' adaptation to API updates without compromising their general code generation capabilities.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "code generation",
        "API updates",
        "dataset",
        "version migration",
        "string similarity metric",
        "reinforcement learning",
        "rule-based",
        "Qwen2.5-Coder-7B",
        "CodeUpdateArena",
        "GRPO",
        "DAPO"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-06-25T10:41:13.000Z",
    "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
    "summary": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20495.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 25
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.20452",
      "authors": [
        {
          "_id": "685cfd21696820ba1f28f30a",
          "name": "Tobias Vontobel",
          "hidden": false
        },
        {
          "_id": "685cfd21696820ba1f28f30b",
          "user": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "isPro": false,
            "fullname": "Seyedmorteza Sadat",
            "user": "msadat97",
            "type": "user"
          },
          "name": "Seyedmorteza Sadat",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:18.462Z",
          "hidden": false
        },
        {
          "_id": "685cfd21696820ba1f28f30c",
          "name": "Farnood Salehi",
          "hidden": false
        },
        {
          "_id": "685cfd21696820ba1f28f30d",
          "user": {
            "_id": "630f7646197cd3f24e7f8e9f",
            "avatarUrl": "/avatars/59bbd4ed38277b313051aac78f6808ac.svg",
            "isPro": false,
            "fullname": "Romann Weber",
            "user": "RMW",
            "type": "user"
          },
          "name": "Romann M. Weber",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:16.404Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T13:58:37.000Z",
      "submittedOnDailyAt": "2025-06-26T06:29:51.626Z",
      "title": "ヒワブ: ワーブレットベースディフュージョンサンプリングによるトレーニング無し高解像度画像生成",
      "submittedOnDailyBy": {
        "_id": "63b4b02a103617b0a5b0ee2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
        "isPro": false,
        "fullname": "Seyedmorteza Sadat",
        "user": "msadat97",
        "type": "user"
      },
      "summary": "ディフュージョンモデルは、画像合成の先進的なアプローチとして現れ、超卓な写実性と多様性を示しています。しかし、高解像度でのディフュージョンモデルの訓練は計算的に困難であり、現在のトライアウト生成技術は、訓練解像度より高い画像の合成において、オブジェクトの重複と空間的な不連続なほどのアーティファクトを含むことが多いです。本論文では、トレーニング無しで、零ショットアプローチとして、超高解像度画像合成での視覚的なフィデリティと構造的な一貫性を大幅に向上させるために、事前学習されたディフュージョンモデルを使用してHiWaveを介して実現する方法を介しています。我々の方法は、2段階パイプラインを使用しています：事前学習モデルからベース画像を生成し、パッチワイズDDIM逆転写真ステップと新しいワブレットベースの詳細拡大モジュールを続けます。特に、我々は最初に、逆転写真手法を使用して、ベース画像からグローバル的な一貫性を保持する初期ノイズベクトルを得ます。その後、サンプリングの際、我々のワブレット領域の詳細拡大モジュールは、ベース画像からの低周波成分を残し、構造的な一貫性を確保し、選択的に高周波成分をガイドして、詳細のデテールとテクスチャを増進させます。Stable Diffusion XLを使用した拡張評価により、HiWaveは、先行方法で見られる一般的な視覚的なアーティファクトを効果的に軽減し、上位視覚的な品質を達成します。ユーザーステージでは、HiWaveの性能が確認され、状態の最先端の代替と比較して80%以上の比較で優しく捨てられ、高品質の超高解像度画像合成の効果性を強調します。",
      "upvotes": 4,
      "discussionId": "685cfd22696820ba1f28f30e",
      "ai_summary": "HiWave enhances ultra-high-resolution image synthesis using pretrained diffusion models through a two-stage pipeline involving DDIM inversion and wavelet-based detail enhancement, improving visual fidelity and reducing artifacts.",
      "ai_keywords": [
        "diffusion models",
        "image synthesis",
        "photorealism",
        "high resolutions",
        "zero-shot generation",
        "artifacts",
        "object duplication",
        "spatial incoherence",
        "pretrained diffusion models",
        "two-stage pipeline",
        "DDIM inversion",
        "wavelet-based detail enhancer",
        "structural consistency",
        "fine details",
        "textures",
        "perceptual quality",
        "user study"
      ]
    },
    "publishedAt": "2025-06-25T09:58:37.000Z",
    "title": "HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based\n  Diffusion Sampling",
    "summary": "Diffusion models have emerged as the leading approach for image synthesis,\ndemonstrating exceptional photorealism and diversity. However, training\ndiffusion models at high resolutions remains computationally prohibitive, and\nexisting zero-shot generation techniques for synthesizing images beyond\ntraining resolutions often produce artifacts, including object duplication and\nspatial incoherence. In this paper, we introduce HiWave, a training-free,\nzero-shot approach that substantially enhances visual fidelity and structural\ncoherence in ultra-high-resolution image synthesis using pretrained diffusion\nmodels. Our method employs a two-stage pipeline: generating a base image from\nthe pretrained model followed by a patch-wise DDIM inversion step and a novel\nwavelet-based detail enhancer module. Specifically, we first utilize inversion\nmethods to derive initial noise vectors that preserve global coherence from the\nbase image. Subsequently, during sampling, our wavelet-domain detail enhancer\nretains low-frequency components from the base image to ensure structural\nconsistency, while selectively guiding high-frequency components to enrich fine\ndetails and textures. Extensive evaluations using Stable Diffusion XL\ndemonstrate that HiWave effectively mitigates common visual artifacts seen in\nprior methods, achieving superior perceptual quality. A user study confirmed\nHiWave's performance, where it was preferred over the state-of-the-art\nalternative in more than 80% of comparisons, highlighting its effectiveness for\nhigh-quality, ultra-high-resolution image synthesis without requiring\nretraining or architectural modifications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20452.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b4b02a103617b0a5b0ee2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
      "fullname": "Seyedmorteza Sadat",
      "name": "msadat97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18403",
      "authors": [
        {
          "_id": "685a555f0e4ad7e2197586b1",
          "user": {
            "_id": "65eef9ce7443c09267513796",
            "avatarUrl": "/avatars/62547f99130557f54093b2ff4d6c9c24.svg",
            "isPro": false,
            "fullname": "Muntasir Adnan",
            "user": "adnaan525",
            "type": "user"
          },
          "name": "Muntasir Adnan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:33:02.852Z",
          "hidden": false
        },
        {
          "_id": "685a555f0e4ad7e2197586b2",
          "name": "Carlos C. N. Kuhn",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T08:40:45.000Z",
      "submittedOnDailyAt": "2025-06-26T01:42:45.705Z",
      "title": "デバッグダウンゼットインデックス：コードLLMのデバッグ戦略を再考する",
      "submittedOnDailyBy": {
        "_id": "65eef9ce7443c09267513796",
        "avatarUrl": "/avatars/62547f99130557f54093b2ff4d6c9c24.svg",
        "isPro": false,
        "fullname": "Muntasir Adnan",
        "user": "adnaan525",
        "type": "user"
      },
      "summary": "AIデバッグの効果性は予測可能な指数関数的減衰パターンを認める；多くのモデルは2-3回の試行で60-80%のデバッグ能力を失い、複数回の試行を繰り返すことが実用的なコード生成システムの重要な能力であることに認められる。私たちは、デバッグが無効になるときを定量化し、介入点を予測するための数学的フレームワークであるデバッグデカインデックス（DDI）を紹介する。我々の戦略的な新たなスタートアプローチは、デバッグプロセスの戦略的な点で従来の採用に向けて探索に移り、その時間の適切な介入がデバッグの効果性を救出できることを示している。DDIは現在のAIデバッグの根本的な制限を明らかにし、最初の定量的なフレームワークとして、複数回の試行を繰り返すコード生成戦略を最適化するためのフレームワークを提供している。",
      "upvotes": 2,
      "discussionId": "685a555f0e4ad7e2197586b3",
      "ai_summary": "The Debugging Decay Index (DDI) quantifies and optimizes the effectiveness of iterative AI debugging by predicting intervention points to revive and enhance debugging capability.",
      "ai_keywords": [
        "AI debugging",
        "Debugging Decay Index (DDI)",
        "iterative debugging",
        "code generation",
        "effectiveness",
        "intervention points"
      ]
    },
    "publishedAt": "2025-06-23T04:40:45.000Z",
    "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs",
    "summary": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18403.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65eef9ce7443c09267513796",
      "avatarUrl": "/avatars/62547f99130557f54093b2ff4d6c9c24.svg",
      "fullname": "Muntasir Adnan",
      "name": "adnaan525",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.20512",
      "authors": [
        {
          "_id": "685cb8d7696820ba1f28f296",
          "name": "Zengzhi Wang",
          "hidden": false
        },
        {
          "_id": "685cb8d7696820ba1f28f297",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "685cb8d7696820ba1f28f298",
          "name": "Xuefeng Li",
          "hidden": false
        },
        {
          "_id": "685cb8d7696820ba1f28f299",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62cbeb2d72dfd24b86bdf977/cZoks9vqpXBnkpA9PgVRV.png"
      ],
      "publishedAt": "2025-06-25T14:58:13.000Z",
      "submittedOnDailyAt": "2025-06-26T07:21:37.577Z",
      "title": "オクトティハイナー：中間学習で強化学習のスケーリングを奨励する",
      "submittedOnDailyBy": {
        "_id": "62cbeb2d72dfd24b86bdf977",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62cbeb2d72dfd24b86bdf977/UcGYYSBNrCvPM5K9v-sro.png",
        "isPro": false,
        "fullname": "Zengzhi Wang",
        "user": "SinclairWang",
        "type": "user"
      },
      "summary": "異なる基底言語モデルの家族、例えばLlamaとQwenは、強化学習（RL）の後編修期間で相異なる行動を示す。基底言語モデルが強化学習に適していることは何か。この問題に深い理解を得ることは次世代のRLスケーラブルな基盤モデルの開発に重要である。本研究では、中盤学習戦略がRLのダイナミクスをどのように影響しているかを調査し、代表的な2つのモデルの家族、QwenとLlamaを焦点にしています。本研究は、(1)高品質の数学コーパス、MegaMath-Web-Proなどが基底モデルとRLの両方の性能を大幅に向上させ、現在の代替コーパス（例えばFineMath-4plus）がこれを達成しないことを示しています；(2)QAスタイルのデータを追加し、特に長いChain-of-Thought（CoT）推理例を含むデータを追加することがRLの結果を向上させ、指示データがこの効果を引き出すことを示しています；(3)長いCoTが推理の深さを向上させるが、同時にモデルのレスポンスの長さとRLトレーニングの不穩定さを招き、データのフォーマットの重要性を強調しています；(4)中盤学習のスケーリングは、次々と強力な下流のRL性能を示すことを示しています。これらの洞察に基づき、2ステップの中盤学習戦略、Stable-then-Decayを導入し、基底モデルは200Bトークンで定数学習率で学習され、その後、3つのCoTフォーカスのブランチで20Bトークンを学習させ、学習率の衰減を行います。これにより、OctoThinkerモデルの家族が強力なRLの相容性を示し、よりRLに適したモデルの家族との性能の間隔を閉じることができます。本研究は、RL時代の基盤モデルの予ち習得戦略を形成することを望んでいます。さらなる研究のために、本研究では、オープンソースモデルと700億トークン以上の数理論解力を持つコーパス（MegaMath-Web-Pro-Max）を公開しています。",
      "upvotes": 1,
      "discussionId": "685cb8d7696820ba1f28f29a",
      "githubRepo": "https://github.com/GAIR-NLP/OctoThinker",
      "ai_summary": "Investigating mid-training strategies reveals that high-quality mathematical corpora and well-formatted chain-of-thought reasoning examples enhance reinforcement learning performance in language models, leading to the development of OctoThinker.",
      "ai_keywords": [
        "reinforcement learning",
        "base language model",
        "mid-training strategy",
        "MegaMath-Web-Pro",
        "QA-style data",
        "chain-of-thought (CoT) reasoning",
        "data formatting",
        "learning rate decay",
        "OctoThinker",
        "MegaMath-Web-Pro-Max"
      ],
      "githubStars": 66
    },
    "publishedAt": "2025-06-25T10:58:13.000Z",
    "title": "OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling",
    "summary": "Different base language model families, such as Llama and Qwen, exhibit\ndivergent behaviors during post-training with reinforcement learning (RL),\nespecially on reasoning-intensive tasks. What makes a base language model\nsuitable for reinforcement learning? Gaining deeper insight into this question\nis essential for developing RL-scalable foundation models of the next\ngeneration. In this work, we investigate how mid-training strategies shape RL\ndynamics, focusing on two representative model families: Qwen and Llama. Our\nstudy reveals that (1) high-quality mathematical corpora, such as\nMegaMath-Web-Pro, significantly improve both base model and RL performance,\nwhile existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further\nadding QA-style data, particularly long chain-of-thought (CoT) reasoning\nexamples, enhances RL outcomes, and instruction data further unlocks this\neffect; (3) while long-CoT improves reasoning depth, it can also induce\nverbosity of model responses and unstability of RL training, underscoring the\nimportance of data formatting; (4) scaling mid-training consistently leads to\nstronger downstream RL performance. Building on these insights, we introduce a\ntwo-stage mid-training strategy, Stable-then-Decay, in which base models are\nfirst trained on 200B tokens with a constant learning rate, followed by 20B\ntokens across three CoT-focused branches with learning rate decay. This yields\nOctoThinker, a family of models demonstrating strong RL compatibility and\nclosing the performance gap with more RL-friendly model families, i.e., Qwen.\nWe hope our work will help shape pre-training strategies for foundation models\nin the RL era. To support further research, we release our open-source models\nalong with a curated math reasoning-intensive corpus of over 70 billion tokens\n(i.e., MegaMath-Web-Pro-Max).",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62cbeb2d72dfd24b86bdf977/cZoks9vqpXBnkpA9PgVRV.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20512.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62cbeb2d72dfd24b86bdf977",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62cbeb2d72dfd24b86bdf977/UcGYYSBNrCvPM5K9v-sro.png",
      "fullname": "Zengzhi Wang",
      "name": "SinclairWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19502",
      "authors": [
        {
          "_id": "685c02aadf8a0d6c70bbf918",
          "user": {
            "_id": "674ed490fc7f50ef61c3a7bd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
            "isPro": false,
            "fullname": "Aleksandr Algazinov",
            "user": "AleksandrAlgazinov",
            "type": "user"
          },
          "name": "Aleksandr Algazinov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T20:59:09.301Z",
          "hidden": false
        },
        {
          "_id": "685c02aadf8a0d6c70bbf919",
          "name": "Matt Laing",
          "hidden": false
        },
        {
          "_id": "685c02aadf8a0d6c70bbf91a",
          "name": "Paul Laban",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T10:40:23.000Z",
      "submittedOnDailyAt": "2025-06-26T01:26:07.991Z",
      "title": "MATE: モバイルロボットエンジニアリングターミナルプログラミングエンヴィロンフォーアクセシビリティアプリケーション",
      "submittedOnDailyBy": {
        "_id": "674ed490fc7f50ef61c3a7bd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
        "isPro": false,
        "fullname": "Aleksandr Algazinov",
        "user": "AleksandrAlgazinov",
        "type": "user"
      },
      "summary": "アクセシビリティは現代社会では重要な課題であり、多くのテクノロジーは完全なユーザーの需要を満たしていないためである。現存する多エージェントシステム（MAS）は、クローズドソースデザインによるカスタマイズ不足により、必要なユーザーに完全な助けを提供できないことが多い。その結果、機能障害者は、デジタル環境とのインターフェースを取り掛ける際に大きな壁を見たことがある。私たちは、ユーザーの需要に基づくモディバル変換を行うための多モディアルアクセシビリティMASを紹介します。このシステムは、データを理解可能なフォーマットに変換し、機能障害者を支援することができます。例えば、ユーザーが視力が低い場合、システムは画像を音声説明に変換します。MATEは、健康ケアなどの幅広い領域にも適用可能で、多くのユーザーグループに役立つ助手となることができます。このシステムは、LLM APIの呼び出しからカスタムマシン学習（ML）チャラクターズの使用まで様々なモデルをサポートしています。この柔軟性は、システムが様々な需要に応えられ、様々なハードウェアとの相容性を保つことを確保しています。システムは、ローカルで実行されることを想定しており、重要な情報のプライバシーとセキュリティを保証します。また、フレームワークは、機関のテクノロジーと（例えば、デジタルハードケアサービス）に効果的に統合でき、リアルタイムユーザーアシスタンスを提供することができます。また、ModCon-Task-Identifierを紹介します。このモデルは、ユーザーの入力から正確なモディバル変換タスクを抽出することができるものです。複数の実験は、ModCon-Task-Identifierが我々のカスタムデータに対して他のLLMsや統計モデルよりも一貫して優れていることを示しています。私たちのコードおよびデータは、https://github.com/AlgazinovAleksandr/Multi-Agent-MATE で公開しています。",
      "upvotes": 1,
      "discussionId": "685c02abdf8a0d6c70bbf91b",
      "ai_summary": "MATE, a multimodal accessibility multi-agent system, converts data into understandable formats based on user needs, supporting various disabilities and integrating with institutional technologies.",
      "ai_keywords": [
        "MULTIAGENT SYSTEMS",
        "MAS",
        "MODALITY CONVERSIONS",
        "LLM API",
        "CUSTOM MACHINE LEARNING CLASSIFIERS",
        "MODCON-TASK-IDENTIFIER",
        "LLM",
        "STATISTICAL MODELS"
      ]
    },
    "publishedAt": "2025-06-24T06:40:23.000Z",
    "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications",
    "summary": "Accessibility remains a critical concern in today's society, as many\ntechnologies are not developed to support the full range of user needs.\nExisting multi-agent systems (MAS) often cannot provide comprehensive\nassistance for users in need due to the lack of customization stemming from\nclosed-source designs. Consequently, individuals with disabilities frequently\nencounter significant barriers when attempting to interact with digital\nenvironments. We introduce MATE, a multimodal accessibility MAS, which performs\nthe modality conversions based on the user's needs. The system is useful for\nassisting people with disabilities by ensuring that data will be converted to\nan understandable format. For instance, if the user cannot see well and\nreceives an image, the system converts this image to its audio description.\nMATE can be applied to a wide range of domains, industries, and areas, such as\nhealthcare, and can become a useful assistant for various groups of users. The\nsystem supports multiple types of models, ranging from LLM API calling to using\ncustom machine learning (ML) classifiers. This flexibility ensures that the\nsystem can be adapted to various needs and is compatible with a wide variety of\nhardware. Since the system is expected to run locally, it ensures the privacy\nand security of sensitive information. In addition, the framework can be\neffectively integrated with institutional technologies (e.g., digital\nhealthcare service) for real-time user assistance. Furthermore, we introduce\nModCon-Task-Identifier, a model that is capable of extracting the precise\nmodality conversion task from the user input. Numerous experiments show that\nModCon-Task-Identifier consistently outperforms other LLMs and statistical\nmodels on our custom data. Our code and data are publicly available at\nhttps://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674ed490fc7f50ef61c3a7bd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674ed490fc7f50ef61c3a7bd/j260ICQmNl42aXehBEd6P.jpeg",
      "fullname": "Aleksandr Algazinov",
      "name": "AleksandrAlgazinov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.20331",
      "authors": [
        {
          "_id": "685d0b5d696820ba1f28f349",
          "user": {
            "_id": "62a9b0acf6708cb85014f9dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9b0acf6708cb85014f9dc/Sem1qcBt1lJjFEPK-xz4_.jpeg",
            "isPro": false,
            "fullname": "Rian Touchent",
            "user": "rntc",
            "type": "user"
          },
          "name": "Rian Touchent",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-26T09:23:08.325Z",
          "hidden": false
        },
        {
          "_id": "685d0b5d696820ba1f28f34a",
          "name": "Nathan Godey",
          "hidden": false
        },
        {
          "_id": "685d0b5d696820ba1f28f34b",
          "name": "Eric de la Clergerie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-25T11:30:25.000Z",
      "submittedOnDailyAt": "2025-06-26T07:44:08.382Z",
      "title": "バイオメディ・エンリッチド：LLMを用いて予習と稀少や隠れた内容を抽出するためのバイオメディデータセットの拡充",
      "submittedOnDailyBy": {
        "_id": "62a9b0acf6708cb85014f9dc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9b0acf6708cb85014f9dc/Sem1qcBt1lJjFEPK-xz4_.jpeg",
        "isPro": false,
        "fullname": "Rian Touchent",
        "user": "rntc",
        "type": "user"
      },
      "summary": "ビオメド・エニッチュード、PubMedから構築されたバイオメディカルテキストデータセットを紹介します。このデータセットは2段階のアノテーションプロセスを通じて構築されています。1段階目では、大規模な言語モデルがPubMedの科学論文から400Kページをアノテートし、それぞれの種類（レビュー、研究、臨床事例、その他）、領域（臨床、バイオメディカル、その他）、教育質量に対するスコアを割り当てます。教育質量スコア（1から5に評価される）は、そのページが大学レベルの学習にどのように役立つかを評価します。これらのアノテーションは、小規模な言語モデルの微調校に使用され、PMC-OAコーパス全体にラベルを伝播させます。このようになったメタデータは、臨床事例の精選サブセットを抽出し、商用利用許可の論文から450K件以上の高品質のサブセットを構築することができます。また、品質フィルタリングと領域の上昇サンプリングを通じて、数々のバージョンを構築します。\n\n臨床テキストは、隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠れられた隠",
      "upvotes": 0,
      "discussionId": "685d0b5d696820ba1f28f34c",
      "ai_summary": "A biomedical text dataset, constructed from PubMed, uses a two-stage annotation process involving large and small language models to fine-tune and extract subsets for clinical NLP, improving pretraining efficiency and performance.",
      "ai_keywords": [
        "Biomed-Enriched",
        "PubMed",
        "large language model",
        "small language model",
        "fine-tuning",
        "PMC-OA corpus",
        "educational quality",
        "clinical cases",
        "biomedical NLP",
        "continual-pretraining",
        "OLMo2",
        "MMLU ProfMed",
        "MedQA",
        "MedMCQA",
        "training tokens"
      ]
    },
    "publishedAt": "2025-06-25T07:30:25.000Z",
    "title": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining\n  and Extracting Rare and Hidden Content",
    "summary": "We introduce Biomed-Enriched, a biomedical text dataset constructed from\nPubMed via a two-stage annotation process. In the first stage, a large language\nmodel annotates 400K paragraphs from PubMed scientific articles, assigning\nscores for their type (review, study, clinical case, other), domain (clinical,\nbiomedical, other), and educational quality. The educational quality score\n(rated 1 to 5) estimates how useful a paragraph is for college-level learning.\nThese annotations are then used to fine-tune a small language model, which\npropagates the labels across the full PMC-OA corpus. The resulting metadata\nallows us to extract refined subsets, including 2M clinical case paragraphs\nwith over 450K high-quality ones from articles with commercial-use licenses,\nand to construct several variants via quality filtering and domain upsampling.\nClinical text is typically difficult to access due to privacy constraints, as\nhospital records cannot be publicly shared. Hence, our dataset provides an\nalternative large-scale, openly available collection of clinical cases from\nPubMed, making it a valuable resource for biomedical and clinical NLP.\nPreliminary continual-pretraining experiments with OLMo2 suggest these curated\nsubsets enable targeted improvements, with clinical upsampling boosting\nperformance by ~5% on MMLU ProfMed and educational quality filtering improving\nMedQA and MedMCQA by ~1%. Combinations of these techniques led to faster\nconvergence, reaching same performance with a third of training tokens,\nindicating potential for more efficient and effective biomedical pretraining\nstrategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20331.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a9b0acf6708cb85014f9dc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9b0acf6708cb85014f9dc/Sem1qcBt1lJjFEPK-xz4_.jpeg",
      "fullname": "Rian Touchent",
      "name": "rntc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": true
  }
]