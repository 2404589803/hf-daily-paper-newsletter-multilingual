[
  {
    "paper": {
      "id": "2504.15376",
      "authors": [
        {
          "_id": "680bda9c34c8d0bd08e01a25",
          "user": {
            "_id": "64c170190bfb901b04399295",
            "avatarUrl": "/avatars/c30ce7566ae3497ddc989ec8918d37cc.svg",
            "isPro": false,
            "fullname": "Zhiqiu Lin",
            "user": "zhiqiulin",
            "type": "user"
          },
          "name": "Zhiqiu Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:01.030Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a26",
          "user": {
            "_id": "65f82fb0de5e636ca20184fa",
            "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
            "isPro": false,
            "fullname": "Alan",
            "user": "syCen",
            "type": "user"
          },
          "name": "Siyuan Cen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:05.915Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a27",
          "name": "Daniel Jiang",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a28",
          "name": "Jay Karhade",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a29",
          "user": {
            "_id": "67b2db158904ba09ca8feb79",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b2db158904ba09ca8feb79/faCKKdyroDNCcylEAQZKu.png",
            "isPro": false,
            "fullname": "Hewei Wang",
            "user": "Stephen624",
            "type": "user"
          },
          "name": "Hewei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:53:03.301Z",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2a",
          "name": "Chancharik Mitra",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2b",
          "name": "Tiffany Ling",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2c",
          "name": "Yuhan Huang",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2d",
          "name": "Sifan Liu",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2e",
          "name": "Mingyu Chen",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a2f",
          "name": "Rushikesh Zawar",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a30",
          "name": "Xue Bai",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a31",
          "name": "Yilun Du",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a32",
          "name": "Chuang Gan",
          "hidden": false
        },
        {
          "_id": "680bda9c34c8d0bd08e01a33",
          "name": "Deva Ramanan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T18:34:57.000Z",
      "submittedOnDailyAt": "2025-04-28T00:10:15.204Z",
      "title": "Towards Understanding Camera Motions in Any Video",
      "submittedOnDailyBy": {
        "_id": "65f82fb0de5e636ca20184fa",
        "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
        "isPro": false,
        "fullname": "Alan",
        "user": "syCen",
        "type": "user"
      },
      "summary": "カメラベンチは、カメラの動きを理解することを評価し、改善するために設計された大規模なデータセットとベンチマークです。カメラベンチは、約3,000個の多様なインターネットビデオから構成され、専門家が厳密な多段階の品質管理プロセスを通じて注釈されています。私たちの貢献の一つは、カメラの動きの基本要素のタクロニマイズムで、映画師との協力で設計されています。例えば、「追跡」（またはタラック）のような動きは、動く主題のようなスケーン内容を理解する必要があることを見出しました。私たちは、人間の注釈性能を定量化するために大規模な人間調査を行い、ドメインの専門知識とチュートリアルベースの訓練が精度を大幅に向上させることを明らかにしました。例えば、新手は「ゾームイン」（イントリンシックの変化）と「進む」（エクストリンシックの変化）を混同するかもしれないが、これらを区別することができるように訓練されることができます。カメラベンチを用いて、構造からの動き（SfM）とビデオ言語モデル（VLM）を評価し、SfMモデルはスケーン内容に依存する語義的な基本要素を捉えやすくなり、VLMは精度の高い軌道の推定が必要なジェオメトリ的な基本要素を捉えやすくなります。その後、カメラベンチを用いて生成的VLMを微調節し、両方のような点を最適化し、モーション付加されたサブシーズ、ビデオクエスチョンの回答、ビデオテキストの検索などの應用を示しました。私たちは、タクロニマイズム、ベンチマーク、チュートリアルを用いて、ビデオ内のカメラの動きを理解する最終的な目標に向けた将来の努力を進めることを望んでいます。",
      "upvotes": 110,
      "discussionId": "680bda9e34c8d0bd08e01ae9",
      "projectPage": "https://linzhiqiu.github.io/papers/camerabench/",
      "githubRepo": "https://github.com/sy77777en/CameraBench",
      "ai_keywords": [
        "Structure-from-Motion (SfM)",
        "Video-Language Models (VLMs)",
        "semantic primitives",
        "geometric primitives",
        "generative VLM",
        "motion-augmented captioning",
        "video question answering",
        "video-text retrieval"
      ]
    },
    "publishedAt": "2025-04-21T14:34:57.000Z",
    "title": "Towards Understanding Camera Motions in Any Video",
    "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15376.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f82fb0de5e636ca20184fa",
      "avatarUrl": "/avatars/82974f2e66fa30ecb6d101b19e023910.svg",
      "fullname": "Alan",
      "name": "syCen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16656",
      "authors": [
        {
          "_id": "6809a4ac81a95c83f0c81c83",
          "name": "Chris",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c84",
          "name": "Yichen Wei",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c85",
          "name": "Yi Peng",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c86",
          "name": "Xiaokun Wang",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c87",
          "name": "Weijie Qiu",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c88",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c89",
          "name": "Tianyidan Xie",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8a",
          "name": "Jiangbo Pei",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8b",
          "name": "Jianhao Zhang",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8c",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8d",
          "user": {
            "_id": "6462b241b438438da3c25a5d",
            "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
            "isPro": false,
            "fullname": "Xuchen Song",
            "user": "xuchensong",
            "type": "user"
          },
          "name": "Xuchen Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:35:17.241Z",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8e",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6809a4ac81a95c83f0c81c8f",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T12:24:10.000Z",
      "submittedOnDailyAt": "2025-04-28T05:19:19.230Z",
      "title": "Skywork R1V2: 多モデルハイブリッド強化学習を用いた論理",
      "submittedOnDailyBy": {
        "_id": "6462b241b438438da3c25a5d",
        "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
        "isPro": false,
        "fullname": "Xuchen Song",
        "user": "xuchensong",
        "type": "user"
      },
      "summary": "Skywork R1V2を紹介します。これは次世代の多モデル認知モデルで、前作のSkywork R1Vより大幅に進化しています。R1V2の核心は、報酬モデルのガイドとルールベースの戦略を融合させるハイブリッド強化学習パラダイムです。これにより、複雑な認知能力と広範囲の一般化のバランスを解決する長年の課題を解決します。また、トレーニングの効率化を進めるために、選択的なサンプルバッファ（SSB）機構を提案しています。これは、グループ相対ポリシー最適化（GRPO）における「消滅するアドバンテージ」の問題を解決するために、最適化プロセス中で高価値のサンプルを優先します。特に、過度な強化信号が視覚のハロウィングを引き起こすことを観察し、トレーニングプロセス中で調整された報酬シュールドラーを通じてこの現象をシステマチックに観測し、軽減しています。実験結果は、R1V2の超卓な能力を証明し、オリンピックベンチャーハンドル（OlympiadBench）で62.6、AIME2024で79.0、LiveCodeBenchで63.6、MMMUで74.0のベンチマークリードの性能を示しています。これらの結果は、現在の開放ソースモデルを上回る優れた性能を示し、ゲミニ2.5とOpenAI o4-miniなどの先進プロプライターシステムとの性能間隔を狭める進歩を示しています。Skywork R1V2モデルの重みは公開にされ、開放性と再現性を促進するために公開されています。ホームページは、https://huggingface.co/Skywork/Skywork-R1V2-38Bです。",
      "upvotes": 38,
      "discussionId": "6809a4ae81a95c83f0c81cda",
      "githubRepo": "https://github.com/SkyworkAI/Skywork-R1V",
      "ai_keywords": [
        "reinforcement learning",
        "reward-model guidance",
        "rule-based strategies",
        "Selective Sample Buffer (SSB)",
        "Vanishing Advantages",
        "Group Relative Policy Optimization (GRPO)",
        "visual hallucinations",
        "calibrated reward thresholds",
        "benchmark-leading performances",
        "OlympiadBench",
        "AIME2024",
        "LiveCodeBench",
        "MMMU",
        "Skywork R1V2-38B"
      ]
    },
    "publishedAt": "2025-04-23T08:24:10.000Z",
    "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning",
    "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a\nmajor leap forward from its predecessor, Skywork R1V. At its core, R1V2\nintroduces a hybrid reinforcement learning paradigm that harmonizes\nreward-model guidance with rule-based strategies, thereby addressing the\nlong-standing challenge of balancing sophisticated reasoning capabilities with\nbroad generalization. To further enhance training efficiency, we propose the\nSelective Sample Buffer (SSB) mechanism, which effectively counters the\n``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization\n(GRPO) by prioritizing high-value samples throughout the optimization process.\nNotably, we observe that excessive reinforcement signals can induce visual\nhallucinations--a phenomenon we systematically monitor and mitigate through\ncalibrated reward thresholds throughout the training process. Empirical results\naffirm the exceptional capability of R1V2, with benchmark-leading performances\nsuch as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and\n74.0 on MMMU. These results underscore R1V2's superiority over existing\nopen-source models and demonstrate significant progress in closing the\nperformance gap with premier proprietary systems, including Gemini 2.5 and\nOpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to\npromote openness and reproducibility\nhttps://huggingface.co/Skywork/Skywork-R1V2-38B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16656.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6462b241b438438da3c25a5d",
      "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
      "fullname": "Xuchen Song",
      "name": "xuchensong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.18415",
      "authors": [
        {
          "_id": "680ef1549cc294f617fb14b4",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "680ef1549cc294f617fb14b5",
          "name": "Shuming Ma",
          "hidden": false
        },
        {
          "_id": "680ef1549cc294f617fb14b6",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-25T15:17:52.000Z",
      "submittedOnDailyAt": "2025-04-28T01:39:22.422Z",
      "title": "BitNet v2: 本地4ビットアクティベーションにヘイダマード変換を用いた1ビットLLM",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "1-bit Large Language Models (LLMs)の効率的な部署は、活性化オファルター（activation outliers）によって妨害されています。これらのオファルターは、低ビット幅への量化（quantization）を複雑にすることにより、効率的な部署が難しくなります。我々は、1-bit LLMsの原生4ビット活性化の量化を可能にする新しいフレームワークBitNet v2を紹介します。特に、注意力ネットワークと前向きネットワークの活性化オファルターを解決するために、オンラインハデマード変換（Hadamard transformation）を適用するH-BitLinearモジュールを提案します。この変換は、活性化分布をガウス分布のような形に平滑化し、低ビット表現に適した形に変換します。実験結果によると、8ビット活性化でのBitNet v2の学習は、BitNet b1.58と同等の性能を示します。重要なことに、BitNet v2は4ビットの原生活性化を使用して学習することで、最小限の性能低下を見落とし、バッチ推論のメモリフットプリントと計算コストを大幅に削減できます。",
      "upvotes": 17,
      "discussionId": "680ef1559cc294f617fb1536",
      "ai_keywords": [
        "BitNet v2",
        "1-bit Large Language Models (LLMs)",
        "activation outliers",
        "quantization",
        "4-bit activation quantization",
        "H-BitLinear",
        "Hadamard transformation",
        "activation distributions",
        "Gaussian-like forms",
        "low-bit representation",
        "8-bit activations",
        "BitNet b1.58",
        "batched inference"
      ]
    },
    "publishedAt": "2025-04-25T11:17:52.000Z",
    "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs",
    "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.18415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17821",
      "authors": [
        {
          "_id": "680f56b8da9639d22c64443f",
          "name": "Xinyu Chen",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644440",
          "name": "Yunxin Li",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644441",
          "name": "Haoyuan Shi",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644442",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644443",
          "name": "Wenhan Luo",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644444",
          "name": "Yaowei Wang",
          "hidden": false
        },
        {
          "_id": "680f56b8da9639d22c644445",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T13:47:30.000Z",
      "submittedOnDailyAt": "2025-04-28T09:15:13.533Z",
      "title": "VideoVista-CulturalLingo: 360°の境界 - 映画理解で文化、言語、領域を結ぶ",
      "submittedOnDailyBy": {
        "_id": "62fdb01bc1588e1d4c6c1a7c",
        "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
        "isPro": false,
        "fullname": "Yunxin Li",
        "user": "YunxinLi",
        "type": "user"
      },
      "summary": "ビデオ理解能力を評価するための多模態AIシステムの評価は、彼らの理解と理由論の能力を効果的に測定できます。ビデオ評価ベンチマークは、通常英語を中心とした1言語限定であり、主に西洋文化の背景からのビデオを特徴としています。本論文では、VideoVista-CulturalLingoという最初のビデオ評価ベンチマークを紹介します。これは、ビデオ理解の文化的、言語的、領域的な隙間を結ぶものです。私たちの研究は、以下の点で現存するベンチマークと異なります：\n\n1. 文化多様性：中国、北米、ヨーロッパの文化を採用します。\n2. 多言語性：中国語と英語での質問が含まれています。これらは最も広く使用される言語の2つです。\n3. 広い領域：人間が作り出した数百の領域からのビデオを採用しています。\n\nVideoVista-CulturalLingoは1,389個のビデオと3,134個のQAペアを含み、24つの最近の開放ソースまたは専有ビデオ大モデルを評価しました。実験結果から以下のことが見落とせました：\n\n1. 現在のモデルは、中国語中心的な質問に対して西洋中心的な質問よりも悪い性能を示し、特に中国の歴史に関する質問に対しては特に悪い性能を示します。\n2. 現在の開放ソースモデルは、時系列的理解において特に限界があり、特にイベントロカライゼーションタスクでは最大45.2%のスコアを達成できません。\n3. 主流のモデルは一般的な科学的な質問に強い性能を示し、一方で開放ソースモデルは数学に対して弱い性能を示します。",
      "upvotes": 14,
      "discussionId": "680f56bdda9639d22c64456b",
      "projectPage": "https://videovista-culturallingo.github.io/",
      "githubRepo": "https://github.com/HITsz-TMG/VideoVista"
    },
    "publishedAt": "2025-04-23T09:47:30.000Z",
    "title": "VideoVista-CulturalLingo: 360^circ Horizons-Bridging Cultures,\n  Languages, and Domains in Video Comprehension",
    "summary": "Assessing the video comprehension capabilities of multimodal AI systems can\neffectively measure their understanding and reasoning abilities. Most video\nevaluation benchmarks are limited to a single language, typically English, and\npredominantly feature videos rooted in Western cultural contexts. In this\npaper, we present VideoVista-CulturalLingo, the first video evaluation\nbenchmark designed to bridge cultural, linguistic, and domain divide in video\ncomprehension. Our work differs from existing benchmarks in the following ways:\n1) Cultural diversity, incorporating cultures from China, North America, and\nEurope; 2) Multi-linguistics, with questions presented in Chinese and\nEnglish-two of the most widely spoken languages; and 3) Broad domain, featuring\nvideos sourced from hundreds of human-created domains. VideoVista-CulturalLingo\ncontains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent\nopen-source or proprietary video large models. From the experiment results, we\nobserve that: 1) Existing models perform worse on Chinese-centric questions\nthan Western-centric ones, particularly those related to Chinese history; 2)\nCurrent open-source models still exhibit limitations in temporal understanding,\nespecially in the Event Localization task, achieving a maximum score of only\n45.2%; 3) Mainstream models demonstrate strong performance in general\nscientific questions, while open-source models demonstrate weak performance in\nmathematics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fdb01bc1588e1d4c6c1a7c",
      "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
      "fullname": "Yunxin Li",
      "name": "YunxinLi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16427",
      "authors": [
        {
          "_id": "680c48805ec65044c2861a6a",
          "user": {
            "_id": "669090c01e3f5b16ce22b535",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
            "isPro": false,
            "fullname": "Hanlei Zhang",
            "user": "HanleiZhang",
            "type": "user"
          },
          "name": "Hanlei Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-26T08:52:58.965Z",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6b",
          "name": "Zhuohang Li",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6c",
          "name": "Yeshuang Zhu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6d",
          "name": "Hua Xu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6e",
          "name": "Peiwu Wang",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a6f",
          "name": "Haige Zhu",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a70",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "680c48805ec65044c2861a71",
          "name": "Jinchao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T05:25:13.000Z",
      "submittedOnDailyAt": "2025-04-28T00:53:49.838Z",
      "title": "ラージー・ランゲージ・モデルは、多モーダル言語解析に役立つことができるか？MMLA: コンプレックスなベンチマーク",
      "submittedOnDailyBy": {
        "_id": "669090c01e3f5b16ce22b535",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
        "isPro": false,
        "fullname": "Hanlei Zhang",
        "user": "HanleiZhang",
        "type": "user"
      },
      "summary": "多タイプ言語分析は、多様なモディュールを利用して、人間の会話の深いセマンティクスを理解するために急速に進化している分野です。その重要性に対して、多タイプ大語言モデル（MLLMs）が認知レベルのセマンティクスを理解する能力についての研究は少ないです。本論文では、この空白を解決するために特に設計された詳細なベンチマークMMLAを紹介します。MMLAは、ステージされたシナリオと実世界のシナリオから構成された61K以上の多タイプの会話を含み、多タイプセマンティクスの6つの核心的な次元を覆います：意図、感情、ダイラギューアクト、センチメント、話し方、コミュニケーションの行動です。LLMsとMLLMsの8つの主流分野を3つの方法で評価します：ゼロショット推論、サブプロジェクト調整、マニュアル調整。拡大的な実験により、それほど調整されたモデルでも約60%~70%の精度を達成することがわかり、現在のMLLMsが複雑な人間の言語を理解する能力の限界を明らかにします。MMLAは、大語言モデルの多タイプ言語分析の可能性を探索するための堅固な基盤として役立つことを信じ、この分野の進歩に役立つ有價のリソースを提供します。データセットとコードは、https://github.com/thuiar/MMLAでオープンソースとして提供されています。",
      "upvotes": 9,
      "discussionId": "680c48825ec65044c2861ac4",
      "githubRepo": "https://github.com/thuiar/MMLA",
      "ai_keywords": [
        "multimodal language models (MLLMs)",
        "MMLA (Multimodal Language Analysis)",
        "multimodal utterances",
        "intent",
        "emotion",
        "dialogue act",
        "sentiment",
        "speaking style",
        "communication behavior",
        "zero-shot inference",
        "supervised fine-tuning",
        "instruction tuning",
        "large language models (LLMs)"
      ]
    },
    "publishedAt": "2025-04-23T01:25:13.000Z",
    "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark",
    "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669090c01e3f5b16ce22b535",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669090c01e3f5b16ce22b535/AT-k66Mt5FtbImnhNQJ_J.jpeg",
      "fullname": "Hanlei Zhang",
      "name": "HanleiZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.17768",
      "authors": [
        {
          "_id": "680f2668db85fd31cd5080ff",
          "name": "Piotr Nawrot",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508100",
          "name": "Robert Li",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508101",
          "name": "Renjie Huang",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508102",
          "name": "Sebastian Ruder",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508103",
          "name": "Kelly Marchisio",
          "hidden": false
        },
        {
          "_id": "680f2668db85fd31cd508104",
          "name": "Edoardo M. Ponti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-24T17:39:25.000Z",
      "submittedOnDailyAt": "2025-04-28T05:26:26.185Z",
      "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
      "submittedOnDailyBy": {
        "_id": "640deb5d3c82bd463ee44735",
        "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
        "isPro": false,
        "fullname": "Piotr Nawrot",
        "user": "pnawrot",
        "type": "user"
      },
      "summary": "Sparse attentionはTransformer LLMの長文脈能力を拡張する有望な戦略であるが、その可能性、効率性と精度のトレードオフ、そしてシステマティックなスケーリング研究はまだ調査されていません。この空白を補うために、さまざまな長文脈タスクのコレクション（これには自然言語を基にした新しいタスクも含まれていますが、制御可能で評価が簡単であることを保ちます）において、異なるモデルサイズ、文脈長さ、スパースさレベルでのトレーニング無しスパースアテンションメソッドをより詳細に比較します。実験の基礎上、以下のような主要な発見を報告します：1) isoFLOPS分析により、非常に長い文脈の場合、大きいそして高度にスパースなモデルは小さいそして密なモデルよりも優しいことが明らかになります。2) 統計的に精度を保つためのスパースさのレベルは、解碼時よりも予約時に高く、これは解確時にそのようなものがモデルサイズに関係しています。3) 任務やステージ全体で最適なスティラテジは明確ではありません、それに対して、異なるスパース化ユニットやバジュアルアダプティビティが必要となるシナリオごとに異なります。それにより、中度のスパースさレベルでは少なくとも1つのタスクでは性能の低下が大きく見られます、これは稀疏アテンションは普遍的な解決策ではないことを強調します。4) 稀疏アテンションに特に適したスケーリング法則を提案し、それらの法則が我々の実験範囲を超えても真実的であることを証明します。これらの見解をもとに、稀疏アテンションはTransformer LLMの長文脈処理能力を向上させる重要なツールであることを示し、性能シンプレックスのアプリケーションでは評価のトレードオフを慎重に行う必要があることを示します。",
      "upvotes": 7,
      "discussionId": "680f2669db85fd31cd50815e",
      "ai_keywords": [
        "Sparse attention",
        "Transformer LLMs",
        "Training-free",
        "IsoFLOPS analysis",
        "Sequence lengths",
        "Sparsity levels",
        "Long-sequence tasks",
        "Natural language",
        "Accuracy preservation",
        "Decoding",
        "Prefilling",
        "Budget adaptivity",
        "Performance degradation",
        "Scaling laws"
      ]
    },
    "publishedAt": "2025-04-24T13:39:25.000Z",
    "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
    "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17768.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "640deb5d3c82bd463ee44735",
      "avatarUrl": "/avatars/0e748d7c91d97526b280e40ccb25c9e0.svg",
      "fullname": "Piotr Nawrot",
      "name": "pnawrot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.17816",
      "authors": [
        {
          "_id": "680ed2679e529f7799a0689f",
          "user": {
            "_id": "636b20591340f879a2eb98d0",
            "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
            "isPro": false,
            "fullname": "Daneul Kim",
            "user": "carpedkm",
            "type": "user"
          },
          "name": "Daneul Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-28T07:38:37.502Z",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a0",
          "name": "Jingxu Zhang",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a1",
          "name": "Wonjoon Jin",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a2",
          "name": "Sunghyun Cho",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a3",
          "user": {
            "_id": "65115c00a588fdb36558b673",
            "avatarUrl": "/avatars/1f36263dc4bfaf696a4aa959a6aab1e1.svg",
            "isPro": false,
            "fullname": "Qi Dai",
            "user": "daiqi",
            "type": "user"
          },
          "name": "Qi Dai",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-28T00:57:12.914Z",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a4",
          "name": "Jaesik Park",
          "hidden": false
        },
        {
          "_id": "680ed2679e529f7799a068a5",
          "user": {
            "_id": "676a328148d749b7086782d0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Tt7u8l8f_1oVBWmBp7tkm.png",
            "isPro": false,
            "fullname": "Chong Luo",
            "user": "cluo-ms",
            "type": "user"
          },
          "name": "Chong Luo",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-28T00:57:12.914Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/636b20591340f879a2eb98d0/ZUDDcDv2cTWIFx65RIEbG.mp4"
      ],
      "publishedAt": "2025-04-23T06:48:31.000Z",
      "submittedOnDailyAt": "2025-04-28T07:02:51.113Z",
      "title": "主題駆動のビデオ生成における違別化されたアイデンティティとモーション",
      "submittedOnDailyBy": {
        "_id": "636b20591340f879a2eb98d0",
        "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
        "isPro": false,
        "fullname": "Daneul Kim",
        "user": "carpedkm",
        "type": "user"
      },
      "summary": "We propose to train a subject-driven customized video generation model through decoupling the subject-specific learning from temporal dynamics in zero-shot without additional tuning. A traditional method for video customization that is tuning-free often relies on large, annotated video datasets, which are computationally expensive and require extensive annotation. In contrast to the previous approach, we introduce the use of an image customization dataset directly on training video customization models, factorizing the video customization into two folds: (1) identity injection through image customization dataset and (2) temporal modeling preservation with a small set of unannotated videos through the image-to-video training method. Additionally, we employ random image token dropping with randomized image initialization during image-to-video fine-tuning to mitigate the copy-and-paste issue. To further enhance learning, we introduce stochastic switching during joint optimization of subject-specific and temporal features, mitigating catastrophic forgetting. Our method achieves strong subject consistency and scalability, outperforming existing video customization models in zero-shot settings, demonstrating the effectiveness of our framework.",
      "upvotes": 5,
      "discussionId": "680ed2689e529f7799a06907",
      "projectPage": "https://carpedkm.github.io/projects/disentangled_sub/",
      "githubRepo": "https://github.com/carpedkm/disentangled-subject-to-vid",
      "ai_keywords": [
        "subject-specific learning",
        "temporal dynamics",
        "image customization dataset",
        "identity injection",
        "temporal modeling",
        "image-to-video training method",
        "random image token dropping",
        "randomized image initialization",
        "image-to-video fine-tuning",
        "stochastic switching",
        "joint optimization",
        "catastrophic forgetting",
        "subject consistency",
        "zero-shot settings"
      ]
    },
    "publishedAt": "2025-04-23T02:48:31.000Z",
    "title": "Subject-driven Video Generation via Disentangled Identity and Motion",
    "summary": "We propose to train a subject-driven customized video generation model\nthrough decoupling the subject-specific learning from temporal dynamics in\nzero-shot without additional tuning. A traditional method for video\ncustomization that is tuning-free often relies on large, annotated video\ndatasets, which are computationally expensive and require extensive annotation.\nIn contrast to the previous approach, we introduce the use of an image\ncustomization dataset directly on training video customization models,\nfactorizing the video customization into two folds: (1) identity injection\nthrough image customization dataset and (2) temporal modeling preservation with\na small set of unannotated videos through the image-to-video training method.\nAdditionally, we employ random image token dropping with randomized image\ninitialization during image-to-video fine-tuning to mitigate the copy-and-paste\nissue. To further enhance learning, we introduce stochastic switching during\njoint optimization of subject-specific and temporal features, mitigating\ncatastrophic forgetting. Our method achieves strong subject consistency and\nscalability, outperforming existing video customization models in zero-shot\nsettings, demonstrating the effectiveness of our framework.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/636b20591340f879a2eb98d0/ZUDDcDv2cTWIFx65RIEbG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.17816.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636b20591340f879a2eb98d0",
      "avatarUrl": "/avatars/4fc5cb13f916bcbc842ccf387bd5f6c0.svg",
      "fullname": "Daneul Kim",
      "name": "carpedkm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15716",
      "authors": [
        {
          "_id": "680dcc5d3478de07603a8036",
          "user": {
            "_id": "642656cbad1e3b0e6e91b752",
            "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
            "isPro": false,
            "fullname": "Jie Zhu",
            "user": "amazingj",
            "type": "user"
          },
          "name": "Jie Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-28T07:39:02.713Z",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8037",
          "name": "Qian Chen",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8038",
          "name": "Huaixia Dou",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a8039",
          "name": "Junhui Li",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803a",
          "name": "Lifan Guo",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803b",
          "name": "Feng Chen",
          "hidden": false
        },
        {
          "_id": "680dcc5d3478de07603a803c",
          "name": "Chi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T09:01:04.000Z",
      "submittedOnDailyAt": "2025-04-28T06:16:26.234Z",
      "title": "DianJin-R1: 大規模言語モデルの財務理由論の評価と向上",
      "submittedOnDailyBy": {
        "_id": "642656cbad1e3b0e6e91b752",
        "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
        "isPro": false,
        "fullname": "Jie Zhu",
        "user": "amazingj",
        "type": "user"
      },
      "summary": "有效な理由論理は、大規模言語モデル（LLMs）において金融領域では核心的な課題です。これらの課題は、領域特有の知識、精密な数値計算、そして厳格な違反ルールの遵守が求められるためです。我々は、理由論理を強化するフレームワークDianJin-R1を提案し、理由論理を強化することでこれらの課題を解決することを目指しています。我々のアプローチの中心はDianJin-R1-Dataです。DianJin-R1-Dataは、CFLUE、FinQA、そして専有の違反コーパス（中国違反チェック、CCC）から構築された高品質なデータセットです。これらは、多様な金融の理由論理スケーラと確認された注釈を組み合わせて構成されています。我々のモデル、DianJin-R1-7BとDianJin-R1-32Bは、Qwen2.5-7B-InstructとQwen2.5-32B-Instructから構築され、構造化されたフォーマットを使用して理由論理ステップと最終的な答えを生成することで微調節されています。理由論理の品質を進一步に改善するために、我々はGroup Relative Policy Optimization（GRPO）を適用し、これは強化学習手法で、二つの報酬信号を含む：一つは構造化された出力を促す、もう一つは答えの正確性を報酬するものです。我々のモデルは、5つのベンチマークで評価されます：3つの金融データセット（CFLUE、FinQA、CCC）と2つの一般的な理由論理ベンチマーク（MATH-500、GPQA-Diamond）です。実験結果は、DianJin-R1モデルが、特に複雑な金融タスクで理由論理のないものよりも一貫的に優れていることを示しています。また、実世界的なCCCデータセットでは、我々の単一コールの理由論理モデルは、計算コストが大幅に多く必要な多エージェントシステムの性能を追い越すこともできます。これらの発見は、DianJin-R1が構造化されたサブジェクションと報酬に合わせた学習を通じて金融の理由論理を強化することの効果性を示し、実世界的なアプリケーションに対してスケーラブルで実用的な解決策を提供していることを示しています。",
      "upvotes": 5,
      "discussionId": "680dcc5e3478de07603a807e",
      "ai_keywords": [
        "reasoning-enhanced framework",
        "reasoning-augmented supervision",
        "reinforcement learning",
        "DianJin-R1-Data",
        "CFLUE",
        "FinQA",
        "Chinese Compliance Check (CCC)",
        "high-quality dataset",
        "DianJin-R1-7B",
        "DianJin-R1-32B",
        "Qwen2.5-7B-Instruct",
        "Qwen2.5-32B-Instruct",
        "structured format",
        "reasoning steps",
        "Group Relative Policy Optimization (GRPO)",
        "dual reward signals",
        "structured outputs",
        "answer correctness",
        "MATH-500",
        "GPQA-Diamond",
        "financial datasets",
        "single-call reasoning models",
        "multi-agent systems",
        "real-world applications"
      ]
    },
    "publishedAt": "2025-04-22T05:01:04.000Z",
    "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large\n  Language Models",
    "summary": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15716.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642656cbad1e3b0e6e91b752",
      "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
      "fullname": "Jie Zhu",
      "name": "amazingj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.12080",
      "authors": [
        {
          "_id": "680afc5f2c4b584e1d786eee",
          "name": "Mengshi Qi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786eef",
          "user": {
            "_id": "66a8c8e4f5cda7b8690205ef",
            "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
            "isPro": false,
            "fullname": "Pengfei Zhu",
            "user": "zaplm",
            "type": "user"
          },
          "name": "Pengfei Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-25T08:34:30.195Z",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef0",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef1",
          "name": "Xiaoyang Bi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef2",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef3",
          "name": "Huadong Ma",
          "hidden": false
        },
        {
          "_id": "680afc5f2c4b584e1d786ef4",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T13:41:59.000Z",
      "submittedOnDailyAt": "2025-04-28T02:11:37.182Z",
      "title": "DC-SAM: 画像とビデオでコンテキスト内でのセグメント化を行うための双重一致性による方法",
      "submittedOnDailyBy": {
        "_id": "66a8c8e4f5cda7b8690205ef",
        "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
        "isPro": false,
        "fullname": "Pengfei Zhu",
        "user": "zaplm",
        "type": "user"
      },
      "summary": "グラフィックデータを一つのラベル例を与えた場合、in-context segmentationは対応する物体の分割を目指します。この設定は、few-shot learningの中でオーナーショット segmentationとして知られ、スケーン理解や画像/ビデオ編集などの多様な視覚タスクに応用されています。最近のSegment Anything Modelsは、交互的なsegmentationで最先端の結果を達成しましたが、これらのアプローチは直接in-context segmentationに適用できません。本稿では、prompt-tuningに基づいたDual Consistency SAM (DC-SAM)方法を提案し、SAMとSAM2を画像とビデオの両方のin-context segmentationに適用します。主なアイデアは、SAMのprompt encoderの特徴を高品質な可視プロンプトを提供することで強化します。マスクプロイダーを生成する際には、SAMの特徴量を融合し、prompt encoderとより良く合わせることです。次に、融合された特徴量と初期プロンプトを用いて循環的に一致するcross-attentionを設計します。その後、prompt encoder内の判別的な正負プロンプトを用いて二分支デザインを提供します。また、簡単なマスクチーブルトレーニングスタラテジを設計し、提案した二重一致性方法をマスクチーブに適用します。DC-SAMは主に画像に適用されていますが、SAM2のサポートによりビデオドメインにもシームレスに拡張できます。ビデオドメインでのin-context segmentationがないため、IC-VOSという最初のベンチマークを手動で整頓し、現存するビデオ分割データセットから構築します。このベンチマークを用いて、拡張的な実験は、COCO-20iでは55.5 (+1.4) mIoU、PASCAL-5iでは73.0 (+1.1) mIoU、提案したIC-VOSベンチマークではJ&Fスコア71.52を達成しました。このDC-SAMのソースコードとベンチマークは、https://github.com/zaplm/DC-SAMに公開されています。",
      "upvotes": 5,
      "discussionId": "680afc622c4b584e1d786f9e",
      "ai_keywords": [
        "prompt-tuning",
        "prompt encoder",
        "mask prior",
        "cycle-consistent cross-attention",
        "dual-branch design",
        "discriminative positive prompts",
        "negative prompts",
        "mask-tube",
        "In-Context Video Object Segmentation (IC-VOS)",
        "mIoU"
      ]
    },
    "publishedAt": "2025-04-16T09:41:59.000Z",
    "title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency",
    "summary": "Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.12080.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66a8c8e4f5cda7b8690205ef",
      "avatarUrl": "/avatars/5c43b7b50aeb3d1459307334ddcd1d1b.svg",
      "fullname": "Pengfei Zhu",
      "name": "zaplm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]