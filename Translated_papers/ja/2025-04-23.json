[
  {
    "paper": {
      "id": "2504.15120",
      "authors": [
        {
          "_id": "680733cf7722bb6407ca0787",
          "user": {
            "_id": "65276c7911a8a521c91bc10f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
            "isPro": false,
            "fullname": "Khalil Hennara",
            "user": "Hennara",
            "type": "user"
          },
          "name": "Khalil Hennara",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-22T09:37:47.479Z",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca0788",
          "name": "Sara Chrouf",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca0789",
          "user": {
            "_id": "63aa7667769a10efc404fbbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63aa7667769a10efc404fbbc/tn8ZxUmTEMS0Gze7_F7JL.jpeg",
            "isPro": false,
            "fullname": "Mohamed Motasim Hamed",
            "user": "Moatasem444",
            "type": "user"
          },
          "name": "Mohamed Motaism Hamed",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:37:25.702Z",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078a",
          "user": {
            "_id": "65704741e1cfce1764ce652e",
            "avatarUrl": "/avatars/9189aaf417426af4ebe381ed364a6c0e.svg",
            "isPro": false,
            "fullname": "Zeina Aldallal",
            "user": "ZeinaD",
            "type": "user"
          },
          "name": "Zeina Aldallal",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-23T05:30:42.569Z",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078b",
          "name": "Omar Hadid",
          "hidden": false
        },
        {
          "_id": "680733cf7722bb6407ca078c",
          "name": "Safwan AlModhayan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T14:17:25.000Z",
      "submittedOnDailyAt": "2025-04-23T03:28:02.778Z",
      "title": "クワイン 1.5B: 言語注入によるアラビア語 SLM",
      "submittedOnDailyBy": {
        "_id": "65276c7911a8a521c91bc10f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
        "isPro": false,
        "fullname": "Khalil Hennara",
        "user": "Hennara",
        "type": "user"
      },
      "summary": "AI開発の重要な一環として、既存モデルに新しい知識を追加することがあります。本論文では、大規模言語モデル（LLM）に新しい言語を統合する新しい方法を紹介します。我々のアプローチは、既存のLLMに前に見られなかったターゲット言語を収録し、その知識を破壊しないように成功しました。英語で主に訓練された小さなオープンソースモデルにアラビア語を注入し、15億パラメータを持つ小さなモデル「Kuwain」を訓練しました。我々の方法は、多様なベンチマークで平均8%のアラビア語の性能向上を示し、そのまま既存の知識を保持し、元モデルのデータの最小量で訓練されたものです。これは、英語とアラビア語の両方で詳細なモデルを訓練する代わりに、コスト効率的な選択肢となります。結果は、拡張された言語モデルの効率的な、特定の言語に向けた拡張の可能性を示し、拡張や資源強制のプロセスを避けることができることを明らかにします。",
      "upvotes": 58,
      "discussionId": "680733d07722bb6407ca07da",
      "githubRepo": "https://github.com/misraj-ai/Kuwain-Arabic-cleaner",
      "ai_keywords": [
        "large language model (LLM)",
        "tiny model",
        "Kuwain",
        "language integration",
        "Arabic language",
        "benchmarks",
        "language model expansion"
      ]
    },
    "publishedAt": "2025-04-21T10:17:25.000Z",
    "title": "Kuwain 1.5B: An Arabic SLM via Language Injection",
    "summary": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15120.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65276c7911a8a521c91bc10f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
      "fullname": "Khalil Hennara",
      "name": "Hennara",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16084",
      "authors": [
        {
          "_id": "6808558a07e80b69b2e351b5",
          "name": "Yuxin Zuo",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b6",
          "user": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "isPro": false,
            "fullname": "Kaiyan Zhang",
            "user": "iseesaw",
            "type": "user"
          },
          "name": "Kaiyan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:51.438Z",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b7",
          "name": "Shang Qu",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b8",
          "name": "Li Sheng",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351b9",
          "name": "Xuekai Zhu",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351ba",
          "name": "Biqing Qi",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bb",
          "name": "Youbang Sun",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bc",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351bd",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "6808558a07e80b69b2e351be",
          "name": "Bowen Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:59:56.000Z",
      "submittedOnDailyAt": "2025-04-23T01:22:31.055Z",
      "title": "TTRL: 検証時の強化学習",
      "submittedOnDailyBy": {
        "_id": "60bc94cd85a3ab33829b6211",
        "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
        "isPro": false,
        "fullname": "Kaiyan Zhang",
        "user": "iseesaw",
        "type": "user"
      },
      "summary": "この論文は、大語言モデル（LLMs）での推論時に明示的なラベルがないデータ上での推論任務に関する強化学習（RL）を調査します。問題の核心的な課題は、真値情報にアクセスできない状況で推論時に報酬の評価です。この設定はそのまま見えているようですが、テストタイムスケーリング（TTS）の一般的な実践である多数決や、それにより驚くほど効果的な報酬を得ることが見られます。この研究では、無ラベルデータ上でのRLを用いたLLMsの訓練の新しい方法であるテストタイム強化学習（TTRL）を紹介します。TTRLは、事前学習モデルの先驭を利用してLLMsの自己進化を可能にします。実験結果から、TTRLは多様なタスクとモデルで経験的に性能向上を示しています。特に、AIME 2024では、無ラベルテストデータのみを用いてQwen-2.5-Math-7Bのpass@1性能を約159%向上させます。また、TTRLはマジ@Nメトリックのみによるサポートを受けているが、初期モデルの性能の上限を超え、テストデータと真値ラベルを用いて直接訓練されたモデルの性能に近づきます。実験結果は、多様なタスクでのTTRLの一般的な効果性を証明し、さらに、TTRLの広範囲のタスクと領域の可能性を明らかにします。GitHub: https://github.com/PRIME-RL/TTRL",
      "upvotes": 44,
      "discussionId": "6808558b07e80b69b2e351f3",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "Large Language Models (LLMs)",
        "reward estimation",
        "Test-Time Scaling (TTS)",
        "majority voting",
        "Test-Time Reinforcement Learning (TTRL)",
        "self-evolution",
        "pre-trained models",
        "pass@1",
        "Qwen-2.5-Math-7B",
        "AIME 2024",
        "Maj@N metric"
      ]
    },
    "publishedAt": "2025-04-22T13:59:56.000Z",
    "title": "TTRL: Test-Time Reinforcement Learning",
    "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60bc94cd85a3ab33829b6211",
      "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
      "fullname": "Kaiyan Zhang",
      "name": "iseesaw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15521",
      "authors": [
        {
          "_id": "6808458f07e80b69b2df2440",
          "name": "Minghao Wu",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2441",
          "name": "Weixuan Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2442",
          "name": "Sinuo Liu",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2443",
          "name": "Huifeng Yin",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2444",
          "name": "Xintong Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2445",
          "name": "Yu Zhao",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2446",
          "user": {
            "_id": "6527d8b077bceabaab382a75",
            "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
            "isPro": false,
            "fullname": "Chenyang Lyu",
            "user": "ChenyangLyu",
            "type": "user"
          },
          "name": "Chenyang Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:16.770Z",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2447",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2448",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "6808458f07e80b69b2df2449",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T01:47:37.000Z",
      "submittedOnDailyAt": "2025-04-23T00:13:52.385Z",
      "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
      "submittedOnDailyBy": {
        "_id": "62d4bf8c97ab9eb08762a975",
        "avatarUrl": "/avatars/73c6228e317cf37b4e3c3e7a4b3d8ae8.svg",
        "isPro": false,
        "fullname": "Minghao Wu",
        "user": "minghaowu",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）の言語能力が進化し続ける中、強力的な多言語評価が技術的な進歩の公平性を促進するために重要になってきました。この立場論文では、2021年から2024年にかけて148国から公開された2,000点以上の多言語（英語以外）ベンチマークを評価し、過去、現在、将来の多言語ベンチマークの実践を評価します。我々の調査結果から、数十百万ドル以上の大規模な投資の中でも、英語がこれらのベンチマークにもっとも過剰に表現されていることが明らかになりました。また、ほとんどのベンチマークは、翻訳されたものではなく、元の言語のコンテンツを使用していて、中国、インド、ドイツ、イギリス、アメリカ合衆国などの資源豊富な国からのデータが主な資源となっています。また、ベンチマークの性能と人間の判断との比較により、明らかになった差異があります。STEM関連のタスクは、人間の評価と強い相関関係を示している（0.70から0.85），一方、問答システム（例：XQuAD）などの伝統的なNLPタスクは、この傾向に比べて弱い相関関係（0.11から0.30）を示しています。また、英語ベンチマークを他の言語に翻訳することは十分ではなく、地域的なベンチマークは、地域の人間の判断との高い一致率（0.68）を示し、翻訳されたベンチマーク（0.47）に比べても高いことが明らかになりました。これは、文化的および言語的にマッチさせたベンチマークの重要性を強調し、翻訳だけを依存させないことの重要性を示しています。この詳細な分析を通じて、現在の多言語評価の実践の中での6つの重要な制限点を指摘し、効果的な多言語ベンチマークのための指導原理を提案し、5つの重要な研究方向を明確にします。最後に、実世界的な応用を重視した人間に合わせたベンチマークの開発に向けて、国際的な協力を呼びかけます。",
      "upvotes": 41,
      "discussionId": "6808459007e80b69b2df249e",
      "ai_keywords": [
        "multilingual large language models (LLMs)",
        "multilingual benchmarks",
        "benchmark performance",
        "human judgments",
        "STEM-related tasks",
        "question answering (e.g., XQuAD)",
        "culturally and linguistically tailored benchmarks",
        "human-aligned benchmarks",
        "real-world applications"
      ]
    },
    "publishedAt": "2025-04-21T21:47:37.000Z",
    "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
    "summary": "As large language models (LLMs) continue to advance in linguistic\ncapabilities, robust multilingual evaluation has become essential for promoting\nequitable technological progress. This position paper examines over 2,000\nmultilingual (non-English) benchmarks from 148 countries, published between\n2021 and 2024, to evaluate past, present, and future practices in multilingual\nbenchmarking. Our findings reveal that, despite significant investments\namounting to tens of millions of dollars, English remains significantly\noverrepresented in these benchmarks. Additionally, most benchmarks rely on\noriginal language content rather than translations, with the majority sourced\nfrom high-resource countries such as China, India, Germany, the UK, and the\nUSA. Furthermore, a comparison of benchmark performance with human judgments\nhighlights notable disparities. STEM-related tasks exhibit strong correlations\nwith human evaluations (0.70 to 0.85), while traditional NLP tasks like\nquestion answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).\nMoreover, translating English benchmarks into other languages proves\ninsufficient, as localized benchmarks demonstrate significantly higher\nalignment with local human judgments (0.68) than their translated counterparts\n(0.47). This underscores the importance of creating culturally and\nlinguistically tailored benchmarks rather than relying solely on translations.\nThrough this comprehensive analysis, we highlight six key limitations in\ncurrent multilingual evaluation practices, propose the guiding principles\naccordingly for effective multilingual benchmarking, and outline five critical\nresearch directions to drive progress in the field. Finally, we call for a\nglobal collaborative effort to develop human-aligned benchmarks that prioritize\nreal-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15521.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d4bf8c97ab9eb08762a975",
      "avatarUrl": "/avatars/73c6228e317cf37b4e3c3e7a4b3d8ae8.svg",
      "fullname": "Minghao Wu",
      "name": "minghaowu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16072",
      "authors": [
        {
          "_id": "6808467a867c3ef14f8326ce",
          "user": {
            "_id": "63797c273f575acc2f6893c0",
            "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
            "isPro": true,
            "fullname": "Long(Tony) Lian",
            "user": "longlian",
            "type": "user"
          },
          "name": "Long Lian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:14.686Z",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326cf",
          "name": "Yifan Ding",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d0",
          "name": "Yunhao Ge",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d1",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d2",
          "name": "Hanzi Mao",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d3",
          "user": {
            "_id": "620dd3888528f797e88cb9b5",
            "avatarUrl": "/avatars/af04728788d78fe7d6375e19e32a535e.svg",
            "isPro": false,
            "fullname": "Boyi Li",
            "user": "Boyiliee",
            "type": "user"
          },
          "name": "Boyi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:09.738Z",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d4",
          "name": "Marco Pavone",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d5",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d6",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d7",
          "user": {
            "_id": "6333a9195a032dcd095dda13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664329996201-noauth.jpeg",
            "isPro": false,
            "fullname": "Adam Yala",
            "user": "yala",
            "type": "user"
          },
          "name": "Adam Yala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:12.415Z",
          "hidden": false
        },
        {
          "_id": "6808467a867c3ef14f8326d8",
          "user": {
            "_id": "649f05367b57fab3a5b27c8b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649f05367b57fab3a5b27c8b/UDJB4yqF2NmaRwCyTOfcl.jpeg",
            "isPro": true,
            "fullname": "Yin Cui",
            "user": "richardaecn",
            "type": "user"
          },
          "name": "Yin Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:06.739Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63797c273f575acc2f6893c0/37vXp0iDKwhBbVyNnCGHy.qt"
      ],
      "publishedAt": "2025-04-22T17:51:41.000Z",
      "submittedOnDailyAt": "2025-04-23T00:22:38.011Z",
      "title": "Describe Anything: 詳細な地域別画像と動画のキャプチング",
      "submittedOnDailyBy": {
        "_id": "63797c273f575acc2f6893c0",
        "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
        "isPro": true,
        "fullname": "Long(Tony) Lian",
        "user": "longlian",
        "type": "user"
      },
      "summary": "特定画像と映像の特定領域に詳細かつ正確な説明を生成することは、視覚言語モデルにとって基本的な課題です。ここでは、詳細化の局所的なキャプチャ（DLC）を目的としたモデルを紹介します。Describe Anything Model (DAM) は、2つの鍵の革新で局所的な詳細とグローバルなコンテキストを両方保持します。それは、フォーカルプロンプトで目標領域の高解像度のエンコーディングを確保し、局所的な説明とグローバルなコンテキストを統合する局所的な視覚バックボーンです。高品質なDLCデータの不足を解決するために、Semi-supervised learning (SSL) に基づくデータパイプライン（DLC-SDP）を提案します。DLC-SDPは、現存する分割データセットから始まり、SSLを用いて無ラベルのウェブ画像に拡張します。DLC-Benchというベンチマークを紹介します。これは、参照キャプチャを依存せずにDLCを評価するために設計されています。DAMは、7つのベンチマークで新たな最先端となります。これらは、キーワードレベル、フレーズレベル、および詳細な多文句の局所的な画像と映像キャプチャです。",
      "upvotes": 28,
      "discussionId": "6808467e867c3ef14f832831",
      "projectPage": "https://describe-anything.github.io",
      "githubRepo": "https://github.com/NVlabs/describe-anything",
      "ai_keywords": [
        "focal prompt",
        "localized vision backbone",
        "Semi-supervised learning (SSL)-based Data Pipeline (DLC-SDP)",
        "segmentation datasets",
        "DLC-Bench",
        "keyword-level",
        "phrase-level",
        "detailed multi-sentence localized image and video captioning"
      ]
    },
    "publishedAt": "2025-04-22T13:51:41.000Z",
    "title": "Describe Anything: Detailed Localized Image and Video Captioning",
    "summary": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63797c273f575acc2f6893c0/37vXp0iDKwhBbVyNnCGHy.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16072.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63797c273f575acc2f6893c0",
      "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
      "fullname": "Long(Tony) Lian",
      "name": "longlian",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15466",
      "authors": [
        {
          "_id": "6808480c49c8f78b6a4e492f",
          "name": "Jiayi Pan",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4930",
          "user": {
            "_id": "644570ba2d91b15b4c7f6311",
            "avatarUrl": "/avatars/d5e66012066d0c330b8f23718b1499d8.svg",
            "isPro": false,
            "fullname": "Xiuyu Li",
            "user": "xiuyul",
            "type": "user"
          },
          "name": "Xiuyu Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:59.248Z",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4931",
          "name": "Long Lian",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4932",
          "name": "Charlie Snell",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4933",
          "name": "Yifei Zhou",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4934",
          "user": {
            "_id": "6333a9195a032dcd095dda13",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664329996201-noauth.jpeg",
            "isPro": false,
            "fullname": "Adam Yala",
            "user": "yala",
            "type": "user"
          },
          "name": "Adam Yala",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:02.029Z",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4935",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4936",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "6808480c49c8f78b6a4e4937",
          "name": "Alane Suhr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T22:29:02.000Z",
      "submittedOnDailyAt": "2025-04-23T00:30:52.876Z",
      "title": "学習アダプティブパラレル論理",
      "submittedOnDailyBy": {
        "_id": "63797c273f575acc2f6893c0",
        "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
        "isPro": true,
        "fullname": "Long(Tony) Lian",
        "user": "longlian",
        "type": "user"
      },
      "summary": "スカライングの推論時計算の拡大は、言語モデルの理由能力を大幅に向上させました。しかし、現在の方法には重要な制限があります: 連鎖コンシステンシーアプローチは過度に長い出力を生成し、ラテンシータとコンテキストウィンドウの終わりによる増加させています。一方で、並列方法のように自分の一致性による方法は、十分な協調性の不足により、冗長な計算と性能の限界などの問題を見落としています。これらの欠点を解決するために、私たちは、言語モデルが並列計算と連鎖計算を統一したエンドツーエンドの計算を協調するための新しい理由フレームワークを提案します。APRは、現在の理由モデルを一般化し、スパノン()とジョイン()操作を用いた適応的な多タイプ計算を可能にします。鍵の革新的な点は、両方の親と子の推論タイムを最適化するための端末からの強化学習戦略で、推論構造の事前定義が必要とならないようにタスクの成功率を向上させます。カウントダウン理由タスクの実験では、APRの大きな利益が示されました: (1)同じコンテキストウィンドウ内での高い性能(83.4% vs. 60.0% at 4k context); (2) 増加された計算による上位のスケーラブル性(80.1% vs. 66.6% at 20k total tokens); (3) 同等のラテンシータでの精度向上(75.2% vs. 57.3% at approximately 5,000ms)。APRは、言語モデルが自動的に計算の適応的分配を通じて理由プロセスを最適化することを可能にします。",
      "upvotes": 27,
      "discussionId": "6808480c49c8f78b6a4e4968",
      "githubRepo": "https://github.com/Parallel-Reasoning/APR",
      "ai_keywords": [
        "Adaptive Parallel Reasoning (APR)",
        "serialized chain-of-thought approaches",
        "parallel methods",
        "self-consistency",
        "adaptive multi-threaded inference",
        "spawn()",
        "join()",
        "reinforcement learning strategy",
        "parent inference threads",
        "child inference threads",
        "Countdown reasoning task",
        "context window",
        "scalability",
        "total tokens",
        "reasoning processes",
        "adaptive allocation of computation"
      ]
    },
    "publishedAt": "2025-04-21T18:29:02.000Z",
    "title": "Learning Adaptive Parallel Reasoning with Language Models",
    "summary": "Scaling inference-time computation has substantially improved the reasoning\ncapabilities of language models. However, existing methods have significant\nlimitations: serialized chain-of-thought approaches generate overly long\noutputs, leading to increased latency and exhausted context windows, while\nparallel methods such as self-consistency suffer from insufficient\ncoordination, resulting in redundant computations and limited performance\ngains. To address these shortcomings, we propose Adaptive Parallel Reasoning\n(APR), a novel reasoning framework that enables language models to orchestrate\nboth serialized and parallel computations end-to-end. APR generalizes existing\nreasoning methods by enabling adaptive multi-threaded inference using spawn()\nand join() operations. A key innovation is our end-to-end reinforcement\nlearning strategy, optimizing both parent and child inference threads to\nenhance task success rate without requiring predefined reasoning structures.\nExperiments on the Countdown reasoning task demonstrate significant benefits of\nAPR: (1) higher performance within the same context window (83.4% vs. 60.0% at\n4k context); (2) superior scalability with increased computation (80.1% vs.\n66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%\nvs. 57.3% at approximately 5,000ms). APR represents a step towards enabling\nlanguage models to autonomously optimize their reasoning processes through\nadaptive allocation of computation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15466.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63797c273f575acc2f6893c0",
      "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg",
      "fullname": "Long(Tony) Lian",
      "name": "longlian",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15415",
      "authors": [
        {
          "_id": "68084b04ba1dd0e6a077e09f",
          "user": {
            "_id": "64c910233d5a0dfed5ce5abb",
            "avatarUrl": "/avatars/8c73f380219c05ae7e7c2fad75a570d8.svg",
            "isPro": false,
            "fullname": "dma",
            "user": "mdh98",
            "type": "user"
          },
          "name": "David Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:56.437Z",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a0",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a1",
          "user": {
            "_id": "6704ee27386892c420db1938",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6704ee27386892c420db1938/lb5mtEwYhn47RawkynYPs.jpeg",
            "isPro": false,
            "fullname": "JinCheng Ren",
            "user": "JinChengRen",
            "type": "user"
          },
          "name": "Jincheng Ren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:36:46.274Z",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a2",
          "name": "Jarvis Guo",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a3",
          "name": "Yifan Yao",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a4",
          "name": "Zhenlin Wei",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a5",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a6",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a7",
          "name": "Boyu Feng",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a8",
          "name": "Jun Ma",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0a9",
          "name": "Xiao Gu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0aa",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ab",
          "name": "King Zhu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ac",
          "name": "Yancheng He",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ad",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0ae",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0af",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b0",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b1",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "68084b04ba1dd0e6a077e0b2",
          "name": "Xiaojie Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T19:53:44.000Z",
      "submittedOnDailyAt": "2025-04-23T00:59:32.168Z",
      "title": "IV-Bench: 画像基底化ビデオ認識と理由論理のベンチマーク\n\nIV-Bench: 画像基底化ビデオ認識と理由論理のベンチマーク",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "現在のMultimodal Large Language Models (MLLMs)の評価フレームワークは、主に画像の理由論や一般的な動画理解タスクに焦点を当て、動画理解における画像コンテキストの重要な役割を大きく見落としています。この空間を埋めるために、IV-Benchという最初の総合的なベンチマークを提案します。IV-Benchは967個の動画と2,585個の精密に注釈された画像-テキストクエリを、13タスク（7ポーズと6理由論タスク）と5タイプの代表的なカテゴリにわたって構成されています。最新の開放ソース（例：InternVL2.5, Qwen2.5-VL）とクローズドソース（例：GPT-4o, Gemini2-Flash, Gemini2-Pro）MLLMの詳細な評価により、現在のモデルは画像コンテキストベースの動画の認識と理由論において大幅に悪い性能を示し、最高でも28.9%の精度を達成します。進ける分析は、IV-Benchでのモデルの性能に影響する要因を明らかにし、推論パターン、フレーム数、解像度などを含むことを示します。また、簡単なデータ合成アプローチを通じて、IV-Benchの課題が、学習プロセスでのデータフォーマットの対応だけでなく、さらに広く範囲が及ぶことを示します。これらの見つけは、将来の研究において有價値なヒントを提供します。コードとデータは、https://github.com/multimodal-art-projection/IV-Benchで公開されています。",
      "upvotes": 15,
      "discussionId": "68084b0bba1dd0e6a077e279",
      "githubRepo": "https://github.com/multimodal-art-projection/IV-Bench",
      "ai_keywords": [
        "Image-Grounded Video Perception and Reasoning",
        "IV-Bench",
        "image-text queries",
        "frame number",
        "resolution"
      ]
    },
    "publishedAt": "2025-04-21T15:53:44.000Z",
    "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning\n  in Multimodal LLMs",
    "summary": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)\nprimarily focus on image reasoning or general video understanding tasks,\nlargely overlooking the significant role of image context in video\ncomprehension. To bridge this gap, we propose IV-Bench, the first comprehensive\nbenchmark for evaluating Image-Grounded Video Perception and Reasoning.\nIV-Bench consists of 967 videos paired with 2,585 meticulously annotated\nimage-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5\nrepresentative categories. Extensive evaluations of state-of-the-art\nopen-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,\nGemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models\nsubstantially underperform in image-grounded video Perception and Reasoning,\nmerely achieving at most 28.9% accuracy. Further analysis reveals key factors\ninfluencing model performance on IV-Bench, including inference pattern, frame\nnumber, and resolution. Additionally, through a simple data synthesis approach,\nwe demonstratethe challenges of IV- Bench extend beyond merely aligning the\ndata format in the training proecss. These findings collectively provide\nvaluable insights for future research. Our codes and data are released in\nhttps://github.com/multimodal-art-projection/IV-Bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 46
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14538",
      "authors": [
        {
          "_id": "680863ed3767f6ed7c969fbf",
          "name": "Yiting Ran",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc0",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc1",
          "name": "Tian Qiu",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc2",
          "name": "Jiaqing Liang",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc3",
          "name": "Yanghua Xiao",
          "hidden": false
        },
        {
          "_id": "680863ed3767f6ed7c969fc4",
          "name": "Deqing Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-20T08:56:27.000Z",
      "submittedOnDailyAt": "2025-04-23T02:23:09.187Z",
      "title": "BookWorld: 小説から創造的な物語の生成を目指すインタラクティブな効果團体へ",
      "submittedOnDailyBy": {
        "_id": "64c7bf2c4524c2aea7eac0b3",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7bf2c4524c2aea7eac0b3/5ocZ69MvN4RFv86Aa7ks3.png",
        "isPro": false,
        "fullname": "Xintao Wang",
        "user": "Neph0s",
        "type": "user"
      },
      "summary": "最近の大規模言語モデル（LLMs）の進展により、多エージェントシステムを用いたシミュレーションが可能になりました。先行の試みは、新しく定義されたピースナを割り当てたエージェントソシエイティーを作成していましたが、既に確立した虚構的な世界とキャラクターをシミュレートすることは、実用的な価値があるにも関わらず、大きく調査されていません。本論文では、本ベースの多エージェントソシエイティーを構築し、シミュレートするための機能を統合したシステム「BookWorld」を紹介します。「BookWorld」の設計は、多様性と動的なキャラクター、虚構的な世界観、地理学的制約および変化など、実世界的複雑性を幅広く取り入れています。「BookWorld」は、物語生成、インタラクティブゲーム、ソシャルシミュレーションなど、多様なアプリケーションを提供し、愛される虚構的な作品を拡張し、探索する新しい方法を提供します。拡張した実験を通じて、「BookWorld」が、ソースブックに忠実な高品質のシンプレックスを生成し、前の方法を超えることを示しました（勝率：75.36%）。本論文のコードは、プロジェクトページで公開されています：https://bookworld2025.github.io/。",
      "upvotes": 13,
      "discussionId": "680863ef3767f6ed7c96a026",
      "ai_keywords": [
        "large language models (LLMs)",
        "social simulation",
        "multi-agent systems",
        "agent societies",
        "personas",
        "book-based",
        "comprehensive real-world intricacies",
        "diverse and dynamic characters",
        "fictional worldviews",
        "geographical constraints",
        "story generation",
        "interactive games",
        "creative, high-quality stories",
        "fidelity to the source books"
      ]
    },
    "publishedAt": "2025-04-20T04:56:27.000Z",
    "title": "BookWorld: From Novels to Interactive Agent Societies for Creative Story\n  Generation",
    "summary": "Recent advances in large language models (LLMs) have enabled social\nsimulation through multi-agent systems. Prior efforts focus on agent societies\ncreated from scratch, assigning agents with newly defined personas. However,\nsimulating established fictional worlds and characters remain largely\nunderexplored, despite its significant practical value. In this paper, we\nintroduce BookWorld, a comprehensive system for constructing and simulating\nbook-based multi-agent societies. BookWorld's design covers comprehensive\nreal-world intricacies, including diverse and dynamic characters, fictional\nworldviews, geographical constraints and changes, e.t.c. BookWorld enables\ndiverse applications including story generation, interactive games and social\nsimulation, offering novel ways to extend and explore beloved fictional works.\nThrough extensive experiments, we demonstrate that BookWorld generates\ncreative, high-quality stories while maintaining fidelity to the source books,\nsurpassing previous methods with a win rate of 75.36%. The code of this paper\ncan be found at the project page: https://bookworld2025.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14538.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c7bf2c4524c2aea7eac0b3",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7bf2c4524c2aea7eac0b3/5ocZ69MvN4RFv86Aa7ks3.png",
      "fullname": "Xintao Wang",
      "name": "Neph0s",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.14992",
      "authors": [
        {
          "_id": "68074ed102571b837f03463c",
          "user": {
            "_id": "64722a616facfb01d8ae8349",
            "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
            "isPro": false,
            "fullname": "Wu Bohong",
            "user": "bongbohong",
            "type": "user"
          },
          "name": "Bohong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:15.811Z",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463d",
          "name": "Shen Yan",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463e",
          "name": "Sijun Zhang",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f03463f",
          "name": "Jianqiao Lu",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034640",
          "user": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "isPro": false,
            "fullname": "Yutao Zeng",
            "user": "Taoer",
            "type": "user"
          },
          "name": "Yutao Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:50:18.659Z",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034641",
          "name": "Ya Wang",
          "hidden": false
        },
        {
          "_id": "68074ed102571b837f034642",
          "name": "Xun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T09:41:26.000Z",
      "submittedOnDailyAt": "2025-04-23T00:38:26.026Z",
      "title": "Efficient Pretraining Length Scaling",
      "submittedOnDailyBy": {
        "_id": "64722a616facfb01d8ae8349",
        "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
        "isPro": false,
        "fullname": "Wu Bohong",
        "user": "bongbohong",
        "type": "user"
      },
      "summary": "最近の大規模言語モデルの進展は、訓練後の長さスケーリングの効果性を示したが、訓練前の長さスケーリングの可能性はまだ調査不足である。我々は、訓練前にも長さスケーリングを効率的に行うための新しいフレームワークを提案します。それは、Parallel Hidden Decoding Transformer (PHD-Transformer)と呼ばれ、推論の効率性を維持する同時に訓練前でも長さスケーリングを可能にします。PHD-Transformerは、KVキャッシュ管理戦略を革新的に導入し、元のトークンと隠れデコーディングトークンを区別します。長距離依存関係を維持するために、元のトークンのみのKVキャッシュを残し、使用後すぐに隠れデコーディングトークンを捨てることで、ベーニーフォームのタンフォーマーと同じサイズのKVキャッシュを維持します。さらに、性能を向上させるために、2つの最適化バージョンを導入します。PHD-SWAは、スライディングウィンドウアテンションを用いて局所的依存関係を保存し、PHD-CSWAは、プレフィルティング時間の線形成長を排除するためにチャンクワイズスライディングウィンドウアテンションを実装します。詳細な実験は、複数のベンチマークで一致した改善を示します。",
      "upvotes": 12,
      "discussionId": "68074ed202571b837f03468b",
      "ai_keywords": [
        "KV cache",
        "original tokens",
        "hidden decoding tokens",
        "long-range dependencies",
        "local dependencies",
        "sliding window attention",
        "chunk-wise sliding window attention"
      ]
    },
    "publishedAt": "2025-04-21T05:41:26.000Z",
    "title": "Efficient Pretraining Length Scaling",
    "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(PHD-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\nPHD-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: PHD-SWA employs\nsliding window attention to preserve local dependencies, while\nPHD-CSWA implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14992.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64722a616facfb01d8ae8349",
      "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
      "fullname": "Wu Bohong",
      "name": "bongbohong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.13820",
      "authors": [
        {
          "_id": "6805ab2c40034a5a792a26b2",
          "user": {
            "_id": "63f1d16fbe95ed4c9a9418fe",
            "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
            "isPro": false,
            "fullname": "Yang Yue",
            "user": "yueyang2000",
            "type": "user"
          },
          "name": "Yang Yue",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T09:59:59.547Z",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b3",
          "name": "Yulin Wang",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b4",
          "name": "Chenxin Tao",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b5",
          "name": "Pan Liu",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b6",
          "name": "Shiji Song",
          "hidden": false
        },
        {
          "_id": "6805ab2c40034a5a792a26b7",
          "name": "Gao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-18T17:50:43.000Z",
      "submittedOnDailyAt": "2025-04-23T01:09:25.550Z",
      "title": "CheXWorld: 放射画像の表現に向けた画像世界モデリングの検討\n学習",
      "submittedOnDailyBy": {
        "_id": "63f1d16fbe95ed4c9a9418fe",
        "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
        "isPro": false,
        "fullname": "Yang Yue",
        "user": "yueyang2000",
        "type": "user"
      },
      "summary": "人間は、共通の知識を記述し、世界がどのように機能しているかを理解し、自分の行動の後果を予測するための内部の世界モデルを開発できます。この概念は、最近の初步的な研究で一般的なモジュールを構築する有望な方向として登場しました。例えば、可視的表現学習においても。本論文では、CheXWorldという最初の自動調節された世界モデルのための試みを紹介します。特に、我々の研究は、資格のある放射線医師に必要な医学知識の3つの面を同時にモデル化する統一フレームワークを開発しました。1) 局所解剖構造は、局所組織の細かな特徴（例えば、構造、形状、およびテクスチャ）を説明します。2) 局所解剖構造は、人間の体の構造を説明します（例えば、器官と骨骼の配置）。3) 領域の変化は、CheXWorldが放射線写真の異なる外観領域の遷移をモデル化するように促すことを目的とします（例えば、写真の採用による明るさ、比較度、および照射による変化）。実験的には、適切な質的的および量的な分析を設計し、CheXWorldがこれらの3つの医学知識の次元を成功して捉えていることを明らかにしました。また、8つの医学画像分類と分割ベンチマークにおいての転移学習実験は、CheXWorldが現在のSSL方法と大規模な医学ベースモデルを大幅に超えることを示しました。コードと事前学習モデルは、https://github.com/LeapLabTHU/CheXWorldに提供されています。",
      "upvotes": 11,
      "discussionId": "6805ab2f40034a5a792a27c8",
      "githubRepo": "https://github.com/LeapLabTHU/CheXWorld",
      "ai_keywords": [
        "CheXWorld",
        "self-supervised world model",
        "radiographic images",
        "local anatomical structures",
        "global anatomical layouts",
        "organs",
        "skeletons",
        "domain variations",
        "medical image classification",
        "medical image segmentation",
        "SSL methods",
        "medical foundation models"
      ]
    },
    "publishedAt": "2025-04-18T13:50:43.000Z",
    "title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation\n  Learning",
    "summary": "Humans can develop internal world models that encode common sense knowledge,\ntelling them how the world works and predicting the consequences of their\nactions. This concept has emerged as a promising direction for establishing\ngeneral-purpose machine-learning models in recent preliminary works, e.g., for\nvisual representation learning. In this paper, we present CheXWorld, the first\neffort towards a self-supervised world model for radiographic images.\nSpecifically, our work develops a unified framework that simultaneously models\nthree aspects of medical knowledge essential for qualified radiologists,\nincluding 1) local anatomical structures describing the fine-grained\ncharacteristics of local tissues (e.g., architectures, shapes, and textures);\n2) global anatomical layouts describing the global organization of the human\nbody (e.g., layouts of organs and skeletons); and 3) domain variations that\nencourage CheXWorld to model the transitions across different appearance\ndomains of radiographs (e.g., varying clarity, contrast, and exposure caused by\ncollecting radiographs from different hospitals, devices, or patients).\nEmpirically, we design tailored qualitative and quantitative analyses,\nrevealing that CheXWorld successfully captures these three dimensions of\nmedical knowledge. Furthermore, transfer learning experiments across eight\nmedical image classification and segmentation benchmarks showcase that\nCheXWorld significantly outperforms existing SSL methods and large-scale\nmedical foundation models. Code & pre-trained models are available at\nhttps://github.com/LeapLabTHU/CheXWorld.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.13820.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f1d16fbe95ed4c9a9418fe",
      "avatarUrl": "/avatars/a1bdfa97323693808f2f16ec74698ed3.svg",
      "fullname": "Yang Yue",
      "name": "yueyang2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16030",
      "authors": [
        {
          "_id": "68084e2c59762f55a5a8b5f3",
          "name": "Joya Chen",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f4",
          "name": "Ziyun Zeng",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f5",
          "name": "Yiqi Lin",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f6",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f7",
          "name": "Zejun Ma",
          "hidden": false
        },
        {
          "_id": "68084e2c59762f55a5a8b5f8",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642435a1a3adbc7142c3b0a6/8JExSHYg9ME-w-L_VUt4W.mp4"
      ],
      "publishedAt": "2025-04-22T16:52:09.000Z",
      "submittedOnDailyAt": "2025-04-23T00:56:24.970Z",
      "title": "LiveCC: 大規模ストリーミングスピーチトランスクリプションを用いたビデオLLM学習",
      "submittedOnDailyBy": {
        "_id": "642435a1a3adbc7142c3b0a6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642435a1a3adbc7142c3b0a6/EBmQ7LnfdTdyuhSUti0-d.png",
        "isPro": true,
        "fullname": "Joya Chen",
        "user": "chenjoya",
        "type": "user"
      },
      "summary": "最近のビデオ大語言モデル（Video LLMs）は、コスト高い人間のアノテーションや所有権のモデルAPI（例：GPT-4o）を使用してトレーニングデータを生成するため、スケールアップのトレーニングに制限がある。本論文では、コスト低い自動的語音識別（ASR）トランスクリプトを使用してVideo LLMの大規模トレーニングを調査します。特に、ASRの時間スタンプに従ってASRの単語とビデオフレームを稠密に交差させる新しいストリーミングトレーニングアプローチを提案します。ASRにおけるビジョン・言語表現の先行研究と比較して、我々の方法は自然にASRのストリーミング特性を適合し、モデルが時間的にアラインされた、細かいビジョン・言語モデリングを学習することができます。トレーニングアルゴリズムのサポートを提供するために、YouTubeビデオおよびコーナーキャプション（CC、ASRと同じ）を処理するデータ生成パイプラインを導入し、Live-CC-5Mデータセットを予ちディレクトロンとして、Live-WhisperX-526Kデータセットを高品質の制御付フィードバックトレーニング（SFT）に用いることにより、このデータセットを作成します。特に、SFTがない場合、ASRだけの予ちディレクトロンモデルであるLiveCC-7B-Baseモデルは、実時間ビデオコメンタリーの新しい能力を示し、一般的なビデオQA性能に対して競争的な性能を示します。これを評価するために、LLM-as-a-judgeを使用して新しいLiveSports-3Kベンチマークを設計します。実験は、我々の最終的なLiveCC-7B-Instructモデルが、実時間モードでも先進的な72Bモデル（Qwen2.5-VL-72B-Instruct、LLaVA-Video-72B）のコメンタリー質量を超えることを示し、ビデオMMEとOVOBenchといったビデオQAベンチマークで7B/8Bスケールで最先端の結果を収め、我々のアプローチの広い一般化能力を示します。本論文のすべてのリソースは、https://showlab.github.io/liveccに公開されています。",
      "upvotes": 8,
      "discussionId": "68084e2f59762f55a5a8b721",
      "projectPage": "https://showlab.github.io/livecc/",
      "githubRepo": "https://github.com/showlab/livecc",
      "ai_keywords": [
        "Video LLMs",
        "automatic speech recognition (ASR)",
        "streaming training",
        "timestamps",
        "vision-language representation",
        "temporally-aligned",
        "fine-grained vision-language modeling",
        "data production pipeline",
        "YouTube videos",
        "closed captions (CC)",
        "Live-CC-5M",
        "Live-WhisperX-526K",
        "supervised fine-tuning (SFT)",
        "general video QA",
        "real-time video commentary",
        "LiveSports-3K benchmark",
        "LLM-as-a-judge",
        "LiveCC-7B-Base",
        "LiveCC-7B-Instruct",
        "Qwen2.5-VL-72B-Instruct",
        "LLaVA-Video-72B",
        "VideoMME",
        "OVOBench"
      ]
    },
    "publishedAt": "2025-04-22T12:52:09.000Z",
    "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
    "summary": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642435a1a3adbc7142c3b0a6/8JExSHYg9ME-w-L_VUt4W.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16030.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642435a1a3adbc7142c3b0a6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642435a1a3adbc7142c3b0a6/EBmQ7LnfdTdyuhSUti0-d.png",
      "fullname": "Joya Chen",
      "name": "chenjoya",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15681",
      "authors": [
        {
          "_id": "680846defa5a6cc6bd9d2cf3",
          "name": "Vidi Team",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf4",
          "name": "Celong Liu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf5",
          "name": "Chia-Wen Kuo",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf6",
          "user": {
            "_id": "6476af4402fc644c810b29a2",
            "avatarUrl": "/avatars/68aefabe6b000443f4601137e6672187.svg",
            "isPro": false,
            "fullname": "Dawei Du",
            "user": "daviddousa",
            "type": "user"
          },
          "name": "Dawei Du",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:04.152Z",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf7",
          "name": "Fan Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf8",
          "name": "Guang Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cf9",
          "name": "Jiamin Yuan",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfa",
          "name": "Lingxi Zhang",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfb",
          "name": "Lu Guo",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfc",
          "name": "Lusha Li",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfd",
          "name": "Longyin Wen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cfe",
          "name": "Qingyu Chen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2cff",
          "name": "Rachel Deng",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d00",
          "name": "Sijie Zhu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d01",
          "name": "Stuart Siew",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d02",
          "name": "Tong Jin",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d03",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d04",
          "name": "Wen Zhong",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d05",
          "name": "Xiaohui Shen",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d06",
          "name": "Xin Gu",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d07",
          "name": "Xing Mei",
          "hidden": false
        },
        {
          "_id": "680846defa5a6cc6bd9d2d08",
          "name": "Xueqiong Qu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T08:04:45.000Z",
      "submittedOnDailyAt": "2025-04-23T00:19:34.185Z",
      "title": "Vidi: 大規模多モデルの映画理解と編集",
      "submittedOnDailyBy": {
        "_id": "65cbdea6d6c974694f09249a",
        "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
        "isPro": false,
        "fullname": "Jay",
        "user": "Zilence006",
        "type": "user"
      },
      "summary": "人間は自然と親織りのある人々と情報を共有し、映像はインターネットでのコミュニケーションと表現の主なメディアとして構成されています。高品質な大規模な映像内容の作成を支援するために、現代のパイプラインは、生データ（例：カメラで撮影された未編集フードアウト）と編集コンポーネント（例：可視効果）の両方についての詳細な理解が必要です。映像編集の場合、モデルは強い知識を持つことと、柔軟な入力長さ（例：1時間の生映像）を処理する必要があり、これは伝統的なモデルにとって大きな課題となります。このレポートでは、Vidiという大規模な多様性モデル（LMMs）の家族を、広範囲の映像理解編集スキャンを支援するために紹介します。最初のリリースは、時系列的な検索（temporal retrieval）に焦点を当てています。つまり、与えられたテキストクエリに対応する入力映像の時間範囲を特定することで、智能な編集に重要な役割を果たします。このモデルは、時系列的理解能力が強く、例えば特定のクエリに対して時間範囲を検索することができます。実世界的なスキャンでの詳細な評価を支援するために、VUE-TRベンチマークも紹介します。これは5つの進歩点を持ちます。1）映像の長さ：現在の時系列検索データセットよりも長い、2）音声サポート：音声ベースのクエリを含み、3）クエリの形式：多様なクエリの長さ/形式、4）アノテーションの品質：真の時間範囲は手動でアノテーションされています、5）評価指標：複数の時間範囲を評価するための改良されたIoUメトリック。驚くことに、Vidiは時系列検索タスクで最先端のプロプライエドモデル（例：GPT-4oとGemini）を大幅に超え、映像編集スキャンでの優れた性能を示しています。",
      "upvotes": 7,
      "discussionId": "680846dffa5a6cc6bd9d2d59",
      "ai_keywords": [
        "Vidi",
        "Large Multimodal Models (LMMs)",
        "temporal retrieval",
        "video editing scenarios",
        "temporal understanding",
        "VUE-TR benchmark",
        "IoU metric"
      ]
    },
    "publishedAt": "2025-04-22T04:04:45.000Z",
    "title": "Vidi: Large Multimodal Models for Video Understanding and Editing",
    "summary": "Humans naturally share information with those they are connected to, and\nvideo has become one of the dominant mediums for communication and expression\non the Internet. To support the creation of high-quality large-scale video\ncontent, a modern pipeline requires a comprehensive understanding of both the\nraw input materials (e.g., the unedited footage captured by cameras) and the\nediting components (e.g., visual effects). In video editing scenarios, models\nmust process multiple modalities (e.g., vision, audio, text) with strong\nbackground knowledge and handle flexible input lengths (e.g., hour-long raw\nvideos), which poses significant challenges for traditional models. In this\nreport, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a\nwide range of video understand editing scenarios. The first release focuses on\ntemporal retrieval, i.e., identifying the time ranges within the input videos\ncorresponding to a given text query, which plays a critical role in intelligent\nediting. The model is capable of processing hour-long videos with strong\ntemporal understanding capability, e.g., retrieve time ranges for certain\nqueries. To support a comprehensive evaluation in real-world scenarios, we also\npresent the VUE-TR benchmark, which introduces five key advancements. 1) Video\nduration: significantly longer than existing temporal retrival datasets, 2)\nAudio support: includes audio-based queries, 3) Query format: diverse query\nlengths/formats, 4) Annotation quality: ground-truth time ranges are manually\nannotated. 5) Evaluation metric: a refined IoU metric to support evaluation\nover multiple time ranges. Remarkably, Vidi significantly outperforms leading\nproprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,\nindicating its superiority in video editing scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65cbdea6d6c974694f09249a",
      "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
      "fullname": "Jay",
      "name": "Zilence006",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16080",
      "authors": [
        {
          "_id": "680845d997f32b8ffc13569c",
          "name": "Le Zhuo",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569d",
          "name": "Liangbing Zhao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569e",
          "name": "Sayak Paul",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc13569f",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a0",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a1",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a2",
          "name": "Peng Gao",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a3",
          "name": "Mohamed Elhoseiny",
          "hidden": false
        },
        {
          "_id": "680845d997f32b8ffc1356a4",
          "name": "Hongsheng Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5f7fbd813e94f16a85448745/L_S4ww0dm1-ZUiNRLQm_h.png"
      ],
      "publishedAt": "2025-04-22T17:58:07.000Z",
      "submittedOnDailyAt": "2025-04-23T00:16:04.186Z",
      "title": "「反射による最適化を拡大する：文から画像への拡散モデルの推論時最適化」",
      "submittedOnDailyBy": {
        "_id": "5f7fbd813e94f16a85448745",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
        "isPro": false,
        "fullname": "Sayak Paul",
        "user": "sayakpaul",
        "type": "user"
      },
      "summary": "最近のテキストから画像への拡散モデルは、訓練データとモデルパラメータの拡大により驚異的な視覚質量を達成しますが、複雑なスケーンと細かい詳細については難しいです。大規模な言語モデルで現れる自己反省能力をヒントに、私たちはReflectionFlowを提案します。これは推論時のフレームワークで、拡散モデルが出力を反省し、改良することを可能にします。ReflectionFlowは3つの補間する推論時の拡大アクセスを導入します：(1) ノイズレベル拡大は潜在的初期化を最適化します；(2) プロンプトレベル拡大は精確な語意的ガイドを提供します；(3) 特に、反省レベル拡大は前回の生成を反省し、修正するための行動可能な反省を明記します。反省レベル拡大を促進するために、私たちはGenRefを構築します。これは100万タプルからなり、反省画像、欠陥画像、アピーチ画像を含むデータセットです。このデータセットを活用し、最先端の拡散トランジフォーマーFLUX.1-devを推論時の反省チューニングを適切に行います。実験結果は、ReflectionFlowは簡単なノイズレベル拡大メソッドを大幅に超え、難しいタスクで高品質な画像合成に向けてスケーラブルで計算効率的な解決策を提供します。",
      "upvotes": 5,
      "discussionId": "680845de97f32b8ffc1357c7",
      "ai_keywords": [
        "text-to-image diffusion models",
        "visual quality",
        "training data",
        "model parameters",
        "self-reflection capabilities",
        "diffusion models",
        "inference-time framework",
        "noise-level scaling",
        "latent initialization",
        "prompt-level scaling",
        "semantic guidance",
        "reflection-level scaling",
        "GenRef",
        "multimodal inputs",
        "unified framework",
        "image synthesis"
      ]
    },
    "publishedAt": "2025-04-22T13:58:07.000Z",
    "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for\n  Text-to-Image Diffusion Models via Reflection Tuning",
    "summary": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5f7fbd813e94f16a85448745/L_S4ww0dm1-ZUiNRLQm_h.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16080.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f7fbd813e94f16a85448745",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
      "fullname": "Sayak Paul",
      "name": "sayakpaul",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 605
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16078",
      "authors": [
        {
          "_id": "68087a231e425a6eee93570d",
          "name": "Thomas Schmied",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee93570e",
          "name": "Jörg Bornschein",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee93570f",
          "name": "Jordi Grau-Moya",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee935710",
          "name": "Markus Wulfmeier",
          "hidden": false
        },
        {
          "_id": "68087a231e425a6eee935711",
          "name": "Razvan Pascanu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:57:14.000Z",
      "submittedOnDailyAt": "2025-04-23T03:57:30.931Z",
      "title": "LLMsはギリギリ効果の効果：RLの微調節対策による決断能力の影響",
      "submittedOnDailyBy": {
        "_id": "64c3849269b1a6796052eac7",
        "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
        "isPro": false,
        "fullname": "Thomas Schmied",
        "user": "thomasschmied",
        "type": "user"
      },
      "summary": "LLMの成功により、多様なアガント的アプリケーションに興味が熱ましくなりました。主な仮説は、LLMが共通感とChain-of-Thought（CoT）推理を活用して、複雑な領域を効果的に探索し、効率的に解決できることです。しかし、LLMアガントは、最適な探索と知識を実際に行動する能力の間の隙間（knowing-doing gap）に悩まされ、その補正が必要となりました。本研究では、LLMが決断シナリオで最適な性能を示さない理由をシステマティックに調査します。特に、3つの通常の失敗モード、ギリギリ主義、頻度バイアス、知識を実際に行動する能力の間の隙間に焦点を当てて詳細に調査します。自我生成されたCoT理由に基づく強化学習（RL）での微調節を通じて、これらの欠点を補正する方法を提案します。多様なバンデット（multi-armed bandits、contextual bandits）およびチャックチャック（Tic-tac-toe）の実験では、RL微調節は探索を増加させ、知識を実際に行動する能力の間の隙間を狭めることで、LLMの決断能力を向上させることが示されました。最後に、エピソーンギリギリならびにLLM特有のアプローチ（自転補正、自転一貫性）を組み合わせて、より効果的なLLMの微調節を可能にします。",
      "upvotes": 5,
      "discussionId": "68087a241e425a6eee93576b",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Chain-of-Thought (CoT) reasoning",
        "sub-optimal exploration",
        "knowing-doing gap",
        "decision-making scenarios",
        "greediness",
        "frequency bias",
        "fine-tuning",
        "Reinforcement Learning (RL)",
        "self-generated CoT rationales",
        "multi-armed bandits",
        "contextual bandits",
        "Tic-tac-toe",
        "$\\epsilon$-greedy",
        "self-correction",
        "self-consistency"
      ]
    },
    "publishedAt": "2025-04-22T13:57:14.000Z",
    "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making\n  Abilities",
    "summary": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\nepsilon-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16078.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3849269b1a6796052eac7",
      "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
      "fullname": "Thomas Schmied",
      "name": "thomasschmied",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15785",
      "authors": [
        {
          "_id": "6808579f91ba7dbcc19dbd3e",
          "name": "Siyu Zhou",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd3f",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:49.077Z",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd40",
          "name": "Yijun Yang",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd41",
          "name": "Guodong Long",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd42",
          "name": "Deheng Ye",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd43",
          "name": "Jing Jiang",
          "hidden": false
        },
        {
          "_id": "6808579f91ba7dbcc19dbd44",
          "name": "Chengqi Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ESSjdqSUTzyiFhpuhfUhO.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/-ScBiECcYZw2_83Qu3-29.png"
      ],
      "publishedAt": "2025-04-22T10:58:27.000Z",
      "submittedOnDailyAt": "2025-04-23T01:40:28.955Z",
      "title": "WALL-E 2.0: 世界調整を行うニューロシンボリック学習が、ワールドモデルベースのLLMアグエントを改善する",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "LLMsから正確な世界モデルを構築できるか？そして、世界モデルはLLMアガントにどのような利点を与えるか。LLMsの先行知識と特定の環境の動力学の間の隙間が、LLMsの性能をボトルネックとしてしまうことが多い。この隙間を閉じるために、「世界の調整」というトレーニング無制限の方法を提案し、LLMsに補間的な環境の記号的知識を学習する。記号的知識は行動ルール、グラフ、スケーングラフなどを含む、LLMsが探索トラジェクトから抽出し、実行可能なコードにエンコードされて、LLMアガントの政策を調節する。また、RL無制限でモデルベースのアガント「WALL-E 2.0」をモデル予測制御（MPC）フレームワークを通じて提案し、古典的なMPCのような即時計算の費用の高い最適化を不要にし、LLMアガントが認知記号的な世界モデルと相互作用して、将来のステップの行動を効率的に最適化する。LLMアガントの強いヒューリスティック性はMPCで効率的な計画者となり、調整された世界モデルの正確な予測により計画された行動の品質も保証される。これらは新しい環境での学習エフセクティブさを大幅に向上させる。マース（Minecraftより開放世界の挑戦）とALFWorld（構体化室内環境）の開放世界の挑戦で、WALL-E 2.0は現在の方法より显著に優れている。マースでは、成功率は基準を超える16.1%〜51.6%で、スコアは61.7%以上の改善を収める。ALFWorldでは、4イテレーションで98%の新しいリンクを達成する。",
      "upvotes": 5,
      "discussionId": "680857a191ba7dbcc19dbda6",
      "githubRepo": "https://github.com/elated-sawyer/WALL-E",
      "ai_keywords": [
        "world models",
        "large language models (LLMs)",
        "symbolic knowledge",
        "knowledge graphs",
        "scene graphs",
        "exploration trajectories",
        "executable codes",
        "policies",
        "RL-free",
        "model-based agent",
        "WALL-E 2.0",
        "model-predictive control (MPC)",
        "neurosymbolic world model",
        "look-ahead optimizer",
        "heuristics",
        "planner",
        "predictions",
        "learning efficiency",
        "open-world challenges",
        "Mars (Minecraft like)",
        "ALFWorld (embodied indoor environments)",
        "success rate",
        "score"
      ]
    },
    "publishedAt": "2025-04-22T06:58:27.000Z",
    "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World\n  Model-based LLM Agents",
    "summary": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ESSjdqSUTzyiFhpuhfUhO.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/-ScBiECcYZw2_83Qu3-29.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15785.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16082",
      "authors": [
        {
          "_id": "6808486f043aa415b647ca77",
          "name": "Ziqi Pang",
          "hidden": false
        },
        {
          "_id": "6808486f043aa415b647ca78",
          "name": "Yu-Xiong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T17:59:41.000Z",
      "submittedOnDailyAt": "2025-04-23T00:25:18.841Z",
      "title": "マスター・ビデオ：「MapReduce」は、長いビデオの理解の基本的な原理です。",
      "submittedOnDailyBy": {
        "_id": "642a33ea5673845d9854f458",
        "avatarUrl": "/avatars/ea8ed29a0b4ad629a2ba6e7d26cbd923.svg",
        "isPro": false,
        "fullname": "Ziqi Pang",
        "user": "ziqipang",
        "type": "user"
      },
      "summary": "マネジャー・ビデオ、長いビデオの理解を示す効果的なフレームワークです。このフレームワークは、長いビデオの処理において簡単で効果的なMapReduce原理を示します。 (1) Map: 独立して稠密に短いビデオクリップを認識し、 (2) Reduce: すべてのクリップからの情報を共に集約します。シーケンスからシーケンスまでの視覚言語モデル（VLMs）と比較して、マネジャー・ビデオはコンテキストの長さによらず詳細な短いビデオの認識を行います。既存のビデオアジェントと比較して、順序的なキーセグメント選択に依存しているものではなく、Map操作により簡単でスケーラブルな順序平行の短いビデオセグメントの認識が可能になります。Reduceステップでは、より詳細なコンテキストの集約と理由を行い、明示的なキーセグメント検索を超えます。このMapReduce原理は、VLMsとビデオアジェントにも適用可能です。LLMアジェントを使用してその効果性を証明しています。\n\n実際には、マネジャー・ビデオは2つのMapReduceステージを使用しています。 (A) キャプション: 短いビデオクリップにキャプションを生成し、 (map)、重複する文字と物体を共有名に標準化します (reduce); (B) 分析: ユーザーの質問ごとに、個々の短いビデオからの関連情報を分析し、最終的な回答に統合します (map)、 (reduce)。マネジャー・ビデオは、最先端のVLMsとビデオアジェントと比較して、難しいLVBenchで10%以上の精度向上を達成します。\n\nコードは以下のURLで利用可能です: https://github.com/ziqipang/MR-Video",
      "upvotes": 3,
      "discussionId": "68084870043aa415b647caaf",
      "ai_keywords": [
        "MapReduce",
        "short video clips",
        "sequence-to-sequence vision-language models (VLMs)",
        "sequence parallel perception",
        "context aggregation",
        "context reasoning",
        "key segment selection",
        "key segment retrieval",
        "Captioning",
        "standardizing",
        "repeated characters",
        "shared names",
        "Analysis",
        "relevant information",
        "final answer",
        "LVBench"
      ]
    },
    "publishedAt": "2025-04-22T13:59:41.000Z",
    "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding",
    "summary": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16082.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642a33ea5673845d9854f458",
      "avatarUrl": "/avatars/ea8ed29a0b4ad629a2ba6e7d26cbd923.svg",
      "fullname": "Ziqi Pang",
      "name": "ziqipang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11703",
      "authors": [
        {
          "_id": "6800a4f9f16f9f820ed748af",
          "user": {
            "_id": "64f27f74f1b6c235aed4b904",
            "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
            "isPro": false,
            "fullname": "stneng",
            "user": "stneng",
            "type": "user"
          },
          "name": "Tianneng Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-22T10:13:56.123Z",
          "hidden": true
        },
        {
          "_id": "6800a4f9f16f9f820ed748b0",
          "name": "Jingxuan He",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b1",
          "name": "Zhun Wang",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b2",
          "name": "Linyu Wu",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b3",
          "name": "Hongwei Li",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b4",
          "name": "Wenbo Guo",
          "hidden": false
        },
        {
          "_id": "6800a4f9f16f9f820ed748b5",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T01:58:40.000Z",
      "submittedOnDailyAt": "2025-04-23T00:27:37.099Z",
      "title": "Progent: LLMアガント向けのプログラミング可能権限制御",
      "submittedOnDailyBy": {
        "_id": "64f27f74f1b6c235aed4b904",
        "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
        "isPro": false,
        "fullname": "stneng",
        "user": "stneng",
        "type": "user"
      },
      "summary": "LLM アガントは、ラージー・ランゲージ・モデル（LLM）が中心的な構成要素として、多様なツールを利用してユーザーに割り当てられたタスクを完了する新しいエンドプロシーである。それにも関わらず、LLM アガントは大きなセキュリティリスクを抱えている。外部世界との相互作用によって、攻撃者からの悪意のある命令を遭遇する可能性があり、危険な行動を実行することになる。これを解決する望ましい方法として、最低限の権限を持つことを強制することがあります。タスクの完了に必要なことだけを許可し、必要ないものをブロックすることです。しかし、これを実現するには、多様なアガントシナリオを扱う同時に、セキュリティとユーティリティの両方を保つ必要があるので、難しいです。\n\n我々は、Progentという最初の権限制御機構を紹介します。その核心は、アガント実行中に適用される権限制御ポリシーを柔軟に表現するためのドメイン専用言語です。これらのポリシーは、ツール呼び出しに対して細かい制約を与え、ツール呼び出しが許可されるかどうかを決定し、許可されない場合にフォルトバックを指定します。これにより、アガント開発者やユーザーは、特定の使用ケースに合わせたポリシーを作成し、確定的に実行することで安全性を確保できます。モジュール化のデザインにより、Progentの統合はアガント内部を変更させず、アガント実装に最小限の変更が必要なく、実用的さと広くなる可能性を高めます。ポリシーの書き出しを自動化するために、LLMを利用してユーザーのクエリに基づいたポリシーを生成し、動的に更新してセキュリティとユーティリティの向上を図ることができます。我々の拡大的な評価により、AgentDojo、ASB、AgentPoisonの3つの異なるシナリオやベンチマークで強い安全性を維持しながら高いユーティリティを維持することができることが示されます。また、我々は、コアコンポーネントの効果および自動化ポリシー生成の対応性についての詳細な分析を行い、対応攻撃に対する強固性を示します。",
      "upvotes": 3,
      "discussionId": "6800a4faf16f9f820ed748ee",
      "ai_keywords": [
        "LLM agents",
        "large language models (LLMs)",
        "principle of least privilege",
        "privilege control mechanism",
        "domain-specific language",
        "privilege control policies",
        "tool calls",
        "agent execution",
        "fine-grained constraints",
        "fallbacks",
        "security",
        "utility",
        "policy writing",
        "automated policy generation",
        "AgentDojo",
        "ASB",
        "AgentPoison"
      ]
    },
    "publishedAt": "2025-04-15T21:58:40.000Z",
    "title": "Progent: Programmable Privilege Control for LLM Agents",
    "summary": "LLM agents are an emerging form of AI systems where large language models\n(LLMs) serve as the central component, utilizing a diverse set of tools to\ncomplete user-assigned tasks. Despite their great potential, LLM agents pose\nsignificant security risks. When interacting with the external world, they may\nencounter malicious commands from attackers, leading to the execution of\ndangerous actions. A promising way to address this is by enforcing the\nprinciple of least privilege: allowing only essential actions for task\ncompletion while blocking unnecessary ones. However, achieving this is\nchallenging, as it requires covering diverse agent scenarios while preserving\nboth security and utility.\n  We introduce Progent, the first privilege control mechanism for LLM agents.\nAt its core is a domain-specific language for flexibly expressing privilege\ncontrol policies applied during agent execution. These policies provide\nfine-grained constraints over tool calls, deciding when tool calls are\npermissible and specifying fallbacks if they are not. This enables agent\ndevelopers and users to craft suitable policies for their specific use cases\nand enforce them deterministically to guarantee security. Thanks to its modular\ndesign, integrating Progent does not alter agent internals and requires only\nminimal changes to agent implementation, enhancing its practicality and\npotential for widespread adoption. To automate policy writing, we leverage LLMs\nto generate policies based on user queries, which are then updated dynamically\nfor improved security and utility. Our extensive evaluation shows that it\nenables strong security while preserving high utility across three distinct\nscenarios or benchmarks: AgentDojo, ASB, and AgentPoison. Furthermore, we\nperform an in-depth analysis, showcasing the effectiveness of its core\ncomponents and the resilience of its automated policy generation against\nadaptive attacks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11703.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f27f74f1b6c235aed4b904",
      "avatarUrl": "/avatars/1129af4fcf566b87a5ff81375376bf3a.svg",
      "fullname": "stneng",
      "name": "stneng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14977",
      "authors": [
        {
          "_id": "68084dbc2eff5d45775d8f14",
          "name": "Jingkai Zhou",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f15",
          "name": "Yifan Wu",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f16",
          "name": "Shikai Li",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f17",
          "name": "Min Wei",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f18",
          "name": "Chao Fan",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f19",
          "name": "Weihua Chen",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f1a",
          "name": "Wei Jiang",
          "hidden": false
        },
        {
          "_id": "68084dbc2eff5d45775d8f1b",
          "name": "Fan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T09:09:21.000Z",
      "submittedOnDailyAt": "2025-04-23T00:49:31.236Z",
      "title": "RealisDance-DiT: 簡単で強力な基準を構築し、自然界での制御可能なキャラクターアニメーションに向けて",
      "submittedOnDailyBy": {
        "_id": "6434caa64b34368fdb07da48",
        "avatarUrl": "/avatars/8da3d3d1e274ff4ad409234678b1b952.svg",
        "isPro": false,
        "fullname": "Jingkai Zhou",
        "user": "theFoxofSky",
        "type": "user"
      },
      "summary": "制御可能なキャラクターアニメーションは、特に稀少なパウス、スタイリズドなキャラクター、キャラクターとオブジェクトの相互作用、複雑な照明、動的なシーンを処理する場合には、難しい問題です。これらの問題を解決するために、先行研究は主に複雑なバイパスネットワークを用いて姿勢と外観のガイドを注入していましたが、通常は開放ワールドシナリオに対して一般化できません。本論文では、ベースモデルが十分に強力である限り、直さないモデル補正と柔軟な微調節戦略を用いて上記の課題を大きく解決できる新しい視点を提案します。特に、RealisDance-DiTを紹介します。RealisDance-DiTはWan-2.1ビデオベースモデルに基づいて構築されています。我々の十分な分析により、広く採用されているReference Netデザインは、大規模なDiTモデルに対しては最適ではありませんでした。代わりに、我々はベースモデルアーキテクチャに最小限の補正を行い、驚くべき強いベースラインを示しました。また、我々は、低ノイズワームアップと「大バッチと小イテレーション」戦略を提案し、微調節中のモデルの収束を加速しながら、ベースモデルの先駆的性質を最大限に保存することを目指します。また、我々は、多様なリアルウォールチャレンジを捉える新しいテストデータセットを紹介し、TikTokデータセットやUBCファッションビデオデータセットといった既存のベンチマークを補完し、提案した方法を詳細に評価することを目指します。拡大的な実験により、RealisDance-DiTは現在の方法より大幅に優れていることがわかりました。",
      "upvotes": 2,
      "discussionId": "68084dc02eff5d45775d902c",
      "projectPage": "https://thefoxofsky.github.io/project_pages/RealisDance-DiT/index",
      "githubRepo": "https://github.com/damo-cv/RealisDance",
      "ai_keywords": [
        "RealisDance-DiT",
        "Wan-2.1 video foundation model",
        "Reference Net design",
        "DiT models",
        "low-noise warmup",
        "large batches and small iterations strategies",
        "foundation model architecture",
        "model convergence",
        "priors of the foundation model",
        "TikTok dataset",
        "UBC fashion video dataset"
      ]
    },
    "publishedAt": "2025-04-21T05:09:21.000Z",
    "title": "RealisDance-DiT: Simple yet Strong Baseline towards Controllable\n  Character Animation in the Wild",
    "summary": "Controllable character animation remains a challenging problem, particularly\nin handling rare poses, stylized characters, character-object interactions,\ncomplex illumination, and dynamic scenes. To tackle these issues, prior work\nhas largely focused on injecting pose and appearance guidance via elaborate\nbypass networks, but often struggles to generalize to open-world scenarios. In\nthis paper, we propose a new perspective that, as long as the foundation model\nis powerful enough, straightforward model modifications with flexible\nfine-tuning strategies can largely address the above challenges, taking a step\ntowards controllable character animation in the wild. Specifically, we\nintroduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our\nsufficient analysis reveals that the widely adopted Reference Net design is\nsuboptimal for large-scale DiT models. Instead, we demonstrate that minimal\nmodifications to the foundation model architecture yield a surprisingly strong\nbaseline. We further propose the low-noise warmup and \"large batches and small\niterations\" strategies to accelerate model convergence during fine-tuning while\nmaximally preserving the priors of the foundation model. In addition, we\nintroduce a new test dataset that captures diverse real-world challenges,\ncomplementing existing benchmarks such as TikTok dataset and UBC fashion video\ndataset, to comprehensively evaluate the proposed method. Extensive experiments\nshow that RealisDance-DiT outperforms existing methods by a large margin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6434caa64b34368fdb07da48",
      "avatarUrl": "/avatars/8da3d3d1e274ff4ad409234678b1b952.svg",
      "fullname": "Jingkai Zhou",
      "name": "theFoxofSky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15524",
      "authors": [
        {
          "_id": "68084b46fa5a6cc6bd9e6a83",
          "user": {
            "_id": "64560618bfdf9c63ce2d658a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
            "isPro": false,
            "fullname": "Mathsion Wong",
            "user": "QiYao-Wang",
            "type": "user"
          },
          "name": "Qiyao Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:27:53.885Z",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a84",
          "name": "Guhong Chen",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a85",
          "name": "Hongbo Wang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a86",
          "name": "Huaren Liu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a87",
          "name": "Minghui Zhu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a88",
          "name": "Zhifei Qin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a89",
          "name": "Linwei Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8a",
          "name": "Yilin Yue",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8b",
          "name": "Shiqiang Wang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8c",
          "name": "Jiayan Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8d",
          "name": "Yihang Wu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8e",
          "name": "Ziqiang Liu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a8f",
          "name": "Longze Chen",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a90",
          "name": "Run Luo",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a91",
          "name": "Liyang Fan",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a92",
          "name": "Jiaming Li",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a93",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a94",
          "name": "Kan Xu",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a95",
          "name": "Hongfei Lin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a96",
          "name": "Hamid Alinejad-Rokny",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a97",
          "name": "Shiwen Ni",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a98",
          "name": "Yuan Lin",
          "hidden": false
        },
        {
          "_id": "68084b46fa5a6cc6bd9e6a99",
          "name": "Min Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T02:00:41.000Z",
      "submittedOnDailyAt": "2025-04-23T05:32:33.756Z",
      "title": "IPBench: 知的財産に関する大規模言語モデルの知識のベンチマーク",
      "submittedOnDailyBy": {
        "_id": "64560618bfdf9c63ce2d658a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
        "isPro": false,
        "fullname": "Mathsion Wong",
        "user": "QiYao-Wang",
        "type": "user"
      },
      "summary": "知識産権（IP）は、技術的知識と法的知識を統合し、固有の複雑さと知識密集性を持つ特別な分野です。大規模な言語モデル（LLMs）が進歩する中、IPタスクの処理に大きなポテンシャルがあり、IP関連のコンテンツの分析、理解、生成がより効率的に行えるようになります。しかし、現在のデータセットとベンチマークは、狭義に専利だけを焦点としているか、IP分野の限られた面を被り、実世界的なシナリオに合わせていません。この隙を埋めるために、初めての実用的なIPタスクタクノロジーと、8つのIP機構と20つのタスクを扱う大規模な多言語ベンチマーク、IPBenchを紹介します。このベンチマークは、実世界的知識産権モデルを評価するために設計されており、理解と生成の両方を含みます。16つのLLMsをベンチマークし、一般用モデルから領域専用モデルまで範囲を広げています。最良のモデルは75.8%の正確率を達成し、大幅な向上の余地があることが明らかになりました。特に、開放ソースのIPや法専門モデルは、閉じたソースの一般用モデルよりも落ち着いています。IPBenchの全データとコードを公開し、IP関連のタスクを追加して実世界的な知識産権チャレンジをより正確に反映し続けます。",
      "upvotes": 1,
      "discussionId": "68084b4cfa5a6cc6bd9e6c75",
      "projectPage": "https://ipbench.github.io/",
      "githubRepo": "https://github.com/IPBench/IPBench"
    },
    "publishedAt": "2025-04-21T22:00:41.000Z",
    "title": "IPBench: Benchmarking the Knowledge of Large Language Models in\n  Intellectual Property",
    "summary": "Intellectual Property (IP) is a unique domain that integrates technical and\nlegal knowledge, making it inherently complex and knowledge-intensive. As large\nlanguage models (LLMs) continue to advance, they show great potential for\nprocessing IP tasks, enabling more efficient analysis, understanding, and\ngeneration of IP-related content. However, existing datasets and benchmarks\neither focus narrowly on patents or cover limited aspects of the IP field,\nlacking alignment with real-world scenarios. To bridge this gap, we introduce\nthe first comprehensive IP task taxonomy and a large, diverse bilingual\nbenchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is\ndesigned to evaluate LLMs in real-world intellectual property applications,\nencompassing both understanding and generation. We benchmark 16 LLMs, ranging\nfrom general-purpose to domain-specific models, and find that even the\nbest-performing model achieves only 75.8% accuracy, revealing substantial room\nfor improvement. Notably, open-source IP and law-oriented models lag behind\nclosed-source general-purpose models. We publicly release all data and code of\nIPBench and will continue to update it with additional IP-related tasks to\nbetter reflect real-world challenges in the intellectual property domain.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15524.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64560618bfdf9c63ce2d658a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64560618bfdf9c63ce2d658a/GVBWU4yNzRsjdyzKT3z3B.jpeg",
      "fullname": "Mathsion Wong",
      "name": "QiYao-Wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14735",
      "authors": [
        {
          "_id": "6807f85efcd784a902c87126",
          "user": {
            "_id": "621d85a10e35b2fbbf3e6196",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
            "isPro": false,
            "fullname": "Chin-Yun Yu",
            "user": "yoyolicoris",
            "type": "user"
          },
          "name": "Chin-Yun Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:28:18.913Z",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c87127",
          "name": "Marco A. Martínez-Ramírez",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c87128",
          "name": "Junghyun Koo",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c87129",
          "name": "Ben Hayes",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c8712a",
          "name": "Wei-Hsiang Liao",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c8712b",
          "name": "György Fazekas",
          "hidden": false
        },
        {
          "_id": "6807f85efcd784a902c8712c",
          "name": "Yuki Mitsufuji",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/621d85a10e35b2fbbf3e6196/alz9iP58RZiY22SYNBEzr.png"
      ],
      "publishedAt": "2025-04-20T20:52:58.000Z",
      "submittedOnDailyAt": "2025-04-23T08:05:27.097Z",
      "title": "DiffVox: 専門家の効果分布を捉える可微分モデル",
      "submittedOnDailyBy": {
        "_id": "621d85a10e35b2fbbf3e6196",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
        "isPro": false,
        "fullname": "Chin-Yun Yu",
        "user": "yoyolicoris",
        "type": "user"
      },
      "summary": "この研究では、音楽制作における声の効果のマッチングに対して新しいかつ解釈可能なモデル「DiffVox」を紹介します。DiffVoxは「Differentiable Vocal Fx」の略で、パラメーターフィードバック、動的範囲制御、デライー、リベアーバーの効果を有効な微分実装により統合し、パラメーターの評価において勾配に基づく最適化を可能にします。声のプレセットは、MedleyDBからの70トラックと私的コレクションからの365トラックから検索されます。パラメーターの相関分析では、高通フィルタと低シェルフィルタが低音を形成するために続くことや、デライー時間とデライー信号の強度が相関していることが明らかになります。主成分分析では、McAdamsの音色の次元との関連が示され、最も重要な成分は観察された空間性を制御し、2番目の成分はスペクトルの明るさを影響します。統計的検定では、パラメーター分布の非ガウス性が確認され、声の効果の空間の複雑性を強調します。これらの初期の発見は、声の効果モデリングと自動混音における将来の研究の基盤となります。ソースコードとデータセットは、https://github.com/SonyResearch/diffvox からアクセス可能です。",
      "upvotes": 0,
      "discussionId": "6807f860fcd784a902c87194",
      "githubRepo": "https://github.com/SonyResearch/diffvox",
      "ai_keywords": [
        "parametric equalisation",
        "dynamic range control",
        "delay",
        "reverb",
        "differentiable implementations",
        "gradient-based optimisation",
        "parameter estimation",
        "principal component analysis",
        "McAdams' timbre dimensions",
        "perceived spaciousness",
        "spectral brightness",
        "non-Gaussian nature"
      ]
    },
    "publishedAt": "2025-04-20T16:52:58.000Z",
    "title": "DiffVox: A Differentiable Model for Capturing and Analysing Professional\n  Effects Distributions",
    "summary": "This study introduces a novel and interpretable model, DiffVox, for matching\nvocal effects in music production. DiffVox, short for ``Differentiable Vocal\nFx\", integrates parametric equalisation, dynamic range control, delay, and\nreverb with efficient differentiable implementations to enable gradient-based\noptimisation for parameter estimation. Vocal presets are retrieved from two\ndatasets, comprising 70 tracks from MedleyDB and 365 tracks from a private\ncollection. Analysis of parameter correlations highlights strong relationships\nbetween effects and parameters, such as the high-pass and low-shelf filters\noften behaving together to shape the low end, and the delay time correlates\nwith the intensity of the delayed signals. Principal component analysis reveals\nconnections to McAdams' timbre dimensions, where the most crucial component\nmodulates the perceived spaciousness while the secondary components influence\nspectral brightness. Statistical testing confirms the non-Gaussian nature of\nthe parameter distribution, highlighting the complexity of the vocal effects\nspace. These initial findings on the parameter distributions set the foundation\nfor future research in vocal effects modelling and automatic mixing. Our source\ncode and datasets are accessible at https://github.com/SonyResearch/diffvox.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/621d85a10e35b2fbbf3e6196/alz9iP58RZiY22SYNBEzr.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "621d85a10e35b2fbbf3e6196",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
      "fullname": "Chin-Yun Yu",
      "name": "yoyolicoris",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]