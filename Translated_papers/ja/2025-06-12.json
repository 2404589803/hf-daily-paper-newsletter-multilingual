[
  {
    "paper": {
      "id": "2506.09113",
      "authors": [
        {
          "_id": "684a3b0a9b38e1e5a33a683f",
          "name": "Yu Gao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6840",
          "name": "Haoyuan Guo",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6841",
          "name": "Tuyen Hoang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6842",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6843",
          "name": "Lu Jiang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6844",
          "name": "Fangyuan Kong",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6845",
          "name": "Huixia Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6846",
          "name": "Jiashi Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6847",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6848",
          "name": "Xiaojie Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6849",
          "name": "Xunsong Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684a",
          "name": "Yifu Li",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684b",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684c",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684d",
          "name": "Jiawei Liu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684e",
          "name": "Shu Liu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a684f",
          "name": "Xiaonan Nie",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6850",
          "name": "Zhiwu Qing",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6851",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6852",
          "name": "Li Sun",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6853",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6854",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6855",
          "name": "Sen Wang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6856",
          "name": "Guoqiang Wei",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6857",
          "name": "Guohong Wu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6858",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6859",
          "name": "Ruiqi Xia",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685a",
          "name": "Fei Xiao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685b",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685c",
          "name": "Jiangqiao Yan",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685d",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685e",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a685f",
          "name": "Runkai Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6860",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6861",
          "name": "Yihang Yang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6862",
          "name": "Zilyu Ye",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6863",
          "name": "Xuejiao Zeng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6864",
          "name": "Yan Zeng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6865",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6866",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6867",
          "name": "Xiaozheng Zheng",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6868",
          "name": "Peihao Zhu",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a6869",
          "name": "Jiaxin Zou",
          "hidden": false
        },
        {
          "_id": "684a3b0a9b38e1e5a33a686a",
          "name": "Feilong Zuo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:56:11.000Z",
      "submittedOnDailyAt": "2025-06-12T01:08:56.090Z",
      "title": "Seedance 1.0: 映像生成モデルの境界を探る",
      "submittedOnDailyBy": {
        "_id": "6381c5d63680a7cf34e08ca9",
        "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
        "isPro": false,
        "fullname": "wujie10558@gmail.com",
        "user": "wujie10",
        "type": "user"
      },
      "summary": "類似のモデルの現状では、プリンセプトの追跡、動きの可能性、そして視覚品質の同時的なバランスは重要な課題です。このレポートでは、Seedance 1.0を紹介します。これは、多くの核心的な技術的な改善を統合した高性能と推論効率の高いビデオベースモデルです。以下には、具体的な改善点がまとまっています。\n\n(i) 多サンプルデータのカレーティングに精度と意味のあるビデオキャプチャを追加し、多様なスケーナーでの全面的な学習を可能にします。\n(ii) 効率的なアーキテクチャ設計と提案されたトレーニングパラダイムで、多ショット生成と文からビデオ、画像からビデオの両方のタスクの共通学習を可能にします。\n(iii) 複数次元の報酬機構を用いたビデオ特有のRLHFと細かいカスタマイズフィードバックを活用した調整された後プロセスで、全体的な性能向上を実現します。\n(iv) オリエンテーションの効果が高いモデル加速を実現し、多段階のディスタイル化戦略とシステムレベルの最適化を通じて、推論速度を約10倍にします。\n\nSeedance 1.0は、NVIDIA-L20での41.4秒で1080pの5秒ビデオを生成できます。現在の最先端のビデオ生成モデルと比較して、高品質と高速なビデオ生成で、スペクトラルフラミディティビリティと構造的安定性、複雑な多セクターコンテキストでの精密な指示の遵守、原生の多ショットナレイティブの一貫性と一致性のある主題表現に特徴があります。",
      "upvotes": 33,
      "discussionId": "684a3b0b9b38e1e5a33a686b",
      "projectPage": "https://seed.bytedance.com/seedance",
      "ai_summary": "Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.",
      "ai_keywords": [
        "diffusion modeling",
        "multi-source data curation",
        "precision and meaningful video captioning",
        "efficient architecture",
        "training paradigm",
        "multi-shot generation",
        "text-to-video",
        "image-to-video",
        "fine-grained supervised fine-tuning",
        "video-specific RLHF",
        "multi-dimensional reward mechanisms",
        "multi-stage distillation strategies",
        "model acceleration",
        "spatiotemporal fluidity",
        "structural stability",
        "instruction adherence",
        "multi-shot narrative coherence",
        "consistent subject representation"
      ]
    },
    "publishedAt": "2025-06-10T13:56:11.000Z",
    "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
    "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09113.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6381c5d63680a7cf34e08ca9",
      "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
      "fullname": "wujie10558@gmail.com",
      "name": "wujie10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06395",
      "authors": [
        {
          "_id": "68492dcf42e4f9106973f437",
          "user": {
            "_id": "6734e315c1aadce903f73aea",
            "avatarUrl": "/avatars/95d95c49419372debc201cb63c354b86.svg",
            "isPro": false,
            "fullname": "Li Pengyi",
            "user": "LiPengyi29",
            "type": "user"
          },
          "name": "Pengyi Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-11T07:18:40.287Z",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f438",
          "name": "Matvey Skripkin",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f439",
          "name": "Alexander Zubrey",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f43a",
          "name": "Andrey Kuznetsov",
          "hidden": false
        },
        {
          "_id": "68492dcf42e4f9106973f43b",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643984dceb7c5616ef3f5d54/5kHQrpj1ivFhnzHL36xhr.jpeg"
      ],
      "publishedAt": "2025-06-05T19:55:15.000Z",
      "submittedOnDailyAt": "2025-06-12T07:02:06.762Z",
      "title": "自信はすべてである：言語モデルの少ショットRL微調",
      "submittedOnDailyBy": {
        "_id": "643984dceb7c5616ef3f5d54",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
        "isPro": false,
        "fullname": "Andrey Kuznetsov",
        "user": "kuznetsoffandrey",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は理由論に優れていますが、トークン後の訓練は、モデルの行動をタスクの目標に合わせるために重要です。現在の強化学習（RL）方法は、高額な人間のアノテーションや外部の報酬モデルに依存しています。私たちは、モデルの自信を報酬信号として使用するReinforcement Learning via Self-Confidence（RLSC）を提案します。これはラベル、好みモデルや報酬工学の必要性を消去します。Qwen2.5-Math-7Bに対して、16サンプルずつの問題に対して、10か20ステップの訓練を行うだけで、RLSCはAIME2024で+13.4%、MATH500で+21.2%、Minerva Mathで+21.7%、Olympiadbenchで+20.8%、AMC23で+9.7%の精度を向上させます。RLSCは、小数点の数のサンプルと無ラベルのスーパーバイオンを必要とする簡単でスケーラブルなトークン後の訓練方法を提供します。",
      "upvotes": 27,
      "discussionId": "68492dd042e4f9106973f43c",
      "ai_summary": "Reinforcement Learning via Self-Confidence (RLSC) improves large language model accuracy using the model's confidence as a reward signal, eliminating the need for human labels or reward engineering.",
      "ai_keywords": [
        "Reinforcement Learning",
        "Large language models",
        "self-confidence",
        "RLSC"
      ]
    },
    "publishedAt": "2025-06-05T15:55:15.000Z",
    "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models",
    "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,\nRLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on\nMinerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a\nsimple, scalable post-training method for inference models, requiring only a\nsmall number of samples and unlabelled supervision.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643984dceb7c5616ef3f5d54/5kHQrpj1ivFhnzHL36xhr.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06395.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "643984dceb7c5616ef3f5d54",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
      "fullname": "Andrey Kuznetsov",
      "name": "kuznetsoffandrey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09995",
      "authors": [
        {
          "_id": "684a39639b38e1e5a33a6837",
          "name": "Yuanpeng Tu",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a6838",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a6839",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683a",
          "name": "Xiang Bai",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683b",
          "name": "Fan Wang",
          "hidden": false
        },
        {
          "_id": "684a39639b38e1e5a33a683c",
          "name": "Hengshuang Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:59:53.000Z",
      "submittedOnDailyAt": "2025-06-12T00:50:19.796Z",
      "title": "PlayerOne: 自我中心の世界シミュレーター",
      "submittedOnDailyBy": {
        "_id": "644a1b6401e18bf93a6f45c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
        "isPro": false,
        "fullname": "xichen",
        "user": "xichenhku",
        "type": "user"
      },
      "summary": "プレイヤーワン、最初の自中心的な写実世界シミュレータを紹介します。これは、生み出した活気ある環境内で満喫的で制限なしな探索を促進するものです。ユーザからの自中心的なスケーン画像を受け取り、その対応する世界を正確に構築し、自中心的なビデオを生成します。これらのビデオは、ユーザが自中心的なカメラで撮影された実際のスケーンの人間の動きと一致しています。プレイヤーワンは、粗略から細かくなるパイプラインで訓練されています。まず、大規模な自中心的なテキスト・ビデオペアでの予ちゅう訓練を行い、粗略レベルの自中心的な理解を実現します。次に、自中心的・自中心的なビデオデータセットから抽出された同期した動き・ビデオデータによる調整訓練を行います。また、異なるコンポーネントの重要性の差異を考慮し、部分別の動き注入シェムを設計し、部分レベルの動きを精密に制御することができます。また、4Dスケーンとビデオフレームの両方を進歩的にモデル化する両方ともの重み複合重建フレームワークを設計し、長形ビデオ生成でのスケーンの一貫性を確保します。実験結果は、多様なスキナーの世界一致的なモデリングと決定的な制御能力の大きな一般化能力を示しています。これは、自中心的な実世界シミュレーションの最初の試みであり、世界モデリングの新しい境界にチャレンジするコミュニティのための道が開かれます。",
      "upvotes": 22,
      "discussionId": "684a39639b38e1e5a33a683d",
      "projectPage": "https://playerone-hku.github.io/",
      "ai_summary": "PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.",
      "ai_keywords": [
        "egocentric realistic world simulator",
        "coarse-to-fine pipeline",
        "pretraining",
        "finetuning",
        "synchronous motion-video data",
        "automatic construction pipeline",
        "part-disentangled motion injection",
        "joint reconstruction framework",
        "4D scene",
        "video frames",
        "scene consistency",
        "long-form video generation",
        "worldconsistent modeling"
      ]
    },
    "publishedAt": "2025-06-11T13:59:53.000Z",
    "title": "PlayerOne: Egocentric World Simulator",
    "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09995.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644a1b6401e18bf93a6f45c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
      "fullname": "xichen",
      "name": "xichenhku",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09350",
      "authors": [
        {
          "_id": "684a79ca9b38e1e5a33a68bf",
          "name": "Shanchuan Lin",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c0",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c1",
          "name": "Hao He",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c2",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c3",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c4",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c5",
          "name": "Yang Zhao",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c6",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "684a79ca9b38e1e5a33a68c7",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T03:04:23.000Z",
      "submittedOnDailyAt": "2025-06-12T05:25:53.654Z",
      "title": "自動回帰的相手誰もが識別できる後処理トレーニングを用いた時間単位の対話ビデオ生成",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "現在の大規模なビデオ生成モデルは計算量が大きく、時間的に実行されることができないことにより、実時間や相互作用的なアプリケーションでの採用が難しい。本研究では、前学習ラテンビデオディフュージョンモデルを時間的な、相互作用的なビデオジェネレーターに変換するために、自動復元的対抗的後学習（AAPT）を提案します。我々のモデルは、1NFE（1ニューラルフィボリシャーティング）で一時間に1ラテンフレームを自動復元的に生成します。モデルは、ユーザーに時間的に結果をストリーミングし、相互作用的な応答を受け取り、次のラテンフレームを生成することができます。既存のアプローチと異なり、我々の方法は、自動復元的な生成に対する対抗的訓練を有効なパラダイムとして検討します。これは、KVキャッシュを充分に利用することで1ステップの生成によりより効率的なアーキテクチャを設計することを可能にし、また、長いビデオ生成時にエラーの積み上げを減少するために、学生強制訓練のような方法でモデルを訓練することができます。実験結果によると、我々の8Bモデルは、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行し、1ラテンフレームを生成するための1NFEを使用し、時間的に実行",
      "upvotes": 22,
      "discussionId": "684a79ca9b38e1e5a33a68c8",
      "projectPage": "https://seaweed-apt.com/2",
      "ai_summary": "Autoregressive adversarial post-training transforms pre-trained latent video diffusion models into real-time, interactive video generators with reduced computational requirements.",
      "ai_keywords": [
        "autoregressive adversarial post-training",
        "latent video diffusion model",
        "autoregressive generation",
        "neural function evaluation",
        "KV cache",
        "student-forcing",
        "real-time video generation",
        "24fps",
        "736x416 resolution",
        "1280x720 resolution",
        "H100"
      ]
    },
    "publishedAt": "2025-06-10T23:04:23.000Z",
    "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
    "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09350.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7093
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08889",
      "authors": [
        {
          "_id": "684a39599b38e1e5a33a6822",
          "name": "Yizhao Gao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6823",
          "name": "Shuming Guo",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6824",
          "name": "Shijie Cao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6825",
          "name": "Yuqing Xia",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6826",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6827",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6828",
          "name": "Lingxiao Ma",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6829",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682a",
          "name": "Tianzhu Ye",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682b",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682c",
          "name": "Hayden Kwok-Hay So",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682d",
          "name": "Yu Hua",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682e",
          "name": "Ting Cao",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a682f",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "684a39599b38e1e5a33a6830",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T15:17:26.000Z",
      "submittedOnDailyAt": "2025-06-12T00:54:43.454Z",
      "title": "SeerAttention-R: 長期計算のためのスパースアテンションアダプタ",
      "submittedOnDailyBy": {
        "_id": "661c96f48921f03a9dae04c3",
        "avatarUrl": "/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg",
        "isPro": false,
        "fullname": "Yizhao Gao",
        "user": "LongMountain",
        "type": "user"
      },
      "summary": "We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.",
      "upvotes": 15,
      "discussionId": "684a39599b38e1e5a33a6833",
      "githubRepo": "https://github.com/microsoft/SeerAttention",
      "ai_summary": "SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.",
      "ai_keywords": [
        "sparse attention",
        "reasoning models",
        "self-distilled gating mechanism",
        "query pooling",
        "lightweight plug-in gating",
        "AIME benchmark",
        "TileLang",
        "sparse decoding kernel",
        "FlashAttention-3",
        "H100 GPU"
      ]
    },
    "publishedAt": "2025-06-10T11:17:26.000Z",
    "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
    "summary": "We introduce SeerAttention-R, a sparse attention framework specifically\ntailored for the long decoding of reasoning models. Extended from\nSeerAttention, SeerAttention-R retains the design of learning attention\nsparsity through a self-distilled gating mechanism, while removing query\npooling to accommodate auto-regressive decoding. With a lightweight plug-in\ngating, SeerAttention-R is flexible and can be easily integrated into existing\npretrained model without modifying the original parameters. We demonstrate that\nSeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning\naccuracy with 4K token budget in AIME benchmark under large sparse attention\nblock sizes (64/128). Using TileLang, we develop a highly optimized sparse\ndecoding kernel that achieves near-theoretical speedups of up to 9x over\nFlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:\nhttps://github.com/microsoft/SeerAttention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08889.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661c96f48921f03a9dae04c3",
      "avatarUrl": "/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg",
      "fullname": "Yizhao Gao",
      "name": "LongMountain",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09790",
      "authors": [
        {
          "_id": "684a33989b38e1e5a33a6804",
          "name": "Zhenran Xu",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6805",
          "name": "Yiyu Wang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6806",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6807",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6808",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a6809",
          "name": "Kaifu Zhang",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a680a",
          "name": "Baotian Hu",
          "hidden": false
        },
        {
          "_id": "684a33989b38e1e5a33a680b",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T14:35:15.000Z",
      "submittedOnDailyAt": "2025-06-12T00:38:07.422Z",
      "title": "ComfyUI-R1: ワークフロー生成のための理由推論モデルの検討",
      "submittedOnDailyBy": {
        "_id": "639c379cdb7c5f35004066cb",
        "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
        "isPro": false,
        "fullname": "Zhenran Xu",
        "user": "imryanxu",
        "type": "user"
      },
      "summary": "AI生成的内容已从单体模型发展到模块化工作流程，特别是在ComfyUI等平台上，这使得创意管道中的定制化成为可能。然而，创建有效的工作流程需要极高的专业知识来协调众多专业组件，这对用户来说是一个陡峭的学习曲线。为了应对这一挑战，我们介绍了ComfyUI-R1，这是第一个用于自动化工作流程生成的大型推理模型。我们从精心策划的4K工作流程数据集开始，构建了长链式思维（CoT）推理数据，包括节点选择、工作流程规划和代码级工作流程表示。ComfyUI-R1通过两阶段框架进行训练：（1）CoT微调用于冷启动，使模型适应ComfyUI领域；（2）强化学习用于激励推理能力，由细粒度规则-度量混合奖励引导，确保格式有效性、结构完整性和节点级保真度。实验表明，我们的70亿参数模型达到了97%的格式有效性率，以及高通过率、节点级和图级F1分数，显著超越了使用领先的闭源模型（如GPT-4o和Claude系列）的先前最先进方法。进一步的分析强调了推理过程的关键作用以及将工作流程转化为代码的优势。定性比较揭示了我们在合成复杂工作流程和多样化节点方面的优势，突显了长CoT推理在AI艺术创作中的潜力。",
      "upvotes": 14,
      "discussionId": "684a33989b38e1e5a33a680c",
      "projectPage": "https://github.com/AIDC-AI/ComfyUI-Copilot",
      "githubRepo": "https://github.com/AIDC-AI/ComfyUI-Copilot",
      "ai_summary": "ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.",
      "ai_keywords": [
        "modular workflows",
        "ComfyUI",
        "large reasoning model",
        "automated workflow generation",
        "chain-of-thought (CoT) reasoning",
        "node selection",
        "workflow planning",
        "code-level workflow representation",
        "CoT fine-tuning",
        "reinforcement learning",
        "fine-grained rule-metric hybrid reward",
        "format validity",
        "structural integrity",
        "node-level fidelity",
        "GPT-4o",
        "Claude series",
        "pass rate",
        "node-level F1 scores",
        "graph-level F1 scores",
        "intricate workflows",
        "diverse nodes",
        "qualitative comparison",
        "AI art creation"
      ]
    },
    "publishedAt": "2025-06-11T10:35:15.000Z",
    "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation",
    "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09790.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639c379cdb7c5f35004066cb",
      "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
      "fullname": "Zhenran Xu",
      "name": "imryanxu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09003",
      "authors": [
        {
          "_id": "6848eed742e4f9106973f2cf",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d0",
          "name": "Jiaxi Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d1",
          "name": "Min Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d2",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d3",
          "name": "Mouxiang Chen",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d4",
          "name": "Jiajun Zhang",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d5",
          "name": "Zeyu Cui",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d6",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "6848eed742e4f9106973f2d7",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:23:33.000Z",
      "submittedOnDailyAt": "2025-06-12T00:17:18.390Z",
      "title": "SWE-Flow: テスト駆動でのソフトウェア工学データの合成",
      "submittedOnDailyBy": {
        "_id": "64c38871f9cd765462fa1a17",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg",
        "isPro": false,
        "fullname": "Lei Zhang",
        "user": "Lemoncoke",
        "type": "user"
      },
      "summary": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in Test-Driven Development (TDD). Unlike existing software engineering data that rely on human-submitted issues, **SWE-Flow** automatically infers incremental development steps directly from unit tests, which inherently encapsulate high-level requirements. The core of **SWE-Flow** is the construction of a Runtime Dependency Graph (RDG), which precisely captures function interactions, enabling the generation of a structured, step-by-step *development schedule*. At each step, **SWE-Flow** produces a partial codebase, the corresponding unit tests, and the necessary code modifications, resulting in fully verifiable TDD tasks. With this approach, we generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, creating the **SWE-Flow-Eval** benchmark. Our experiments show that fine-tuning open model on this dataset significantly improves performance in TDD-based coding. To facilitate further research, we release all code, datasets, models, and Docker images at [Github](https://github.com/Hambaobao/SWE-Flow).",
      "upvotes": 13,
      "discussionId": "6848eed842e4f9106973f2d8",
      "githubRepo": "https://github.com/Hambaobao/SWE-Flow",
      "ai_summary": "A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.",
      "ai_keywords": [
        "Test-Driven Development (TDD)",
        "Runtime Dependency Graph (RDG)",
        "SWE-Flow",
        "unit tests",
        "development schedule",
        "SWE-Flow-Eval",
        "fine-tuning",
        "open model"
      ]
    },
    "publishedAt": "2025-06-10T13:23:33.000Z",
    "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner",
    "summary": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in\nTest-Driven Development (TDD). Unlike existing software engineering data that\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\ndevelopment steps directly from unit tests, which inherently encapsulate\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\nenabling the generation of a structured, step-by-step *development schedule*.\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\ntests, and the necessary code modifications, resulting in fully verifiable TDD\ntasks. With this approach, we generated 16,061 training instances and 2,020\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\nbenchmark. Our experiments show that fine-tuning open model on this dataset\nsignificantly improves performance in TDD-based coding. To facilitate further\nresearch, we release all code, datasets, models, and Docker images at\n[Github](https://github.com/Hambaobao/SWE-Flow).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09003.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64c38871f9cd765462fa1a17",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg",
      "fullname": "Lei Zhang",
      "name": "Lemoncoke",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09984",
      "authors": [
        {
          "_id": "684a49fa9b38e1e5a33a6884",
          "name": "Zhenzhi Wang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6885",
          "name": "Jiaqi Yang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6886",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6887",
          "name": "Chao Liang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6888",
          "name": "Gaojie Lin",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a6889",
          "name": "Zerong Zheng",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a688a",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "684a49fa9b38e1e5a33a688b",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:57:09.000Z",
      "submittedOnDailyAt": "2025-06-12T02:02:32.983Z",
      "title": "InterActHuman: 多概念ヒューマンアニメーションと並び順に合わせた音声\n条件",
      "submittedOnDailyBy": {
        "_id": "6519346a186bc3b6997c1aaf",
        "avatarUrl": "/avatars/8981bd278962da50f9bbfb92c2abe2bf.svg",
        "isPro": false,
        "fullname": "Zhenzhi Wang",
        "user": "zhenzhiwang",
        "type": "user"
      },
      "summary": "終端から終端までの豊富な多モーダル条件（例：テキスト、画像、音声）を用いた人間アニメーションは、近年にわたって驚異的な進歩を遂げています。しかし、現在の多数の方法は、シングルエンティティのアニメーションを行い、条件をグローバル的な方法で注入し、同じビデオで複数の概念が出現し、豊富な人間間の相互作用と人間・物体間の相互作用を無視しています。このようなグローバル的な仮定は、複数の概念（これには人間や物体も含まれる）の精密な、プロジェクトワイドフィールドごとの制御を妨げ、アプリケーションを妨げています。本稿では、シングルエンティティの仮定を捨て、新しいフレームワークを導入し、モーダルからの条件を各イデンティティの空間時間的な跡跡に強い、領域特有の結合を強制します。複数の概念の参照画像を与えると、我々の方法は、マスクプロデャターを利用して、デノイズ画像と各参照外観の顔色コードをマッチすることで自動的にレイアウト情報を推定します。また、ロカルな音声条件を対応する領域に注入し、複数の概念のレイアウトに一致するモーダルマッチングを進めるために、進行することです。この設計は、制御可能な多概念の人間中心的なビデオの高品質の生成を可能にします。実験結果と消滅研究は、我々の明示的なレイアウト制御の有效性を示し、他の既存の方法と比較して、豊富な多モーダル条件の比較的な効果性を証明します。",
      "upvotes": 9,
      "discussionId": "684a49fa9b38e1e5a33a688c",
      "ai_summary": "A new framework enables precise, per-identity control of multiple concepts in end-to-end human animation by enforcing region-specific binding of multi-modal conditions.",
      "ai_keywords": [
        "human animation",
        "multi-modal conditions",
        "mask predictor",
        "denoised video",
        "layout information",
        "local audio condition",
        "controllable multi-concept videos",
        "explicit layout control"
      ]
    },
    "publishedAt": "2025-06-11T13:57:09.000Z",
    "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions",
    "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09984.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6519346a186bc3b6997c1aaf",
      "avatarUrl": "/avatars/8981bd278962da50f9bbfb92c2abe2bf.svg",
      "fullname": "Zhenzhi Wang",
      "name": "zhenzhiwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09937",
      "authors": [
        {
          "_id": "684a3d639b38e1e5a33a686d",
          "name": "Qiao Gu",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a686e",
          "name": "Yuanliang Ju",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a686f",
          "name": "Shengxiang Sun",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6870",
          "name": "Igor Gilitschenski",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6871",
          "name": "Haruki Nishimura",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6872",
          "name": "Masha Itkina",
          "hidden": false
        },
        {
          "_id": "684a3d639b38e1e5a33a6873",
          "name": "Florian Shkurti",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T16:59:13.000Z",
      "submittedOnDailyAt": "2025-06-12T01:08:05.216Z",
      "title": "SAFE: 視覚・言語・行動モデルの複数タスク失敗検出",
      "submittedOnDailyBy": {
        "_id": "63d1df92f7f31a66a2d7292c",
        "avatarUrl": "/avatars/eb3339c1f2c82742b518b8a7f142e99a.svg",
        "isPro": false,
        "fullname": "Qiao Gu",
        "user": "guqiao",
        "type": "user"
      },
      "summary": "ビジョン-言語-アクションモデル（VLAs）は、多様な操作タスクにおいて有望なロボットの行動を示していますが、新しいタスクでの実装において成功率が限られています。これらの政策が安全に環境と相互作用することを許すために、ロボットが停止、後退し、または助けを求めることを可能にするために、失敗検出器が必要です。しかし、現在の失敗検出器は、VLAsが要求するような、未見のタスクや新しい環境での失敗を検出することを可能にするように一般化することができていません。本論文では、多タスク失敗検出問題を導入し、VLAsなどの一般的なロボット政策に適した失敗検出器SAFEを提案します。VLAの特徴空間を分析し、VLAsは、タスクの成功と失敗に関する高レベルの知識が、異なるタスクにも共通していることを見出しました。この見解に基づいて、SAFEはVLAの内部特徴から学習し、タスクの失敗の可能性を示す単一のスカラーを予測するように設計されました。SAFEは成功したものと失敗したものの両方で訓練され、未見のタスクで評価されます。SAFEは、異なる政策アーキテクチャとの相容性があります。OpenVLA、pi_0、pi_0-FASTの両方で、記述的や実世界の環境で厳密にテストされました。SAFEは、多様なベースラインと比較し、コンフォームプロダクションを用いて精度と検出時間のバランスとして最先端の失敗検出性能を達成しました。詳細な質的な結果は、https://vla-safe.github.io/ から確認できます。",
      "upvotes": 3,
      "discussionId": "684a3d639b38e1e5a33a6874",
      "ai_summary": "SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.",
      "ai_keywords": [
        "vision-language-action models",
        "VLAs",
        "multitask failure detection",
        "failure detector",
        "feature space",
        "high-level knowledge",
        "scalar prediction",
        "rollout",
        "conformal prediction"
      ]
    },
    "publishedAt": "2025-06-11T12:59:13.000Z",
    "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
    "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, pi_0,\nand pi_0-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09937.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d1df92f7f31a66a2d7292c",
      "avatarUrl": "/avatars/eb3339c1f2c82742b518b8a7f142e99a.svg",
      "fullname": "Qiao Gu",
      "name": "guqiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08001",
      "authors": [
        {
          "_id": "684a649f9b38e1e5a33a6895",
          "name": "Zeju Qiu",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6896",
          "name": "Simon Buchholz",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6897",
          "name": "Tim Z. Xiao",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6898",
          "name": "Maximilian Dax",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a6899",
          "name": "Bernhard Schölkopf",
          "hidden": false
        },
        {
          "_id": "684a649f9b38e1e5a33a689a",
          "name": "Weiyang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:59:34.000Z",
      "submittedOnDailyAt": "2025-06-12T03:55:47.829Z",
      "title": "正交等價変換による再定義化LLMトレーニング",
      "submittedOnDailyBy": {
        "_id": "648905d1a15c43c791d4381f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
        "isPro": false,
        "fullname": "Weiyang Liu",
        "user": "wy1iu",
        "type": "user"
      },
      "summary": "ラージー ラングジュージャーモデル（LLMs）が人工知能の急速な進歩を駆動している中、これらの大規模なモデルの効果的かつ信頼性のある訓練は、分野の最大の課題の一つです。この課題に対処するために、私たちは、正交等価変換を用いた新しい再パラメタ化訓練アルゴリズムを提案します。POET。特に、POETは、2つの学習可能な正交行列と固定的な乱数重み行列を用いて各ニューロンを再パラメタ化します。このような設計で、POETは重み行列のスペクトル特性を証明的に保存することで、目的関数の安定した最適化と一般化性能の向上を実現できます。また、POETの柔軟性とスケーラビリティを増強するために、効率的な近似を開発しました。拡散した実験は、POETがLLMsの訓練における効果とスケーラビリティを証明しました。",
      "upvotes": 2,
      "discussionId": "684a649f9b38e1e5a33a689b",
      "projectPage": "https://spherelab.ai/poet/",
      "githubRepo": "https://github.com/Sphere-AI-Lab/poet",
      "ai_summary": "POET is a reParameterized training algorithm using Orthogonal Equivalence Transformation to optimize neurons in large language models, ensuring stable training and improved generalization.",
      "ai_keywords": [
        "POET",
        "reParameterized training algorithm",
        "Orthogonal Equivalence Transformation",
        "orthogonal matrices",
        "spectral properties",
        "weight matrices",
        "large language models",
        "generalization",
        "efficient approximations",
        "training",
        "scalability"
      ]
    },
    "publishedAt": "2025-06-09T13:59:34.000Z",
    "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
    "summary": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648905d1a15c43c791d4381f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
      "fullname": "Weiyang Liu",
      "name": "wy1iu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08900",
      "authors": [
        {
          "_id": "684a96ab9aebf043cf7bdb2e",
          "name": "José Morano",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb2f",
          "name": "Botond Fazekas",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb30",
          "name": "Emese Sükei",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb31",
          "name": "Ronald Fecso",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb32",
          "name": "Taha Emre",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb33",
          "name": "Markus Gumpinger",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb34",
          "name": "Georg Faustmann",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb35",
          "name": "Marzieh Oghbaie",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb36",
          "name": "Ursula Schmidt-Erfurth",
          "hidden": false
        },
        {
          "_id": "684a96ab9aebf043cf7bdb37",
          "name": "Hrvoje Bogunović",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655b3383ed8df831286969f0/FuwagzdL2fBNhZyfwg22J.png"
      ],
      "publishedAt": "2025-06-10T15:25:55.000Z",
      "submittedOnDailyAt": "2025-06-12T07:31:36.181Z",
      "title": "MIRAGE: 多モデルの基盤モデルと総合的な視网膜OCT画像分析ベンチマーク",
      "submittedOnDailyBy": {
        "_id": "655b3383ed8df831286969f0",
        "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
        "isPro": false,
        "fullname": "José Morano",
        "user": "j-morano",
        "type": "user"
      },
      "summary": "人工知能（AI）は、臨床医による眼科画像の分析に役立つ基本的なツールとして機能しています。光学コヒェリックタポグラフィー（OCT）などの眼科画像を分析するためのツールですが、AIモデルの開発は広範囲のアノテーションが必要で、独立した、未見たデータに対しては性能が低くなる傾向があります。基盤モデル（FM）は、大きなAIモデルであり、大規模な無ラベルデータセットを用いて訓練されているもので、これらの挑戦に対して優れた性能を示しています。しかし、現在の眼科用FMは、特に分割タスクにおいて極めて詳細な検証が不足し、一つの画像モデールだけに焦点を当てています。このような背景の下で、私たちはMIRAGE、OCTとスキャンニングレーザーオフトハノミー（SLO）画像の分析に向けた新しい多モデルFMを提案します。また、OCT/SLO分類と分割タスクに向けた新しい評価ベンチマークも提案します。一般的なモデルと専門的なFMや分割手法との比較により、MIRAGEは両種のタスクで優れた性能を示し、視网膜のOCT画像分析に向けた強固なAIシステムの開発において基盤となることを明らかにします。MIRAGEと評価ベンチマークは公開的に利用可能です：https://github.com/j-morano/MIRAGE。",
      "upvotes": 1,
      "discussionId": "684a96ab9aebf043cf7bdb38",
      "githubRepo": "https://github.com/j-morano/MIRAGE",
      "ai_summary": "MIRAGE, a multimodal foundation model, outperforms existing models in the classification and segmentation of OCT and SLO images, demonstrating its potential for robust AI in ophthalmologic image analysis.",
      "ai_keywords": [
        "foundational models",
        "multimodal models",
        "optical coherence tomography",
        "scanning laser ophthalmoscopy",
        "image segmentation",
        "evaluation benchmark"
      ]
    },
    "publishedAt": "2025-06-10T11:25:55.000Z",
    "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive\n  retinal OCT image analysis",
    "summary": "Artificial intelligence (AI) has become a fundamental tool for assisting\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\n(OCT). However, developing AI models often requires extensive annotation, and\nexisting models tend to underperform on independent, unseen data. Foundation\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\npromise in overcoming these challenges. Nonetheless, available FMs for\nophthalmology lack extensive validation, especially for segmentation tasks, and\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\nclassification and segmentation tasks. The comparison with general and\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\nboth types of tasks, highlighting its suitability as a basis for the\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\nand the evaluation benchmark are publicly available:\nhttps://github.com/j-morano/MIRAGE.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655b3383ed8df831286969f0/FuwagzdL2fBNhZyfwg22J.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655b3383ed8df831286969f0",
      "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
      "fullname": "José Morano",
      "name": "j-morano",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.09007",
      "authors": [
        {
          "_id": "6849516c42e4f9106973f4d1",
          "name": "Sophia Tang",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d2",
          "name": "Yinuo Zhang",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d3",
          "name": "Alexander Tong",
          "hidden": false
        },
        {
          "_id": "6849516c42e4f9106973f4d4",
          "name": "Pranam Chatterjee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T17:29:48.000Z",
      "submittedOnDailyAt": "2025-06-12T01:32:31.298Z",
      "title": "分枝シュリンガーブリッジマッチング",
      "submittedOnDailyBy": {
        "_id": "64cd5b3f0494187a9e8b7c69",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
        "isPro": false,
        "fullname": "Pranam Chatterjee",
        "user": "pranamanam",
        "type": "user"
      },
      "summary": "初期とターゲット分布の間の中間的なトラジェクトリーを予測するのは生成モデリングの中心的な問題です。現在のアプローチでは、フローマッチングやシンデローガーブリッヂマッチングなど、2つの分布間のマッピングを学習するために、1つの確率パスをモデル化する方法が効果的です。しかし、これらの方法は本来的に一つのモードの遷移に限定され、一つの共通の起源から複数の異なる結果への枝分けや分岐的な進化を捉えることはできません。これを解決するために、BranchSBM（ブランチシンデローガーブリッヂマッチング）を紹介します。BranchSBMは、枝分けシンデローガーブリッヂを学習するための新しいフレームワークで、時間依存性のある速度フィールドと成長プロセスを複数パラメーター化し、人口レベルの分岐を複数の終端分布にまとめることができます。BranchSBMは、多パス表面ナビゲーション、均一な先祖状態からの細胞遺産分岐モデリング、および摂動に対する細胞反応の分岐をシミュレートすることで、より表現力のあるものであり、これらのタスクには不可欠です。",
      "upvotes": 0,
      "discussionId": "6849516d42e4f9106973f4d5",
      "ai_summary": "BranchSBM, a novel generative modeling framework, extends Schr\\\"odinger Bridge Matching to model branched stochastic paths and multi-path evolution from a single initial distribution to multiple outcomes.",
      "ai_keywords": [
        "flow matching",
        "Schr\\\"odinger Bridge Matching",
        "Branched Schr\\\"odinger Bridge Matching",
        "BranchSBM",
        "time-dependent velocity fields",
        "growth processes",
        "multi-path surface navigation",
        "cell fate bifurcations",
        "cellular responses to perturbations"
      ]
    },
    "publishedAt": "2025-06-10T13:29:48.000Z",
    "title": "Branched Schrödinger Bridge Matching",
    "summary": "Predicting the intermediate trajectories between an initial and target\ndistribution is a central problem in generative modeling. Existing approaches,\nsuch as flow matching and Schr\\\"odinger Bridge Matching, effectively learn\nmappings between two distributions by modeling a single stochastic path.\nHowever, these methods are inherently limited to unimodal transitions and\ncannot capture branched or divergent evolution from a common origin to multiple\ndistinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge\nMatching (BranchSBM), a novel framework that learns branched Schr\\\"odinger\nbridges. BranchSBM parameterizes multiple time-dependent velocity fields and\ngrowth processes, enabling the representation of population-level divergence\ninto multiple terminal distributions. We show that BranchSBM is not only more\nexpressive but also essential for tasks involving multi-path surface\nnavigation, modeling cell fate bifurcations from homogeneous progenitor states,\nand simulating diverging cellular responses to perturbations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09007.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cd5b3f0494187a9e8b7c69",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
      "fullname": "Pranam Chatterjee",
      "name": "pranamanam",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]