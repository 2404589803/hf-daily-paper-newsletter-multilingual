[
  {
    "paper": {
      "id": "2504.15279",
      "authors": [
        {
          "_id": "68070d3b5035e6d88636ae13",
          "user": {
            "_id": "649e5ee29420f68cf1c1470e",
            "avatarUrl": "/avatars/7f6d1ec4fb3f85351e88044016d8ab42.svg",
            "isPro": false,
            "fullname": "Xu Wayen",
            "user": "wilye",
            "type": "user"
          },
          "name": "Weiye Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:11:19.332Z",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae14",
          "user": {
            "_id": "664b4a748dd1bfb5a3a970fe",
            "avatarUrl": "/avatars/37aa9332ab3e8fbb6ae30b875a7e0e5a.svg",
            "isPro": false,
            "fullname": "Jiahao Wang",
            "user": "GenuineWWD",
            "type": "user"
          },
          "name": "Jiahao Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:11:17.358Z",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae15",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae16",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae17",
          "name": "Wengang Zhou",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae18",
          "name": "Aijun Yang",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae19",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1a",
          "name": "Houqiang Li",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1b",
          "name": "Xiaohua Wang",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1c",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1d",
          "name": "Wenhai Wang",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1e",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "68070d3b5035e6d88636ae1f",
          "name": "Jinguo Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:59:53.000Z",
      "submittedOnDailyAt": "2025-04-24T01:22:54.990Z",
      "title": "ビジュロジック：多モーダル大語言モデルでの可視化理由論評価のベンチマーク",
      "submittedOnDailyBy": {
        "_id": "664b4a748dd1bfb5a3a970fe",
        "avatarUrl": "/avatars/37aa9332ab3e8fbb6ae30b875a7e0e5a.svg",
        "isPro": false,
        "fullname": "Jiahao Wang",
        "user": "GenuineWWD",
        "type": "user"
      },
      "summary": "視覚推理は、人間の知能の核心成分であり、高度な多モーダルモデルの重要な能力です。しかし、現在の多モーダル大語言モデル（MLLMs）の推理評価は、通常、文章の説明を基にしていて、言語ベースの推理のスロットチップを許容し、本質的な視覚中心的な推理を測定できません。これに対処し、私たちはVisuLogicを紹介します：6カテゴリ（例えば、数値の移動、空間関係、属性の比較）にわたる1,000問の人間認証プロブラムです。これらの様々なクラスの質問は、MLLMsの視覚推理能力を評価するために、多角的な視点から評価できます。このベンチマークで、私たちは先進的なMLLMsを評価し、その結果を分析し、共通の失敗モードを特定します。多くのモデルは30%の正確率を下回り、それは25%のランダムベースラインをちょっと上回り、51.4%の人間の成績よりも遠く低いことを示し、視覚推理には大きな間違いがあることを明らかにします。また、私たちは、進歩を支援するために、追加の訓練データセットと強化学習ベースラインを提供します。",
      "upvotes": 46,
      "discussionId": "68070d3f5035e6d88636af56",
      "projectPage": "https://visulogic-benchmark.github.io/VisuLogic/",
      "githubRepo": "https://github.com/VisuLogic-Benchmark/VisuLogic-Eval",
      "ai_keywords": [
        "Visual reasoning",
        "multimodal large language models (MLLMs)",
        "text descriptions",
        "language-based reasoning shortcuts",
        "genuine vision-centric reasoning",
        "VisuLogic",
        "human-verified problems",
        "quantitative shifts",
        "spatial relations",
        "attribute comparisons",
        "supplementary training dataset",
        "reinforcement-learning baseline"
      ]
    },
    "publishedAt": "2025-04-21T13:59:53.000Z",
    "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models",
    "summary": "Visual reasoning is a core component of human intelligence and a critical\ncapability for advanced multimodal models. Yet current reasoning evaluations of\nmultimodal large language models (MLLMs) often rely on text descriptions and\nallow language-based reasoning shortcuts, failing to measure genuine\nvision-centric reasoning. To address this, we introduce VisuLogic: a benchmark\nof 1,000 human-verified problems across six categories (e.g., quantitative\nshifts, spatial relations, attribute comparisons). These various types of\nquestions can be evaluated to assess the visual reasoning capabilities of MLLMs\nfrom multiple perspectives. We evaluate leading MLLMs on this benchmark and\nanalyze their results to identify common failure modes. Most models score below\n30% accuracy-only slightly above the 25% random baseline and far below the\n51.4% achieved by humans-revealing significant gaps in visual reasoning.\nFurthermore, we provide a supplementary training dataset and a\nreinforcement-learning baseline to support further progress.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15279.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "664b4a748dd1bfb5a3a970fe",
      "avatarUrl": "/avatars/37aa9332ab3e8fbb6ae30b875a7e0e5a.svg",
      "fullname": "Jiahao Wang",
      "name": "GenuineWWD",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.14509",
      "authors": [
        {
          "_id": "6809dd092e04f68a3f5baa66",
          "name": "Fulong Ye",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa67",
          "name": "Miao Hua",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa68",
          "name": "Pengze Zhang",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa69",
          "name": "Xinghui Li",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa6a",
          "name": "Qichao Sun",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa6b",
          "name": "Songtao Zhao",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa6c",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "6809dd092e04f68a3f5baa6d",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-20T06:53:00.000Z",
      "submittedOnDailyAt": "2025-04-24T05:26:05.811Z",
      "title": "ドリーミッド：高品質と高速なディフュージョンベースの顔交換を実現する三重タイプIDグループ学習法",
      "submittedOnDailyBy": {
        "_id": "6339029a76421c0543167075",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6339029a76421c0543167075/3npT0NxTMV-MLWDThyBy8.png",
        "isPro": false,
        "fullname": "fulong ye",
        "user": "Alon77777",
        "type": "user"
      },
      "summary": "この論文では、IDの類似性、属性の保存、画像の忠実度、高速な推論速度を実現するための、拡散基底の顔交換モデル「DreamID」を紹介します。通常の顔交換の訓練プロセスと異なり、隠れたサブプロバイジングを依存し、満足している結果を達成することが難しい。DreamIDは、Triplet ID Groupデータを構築して明示的なサブプロバイジングを実現し、顔の類似性と属性の保存を大幅に向上させます。拡散モデルのイテレーション的な性質は、画像空間の損失関数の効率的な利用に課題をもたらし、訓練中に生成画像を得るための複数ステップの取り組みが実用的であることが難しい。この問題を解決するために、SD Turboという加速された拡散モデルを利用し、推論ステップを1ステップに抑え、明示的なTriplet ID Groupサブプロバイジングを伴う効率的なピクセルレベルの終端から終端までの訓練を可能にします。また、SwapNet、FaceNet、ID Adapterを構成する改善された拡散基底のモデルアーキテクチャを提案します。この強固なアーキテクチャは、Triplet ID Groupの明示的なサブプロバイジングの力を完全に解放します。最後に、方法の拡張を進めるために、訓練中にTriplet ID Groupデータを明示的に変更し、特に眼鏡や顔の形状などの特定の属性を微調節し、保存することを目的とします。拡張的な実験は、IDの類似性、姿勢と表情の保存、画像の忠実度において最先端の方法を超えることを示します。全体として、DreamIDは512*512の解像度で0.6秒で高品質の顔交換の結果を実現し、複雑な照明、大きな角度、遮蔽などの難しいシナリオでも特に優れた動作を示します。",
      "upvotes": 26,
      "discussionId": "6809dd102e04f68a3f5babf5",
      "projectPage": "https://superhero-7.github.io/DreamID/",
      "githubRepo": "https://github.com/superhero-7/DreamID",
      "ai_keywords": [
        "diffusion-based model",
        "Triplet ID Group",
        "diffusion models",
        "image-space loss functions",
        "SD Turbo",
        "SwapNet",
        "FaceNet",
        "ID Adapter",
        "face swapping",
        "explicit supervision",
        "identity similarity",
        "attribute preservation",
        "image fidelity",
        "pose preservation",
        "expression preservation",
        "high-quality face swapping"
      ]
    },
    "publishedAt": "2025-04-20T02:53:00.000Z",
    "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via\n  Triplet ID Group Learning",
    "summary": "In this paper, we introduce DreamID, a diffusion-based face swapping model\nthat achieves high levels of ID similarity, attribute preservation, image\nfidelity, and fast inference speed. Unlike the typical face swapping training\nprocess, which often relies on implicit supervision and struggles to achieve\nsatisfactory results. DreamID establishes explicit supervision for face\nswapping by constructing Triplet ID Group data, significantly enhancing\nidentity similarity and attribute preservation. The iterative nature of\ndiffusion models poses challenges for utilizing efficient image-space loss\nfunctions, as performing time-consuming multi-step sampling to obtain the\ngenerated image during training is impractical. To address this issue, we\nleverage the accelerated diffusion model SD Turbo, reducing the inference steps\nto a single iteration, enabling efficient pixel-level end-to-end training with\nexplicit Triplet ID Group supervision. Additionally, we propose an improved\ndiffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.\nThis robust architecture fully unlocks the power of the Triplet ID Group\nexplicit supervision. Finally, to further extend our method, we explicitly\nmodify the Triplet ID Group data during training to fine-tune and preserve\nspecific attributes, such as glasses and face shape. Extensive experiments\ndemonstrate that DreamID outperforms state-of-the-art methods in terms of\nidentity similarity, pose and expression preservation, and image fidelity.\nOverall, DreamID achieves high-quality face swapping results at 512*512\nresolution in just 0.6 seconds and performs exceptionally well in challenging\nscenarios such as complex lighting, large angles, and occlusions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.14509.png",
    "numComments": 7,
    "submittedBy": {
      "_id": "6339029a76421c0543167075",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6339029a76421c0543167075/3npT0NxTMV-MLWDThyBy8.png",
      "fullname": "fulong ye",
      "name": "Alon77777",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.15431",
      "authors": [
        {
          "_id": "680879ead6dc8bf64565c975",
          "name": "Sungjun Han",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c976",
          "user": {
            "_id": "6138cc1306dd10833d2db64b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6138cc1306dd10833d2db64b/IRX4y-8M4YlzR_8jOwkKp.jpeg",
            "isPro": false,
            "fullname": "Juyoung Suk",
            "user": "scottsuk0306",
            "type": "user"
          },
          "name": "Juyoung Suk",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:08:21.257Z",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c977",
          "name": "Suyeong An",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c978",
          "name": "Hyungguk Kim",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c979",
          "name": "Kyuseok Kim",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c97a",
          "name": "Wonsuk Yang",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c97b",
          "user": {
            "_id": "6257adfdb98dcaa7e0de7ab4",
            "avatarUrl": "/avatars/ddfc2135104895d09cfce0cd6f10e5fb.svg",
            "isPro": false,
            "fullname": "Seungtaek Choi",
            "user": "hist0613",
            "type": "user"
          },
          "name": "Seungtaek Choi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:11:03.864Z",
          "hidden": false
        },
        {
          "_id": "680879ead6dc8bf64565c97c",
          "name": "Jamin Shin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T20:54:44.000Z",
      "submittedOnDailyAt": "2025-04-24T01:09:32.264Z",
      "title": "Trillion 7B 技術報告",
      "submittedOnDailyBy": {
        "_id": "6138cc1306dd10833d2db64b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6138cc1306dd10833d2db64b/IRX4y-8M4YlzR_8jOwkKp.jpeg",
        "isPro": false,
        "fullname": "Juyoung Suk",
        "user": "scottsuk0306",
        "type": "user"
      },
      "summary": "テキストを日本語に翻訳します。",
      "upvotes": 18,
      "discussionId": "680879ebd6dc8bf64565c9bb",
      "ai_keywords": [
        "Trillion-7B",
        "Cross-lingual Document Attention (XLDA)",
        "language-specific filtering",
        "tailored tokenizer construction",
        "multilingual data",
        "multilingual performance",
        "cross-lingual consistency"
      ]
    },
    "publishedAt": "2025-04-21T16:54:44.000Z",
    "title": "Trillion 7B Technical Report",
    "summary": "We introduce Trillion-7B, the most token-efficient Korean-centric\nmultilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)\nmechanism enables highly efficient and effective knowledge transfer from\nEnglish to target languages like Korean and Japanese. Combined with optimized\ndata mixtures, language-specific filtering, and tailored tokenizer\nconstruction, Trillion-7B achieves competitive performance while dedicating\nonly 10\\% of its 2T training tokens to multilingual data and requiring just\n59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations\nacross 27 benchmarks in four languages demonstrate Trillion-7B's robust\nmultilingual performance and exceptional cross-lingual consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15431.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6138cc1306dd10833d2db64b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6138cc1306dd10833d2db64b/IRX4y-8M4YlzR_8jOwkKp.jpeg",
      "fullname": "Juyoung Suk",
      "name": "scottsuk0306",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15843",
      "authors": [
        {
          "_id": "6809948944114def75aaeb7d",
          "name": "Junshu Pan",
          "hidden": false
        },
        {
          "_id": "6809948944114def75aaeb7e",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6809948944114def75aaeb7f",
          "name": "Shulin Huang",
          "hidden": false
        },
        {
          "_id": "6809948944114def75aaeb80",
          "name": "Qiji Zhou",
          "hidden": false
        },
        {
          "_id": "6809948944114def75aaeb81",
          "name": "Yue Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T12:39:30.000Z",
      "submittedOnDailyAt": "2025-04-24T00:02:36.679Z",
      "title": "DPO前：直接偏好最適化におけるデータ利用の向上について\nガイドリンクモデルを用いて",
      "submittedOnDailyBy": {
        "_id": "6468823272d9180d4ac90bdf",
        "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
        "isPro": false,
        "fullname": "Wei Shen",
        "user": "Swtheking",
        "type": "user"
      },
      "summary": "Direct Preference Optimization (DPO)は、大規模言語モデル（LLMs）の強化学習（RLHF）を人間のフィードバックに直接対応することで簡素化します。DPOの学習過程では、参照モデルはデータの重み調整器の役割を果たします。しかし、DPOで政策モデルと参照モデルを同じように初期化することは、データの効率的な利用を妨げ、性能の天井を課します。一方、Simple Preference Optimization（SimPO）では参照モデルのない状態で学習を行うことで、学習の安定性が低下し、認知的忘却を防ぐために厳しい条件が必要となります。本論文では、Guiding Reference Modelを利用したDPO基盤の効果的な学習パラダイム「Pre-DPO」を提案します。この参照モデルは、学習フィードバックデータから最適な政策状態を予測し、モデルに合適なデータに対して高い重みを、モデルに適しないデータに対して低い重みを割り当てるように対応する指導機能を提供します。AlpacaEval 2.0とArena-Hard v0.1ベンチマークの拡張試験において、Pre-DPOはDPOとSimPOの両方の性能を一貫的に向上させ、外部モデルや追加データを依存せずに効果的です。",
      "upvotes": 12,
      "discussionId": "6809948a44114def75aaebab",
      "ai_keywords": [
        "reinforcement learning from human feedback (RLHF)",
        "large language models (LLMs)",
        "Direct Preference Optimization (DPO)",
        "human preferences",
        "reference model",
        "data weight adjuster",
        "Simple Preference Optimization (SimPO)",
        "catastrophic forgetting",
        "Pre-DPO",
        "guiding reference model",
        "AlpacaEval 2.0",
        "Arena-Hard v0.1"
      ]
    },
    "publishedAt": "2025-04-22T08:39:30.000Z",
    "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
    "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15843.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6468823272d9180d4ac90bdf",
      "avatarUrl": "/avatars/70cb7d65d30ecb944595000ceeeedb1b.svg",
      "fullname": "Wei Shen",
      "name": "Swtheking",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16801",
      "authors": [
        {
          "_id": "6809bebd0f6dfd7bd5159b76",
          "user": {
            "_id": "6545f8922a2a483042ebc8b3",
            "avatarUrl": "/avatars/ab00da8aa841694f3f11093a9148e4c5.svg",
            "isPro": false,
            "fullname": "xiaoxing2001",
            "user": "xiaoxing2001",
            "type": "user"
          },
          "name": "Xiaoxing Hu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-24T04:32:01.063Z",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b77",
          "user": {
            "_id": "63e202f352b7578dba448ab5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
            "isPro": false,
            "fullname": "Yang",
            "user": "Kaichengalex",
            "type": "user"
          },
          "name": "Kaicheng Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:10:22.302Z",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b78",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b79",
          "name": "Haoran Xu",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b7a",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "6809bebd0f6dfd7bd5159b7b",
          "name": "Yupei Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T15:20:53.000Z",
      "submittedOnDailyAt": "2025-04-24T03:04:11.389Z",
      "title": "グローバル-ローカルの連携を分離して構成的理解を向上させる",
      "submittedOnDailyBy": {
        "_id": "63e202f352b7578dba448ab5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
        "isPro": false,
        "fullname": "Yang",
        "user": "Kaichengalex",
        "type": "user"
      },
      "summary": "Contrastive Language-Image Pre-training (CLIP)は、画像とテキストのモデルを調整して複数の次元タスクで成功しました。しかし、グローバル対比学習の性質により、CLIPは構成的な概念（例：関係と属性）を理解することができません。最近の研究は、グローバルの難しい負のサンプルを使用して構成的な理解を改善しようとしていますが、これらの方法は、モデルの固有の一般的な能力を強制的に画像とテキストの埋め込み空間から離れることで大きく補損します。この制限を克服するために、DeGLA（Decoupled Global-Local Alignment）フレームワークを導入します。これは、構成的な理解を改善しながら、一般的な能力の損失を大幅に減少することを目指しています。モデルの固有の能力を保ちつつ最適化するために、グローバル調整プロセス内で自動転熱機構を採用し、指数移動平均からのフローズンドティーチャーモデルから得られる学習可能な画像-テキストエンコーダーを調整します。自動転熱の制約の下で、ファインチューニング中の事前学習された知識のカタストロフィック忘却を有効に抑制します。構成的な理解を改善するために、まず、Large Language Models（LLMs）のコンテキスト学習能力を拡張して、5つの種類にわたる約2Mの高品質の負のカプチャーを構築します。次に、Image-Grounded Contrast（IGC）損失とText-Grounded Contrast（TGC）損失を提案し、視覚言語の構成的な理解を強化します。拡張的な実験結果は、DeGLAフレームワークの効果を示します。先行の最先端の方法と比較して、VALSE、SugarCrepe、AROベンチマークで平均的な効果向上率が3.5%を達成し、11データセットのゼロショット分類タスクで平均的な性能向上率が13.0%を達成しました。我々のコードは、https://github.com/xiaoxing2001/DeGLAで公開します。",
      "upvotes": 11,
      "discussionId": "6809bec10f6dfd7bd5159c38",
      "ai_keywords": [
        "Decoupled Global-Local Alignment (DeGLA)",
        "self-distillation mechanism",
        "learnable image-text encoder",
        "frozen teacher model",
        "exponential moving average",
        "catastrophic forgetting",
        "in-context learning",
        "Large Language Models (LLMs)",
        "high-quality negative captions",
        "Image-Grounded Contrast (IGC) loss",
        "Text-Grounded Contrast (TGC) loss",
        "vision-language compositionally",
        "VALSE",
        "SugarCrepe",
        "ARO benchmarks",
        "zero-shot classification tasks"
      ]
    },
    "publishedAt": "2025-04-23T11:20:53.000Z",
    "title": "Decoupled Global-Local Alignment for Improving Compositional\n  Understanding",
    "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16801.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.16929",
      "authors": [
        {
          "_id": "6809ba7976a4f4f7268546a7",
          "name": "Shaden Alshammari",
          "hidden": false
        },
        {
          "_id": "6809ba7976a4f4f7268546a8",
          "name": "John Hershey",
          "hidden": false
        },
        {
          "_id": "6809ba7976a4f4f7268546a9",
          "name": "Axel Feldmann",
          "hidden": false
        },
        {
          "_id": "6809ba7976a4f4f7268546aa",
          "name": "William T. Freeman",
          "hidden": false
        },
        {
          "_id": "6809ba7976a4f4f7268546ab",
          "name": "Mark Hamilton",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62dae3734398e21bf7f53443/MT5eBEJkF1u7uejE382FI.png"
      ],
      "publishedAt": "2025-04-23T17:59:01.000Z",
      "submittedOnDailyAt": "2025-04-24T02:45:09.509Z",
      "title": "I-Con: 表現学習の統合的フレームワーク",
      "submittedOnDailyBy": {
        "_id": "62dae3734398e21bf7f53443",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658512237806-noauth.jpeg",
        "isPro": false,
        "fullname": "Mark Hamilton",
        "user": "mhamilton723",
        "type": "user"
      },
      "summary": "代表学習の分野が広がりますとともに、問題の種類に対応する異なる損失関数が増えております。私たちは、機械学習の現代的な損失関数の大きな集合を一般化するために、1つの情報理論的な方程式を紹介します。特に、私たちは、2つの条件分布（サブジビューと学習された表現）の積分KLデバイジュランスを最小化するための機械学習の広いクラスの方法を示すフレームワークを紹介します。この視点では、クラスタリング、スペクトル方法、次元削減、対比的学習、および監督的学習の下にある隠れた情報幾何を明らかにします。このフレームワークは、文献の各所からの成功技術を組み合わせて新しい損失関数の開発を可能にします。私たちは、23種類以上のアプローチを結びつけるための証明を提供しますが、これらの理論的な結果を活用して、ImageNet-1Kの無サブジビュー分類で先進的な無サブジビュー画像クラスファイザーを作成し、前の先進的なレベルに+8%の向上を実現します。また、I-Conを使って、対比的表現学習者を改善するための原則的なデビアス処理法を得ることを示します。",
      "upvotes": 9,
      "discussionId": "6809ba7d76a4f4f72685478a",
      "ai_keywords": [
        "information-theoretic equation",
        "KL divergence",
        "conditional distributions",
        "supervisory representations",
        "learned representations",
        "information geometry",
        "clustering",
        "spectral methods",
        "dimensionality reduction",
        "contrastive learning",
        "supervised learning",
        "I-Con",
        "debiasing methods",
        "contrastive representation learners"
      ]
    },
    "publishedAt": "2025-04-23T13:59:01.000Z",
    "title": "I-Con: A Unifying Framework for Representation Learning",
    "summary": "As the field of representation learning grows, there has been a proliferation\nof different loss functions to solve different classes of problems. We\nintroduce a single information-theoretic equation that generalizes a large\ncollection of modern loss functions in machine learning. In particular, we\nintroduce a framework that shows that several broad classes of machine learning\nmethods are precisely minimizing an integrated KL divergence between two\nconditional distributions: the supervisory and learned representations. This\nviewpoint exposes a hidden information geometry underlying clustering, spectral\nmethods, dimensionality reduction, contrastive learning, and supervised\nlearning. This framework enables the development of new loss functions by\ncombining successful techniques from across the literature. We not only present\na wide array of proofs, connecting over 23 different approaches, but we also\nleverage these theoretical results to create state-of-the-art unsupervised\nimage classifiers that achieve a +8% improvement over the prior\nstate-of-the-art on unsupervised classification on ImageNet-1K. We also\ndemonstrate that I-Con can be used to derive principled debiasing methods which\nimprove contrastive representation learners.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62dae3734398e21bf7f53443/MT5eBEJkF1u7uejE382FI.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16929.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62dae3734398e21bf7f53443",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658512237806-noauth.jpeg",
      "fullname": "Mark Hamilton",
      "name": "mhamilton723",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.16915",
      "authors": [
        {
          "_id": "6809b14111003e54bd204d99",
          "name": "Chong Mou",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9a",
          "user": {
            "_id": "639709c2be8a14bb9eeea8f6",
            "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
            "isPro": false,
            "fullname": "Yanze Wu",
            "user": "yanze",
            "type": "user"
          },
          "name": "Yanze Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:10:24.617Z",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9b",
          "name": "Wenxu Wu",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9c",
          "name": "Zinan Guo",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9d",
          "name": "Pengze Zhang",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9e",
          "name": "Yufeng Cheng",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204d9f",
          "name": "Yiming Luo",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da0",
          "name": "Fei Ding",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da1",
          "name": "Shiwen Zhang",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da2",
          "name": "Xinghui Li",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da3",
          "name": "Mengtian Li",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da4",
          "name": "Songtao Zhao",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da5",
          "name": "Jian Zhang",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da6",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "6809b14111003e54bd204da7",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-23T17:41:44.000Z",
      "submittedOnDailyAt": "2025-04-24T02:18:39.286Z",
      "title": "DreamO: 画像カスタマイズー決まりの一つのフレームワーク",
      "submittedOnDailyBy": {
        "_id": "639709c2be8a14bb9eeea8f6",
        "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
        "isPro": false,
        "fullname": "Yanze Wu",
        "user": "yanze",
        "type": "user"
      },
      "summary": "最近、画像のカスタマイズ（例：アイデンティティ、主題、スタイル、背景など）に関する拡大研究は、大規模な生成モデルにおける強力なカスタマイズ能力を示しています。しかし、多数のアプローチは特定のタスクに向けて設計されており、異なるタイプの条件を統合する機能性を制限しています。画像のカスタマイズに関する一連のフレームワークの開発は開放的な課題として残されています。この論文では、広範囲のタスクをサポートし、複数の条件の無間違いな統合を促進する画像カスタマイズフレームワークを紹介します。具体的には、DreamOは異なるタイプの入力を一貫的に処理するために、diffusion transformer (DiT) フレームワークを利用しています。訓練期間には、様々なカスタマイズタスクを含む大規模な訓練データセットを構築し、参照画像から関連する情報の決綁を促進するために特徴ローティング制約を導入しています。また、特定のポストハンドルを条件と関連づけるステージプランティスを設計し、生成データの条件の配置を制御することを可能にしています。また、低品質データによる品質バイアスの補正を目的とした3段階の進歩的な訓練ステージを導入しています。広範囲の実験により、提案されたDreamOは高品質で柔軟な統合を可能に、異なるタイプの制御条件を実現するための多様な画像カスタマイズタスクを効果的に行うことを示しています。",
      "upvotes": 7,
      "discussionId": "6809b14411003e54bd204e51",
      "projectPage": "https://mc-e.github.io/project/DreamO/",
      "githubRepo": "https://github.com/bytedance/DreamO",
      "ai_keywords": [
        "diffusion transformer (DiT)",
        "feature routing constraint",
        "placeholder strategy",
        "progressive training strategy",
        "baseline consistency",
        "customization capabilities",
        "quality alignment stage"
      ]
    },
    "publishedAt": "2025-04-23T13:41:44.000Z",
    "title": "DreamO: A Unified Framework for Image Customization",
    "summary": "Recently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce a feature routing constraint to\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design a placeholder strategy that associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ a\nprogressive training strategy consisting of three stages: an initial stage\nfocused on simple tasks with limited data to establish baseline consistency, a\nfull-scale training stage to comprehensively enhance the customization\ncapabilities, and a final quality alignment stage to correct quality biases\nintroduced by low-quality data. Extensive experiments demonstrate that the\nproposed DreamO can effectively perform various image customization tasks with\nhigh quality and flexibly integrate different types of control conditions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.16915.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639709c2be8a14bb9eeea8f6",
      "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
      "fullname": "Yanze Wu",
      "name": "yanze",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 139
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15585",
      "authors": [
        {
          "_id": "6809c1f389b7cade55b32a6c",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a6d",
          "name": "Guibin Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a6e",
          "name": "Zhenhong Zhou",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a6f",
          "name": "Jiahao Wu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a70",
          "name": "Miao Yu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a71",
          "name": "Shiqian Zhao",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a72",
          "name": "Chenlong Yin",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a73",
          "name": "Jinhu Fu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a74",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a75",
          "name": "Hanjun Luo",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a76",
          "name": "Liang Lin",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a77",
          "name": "Zhihao Xu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a78",
          "name": "Haolang Lu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a79",
          "name": "Xinye Cao",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7a",
          "name": "Xinyun Zhou",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7b",
          "name": "Weifei Jin",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7c",
          "name": "Fanci Meng",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7d",
          "name": "Junyuan Mao",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7e",
          "name": "Hao Wu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a7f",
          "name": "Minghe Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a80",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a81",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a82",
          "name": "Chengwei Liu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a83",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a84",
          "name": "Qiankun Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a85",
          "name": "Chongye Guo",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a86",
          "name": "Yalan Qin",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a87",
          "name": "Yi Ding",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a88",
          "name": "Donghai Hong",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a89",
          "name": "Jiaming Ji",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8a",
          "name": "Xinfeng Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8b",
          "name": "Yifan Jiang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8c",
          "name": "Dongxia Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8d",
          "name": "Yihao Huang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8e",
          "name": "Yufei Guo",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a8f",
          "name": "Jen-tse Huang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a90",
          "name": "Yanwei Yue",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a91",
          "name": "Wenke Huang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a92",
          "name": "Guancheng Wan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a93",
          "name": "Tianlin Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a94",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a95",
          "name": "Jie Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a96",
          "name": "Qing Guo",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a97",
          "name": "Jingyi Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a98",
          "name": "Tianlong Chen",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a99",
          "name": "Joey Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9a",
          "name": "Xiaojun Jia",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9b",
          "name": "Weisong Sun",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9c",
          "name": "Cong Wu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9d",
          "name": "Jing Chen",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9e",
          "name": "Xuming Hu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32a9f",
          "name": "Yiming Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa0",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa1",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:10:18.968Z",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa2",
          "name": "Luu Anh Tuan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa3",
          "name": "Guowen Xu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa4",
          "name": "Tianwei Zhang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa5",
          "name": "Xingjun Ma",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa6",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa7",
          "name": "Bo An",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa8",
          "name": "Jun Sun",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aa9",
          "name": "Mohit Bansal",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aaa",
          "name": "Shirui Pan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aab",
          "name": "Yuval Elovici",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aac",
          "name": "Bhavya Kailkhura",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aad",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aae",
          "name": "Yaodong Yang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aaf",
          "name": "Hongwei Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab0",
          "name": "Wenyuan Xu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab1",
          "name": "Yizhou Sun",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab2",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab3",
          "name": "Qing Li",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab4",
          "name": "Ke Tang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab5",
          "name": "Yu-Gang Jiang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab6",
          "name": "Felix Juefei-Xu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab7",
          "name": "Hui Xiong",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab8",
          "name": "Xiaofeng Wang",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32ab9",
          "name": "Shuicheng Yan",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32aba",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32abb",
          "name": "Philip S. Yu",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32abc",
          "name": "Qingsong Wen",
          "hidden": false
        },
        {
          "_id": "6809c1f389b7cade55b32abd",
          "name": "Yang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T05:02:49.000Z",
      "submittedOnDailyAt": "2025-04-24T03:15:54.692Z",
      "title": "LLM(-Agent) フルスタック安全の全面的調査：データ、訓練、部署",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "LLMの驚異的な成功は、学術および工業的なコミュニティにとって、人工知能（AGI）の実現に向けての希望的なパスウェージを照らし出しています。LLMが多様なアプリケーションで前所未聞の実績を示しているためです。LLMは研究および商業領域での重要性が高まり、その安全性と安全性の影響は、研究者および企業だけでなく、各国にも増加しています。現在のLLM安全に関する調査は、LLMのライフサイクルの特定ステージだけを焦点としています（例えば、実装フェーズまたは微調校フェーズ），LLMの「ライフチェーン」全体の理解が欠けています。この空間を填ぐために、この論文は、LLMの学習、実装、最終的な商業化の全過程をシステマ的に考慮する「フルスタック」安全概念を初めて紹介します。現代のフレーブォールドのLLM安全調査と比較して、私たちの研究は以下の特徴的な優れた点を示しています： （I）厳密な視点。LLMのライフサイクル全体をデータ準備、予プライン、後プライン、実装、最終的な商業化に拡張し、これは、LLM全体のライフサイクルを含む最初の安全調査であると考えられます。 （II）広範囲の文献サポート。私たちの研究は800点以上の論文の厳密なレビューに基づいています、これにより、安全問題の全面的なカバーとシステム的な組織が保証されています。 （III）独自のエンサイン。システム的な文献分析を通じて、各章の信頼性のあるマップと視点を開発しました。私たちの研究は、データ生成の安全性、アラインメントテクニック、モデル編集、LLMベースのアガントシステムの安全性についての有望な研究方向を特定しています。これらのエンサインは、この分野の将来的な研究を追い求める研究者に宝貴なガイドになります。",
      "upvotes": 5,
      "discussionId": "6809c1f789b7cade55b32bf4"
    },
    "publishedAt": "2025-04-22T01:02:49.000Z",
    "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training\n  and Deployment",
    "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15585.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15777",
      "authors": [
        {
          "_id": "6808452d16c1c427ac727816",
          "user": {
            "_id": "67469d6a8407f929491dce06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
            "isPro": true,
            "fullname": "Shangshang Wang",
            "user": "upup-ashton-wang",
            "type": "user"
          },
          "name": "Shangshang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:11:10.259Z",
          "hidden": false
        },
        {
          "_id": "6808452d16c1c427ac727817",
          "name": "Julian Asilis",
          "hidden": false
        },
        {
          "_id": "6808452d16c1c427ac727818",
          "name": "Ömer Faruk Akgül",
          "hidden": false
        },
        {
          "_id": "6808452d16c1c427ac727819",
          "name": "Enes Burak Bilgin",
          "hidden": false
        },
        {
          "_id": "6808452d16c1c427ac72781a",
          "name": "Ollie Liu",
          "hidden": false
        },
        {
          "_id": "6808452d16c1c427ac72781b",
          "name": "Willie Neiswanger",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T10:38:00.000Z",
      "submittedOnDailyAt": "2025-04-24T06:51:34.876Z",
      "title": "Tina: リサイズドモデルによるティニーラボリング",
      "submittedOnDailyBy": {
        "_id": "67469d6a8407f929491dce06",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
        "isPro": true,
        "fullname": "Shangshang Wang",
        "user": "upup-ashton-wang",
        "type": "user"
      },
      "summary": "この英語の日本語翻訳は、コスト効率的な強い理由能力を達成するために、言語モデルでどれだけ効果的に達成できるかという基本的な質問に基づいて、Tinaという家族の小さな理由モデルを提出します。特に、Tinaは、強い理由性能を達成することができることを示し、強化学習（RL）のパラメータ効率的な更新（LoRA）を使用して、既に小さな150億パラメータのベースモデルに適用することで、最小限のリソースを使用することでも強い理由性能を達成できることを示します。この最小主義的なアプローチは、同じベースモデルを基にした現在のSOTAのRL理由モデルと比較して、理由性能が相対的に高く、時にはSOTAを超えるものを実現します。重要なことに、これは、現在のSOTAモデルが使用している計算費用の小さな割合で実現されています。実際には、最良のTinaモデルは、AIME24の理由性能が20％以上上がり、Pass@1の精度が43.33％を達成し、訓練と評価の計算費用は9ドルしかかからない（つまり、推定された260倍のコスト削減を実現します）。私たちの研究は、LoRAを用いた効率的なRL理由の驚異的な効果性を明らかにします。これは、複数のオープンソースの理由データセットと異なる消減設定で検証され、一つの固定したパラメーターセットを始めています。さらに、この効果性と効率性は、LoRAがRLによって賞められる理由の構造フォーマットに迅速にモデルを適応し、ベースモデルの潜在的な知識を大きく破壊しないように、LoRAがモデルを適応することから原因であると仮定しています。アクセス性と開放的な研究のために、すべてのコード、訓練ログ、モデル重みとチェックポイントを完全にオープンソースにしています。",
      "upvotes": 4,
      "discussionId": "6808452f16c1c427ac7278b1",
      "projectPage": "https://shangshangwang.notion.site/tina",
      "githubRepo": "https://github.com/shangshang-wang/Tina",
      "ai_keywords": [
        "parameter-efficient updates",
        "reinforcement learning (RL)",
        "low-rank adaptation (LoRA)",
        "traininng logs",
        "checkpoints"
      ]
    },
    "publishedAt": "2025-04-22T06:38:00.000Z",
    "title": "Tina: Tiny Reasoning Models via LoRA",
    "summary": "How cost-effectively can strong reasoning abilities be achieved in language\nmodels? Driven by this fundamental question, we present Tina, a family of tiny\nreasoning models achieved with high cost-efficiency. Notably, Tina demonstrates\nthat substantial reasoning performance can be developed using only minimal\nresources, by applying parameter-efficient updates during reinforcement\nlearning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B\nparameter base model. This minimalist approach produces models that achieve\nreasoning performance which is competitive with, and sometimes surpasses, SOTA\nRL reasoning models built upon the same base model. Crucially, this is achieved\nat a tiny fraction of the computational post-training cost employed by existing\nSOTA models. In fact, the best Tina model achieves a >20\\% reasoning\nperformance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD\npost-training and evaluation cost (i.e., an estimated 260x cost reduction). Our\nwork reveals the surprising effectiveness of efficient RL reasoning via LoRA.\nWe validate this across multiple open-source reasoning datasets and various\nablation settings starting with a single, fixed set of hyperparameters.\nFurthermore, we hypothesize that this effectiveness and efficiency stem from\nLoRA rapidly adapting the model to the structural format of reasoning rewarded\nby RL, while largely preserving the base model's underlying knowledge. In\nservice of accessibility and open research, we fully open-source all code,\ntraining logs, and model weights \\& checkpoints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15777.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67469d6a8407f929491dce06",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
      "fullname": "Shangshang Wang",
      "name": "upup-ashton-wang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15707",
      "authors": [
        {
          "_id": "6809f6b03e1d48a9bb7f5719",
          "user": {
            "_id": "650971dbce83a0c12a851000",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ec7H2cZpaZz07W02FT5U9.png",
            "isPro": false,
            "fullname": "Yannic Neuhaus",
            "user": "YanNeu",
            "type": "user"
          },
          "name": "Yannic Neuhaus",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:09:36.629Z",
          "hidden": false
        },
        {
          "_id": "6809f6b03e1d48a9bb7f571a",
          "name": "Matthias Hein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-22T08:47:59.000Z",
      "submittedOnDailyAt": "2025-04-24T07:02:13.584Z",
      "title": "RePOPE: アノテーションエラーの影響についてPOPEベンチマークに関する研究",
      "submittedOnDailyBy": {
        "_id": "650971dbce83a0c12a851000",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ec7H2cZpaZz07W02FT5U9.png",
        "isPro": false,
        "fullname": "Yannic Neuhaus",
        "user": "YanNeu",
        "type": "user"
      },
      "summary": "データアノテーションが費用がかかるため、標準的なデータセットは通常既存の画像データセットからラベルを採用しています。本論文では、MSCOCOにおけるラベルエラーの影響を、通常使用される物体の幻想ベンチマークPOPEにおいて評価します。ベンチマーク画像を再アノテートし、各サブセットでのアノテーションエラーの不均衡を識別しました。再アノテートされたラベルで評価したモデルをRePOPEとして、これらのラベルの品質の影響を明らかにしました。コードとデータは、https://github.com/YanNeu/RePOPE から利用可能です。",
      "upvotes": 4,
      "discussionId": "6809f6b13e1d48a9bb7f5790",
      "githubRepo": "https://github.com/YanNeu/RePOPE",
      "ai_keywords": [
        "MSCOCO",
        "object hallucination",
        "POPE",
        "RePOPE"
      ]
    },
    "publishedAt": "2025-04-22T04:47:59.000Z",
    "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark",
    "summary": "Since data annotation is costly, benchmark datasets often incorporate labels\nfrom established image datasets. In this work, we assess the impact of label\nerrors in MSCOCO on the frequently used object hallucination benchmark POPE. We\nre-annotate the benchmark images and identify an imbalance in annotation errors\nacross different subsets. Evaluating multiple models on the revised labels,\nwhich we denote as RePOPE, we observe notable shifts in model rankings,\nhighlighting the impact of label quality. Code and data are available at\nhttps://github.com/YanNeu/RePOPE .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15707.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650971dbce83a0c12a851000",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ec7H2cZpaZz07W02FT5U9.png",
      "fullname": "Yannic Neuhaus",
      "name": "YanNeu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11919",
      "authors": [
        {
          "_id": "6809ae47c6faa064f324307d",
          "user": {
            "_id": "652f979c61ce8120849bb72f",
            "avatarUrl": "/avatars/bac0c1697ef26ffbb455a8b59c16b783.svg",
            "isPro": false,
            "fullname": "Qianjin Yu",
            "user": "USTCYu",
            "type": "user"
          },
          "name": "Qianjin Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-24T09:10:26.728Z",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f324307e",
          "name": "Keyu Wu",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f324307f",
          "name": "Zihan Chen",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f3243080",
          "name": "Chushu Zhang",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f3243081",
          "name": "Manlin Mei",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f3243082",
          "name": "Lingjun Huang",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f3243083",
          "name": "Fang Tan",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f3243084",
          "name": "Yongsheng Du",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f3243085",
          "name": "Kunlin Liu",
          "hidden": false
        },
        {
          "_id": "6809ae47c6faa064f3243086",
          "name": "Yurui Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-16T09:55:34.000Z",
      "submittedOnDailyAt": "2025-04-24T08:12:14.834Z",
      "title": "リティディング・オブ・ハイ・クオリティ・コンテキスト・データの生成を、LLMに適合した質疑問の難易度の評価からして再考する",
      "submittedOnDailyBy": {
        "_id": "652f979c61ce8120849bb72f",
        "avatarUrl": "/avatars/bac0c1697ef26ffbb455a8b59c16b783.svg",
        "isPro": false,
        "fullname": "Qianjin Yu",
        "user": "USTCYu",
        "type": "user"
      },
      "summary": "最近、DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) は複雑なタスクでの優秀な議論能力を示し、公開的にその方法を共有しました。これは小さなサイズの大規模な言語モデル（LLMs）の議論能力を刺激するために潜在的に高品質なChain-of-Thought（CoT）データを提供します。さまざまなLLMsに適した高品質なCoTデータを生成するために、LLM-Adaptiveの問題難易度レベルでの高品質なCoTデータの生成方法を探しています。まず、LLMs自身の議論能力に応じて問題の難易度を評価し、LLM-Adaptiveの問題データベースを構築します。次に、問題データベースに基づいて問題の難易度レベルの分布によってサンプリングし、DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) を用いて正しい回答と共に高品質なCoTデータを生成します。LLM-Adaptiveの難易度レベルでのCoTデータの構築により、データ生成のコストを大幅に減少し、モデルのチャイナーフィーニング（SFT）の効率を向上させました。最後に、複雑な数学コンペティションおよびコード生成タスクの分野で提案された方法の効果性と一般化性能を評価しました。特に、2kの高品質な数学のCoTデータを使っても、我々のZMath-32Bは数学議論タスクでDeepSeek-Distill-32Bを超えました。同様に、2kの高品質なコードのCoTデータを使っても、我々のZCode-32Bはコード議論タスクでDeepSeek-Distill-32Bを超えました。",
      "upvotes": 3,
      "discussionId": "6809ae49c6faa064f32430d1",
      "ai_keywords": [
        "chain-of-thought (CoT) data",
        "LLM-Adaptive questiondifficulty levels",
        "LLM-Adaptive question database",
        "model supervised fine-tuning (SFT)"
      ]
    },
    "publishedAt": "2025-04-16T05:55:34.000Z",
    "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective\n  of LLM-Adaptive Question Difficulty Grading",
    "summary": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its\nexcellent reasoning ability in complex tasks and has publiclyshared its\nmethodology. This provides potentially high-quality chain-of-thought (CoT) data\nfor stimulating the reasoning abilities of small-sized large language models\n(LLMs). To generate high-quality CoT data for different LLMs, we seek an\nefficient method for generating high-quality CoT data with LLM-Adaptive\nquestiondifficulty levels. First, we grade the difficulty of the questions\naccording to the reasoning ability of the LLMs themselves and construct a\nLLM-Adaptive question database. Second, we sample the problem database based on\na distribution of difficulty levels of the questions and then use DeepSeek-R1\n(671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality\nCoT data with correct answers. Thanks to the construction of CoT data with\nLLM-Adaptive difficulty levels, we have significantly reduced the cost of data\ngeneration and enhanced the efficiency of model supervised fine-tuning (SFT).\nFinally, we have validated the effectiveness and generalizability of the\nproposed method in the fields of complex mathematical competitions and code\ngeneration tasks. Notably, with only 2k high-quality mathematical CoT data, our\nZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly,\nwith only 2k high-quality code CoT data, our ZCode-32B surpasses\nDeepSeek-Distill-32B in code reasoning tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11919.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "652f979c61ce8120849bb72f",
      "avatarUrl": "/avatars/bac0c1697ef26ffbb455a8b59c16b783.svg",
      "fullname": "Qianjin Yu",
      "name": "USTCYu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.15254",
      "authors": [
        {
          "_id": "6807bf3e70a0cec724b8a011",
          "user": {
            "_id": "6697abd4be7ce6de07140e72",
            "avatarUrl": "/avatars/1598decabd9975d755082c8fb4e5962d.svg",
            "isPro": false,
            "fullname": "Anirudh Khatry",
            "user": "anirudhkhatry",
            "type": "user"
          },
          "name": "Anirudh Khatry",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:30:04.897Z",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a012",
          "name": "Robert Zhang",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a013",
          "name": "Jia Pan",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a014",
          "name": "Ziteng Wang",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a015",
          "name": "Qiaochu Chen",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a016",
          "name": "Greg Durrett",
          "hidden": false
        },
        {
          "_id": "6807bf3e70a0cec724b8a017",
          "name": "Isil Dillig",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-21T17:33:33.000Z",
      "submittedOnDailyAt": "2025-04-24T05:15:24.237Z",
      "title": "CRUST-Bench: C-to-safe-Rust トランスパイションの完全なベンチマーク",
      "submittedOnDailyBy": {
        "_id": "6697abd4be7ce6de07140e72",
        "avatarUrl": "/avatars/1598decabd9975d755082c8fb4e5962d.svg",
        "isPro": false,
        "fullname": "Anirudh Khatry",
        "user": "anirudhkhatry",
        "type": "user"
      },
      "summary": "C-to-Rust トランスピレーションは、古いCコードを現代化するために重要であり、安全性と現代のRustエコシステムとのインターフェースの向上を実現するためにも重要です。しかし、現在のデータセットでは、システムがCを安全なRustにトランスピレーションできるかどうかを評価することができないものがありません。我々は、CRUST-Benchというデータセットを紹介します。CRUST-Benchは100コのCリポジトリを含み、それぞれに安全なRustで手動で書かれたインターフェースとテストケースが付随しています。これらのテストケースはトランスピレーションの正確性を確認することができます。CRUST-Benchは、インデップントの関数に焦点を当てているのではなく、ファイル間の依存関係を持つ複雑なプロジェクトの翻訳の課題を捉えています。提供されるRustインターフェースは、idiomaticな、メモリ安全なRustパターンに従うことを確保する明示的な規定を提供し、付随するテストケースは機能的正確性を強制しています。我々は、このタスクに最先端の大規模な言語モデル（LLMs）を評価し、安全なおよびidiomaticなRustの生成は、現在の最先端の方法と技術でも難しい問題であることを発見しました。また、LLMsがCから安全なRustにトランスピレーションする際に通常のエラーを説明します。最良の性能を示すモデルはOpenAI o1で、1ステップでのみ15タスクを解決できます。CRUST-Benchの改善は、複雑なシナリオを考慮したトランスピレーションシステムの改善により、Cからメモリ安全な言語（例えばRust）にコードベースの移行を助けることにも期待できます。データセットとコードは、https://github.com/anirudhkhatry/CRUST-benchから取得できます。",
      "upvotes": 1,
      "discussionId": "6807bf3f70a0cec724b8a044",
      "githubRepo": "https://github.com/anirudhkhatry/CRUST-bench"
    },
    "publishedAt": "2025-04-21T13:33:33.000Z",
    "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
    "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.15254.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6697abd4be7ce6de07140e72",
      "avatarUrl": "/avatars/1598decabd9975d755082c8fb4e5962d.svg",
      "fullname": "Anirudh Khatry",
      "name": "anirudhkhatry",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10419",
      "authors": [
        {
          "_id": "68026f762e2023f6cf7f0daa",
          "user": {
            "_id": "680268a7fd1fae58d58a2b49",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P96qvBdInKYnVxij7-MM_.png",
            "isPro": false,
            "fullname": "Michał Turski",
            "user": "mturski",
            "type": "user"
          },
          "name": "Michał Turski",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-18T15:27:52.927Z",
          "hidden": false
        },
        {
          "_id": "68026f762e2023f6cf7f0dab",
          "name": "Mateusz Chiliński",
          "hidden": false
        },
        {
          "_id": "68026f762e2023f6cf7f0dac",
          "name": "Łukasz Borchmann",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:06:59.000Z",
      "submittedOnDailyAt": "2025-04-24T03:55:04.111Z",
      "title": "無視されたチェックボックス：チェックボックスQAを用いて大規模な言語モデルのチェックボックスの視界に欠落を補う",
      "submittedOnDailyBy": {
        "_id": "680268a7fd1fae58d58a2b49",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P96qvBdInKYnVxij7-MM_.png",
        "isPro": false,
        "fullname": "Michał Turski",
        "user": "mturski",
        "type": "user"
      },
      "summary": "チェックボックスは、実世界的ドキュメント処理では重要な役割を果たし、チェックの有無がデータ抽出および決策プロセスに直接情報を与えています。しかし、Large Vision and Language Modelsが幅広いタスクで強力な性能を示すのに対して、チェックボックスの内容を解釈することが難しい問題があります。この課題は、1つのチェックボックスを間違って見落としてしまうと費用のかかる規制や契約の過失による産業に特に厳しくなります。この空間を填ぐために、私たちは、チェックボックス関連のタスクでのモデルの性能を評価し、向上させるためのデータセット「CheckboxQA」を紹介します。このデータセットは現在のモデルの限界を明らかにし、文書理解システムの進歩に役立つ有効なツールです。法律テクノロジーや資産管理などの分野においても重要な影響を及ぼします。\n\nデータセットは以下のURLで公開されています：\nhttps://github.com/Snowflake-Labs/CheckboxQA",
      "upvotes": 1,
      "discussionId": "68026f782e2023f6cf7f0e05"
    },
    "publishedAt": "2025-04-14T13:06:59.000Z",
    "title": "Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large\n  Language Models with CheckboxQA",
    "summary": "Checkboxes are critical in real-world document processing where the presence\nor absence of ticks directly informs data extraction and decision-making\nprocesses. Yet, despite the strong performance of Large Vision and Language\nModels across a wide range of tasks, they struggle with interpreting checkable\ncontent. This challenge becomes particularly pressing in industries where a\nsingle overlooked checkbox may lead to costly regulatory or contractual\noversights. To address this gap, we introduce the CheckboxQA dataset, a\ntargeted resource designed to evaluate and improve model performance on\ncheckbox-related tasks. It reveals the limitations of current models and serves\nas a valuable tool for advancing document comprehension systems, with\nsignificant implications for applications in sectors such as legal tech and\nfinance.\n  The dataset is publicly available at:\nhttps://github.com/Snowflake-Labs/CheckboxQA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "680268a7fd1fae58d58a2b49",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/P96qvBdInKYnVxij7-MM_.png",
      "fullname": "Michał Turski",
      "name": "mturski",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]