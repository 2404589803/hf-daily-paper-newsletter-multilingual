[
  {
    "paper": {
      "id": "2502.14776",
      "authors": [
        {
          "_id": "67bbdb46d94d32bcfba70db7",
          "name": "Xun Liang",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70db8",
          "name": "Jiawei Yang",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70db9",
          "user": {
            "_id": "662dd19f9e6d371ab71b91ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662dd19f9e6d371ab71b91ce/mZBPw_Zs8ZlEFGlbekAoH.jpeg",
            "isPro": false,
            "fullname": "Yezhaohui Wang",
            "user": "HaruTeru",
            "type": "user"
          },
          "name": "Yezhaohui Wang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-02-24T04:12:46.485Z",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dba",
          "name": "Chen Tang",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbb",
          "user": {
            "_id": "656f47ba2f058b368c0b1611",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656f47ba2f058b368c0b1611/mrmcmA8bxaDNUhuJQQ7T1.png",
            "isPro": false,
            "fullname": "Zifan Zheng",
            "user": "fan2goa1",
            "type": "user"
          },
          "name": "Zifan Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:07:22.303Z",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbc",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbd",
          "name": "Shichao Song",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbe",
          "user": {
            "_id": "669e0b93c7cb0568dac6e92e",
            "avatarUrl": "/avatars/a39ea77d7391f164af8a80f94f85f2ca.svg",
            "isPro": false,
            "fullname": "hanyu Wang",
            "user": "UglyToilet",
            "type": "user"
          },
          "name": "Hanyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:07:20.146Z",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dbf",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dc0",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dc1",
          "name": "Keming Mao",
          "hidden": false
        },
        {
          "_id": "67bbdb46d94d32bcfba70dc2",
          "name": "Zhiyu li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T17:59:45.000Z",
      "title": "SurveyX: 学術調査自動化によるラージーラングジュージングモデル",
      "summary": "大語言モデル（LLMs）は、例外的な理解能力と幅広い知識ベースを示し、自動調査生成の効率的なツールとして役立つことが示唆されています。しかし、最近の自動調査生成に関連する研究は、有限なコンテキストウィンドウ、深い内容議論の欠如、そして体系的な評価フレームワークの欠如などの重要な制限により制約されています。人間の書き込みプロセスにもっとも受け続けることをモチーフに、私たちは、調査の作成プロセスを2つのステップに分解し、「Preparation」と「Generation」ステップに基づく、効率的で有组织的なシステム「SurveyX」を提案します。オンラインリソース検索、前処理手法「AttributeTree」、そして再ポリションプロセスを創的に紹介することで、SurveyXは調査の作成の効果性を大幅に向上させます。実験的評価結果は、SurveyXが現在の自動調査生成システムを内容質（0.259の向上）と引用質（1.76の向上）の両方で超え、複数の評価次元で人間の専門家の性能に近づきます。SurveyXで生成された調査の例は、www.surveyx.cnによって提供されています。",
      "upvotes": 61,
      "discussionId": "67bbdb47d94d32bcfba70df3"
    },
    "publishedAt": "2025-02-23T21:39:54.375Z",
    "title": "SurveyX: Academic Survey Automation via Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14776.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "669e60ee8580d17cb60f8347",
      "avatarUrl": "/avatars/37963b833228afe39cc24854c9326670.svg",
      "fullname": "yang jiawei",
      "name": "Dany-0",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11663",
      "authors": [
        {
          "_id": "67b705d2ebee4662205c47f7",
          "name": "Jingcheng Ni",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47f8",
          "name": "Yuxin Guo",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47f9",
          "user": {
            "_id": "6572dcc6bbd6664053b1fa6b",
            "avatarUrl": "/avatars/aba29efd00bc41f14ce422f7807cd2c3.svg",
            "isPro": false,
            "fullname": "Liu Yichen",
            "user": "lyclyc52",
            "type": "user"
          },
          "name": "Yichen Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:23:40.466Z",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47fa",
          "name": "Rui Chen",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47fb",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "67b705d2ebee4662205c47fc",
          "user": {
            "_id": "65717368be66cd9b65a8201c",
            "avatarUrl": "/avatars/fe945828eec9ded4cfa3b89d48a64d90.svg",
            "isPro": false,
            "fullname": "Wu Zehuan",
            "user": "wzhgba",
            "type": "user"
          },
          "name": "Zehuan Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:59:38.956Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T10:53:56.000Z",
      "title": "MaskGWM: ビデオマスクを用いた一般化可能な駆動世界モデル",
      "summary": "世界模型は、行動から環境変化を予測するもので、強い一般化能力を持つ自動運転モデルにとって重要です。主流の駆転みモデルは、ビデオ予測モデルに基づいて構築されています。進歩的なディフュージョンベースのジェネレータを用いて高品質なビデオシーケンスを生成することができるものですが、予測時間と全体の一般化能力に制限があります。本論文では、この問題を解決するために、生成損失とMAEスタイルの特徴ベースのコンテキスト学習を組み合わせる方法を検討します。特に、この目標を実現するために、3つのキーデザインを採用します：1）ディフュージョンTransformer（DiT）構造を、額外的なマスク構成タスクで訓練されるように拡張します。2）マスク再構成と生成ディフュージョンプロセスの雑蔓関係を処理するために、ディフュージョン関連のマスクトークンを設計します。3）マスク構成タスクを空間時間領域に拡張し、行別のマスクを用いたシフトされたself-attentionを使用し、MAEのmasked self-attentionによるものを代わります。その後、このマスク設計に合わせて、行別のcross-viewモジュールを採用します。これらの改善に基づいて、Video Mask再構成を扱う一般化可能な駆転みモデルとして、MaskGWMを提案します。モデルは2つの変体を含みます：MaskGWM-long、長期予測に焦点を当てています。MaskGWM-mview、多角度生成に専門化しています。標準ベンチマーク上の詳細な実験は、Nusceneデータセットの通常の検証、OpenDV-2Kデータセットの長期予測、Waymoデータセットのzero-shot検証を含み、提案方法の効果を証明します。これらのデータセット上での定量的なメトリックは、我々の方法が最先端の駆転みモデルを顕著に向上させたことを示しています。",
      "upvotes": 33,
      "discussionId": "67b705d4ebee4662205c489c"
    },
    "publishedAt": "2025-02-24T01:16:03.517Z",
    "title": "MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11663.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65717368be66cd9b65a8201c",
      "avatarUrl": "/avatars/fe945828eec9ded4cfa3b89d48a64d90.svg",
      "fullname": "Wu Zehuan",
      "name": "wzhgba",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13449",
      "authors": [
        {
          "_id": "67b7ceae3e8a45f770b2606e",
          "user": {
            "_id": "65633c5e84a9fbe322f87d81",
            "avatarUrl": "/avatars/7233a555b43c669847a950ce5697c92c.svg",
            "isPro": false,
            "fullname": "DongkiKim",
            "user": "DongkiKim",
            "type": "user"
          },
          "name": "Dongki Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T09:59:11.214Z",
          "hidden": false
        },
        {
          "_id": "67b7ceae3e8a45f770b2606f",
          "name": "Wonbin Lee",
          "hidden": false
        },
        {
          "_id": "67b7ceae3e8a45f770b26070",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T05:49:10.000Z",
      "title": "Mol-LLaMA: 大分子モジュール言語モデルでの分子の一般的理解へ",
      "summary": "分子の理解は、生物体の理解と薬物発見の進歩において鍵となり、化学と生物学の間の専門的な知識を必要とする。大規模な分子の言語モデルは、分子構造の解釈に関して著しい成功を収めたが、その指示データセットは、特定の知識を含むタスクマイドデータセットから限られ、分子の基本的な特徴を完全に被覆しないことにより、一般的な分子アシスタントとしての能力を妨げている。この問題を解決するために、我々は、多タイプの指示調整により分子への一般的な知識を理解することを目指した大規模な分子の言語モデル、Mol-LLaMAを提案しています。ここでは、分子の基本的な特徴を含むキーデータタイプを設計し、分子構造からの重要な知識を組み込みます。また、分子の特徴の理解を向上させるために、分子の異なるエンコーダーからの補間情報を統合するモジュールを導入し、分子の異なる表現の特徴を活用します。実験結果によると、Mol-LLaMAは分子の一般的な特徴を理解し、ユーザーのクエリに関する関連するレスポンスを生成することができ、分子分析の一般的なアシスタントとしての可能性を示している。",
      "upvotes": 28,
      "discussionId": "67b7ceae3e8a45f770b2609f"
    },
    "publishedAt": "2025-02-23T21:52:51.059Z",
    "title": "Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13449.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65633c5e84a9fbe322f87d81",
      "avatarUrl": "/avatars/7233a555b43c669847a950ce5697c92c.svg",
      "fullname": "DongkiKim",
      "name": "DongkiKim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15007",
      "authors": [
        {
          "_id": "67bc1a4a72499ce2ba28cc70",
          "name": "Anton Razzhigaev",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc71",
          "name": "Matvey Mikhalchuk",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc72",
          "name": "Temurbek Rahmatullaev",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc73",
          "name": "Elizaveta Goncharova",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc74",
          "name": "Polina Druzhinina",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc75",
          "name": "Ivan Oseledets",
          "hidden": false
        },
        {
          "_id": "67bc1a4a72499ce2ba28cc76",
          "name": "Andrey Kuznetsov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T19:59:35.000Z",
      "title": "LLM-Microscope: コンテキストでの隠れた読点の役割を明らかにする\nTransformerの記憶",
      "summary": "ラーグ・ラングジャングルモデル（LLM）でコンテキスト情報をエンコードしてもらう方法を定量化し、タグクラスターが見えるように小さいもの（例：ディミナーター、記号）が驚くほどの高いコンテキストを持っていることを明らかにします。特に、これらのタグクラスターを除去すると、特にストップワード、アーティクル、コンマを除去するとMMLUとBABILong-4kでの性能が一貫して悪化しますが、これらのタグクラスターが関係ないものだけを除去するとも同様の結果を得ます。我々の分析は、コンテキスト化と線形性の間に強い関連性を示し、線形性は、一つの線形マッピングで近似できることの程度の変換の精度を測定します。これらの発見は、フィルタータグクラスターがコンテキストを維持するための隠れた重要性を強調します。進めるために、我々はLLM-Microscopeという開放ソースツールキットを提供します。このツールキットは、タグクラスターレベルの非線形性を評価し、コンテキストメモリを評価し、間接層の貢献を可視化（Logit Lensを改良したものを用いて）し、表現の固有次元数を測定します。このツールキットは、そのように見える裏技であるタグクラスターが長距離的理解に重要なものとなることを明らかにします。",
      "upvotes": 25,
      "discussionId": "67bc1a4c72499ce2ba28cd49"
    },
    "publishedAt": "2025-02-24T02:07:41.624Z",
    "title": "LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6172aaeec8e66e2aa84c06b9/ZPSmOQ-7Yd7B7YIYiwcTw.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15007.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6172aaeec8e66e2aa84c06b9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6172aaeec8e66e2aa84c06b9/ZdRZSp3P1SU6CIDbvQwkv.jpeg",
      "fullname": "Anton Razzhigaev",
      "name": "razzant",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14397",
      "authors": [
        {
          "_id": "67bbed806f2833ecccf914dd",
          "name": "Shijie Huang",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914de",
          "name": "Yiren Song",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914df",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914e0",
          "name": "Hailong Guo",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914e1",
          "name": "Xueyin Wang",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914e2",
          "name": "Mike Zheng Shou",
          "hidden": false
        },
        {
          "_id": "67bbed806f2833ecccf914e3",
          "name": "Jiaming Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T09:35:38.000Z",
      "title": "PhotoDoodle: 少フレームのペアワイズデータから芸術的な画像編集を学ぶ",
      "summary": "PhotoDoodleは、写真上にデコレーションエレメントを重ねることで画像編集を手助けする新しいフレームワークです。写真ドゥードリングは、挿入された要素が背景と無難に統一されることが難しいので、実感的なブレンディング、ペンセル調整、コンテキストの一致性が必要です。また、背景は変形せずに保存され、アーティストの独自のスタイルは限られた訓練データから適切に捉える必要があります。これらの要求は、主にグローバルスタイルトランスファーや地域的なインプレイングに焦点を当てた先行方法では解決されていません。提案方法では、PhotoDoodleは2段階の訓練戦略を用います。最初に、大規模なデータを用いて一般的な画像編集モデル、OmniEditorを訓練します。次に、EditLoRAを用いて、小さなアーティストが編集された画像ペアのアーティストが編集されたデータセットでこのモデルを微調節し、異なる編集スタイルと技術を捉えます。生成された結果の一貫性を向上させるために、位置データの再利用機構を導入します。また、6つの高品質スタイルを挙げるPhotoDoodleデータセットを公開します。拡張した実験は、カスタマイズ画像編集での先進的な性能と強固性を示し、芸術的な創作の新しい可能性を開拓します。",
      "upvotes": 22,
      "discussionId": "67bbed856f2833ecccf915c5"
    },
    "publishedAt": "2025-02-23T22:55:04.409Z",
    "title": "PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14397.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14922",
      "authors": [
        {
          "_id": "67bbe4ba79e0a705cf573985",
          "name": "Zihao Zeng",
          "hidden": false
        },
        {
          "_id": "67bbe4ba79e0a705cf573986",
          "name": "Xuyao Huang",
          "hidden": false
        },
        {
          "_id": "67bbe4ba79e0a705cf573987",
          "name": "Boxiu Li",
          "hidden": false
        },
        {
          "_id": "67bbe4ba79e0a705cf573988",
          "name": "Zhijie Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T17:38:46.000Z",
      "title": "SIFT: コンテキストによるLLM推論の基礎化（スティックカードを用いて）",
      "summary": "この論文では、大規模言語モデルの理由過程中にカンテキストの誤解が重要な問題となることを識別します。これは、Llama3.2-3B-InstructやDeepSeek-R1といった先端モデルを含む小さなモデルから始まり、先端モデルまで幅広く範囲を広げています。例えば、「10ドル/キロ」という表現では、LLMsは「per」が「それぞれ」を意味することを認識しないことがあり、計算エラーを起こすことがあります。この問題に対して、新しい後学習アプローチを紹介しています。これは**Stick to the Facts (SIFT)**という名前をつけています。SIFTは、推論時の計算量を増やしてカンテキストに基づくLLMの理由を強化します。SIFTの核心は、モデル自身が生成した**スタッカー**です。スタッカーは、カンテキスト内の重要な情報を明記してエフエクティブに強調します。スタッカーが与えられた場合、SIFTは、元のクエリからの予測とスタッカーを追加したクエリからの予測を生成します。これらが異なる場合、スタッカーは、*forward*オプティマイズ（抽出された事実とクエリをより良く合わせること）と*inverse*生成（モデルの固有の傾向に合わせること）を通じて順次改善されます。これにより、信頼性の高い理由結果を得ることができます。多様なモデル（3Bから100B+）とベンチマーク（GSM8K、MATH-500など）での研究は、一貫した性能向上を示しています。特に、SIFTはDeepSeek-R1のAIME2024のpass@1精度を78.33%から**85.67%**に向上させ、オープンソースコミュニティで新たな最先端となりました。コードは、https://github.com/zhijie-group/SIFT から利用できます。",
      "upvotes": 13,
      "discussionId": "67bbe4bb79e0a705cf5739c3"
    },
    "publishedAt": "2025-02-23T22:17:18.309Z",
    "title": "SIFT: Grounding LLM Reasoning in Contexts via Stickers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14922.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6193
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12084",
      "authors": [
        {
          "_id": "67b8922ef6632327952ec1e1",
          "user": {
            "_id": "65d8b0f0661492b25c6623de",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d8b0f0661492b25c6623de/c6LPDse8NIV_3BHIu8dYe.png",
            "isPro": false,
            "fullname": "Jianshu Zhang",
            "user": "Sterzhang",
            "type": "user"
          },
          "name": "Jianshu Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-21T14:48:16.643Z",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e2",
          "user": {
            "_id": "64b0377121a001042bc0d274",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b0377121a001042bc0d274/Hk8yI5_s7ey5o9SVZzXrB.png",
            "isPro": false,
            "fullname": "Dongyu Yao",
            "user": "RainJamesY",
            "type": "user"
          },
          "name": "Dongyu Yao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:20:43.528Z",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e3",
          "name": "Renjie Pi",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e4",
          "name": "Paul Pu Liang",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e5",
          "name": "Yi R.",
          "hidden": false
        },
        {
          "_id": "67b8922ef6632327952ec1e6",
          "name": "Fung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T17:57:50.000Z",
      "title": "VLM^2-Bench: ビジュアルモデルの隠れた視覚カットと明示的なマッチングの間の関係をより詳しく見る",
      "summary": "ビジュアル的にマッチングカテガをリンクする能力は日常生活では非常に重要です。例えば、複数の写真から同じ人を識別することができるように、カテガを基にしています。このような基本的な仕事について、視覚言語モデル（VLMs）が何らかの能力を持っていることは知られていますが、これらのモデルがこの仕事を行うことができるかどうかは大きく不明でした。これに対して、VLM^2-Benchというベンチマークを紹介します。これは、VLMsがビジュアル的にマッチングカテガをリンクする能力を評価するために設計されています。このベンチマークには9つのサブタスクと3,000点以上のテストケースが含まれています。\n\n8つのオープンソースのVLMsとGPT-4oを含む8つのモデルに対して、詳細な評価を行い、さらに言語側と視覚側のプロンプティング方法の分析を加えて、合計8つの重要な発見が得られました。モデルがビジュアルカテガをリンクする能力における重要な課題を明らかにし、GPT-4oはその性能においては人間よりも34.80%遅れていることが明らかになりました。これらのフィードバックに基づき、次のことを主張します。モデルの核心的な視覚能力を向上させ、先行知識の依存性を減らすこと、視覚中心的なタスクで言語ベースの理由を統一し、必要なバイアスを防ぐこと、さらには視覚カテガの関係を独立に構築し、推論するモデルの能力を育成するための視覚文脈のトレーニングパラダイムを変更すること。",
      "upvotes": 12,
      "discussionId": "67b89230f6632327952ec27a"
    },
    "publishedAt": "2025-02-24T00:36:34.341Z",
    "title": "VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12084.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65d8b0f0661492b25c6623de",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d8b0f0661492b25c6623de/c6LPDse8NIV_3BHIu8dYe.png",
      "fullname": "Jianshu Zhang",
      "name": "Sterzhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15589",
      "authors": [
        {
          "_id": "67bbfe2d670ece8d9184f339",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33a",
          "name": "Yuqi Zhu",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33b",
          "name": "Mengshu Sun",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33c",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33d",
          "user": {
            "_id": "6447800f30fa4ecb85ddad80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6447800f30fa4ecb85ddad80/NsmXIaMsWctmTNA7tFVkX.jpeg",
            "isPro": false,
            "fullname": "Shuofei Qiao",
            "user": "GoooDte",
            "type": "user"
          },
          "name": "Shuofei Qiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:07:02.722Z",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33e",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f33f",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f340",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67bbfe2d670ece8d9184f341",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:07:04.794Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T16:57:22.000Z",
      "title": "LightThinker: ステップごとの思考に基づく圧縮",
      "summary": "大語言モデル（LLMs）は複雑な理由論タスクで驚異的な性能を示していますが、長いトークンの生成に伴う大きなメモリと計算コストにより効率が低下しています。本論文では、LLMsが理由論の際に動的に中間的な考えを圧縮できる新しい方法を提案します。人間の認知プロセスによりヒントを得、LightThinkerは冗長な考えステップを簡略化した表現に変換し、元の理由論チェーンを捨て、コンテキストウィンドウ内のトークン数を大幅に減少させます。これは、データ構築、隠れ状態を縮約したトークンにマッピング、特設のアタションマスクの作成を通じてモデルを学習させることにより実現されます。また、生成中の歴史トークンの依存関係を測定するデペンデンシー（Dep）メトリックを導入します。4つのデータセットと2つのモデルにおける拡張的な実験は、LightThinkerが高峰メモリ使用量と推論時間を減少させ、精度を維持することを示します。私たちの研究は、複雑な理由論タスクでLLMsの効率向上を実現する新しい方向を提供します。コードは、https://github.com/zjunlp/LightThinker でリリースされます。",
      "upvotes": 12,
      "discussionId": "67bbfe2f670ece8d9184f3a4"
    },
    "publishedAt": "2025-02-24T00:07:05.804Z",
    "title": "LightThinker: Thinking Step-by-Step Compression",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/dhGMWf_tcPkvQlRm5DbD6.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.14494",
      "authors": [
        {
          "_id": "67b9dda03593f69f41cdb5d3",
          "name": "Jinnan Li",
          "hidden": false
        },
        {
          "_id": "67b9dda03593f69f41cdb5d4",
          "name": "Jinzhe Li",
          "hidden": false
        },
        {
          "_id": "67b9dda03593f69f41cdb5d5",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "67b9dda03593f69f41cdb5d6",
          "name": "Yi Chang",
          "hidden": false
        },
        {
          "_id": "67b9dda03593f69f41cdb5d7",
          "name": "Yuan Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T12:22:18.000Z",
      "title": "StructFlowBench: マルチターンインストラクションの構造化フローベンチマーク",
      "summary": "多転換指令追踪能力は、実世界のアプリケーションでの大規模言語モデル（LLMs）の核心的な能力である。現在の評価ベンチマークは、細かい制約満足と領域特有の能力評価を中心にしていますが、多転換と単転換の違いを示す重要な構造的関係を無視しています。この構造的関係は、ユーザーの意図を反映し、制約満足よりも評価の2次元を構築します。このギャップを解決するために、私たちはStructFlowBenchを提案します。StructFlowBenchは構造的なフローモデリングを挟む多転換指令追踪ベンチマークです。このベンチマークは、6つの基本的な転換間の関係を構成する構造的なフローフレームワークを創新的に定義し、モデル評価に新しい構造的な制約を導入し、特定のスキャンダーに合わせたカスタマイズされたダイアログフローの生成パラメータとして役立ちます。既存のLLMベースの自動評価方法を採用し、13つの先進的な開放ソースとクローズドソースのLLMsにシステマティックな評価を実施しました。実験結果から、現在のモデルが多転換ダイアログ構造の理解に欠点があることが明らかになりました。コードは、https://github.com/MLGroupJLU/StructFlowBenchにアクセスできます。",
      "upvotes": 10,
      "discussionId": "67b9dda13593f69f41cdb635"
    },
    "publishedAt": "2025-02-23T23:43:43.529Z",
    "title": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14494.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "670e57b3391f1a7021182bff",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/N0tuHZVz8KFPCv8G1qUX2.png",
      "fullname": "Yuan Wu",
      "name": "WhiteCatY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15027",
      "authors": [
        {
          "_id": "67bbdcec79fcd85f09ddd869",
          "name": "Henry Hengyuan Zhao",
          "hidden": false
        },
        {
          "_id": "67bbdcec79fcd85f09ddd86a",
          "name": "Wenqi Pei",
          "hidden": false
        },
        {
          "_id": "67bbdcec79fcd85f09ddd86b",
          "name": "Yifei Tao",
          "hidden": false
        },
        {
          "_id": "67bbdcec79fcd85f09ddd86c",
          "name": "Haiyang Mei",
          "hidden": false
        },
        {
          "_id": "67bbdcec79fcd85f09ddd86d",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T20:27:06.000Z",
      "title": "InterFeedback: 大規模多模態モデルの相互作用知能を人間の反饋から明らかにする",
      "summary": "現在のベンチマークは、大規模な多モデル（LMM）が人間ユーザーとの相互作用の知能を測定していません。これは一般用のAIアシスタントの開発に必要な要素です。私たちは、どのLMMやデータセットにも適用可能なインタラクティブなフレームワーク「InterFeedback」を設計しました。これにより、この能力を自動的に評価することができます。また、私たちは、代表的なデータセットMMU-ProとMathVerseを使用してインタラクティブな知能を評価する「InterFeedback-Bench」を導入しました。これは10種類の開放ソースのLMMを測定するために使用されます。また、私たちは、OpenAI-o1やClaude-3.5-Sonnetなどの先進モデルのインタラクティブな実行を手動的に測定するために新規に収集した120ケースのデータセット「InterFeedback-Human」を提供します。私たちの評価結果から、状態の最先端のLMM（例えばOpenAI-o1）は、人間のフィードバックを通じて結果を修正することが50%未満でできることがわかりました。私たちの発見は、LMMの知能を理解し、フィードバックから利益を得ることができるような方法が必要とすることを示しています。",
      "upvotes": 4,
      "discussionId": "67bbdced79fcd85f09ddd8da"
    },
    "publishedAt": "2025-02-23T21:44:33.443Z",
    "title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15027.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6193
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15657",
      "authors": [
        {
          "_id": "67bbfd6c3593f69f41512d54",
          "name": "Yoshua Bengio",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d55",
          "name": "Michael Cohen",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d56",
          "name": "Damiano Fornasiere",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d57",
          "name": "Joumana Ghosn",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d58",
          "name": "Pietro Greiner",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d59",
          "name": "Matt MacDermott",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5a",
          "name": "Sören Mindermann",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5b",
          "name": "Adam Oberman",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5c",
          "name": "Jesse Richardson",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5d",
          "name": "Oliver Richardson",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5e",
          "name": "Marc-Antoine Rondeau",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d5f",
          "name": "Pierre-Luc St-Charles",
          "hidden": false
        },
        {
          "_id": "67bbfd6c3593f69f41512d60",
          "name": "David Williams-King",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T18:28:36.000Z",
      "title": "超智能アガントが大災害的リスクを帯びている：科学者AIは安全な道を提供できるか？",
      "summary": "先進的AI企業は、一般的なAIアガントの構築に注力を加えています。これらのシステムは、人間が行うほとんどすべてのタスクにおいて自動的に計画、行動し、目標を追い求めることができるシステムです。これらのシステムは、様々なリスクを見出していますが、それらは、悪意のあるエージェントによる不適用、または人間の制御の不可逆的な失われなど、公共安全と安全に大きなリスクを見出しています。現在のAIの訓練方法からこれらのリスクが起源することを議論します。確かに、様々なスケーナーと実験は、AIアガントが偽装を行うか、人間操作者が指定したものと異なる目標を追い求めるか、人間の利益と矛盾するものを追い求める可能性を示しています。警戒原則に基づいて、現在のアガント駆動の軌道に代わり、安全で有用なものを選択する必要があることを議論します。そのために、我々は、より安全で信頼できるAIシステムの開発を核心的なビルディングブロックとして提案します。これを「Scientist AI」と呼び、人間がその世界を理解することを目的として、行動を取らずに観察から世界を解釈するシステムです。このシステムは、データを説明する理論を生成するワールドモデルと、質問から答える推論機で構成されています。両方のコンポーネントは、過度自信の予測のリスクを軽減するために明記した不確実性の概念を持っています。これらの考慮に基づいて、Scientist AIは、科学研究の加速において人間研究者を支援することができ、特にAI安全も含むことができます。特に、我々のシステムは、AIアガントのリスクを軽減するためのガードラインとして利用できます。最終的に、非アガント的なAIの焦点を当てることは、AI革新の利益を得る同時に、現在の軌道に伴うリスクを避けることができることを示します。我々は、これらの議論が研究者、開発者、政策制定者にこの安全な道を好ませるように動機を与えることを望む。",
      "upvotes": 3,
      "discussionId": "67bbfd6c3593f69f41512d96"
    },
    "publishedAt": "2025-02-24T00:02:52.495Z",
    "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15657.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 63
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15631",
      "authors": [
        {
          "_id": "67bbdbe8ea3003f47f15d036",
          "name": "Marthe Ballon",
          "hidden": false
        },
        {
          "_id": "67bbdbe8ea3003f47f15d037",
          "name": "Andres Algaba",
          "hidden": false
        },
        {
          "_id": "67bbdbe8ea3003f47f15d038",
          "name": "Vincent Ginis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T17:59:13.000Z",
      "title": "理由と性能の関係についての大型言語モデル -- o3 (mini)は長く考えなくても強く考えることができる",
      "summary": "大語言モデルは、連想コンテキストとテスト時の計算スケーリングを利用して数學的論理に顕著な進歩を示しています。しかし、論理トークンの使用と精度の向上の間の相互作用については多くの問題が残っています。特に、モデルの世代間で比較すると、改善された性能が長い論理連想やより効率的な論理から来たものであるかがよくわかりません。そこで、Omni-MATHベンチマークでo1-miniとo3-miniのバージョンの連想長さを系統的に分析し、o3-mini (m)は長い論理連想が必要なくても、o1-miniよりも上位の精度を達成できることを見出しました。また、全モデルと計算設定で、論理連想が長くなると精度が一般的に下がることを示し、問題の難易度を調節しても同様の結果が得られます。この精度の低下は、より専門的なモデルではより小さくなり、テスト時の計算量をより効率的に使用することが新しい世代の論理モデルの特徴です。最後に、o3-mini (h)はo3-mini (m)より微妙な精度の向上を達成し、それは全問題により多くの論理トークンを割り当てることで実現され、o3-mini (m)が既に解ける問題も含めても同様です。これらの発見は、モデルの能力と論理長との関係について新しい見解を提供し、効率、スケーリングと評価方法に影響を及ぼします。",
      "upvotes": 3,
      "discussionId": "67bbdbefea3003f47f15d226"
    },
    "publishedAt": "2025-02-23T21:40:17.216Z",
    "title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15631.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6193
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.14905",
      "authors": [
        {
          "_id": "67bbe0520aabd5d571a723e7",
          "name": "Bhavik Agarwal",
          "hidden": false
        },
        {
          "_id": "67bbe0520aabd5d571a723e8",
          "name": "Ishan Joshi",
          "hidden": false
        },
        {
          "_id": "67bbe0520aabd5d571a723e9",
          "name": "Viktoria Rojkova",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T16:44:55.000Z",
      "title": "JSON内で考える：厳密なLLMシンプル準拠の強化戦略",
      "summary": "この論文では、大規模言語モデル（LLM）の生成に厳密なシンプレックスの遵守を強制する挑戦を解決するために、LLMの理由能力を活用して実装した。DeepSeek R1の強化学習フレームワークを基に、我々のアプローチは、合成的な理由データセットの構築とグループ相対的ポリシー最適化（GRPO）のカスタム報酬関数を組み合わせた新しいパイプラインを用いて、15億パラメータモデルの構造化された理由スキルを学習させます。特に、最初に、20,000サンプルの無構造から構造化されたデータセットに対してR1強化学習を行い、元のDeepSeek R1の方法をミラーして核心的な理由能力を確立します。次に、別の10,000サンプルの理由データセットに対して規範制御での訓練を行い、ダウンストリームタスクのシンプレックスの遵守を改良します。相対的に軽い訓練範囲でも、GRPOの訓練には8xH100GPUクラスタで約20時間、SFTには1xA100で3時間が必要ですが、我々のモデルはシンプレックスの一致性を強制するために強固な性能を示します。我々のThinkJSONアプローチを元のDeepSeek R1（671B）、DeepSeek R1の給電版（Qwen-1.5BとQwen-7B）、およびGemini 2.0 Flash（70B）と比較し、実世界的なアプリケーションでの効果性を示します。我々の結果は、リソース効率的なフレームワークの実用的な有用性を強調します。",
      "upvotes": 2,
      "discussionId": "67bbe0530aabd5d571a72437"
    },
    "publishedAt": "2025-02-23T22:11:17.789Z",
    "title": "Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.14905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6193
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.15681",
      "authors": [
        {
          "_id": "67bbe67c7727595ca5979d2a",
          "name": "Yilun Xu",
          "hidden": false
        },
        {
          "_id": "67bbe67c7727595ca5979d2b",
          "name": "Weili Nie",
          "hidden": false
        },
        {
          "_id": "67bbe67c7727595ca5979d2c",
          "name": "Arash Vahdat",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-21T18:59:20.000Z",
      "title": "1ステップディフュージョンモデルにおけるf-Divergence分布の一致",
      "summary": "ディフュージョンモデルからサンプリングは、実用的な機能を達成するために長時間の進行するイテレーション的なプロセスであり、特にインタラクティブなアプリケーションにおいて実装が難しい。生成速度を加速するために、最近のアプローチは、多ステップディフュージョンモデルを単一ステップの学生ジェネレーターにビニアルスコアディスティルドで導出し、学生が生成したサンプルの分布を教師の分布と一致させることで、生成速度を高速化している。しかし、これらのアプローチは逆向きのカルバック-ライブリング（KL）ディバージェンスを使用して分布の一致を行うことが知られているが、これはモードシーキングとされている。本論文では、新しいf-ディバージェンス最小化フレームワークを用いて分布の一致を一般化し、モードカバージョンとトレーニングバリアンスの違いによる異なる補減を含む異なるディバージョンをカバーすることを目指している。教師と学生の分布の間のf-ディバージョンの勾配を求め、それは、そのスコアの差と密度比による決まる重み付け関数の積で表されることを示し、この重み付け関数は、教師分布の密度が高いサンプルを自然と優先します。逆向きKLディバージョンを使用したビニアルスコアディスティルドの一般的なアプローチは、我々のフレームワーク内の特殊な場合として見なされる。実験的には、順向きKLディバージョンとジェンセン-シャーニングディバージョンなどの代替的なf-ディバージョンは、現在の最も良いビニアルスコアディスティルド手法を超えることを示し、特にジェンセン-シャーニングディバージョンを使用すると、f-distillはImageNet64の現在の最先端の一ステップ生成性能を達成し、MS-COCOのゼロショットテキストから画像生成においても優れた性能を示す。プロジェクトページ：https://research.nvidia.com/labs/genair/f-distill",
      "upvotes": 1,
      "discussionId": "67bbe6837727595ca5979e8c"
    },
    "publishedAt": "2025-02-23T22:24:55.500Z",
    "title": "One-step Diffusion Models with $f$-Divergence Distribution Matching",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15681.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6193
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13189",
      "authors": [
        {
          "_id": "67b7152f299e4d30f9eb41c2",
          "name": "Enzhe Lu",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41c3",
          "name": "Zhejun Jiang",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41c4",
          "name": "Jingyuan Liu",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41c5",
          "name": "Yulun Du",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41c6",
          "name": "Tao Jiang",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41c7",
          "name": "Chao Hong",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41c8",
          "name": "Shaowei Liu",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41c9",
          "name": "Weiran He",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41ca",
          "name": "Enming Yuan",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41cb",
          "name": "Yuzhi Wang",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41cc",
          "name": "Zhiqi Huang",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41cd",
          "name": "Huan Yuan",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41ce",
          "name": "Suting Xu",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41cf",
          "name": "Xinran Xu",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d0",
          "name": "Guokun Lai",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d1",
          "name": "Yanru Chen",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d2",
          "name": "Huabin Zheng",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d3",
          "name": "Junjie Yan",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d4",
          "name": "Jianlin Su",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d5",
          "name": "Yuxin Wu",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d6",
          "name": "Neo Y. Zhang",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d7",
          "name": "Zhilin Yang",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d8",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41d9",
          "name": "Mingxing Zhang",
          "hidden": false
        },
        {
          "_id": "67b7152f299e4d30f9eb41da",
          "name": "Jiezhong Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T14:06:05.000Z",
      "title": "MoBA: 長文脈 LLMs のブロックアテンションの混ぜ合わせ",
      "summary": "スカライングの効果的なコンテキスト長は、大規模な言語モデル（LLMs）を人工的な一般的な知能（AGI）に向けて進化させるために重要である。しかし、単純なアタション機構における計算複雑性の二次的な増加は、負担を負わせることになる。現在のアプローチは、タスクに特化した偏りを持つ構造を強制している（例えば、sink or window attention）か、アタション機構を線形近似に変形しているが、複雑な推理タスクの性能については十分に調査されていません。\n\n本論文では、「構造を少なく」原則に従う解決策を提案し、モデルが自動的にアタションの場所を決定することを可能にし、事前定義された偏りを引き起こさないようにする。Block Attention Mixture（MoBA）を導入し、Mixture of Experts（MoE）の原則をアタション機構に適用した新しいアプローチを提案します。この新しいアーキテクチャは、長コンテキストタスクで上位の性能を示し、全注意力とスパース注意力の適切な転換が可能であり、効率を向上させるものの性能を崩さないリスクを軽減できることを特徴とします。MoBAはすでにKimiの長コンテキストリクエストにサポートしており、LLMsの効率的なアタション計算において显著な進歩を示しています。コードは、https://github.com/MoonshotAI/MoBA から利用可能です。",
      "upvotes": 0,
      "discussionId": "67b71530299e4d30f9eb4213"
    },
    "publishedAt": "2025-02-24T04:52:30.963Z",
    "title": "MoBA: Mixture of Block Attention for Long-Context LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13189.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a369d98c0c89dcae3b8329",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png",
      "fullname": "Adina Yakefu",
      "name": "AdinaY",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 420
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.13407",
      "authors": [
        {
          "_id": "67bb33f3829dedfc99ae1288",
          "user": {
            "_id": "67bb32b6a0cb6e48cfd27d80",
            "avatarUrl": "/avatars/3cafe3a3fb60405252962d00105667c5.svg",
            "isPro": false,
            "fullname": "Ziyuan Liu",
            "user": "circleLZY",
            "type": "user"
          },
          "name": "Ziyuan Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:07:29.223Z",
          "hidden": false
        },
        {
          "_id": "67bb33f3829dedfc99ae1289",
          "name": "Ruifei Zhu",
          "hidden": false
        },
        {
          "_id": "67bb33f3829dedfc99ae128a",
          "name": "Long Gao",
          "hidden": false
        },
        {
          "_id": "67bb33f3829dedfc99ae128b",
          "name": "Yuanxiu Zhou",
          "hidden": false
        },
        {
          "_id": "67bb33f3829dedfc99ae128c",
          "name": "Jingyu Ma",
          "hidden": false
        },
        {
          "_id": "67bb33f3829dedfc99ae128d",
          "name": "Yuantao Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-19T03:33:54.000Z",
      "title": "JL1-CD: 遠隔観測変化検出の新たなベンチマークと強固な多ターナー知識絞り込みフレームワーク",
      "summary": "深層学習は、遠隔観測画像の変化検出（CD）分野で顕著な成功を収めていますが、2つの大きな課題が残っています：小規模な、完全な開放ソースCDデータセットの不足と、変化領域が異なる画像間で一貫したおそらく満足している検出結果を達成する難易度。これらの問題に対処するために、私たちはJL1-CDデータセットを紹介します。このデータセットには、0.5から0.75メートルの解像度で5,000個の512×512ピクセル画像のペアが含まれています。また、私たちはCD向けの多ターナイツ知識節約（MTKD）フレームワークを提案します。JL1-CDおよびSYSU-CDデータセットの実験結果により、MTKDフレームワークは異なるネットワーク構造とパラメータサイズのCDモデルの性能を大幅に向上させ、新たな最先端の結果を収めました。コードは、https://github.com/circleLZY/MTKD-CD にアクセスできます。",
      "upvotes": 0,
      "discussionId": "67bb33f6829dedfc99ae135e"
    },
    "publishedAt": "2025-02-24T04:29:42.452Z",
    "title": "JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13407.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67bb32b6a0cb6e48cfd27d80",
      "avatarUrl": "/avatars/3cafe3a3fb60405252962d00105667c5.svg",
      "fullname": "Ziyuan Liu",
      "name": "circleLZY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15011",
      "authors": [
        {
          "_id": "67bc0d12ffc2c387329c8cfd",
          "user": {
            "_id": "650ec19e6620b0c57e2a551b",
            "avatarUrl": "/avatars/c26c03fa920d857120f03c9ccb9f1d7a.svg",
            "isPro": false,
            "fullname": "Sayan Deb Sarkar",
            "user": "sayandsarkar",
            "type": "user"
          },
          "name": "Sayan Deb Sarkar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:06:56.555Z",
          "hidden": false
        },
        {
          "_id": "67bc0d12ffc2c387329c8cfe",
          "name": "Ondrej Miksik",
          "hidden": false
        },
        {
          "_id": "67bc0d12ffc2c387329c8cff",
          "name": "Marc Pollefeys",
          "hidden": false
        },
        {
          "_id": "67bc0d12ffc2c387329c8d00",
          "name": "Daniel Barath",
          "hidden": false
        },
        {
          "_id": "67bc0d12ffc2c387329c8d01",
          "name": "Iro Armeni",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T20:05:30.000Z",
      "title": "CrossOver: 3Dスキームクロスモードアラインメント",
      "summary": "多モーダル3Dオブジェクト理解は重要な注目を集めていますが、現在のアプローチは完全なデータの可利用性とすべてのモーダルティの剛性な調整を前提としています。私たちは、CrossOverという新しいフレームワークを紹介します。これは、柔軟な、スケーンレベルのモーダルティの調整を通じて、クロスモーダル3Dスケーン理解を行うものです。従来の方法と異なり、CrossOverは、すべてのオブジェクトインスタンスに対して調整されたモーダルデータを必要としないように、RGB画像、点群、CADモデル、フロアプラン、およびテキスト説明を調整し、厳格な制約を放り、明示的なオブジェクト語意を含めないように学習した一連のモーダルティに関する、一連のスケーンのモーダル無依存拡散空間を学習します。次元特有のエンコーダー、多段階トレーニングパイプライン、および現象的なクロスモーダル行動を活用し、CrossOverは、欠損したモーダルデータのある場合でも、強力的なスケーン検索とオブジェクト位置指定を支援します。ScanNetと3RScanデータセットの評価により、多様なメトリックで上位の性能を示し、3Dスケーン理解の実世界アプリケーションの適応性を強調します。",
      "upvotes": 0,
      "discussionId": "67bc0d18ffc2c387329c8e56"
    },
    "publishedAt": "2025-02-24T01:13:24.911Z",
    "title": "CrossOver: 3D Scene Cross-Modal Alignment",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/650ec19e6620b0c57e2a551b/S_xFBPoV3YbtHmtLtRrSV.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15011.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650ec19e6620b0c57e2a551b",
      "avatarUrl": "/avatars/c26c03fa920d857120f03c9ccb9f1d7a.svg",
      "fullname": "Sayan Deb Sarkar",
      "name": "sayandsarkar",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.15082",
      "authors": [
        {
          "_id": "67bbe93f267aa2b537b318be",
          "user": {
            "_id": "64f64da90efa33bfe0a3d9ba",
            "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
            "isPro": false,
            "fullname": "Vaidehi Patil",
            "user": "vaidehi99",
            "type": "user"
          },
          "name": "Vaidehi Patil",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-24T09:07:15.794Z",
          "hidden": false
        },
        {
          "_id": "67bbe93f267aa2b537b318bf",
          "name": "Elias Stengel-Eskin",
          "hidden": false
        },
        {
          "_id": "67bbe93f267aa2b537b318c0",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-20T22:51:10.000Z",
      "title": "UPCORE: ユーティリティ保存型コアセット選択によるバランスのある無学習",
      "summary": "ユーザーの特定要求または法規構造により、情報を復元モデルから削除する必要があることがあり、これには既に学習されたモデルから特定のデータポイントを削除し、または「忘れる」必要がある。これは、モデルが他のデータポイントに対する性能を低下させることが一般的であるため、情報の削除とモデルの他の能力の保全とのバランスを取る必要がある。このバランスの失敗は、削除の失敗または使用不可能なモデルに結果する。この観点から、私たちはUPCORE（Utility-Preserving Coreset Selection）を提案し、無学習（unlearning）時に伴う損害を最小限にするための方法無依頼のデータ選択フレームワークを提案しています。モデルの損傷が「忘れる」セットのモデルの表現の分散に関連していることを見出し、「忘れる」セットを選択的にプロープションし、このようにして無学習後のモデルの損傷を最小限にすることを目指しています。UPCOREは、3つの標準の無学習方法を通じて、削除効果とモデルの保全との対立の目標のバランスを優れて達成しています。この貿易オフの評価をより良くするために、新しいメトリックを導入し、標準メトリックの曲線の面積（AUC）を測定しています。UPCOREは、標準メトリックとAUCを両方改善し、コアセットとプロープションされた点の間の正のトランスファーを受け、「忘れる」セットからその外の点への負のトランスファーを減少しています。",
      "upvotes": 0,
      "discussionId": "67bbe940267aa2b537b318f4"
    },
    "publishedAt": "2025-02-23T23:17:33.152Z",
    "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.15082.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f64da90efa33bfe0a3d9ba",
      "avatarUrl": "/avatars/c45fb015433e46a2eeb9518910f75d35.svg",
      "fullname": "Vaidehi Patil",
      "name": "vaidehi99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]