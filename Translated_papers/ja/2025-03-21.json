[
  {
    "paper": {
      "id": "2503.13358",
      "authors": [
        {
          "_id": "67dd2ed0d2550735426e7b6f",
          "user": {
            "_id": "64a42977250bfdecd9570a9e",
            "avatarUrl": "/avatars/df5d7cf159e6bb9e961e1c77d1b89d36.svg",
            "isPro": false,
            "fullname": "Daniil Selikhanovych",
            "user": "apryc1",
            "type": "user"
          },
          "name": "Daniil Selikhanovych",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-21T09:18:55.946Z",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b70",
          "name": "David Li",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b71",
          "name": "Aleksei Leonov",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b72",
          "name": "Nikita Gushchin",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b73",
          "name": "Sergei Kushneriuk",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b74",
          "name": "Alexander Filippov",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b75",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b76",
          "name": "Iaroslav Koshelev",
          "hidden": false
        },
        {
          "_id": "67dd2ed0d2550735426e7b77",
          "name": "Alexander Korotin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T16:44:08.000Z",
      "submittedOnDailyAt": "2025-03-21T07:50:23.779Z",
      "title": "一ステップ残差シフトディフューションによる画像超解像化のディスタイル化",
      "submittedOnDailyBy": {
        "_id": "64a42977250bfdecd9570a9e",
        "avatarUrl": "/avatars/df5d7cf159e6bb9e961e1c77d1b89d36.svg",
        "isPro": false,
        "fullname": "Daniil Selikhanovych",
        "user": "apryc1",
        "type": "user"
      },
      "summary": "ディフュージョンモデルを用いた超解像度(SR)において、高品質の視覚的な結果を出すが、計算費用が高い問題がある。ディフュージョンベースのSRモデルを加速するための数々の方法が開発されたが、その中でも(例: SinSR)写実的な視覚的な詳細を生成することができないものがあり、(例: OSEDiff)は存在しない構造をハロウィングすることもある。これらの問題を克服するために、ResShiftの一つの最先端のディフュージョンベースのSRモデルに対して新しい煉成法を提案します。私たちの方法は、ティーチャーモデルと同じくらいの結果を出すために学生ネットワークを訓練することに基づいています。RSDは単一ステップでのリファイル化を実現し、ティーチャーモデルを大幅に超える性能を示します。私たちの煉成法は、ResShiftに対する他の煉成法(SinSR)を超え、最先端のディフュージョンベースのSR煉成法と同等になることを示します。テキストから画像への予測モデルを用いたSR手法に比べ、RSDは競争的な視覚的な品質を提供し、悪化された入力画像によりより良い対応の画像を提供し、パラメータ数とGPUメモリの使用量を減らすことができます。RealSR、RealSet65、DRealSR、ImageNet、DIV2Kなどの多様な実世界と合成データセットにおいて実験結果を提供します。",
      "upvotes": 39,
      "discussionId": "67dd2ed7d2550735426e7d7f",
      "ai_keywords": [
        "diffusion models",
        "super-resolution (SR)",
        "ResShift",
        "distillation method",
        "fake ResShift model",
        "single-step restoration",
        "SinSR",
        "perceptual quality",
        "degraded input images",
        "parameters",
        "GPU memory"
      ]
    },
    "publishedAt": "2025-03-17T12:44:08.000Z",
    "title": "One-Step Residual Shifting Diffusion for Image Super-Resolution via\n  Distillation",
    "summary": "Diffusion models for super-resolution (SR) produce high-quality visual\nresults but require expensive computational costs. Despite the development of\nseveral methods to accelerate diffusion-based SR models, some (e.g., SinSR)\nfail to produce realistic perceptual details, while others (e.g., OSEDiff) may\nhallucinate non-existent structures. To overcome these issues, we present RSD,\na new distillation method for ResShift, one of the top diffusion-based SR\nmodels. Our method is based on training the student network to produce such\nimages that a new fake ResShift model trained on them will coincide with the\nteacher model. RSD achieves single-step restoration and outperforms the teacher\nby a large margin. We show that our distillation method can surpass the other\ndistillation-based method for ResShift - SinSR - making it on par with\nstate-of-the-art diffusion-based SR distillation methods. Compared to SR\nmethods based on pre-trained text-to-image models, RSD produces competitive\nperceptual quality, provides images with better alignment to degraded input\nimages, and requires fewer parameters and GPU memory. We provide experimental\nresults on various real-world and synthetic datasets, including RealSR,\nRealSet65, DRealSR, ImageNet, and DIV2K.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13358.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a42977250bfdecd9570a9e",
      "avatarUrl": "/avatars/df5d7cf159e6bb9e961e1c77d1b89d36.svg",
      "fullname": "Daniil Selikhanovych",
      "name": "apryc1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16419",
      "authors": [
        {
          "_id": "67dcdbfc71027d42fa46e3f2",
          "name": "Yang Sui",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f3",
          "name": "Yu-Neng Chuang",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f4",
          "name": "Guanchu Wang",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f5",
          "name": "Jiamu Zhang",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f6",
          "name": "Tianyi Zhang",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f7",
          "name": "Jiayi Yuan",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f8",
          "name": "Hongyi Liu",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f9",
          "name": "Andrew Wen",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3fa",
          "name": "Shaochen",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3fb",
          "name": "Zhong",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3fc",
          "name": "Hanjie Chen",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3fd",
          "name": "Xia Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:38.000Z",
      "submittedOnDailyAt": "2025-03-21T01:56:58.604Z",
      "title": "ストップ・オーバーテンション：大規模言語モデルの効率的な理由の調査",
      "submittedOnDailyBy": {
        "_id": "63787b13500186f250ba377c",
        "avatarUrl": "/avatars/96bb6051662109e9cd25e6df7d738f17.svg",
        "isPro": false,
        "fullname": "yangsui",
        "user": "yangsui",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は複雑なタスクで卓越した能力を示しています。最近の大論理モデル（LRMs）の進展、特にOpenAI o1とDeepSeek-R1による進展は、数学やプログラミングのようなシステム2の論理領域での性能を進めることを可能にしました。これらは、監督学習（SFT）と強化学習（RL）の技術を利用してChain-of-Thought（CoT）論理を強化することにより実現されました。しかし、長いCoT論理のシーケンスは性能向上を伴い、冗長な出力によって計算量の重負を引き起こし、「オーバーチューン現象」として知られています。本論文では、LLMsの効率的な論理を達成するための現在の進歩を体系的に調査し、探索するための最初の構造化された調査を提供します。全体として、LLMsの固有機構を基に、現在の研究を以下の数々の重要な方向に分類します：（1）モデルベースの効率的な論理、全長の論理モデルをより簡潔な論理モデルにまたぐり、直接効率的な論理モデルを学習することを考慮します；（2）論理出力ベースの効率的な論理、推論中に理由のステップと長さを動的に削減することを目的とします；（3）入力プロンプトベースの効率的な論理、入力プロンプトの性質（例えば難易度や長さの制御）に基づいて論理の効率化を強化することを試みます。また、効率的なデータの使用をもとに論理モデルの学習、小語言モデルの論理能力、評価方法とベンチマークを議論します。",
      "upvotes": 27,
      "discussionId": "67dcdbfd71027d42fa46e439",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Large Reasoning Models (LRMs)",
        "OpenAI o1",
        "DeepSeek-R1",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "Chain-of-Thought (CoT) reasoning",
        "overthinking phenomenon",
        "model-based efficient reasoning",
        "reasoning output-based efficient reasoning",
        "input prompts-based efficient reasoning",
        "efficient data",
        "small language models",
        "evaluation methods",
        "benchmarking"
      ]
    },
    "publishedAt": "2025-03-20T13:59:38.000Z",
    "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63787b13500186f250ba377c",
      "avatarUrl": "/avatars/96bb6051662109e9cd25e6df7d738f17.svg",
      "fullname": "yangsui",
      "name": "yangsui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16302",
      "authors": [
        {
          "_id": "67dce2d2068292e7ef79b3dd",
          "name": "Zeqiang Lai",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3de",
          "name": "Yunfei Zhao",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3df",
          "name": "Zibo Zhao",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e0",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e1",
          "name": "Fuyun Wang",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e2",
          "name": "Huiwen Shi",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e3",
          "name": "Xianghui Yang",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e4",
          "name": "Qinxiang Lin",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e5",
          "name": "Jinwei Huang",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e6",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e7",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e8",
          "name": "Chunchao Guo",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e9",
          "name": "Xiangyu Yue",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63044b89eedc089484c995ad/ukrgJYM5cBYEzAo7b9J7U.mp4"
      ],
      "publishedAt": "2025-03-20T16:23:44.000Z",
      "submittedOnDailyAt": "2025-03-21T02:25:30.177Z",
      "title": "Vecset ディフュージョンモデルを活用して高速な形状生成を実現する",
      "submittedOnDailyBy": {
        "_id": "63044b89eedc089484c995ad",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
        "isPro": false,
        "fullname": "Zeqiang Lai",
        "user": "ZeqiangLai",
        "type": "user"
      },
      "summary": "3D形状生成は、「ネイティブ」3Dディフュージョンの開発により大幅に繁栄し、特にVecsetディフュージョンモデル（VDM）が中心的な役割を果たしています。最近の進展は高解像度の3D形状の生成に望ましい結果を示していますが、VDMは高速生成に難しい問題を抱えています。これらの課題は、前の研究では調査不足していたディフュージョンサンプリングの加速およびVDMのVAE解確認の領域により原因があります。これらの課題を解決するために、我々はFlashVDM、VAEとDiTの両方を加速するためのシステム的なフレームワークを提出します。DiTにおいては、FlashVDMは新たに導入されたProgressive Flow Distillationを用いて、5ステップでの柔軟なディフュージョンサンプリングを可能にし、比較的質が保証されます。VAEにおいては、Adaptive KV Selection、Hierarchical Volume Decoding、Efficient Network Designを採用したLightning vecset解確認を導入します。vecsetの局在性と形状表面の稀疏性を利用して、我々の解確認はFLOPsを大幅に減少させ、全体的な解確認オーバーヘッドを最小化します。FlashVDMをHunyuan3D-2に適用して、Hunyuan3D-2 Turboを取得しました。システム的な評価を通じて、我々のモデルは既存の高速3D生成方法を大幅に超え、状態の最先端と比較的性能を達成し、再構築時間を45倍以上、生成時間を32倍以上短縮しました。コードとモデルは以下のURLから利用可能です。\nhttps://github.com/Tencent/FlashVDM",
      "upvotes": 23,
      "discussionId": "67dce2d6068292e7ef79b556",
      "githubRepo": "https://github.com/Tencent/FlashVDM",
      "ai_keywords": [
        "3D diffusion",
        "Vecset Diffusion Model (VDM)",
        "diffusion sampling",
        "VAE",
        "DiT",
        "Progressive Flow Distillation",
        "lightning vecset decoder",
        "Adaptive KV Selection",
        "Hierarchical Volume Decoding",
        "Efficient Network Design",
        "FLOPs",
        "decoding overhead",
        "Hunyuan3D-2",
        "Hunyuan3D-2 Turbo"
      ]
    },
    "publishedAt": "2025-03-20T12:23:44.000Z",
    "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
    "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63044b89eedc089484c995ad/ukrgJYM5cBYEzAo7b9J7U.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16302.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63044b89eedc089484c995ad",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
      "fullname": "Zeqiang Lai",
      "name": "ZeqiangLai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.14487",
      "authors": [
        {
          "_id": "67da83d1b05eff6d87a41f81",
          "user": {
            "_id": "662887715d246621f33d2ce6",
            "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg",
            "isPro": false,
            "fullname": "Shi Minglei",
            "user": "MingleiShi",
            "type": "user"
          },
          "name": "Minglei Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-19T09:43:55.360Z",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f82",
          "name": "Ziyang Yuan",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f83",
          "name": "Haotian Yang",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f84",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f85",
          "name": "Mingwu Zheng",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f86",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f87",
          "name": "Wenliang Zhao",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f88",
          "name": "Wenzhao Zheng",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f89",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f8a",
          "name": "Jiwen Lu",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f8b",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f8c",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "67da83d1b05eff6d87a41f8d",
          "name": "Kun Gai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T17:57:07.000Z",
      "submittedOnDailyAt": "2025-03-21T06:35:23.843Z",
      "title": "DiffMoE: ダイナミックトークン選択によるスケーラブルなDiffusion Transformers",
      "submittedOnDailyBy": {
        "_id": "662887715d246621f33d2ce6",
        "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg",
        "isPro": false,
        "fullname": "Shi Minglei",
        "user": "MingleiShi",
        "type": "user"
      },
      "summary": "Diffusionモデルは、多様な画像生成タスクで驚異的な成功を示していますが、入力の一貫した処理が条件やノイズレベルの異なりを制限しています。この制限を解決するために、我々は、ディフュージョンプロセスの固有な不均一性を活用する新しいアプローチを提案します。我々の方法では、DiffMoE、バッチレベルのグローバルトークンポールを導入し、トレーニング中にエクスプローラーがグローバルトークン分布をアクセスできるようにし、専門的なエクスプローラーの行動を促します。ディフュージョンプロセスの全力を発揮させるために、DiffMoEは、ノイズレベルとサンプル複雑性に基づいて計算コンピューティングリソースを動的に割り当てる能力予測器を採用します。詳細な評価を通じて、DiffMoEはImageNetベンチマークで最先端の性能を達成し、3倍の活性パラメーターを持つ密なアーキテクチャと既存のMoEアプローチを大幅に上回り、1倍の活性パラメーターを保持しています。我々のアプローチの効果は、クラス条件付き生成よりも難しいタスクへと拡張でき、テキストから画像生成などのより複雑なタスクでも効果的であり、異なるディフュージョンモデルアプリケーションの広範囲的な適用性を示しています。プロジェクトページ：https://shiml20.github.io/DiffMoE/",
      "upvotes": 19,
      "discussionId": "67da83d3b05eff6d87a42049",
      "projectPage": "https://shiml20.github.io/DiffMoE/",
      "githubRepo": "https://github.com/KwaiVGI/DiffMoE",
      "ai_keywords": [
        "diffusion models",
        "ImageNet",
        "batch-level global token pool",
        "experts",
        "global token distributions",
        "capacity predictor",
        "computational resources",
        "noise levels",
        "sample complexity",
        "class-conditional generation",
        "text-to-image generation"
      ]
    },
    "publishedAt": "2025-03-18T13:57:07.000Z",
    "title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers",
    "summary": "Diffusion models have demonstrated remarkable success in various image\ngeneration tasks, but their performance is often limited by the uniform\nprocessing of inputs across varying conditions and noise levels. To address\nthis limitation, we propose a novel approach that leverages the inherent\nheterogeneity of the diffusion process. Our method, DiffMoE, introduces a\nbatch-level global token pool that enables experts to access global token\ndistributions during training, promoting specialized expert behavior. To\nunleash the full potential of the diffusion process, DiffMoE incorporates a\ncapacity predictor that dynamically allocates computational resources based on\nnoise levels and sample complexity. Through comprehensive evaluation, DiffMoE\nachieves state-of-the-art performance among diffusion models on ImageNet\nbenchmark, substantially outperforming both dense architectures with 3x\nactivated parameters and existing MoE approaches while maintaining 1x activated\nparameters. The effectiveness of our approach extends beyond class-conditional\ngeneration to more challenging tasks such as text-to-image generation,\ndemonstrating its broad applicability across different diffusion model\napplications. Project Page: https://shiml20.github.io/DiffMoE/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14487.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "662887715d246621f33d2ce6",
      "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg",
      "fullname": "Shi Minglei",
      "name": "MingleiShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16416",
      "authors": [
        {
          "_id": "67dd1d595fd14aedd30bb94a",
          "name": "Asaf Yehudai",
          "hidden": false
        },
        {
          "_id": "67dd1d595fd14aedd30bb94b",
          "name": "Lilach Eden",
          "hidden": false
        },
        {
          "_id": "67dd1d595fd14aedd30bb94c",
          "name": "Alan Li",
          "hidden": false
        },
        {
          "_id": "67dd1d595fd14aedd30bb94d",
          "name": "Guy Uziel",
          "hidden": false
        },
        {
          "_id": "67dd1d595fd14aedd30bb94e",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "67dd1d595fd14aedd30bb94f",
          "name": "Roy Bar-Haim",
          "hidden": false
        },
        {
          "_id": "67dd1d595fd14aedd30bb950",
          "name": "Arman Cohan",
          "hidden": false
        },
        {
          "_id": "67dd1d595fd14aedd30bb951",
          "name": "Michal Shmueli-Scheuer",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:23.000Z",
      "submittedOnDailyAt": "2025-03-21T06:34:12.447Z",
      "title": "LLMベースエージェント評価調査",
      "submittedOnDailyBy": {
        "_id": "638324f862badff43269e588",
        "avatarUrl": "/avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg",
        "isPro": false,
        "fullname": "Asaf Yehudai",
        "user": "Asaf-Yehudai",
        "type": "user"
      },
      "summary": "LLMベースのアガントの出現はAIのパラダイムシフトを意味し、自動軌制システムが戦略的に計画、理由をする、ツールを使用し、メモリを維持しながら動的な環境との相互作用を可能にしています。本論文は、これらの能力を増加させていくアガントの評価手法の最初の詳細な調査を提供します。我々は、基本的なアガント能力、アプリケーション特有のベンチマーク、一般的なアガントのベンチマーク、アガントの評価に用いるフレームワークの4つの重要な次元でシステマティックに分析します。分析では、実際的な、難しい評価の傾向や、連続的に更新されるベンチマークの傾向が明らかになります。また、将来の研究に必要な重要な欠陥も明らかにします。特に、コスト・エフェクティブさ、安全性、強固性の評価、そして、フィンエルスギャインされた、スケーラブルな評価方法の開発が必要です。この調査は、アガント評価の急速に変化するランドシューを地図化し、場の傾向を明らかにし、現在の限界を特定し、将来の研究の方向を提案します。",
      "upvotes": 17,
      "discussionId": "67dd1d5a5fd14aedd30bb999",
      "ai_keywords": [
        "LLM-based agents",
        "planning",
        "tool use",
        "self-reflection",
        "memory",
        "evaluation benchmarks",
        "evaluation frameworks",
        "fundamental agent capabilities",
        "application-specific benchmarks",
        "web agents",
        "software engineering agents",
        "scientific agents",
        "conversational agents",
        "generalist agents",
        "cost-efficiency",
        "safety",
        "robustness",
        "fine-grained evaluation methods",
        "scalable evaluation methods"
      ]
    },
    "publishedAt": "2025-03-20T13:59:23.000Z",
    "title": "Survey on Evaluation of LLM-based Agents",
    "summary": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16416.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638324f862badff43269e588",
      "avatarUrl": "/avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg",
      "fullname": "Asaf Yehudai",
      "name": "Asaf-Yehudai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15558",
      "authors": [
        {
          "_id": "67dcadafb2cd7d4f3a266037",
          "name": "NVIDIA",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266039",
          "name": "Alisson Azzolini",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603a",
          "name": "Hannah Brandon",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603b",
          "name": "Prithvijit Chattopadhyay",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603c",
          "name": "Huayu Chen",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603d",
          "name": "Jinju Chu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603e",
          "name": "Yin Cui",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603f",
          "name": "Jenna Diamond",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266040",
          "name": "Yifan Ding",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266041",
          "name": "Francesco Ferroni",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266042",
          "name": "Rama Govindaraju",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266043",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266044",
          "name": "Siddharth Gururani",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266045",
          "name": "Imad El Hanafi",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266046",
          "name": "Zekun Hao",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266047",
          "name": "Jacob Huffman",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266048",
          "name": "Jingyi Jin",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266049",
          "name": "Brendan Johnson",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604a",
          "name": "Rizwan Khan",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604b",
          "name": "George Kurian",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604c",
          "name": "Elena Lantz",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604d",
          "name": "Nayeon Lee",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604e",
          "name": "Zhaoshuo Li",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604f",
          "name": "Xuan Li",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266050",
          "name": "Tsung-Yi Lin",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266051",
          "name": "Yen-Chen Lin",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266052",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266053",
          "name": "Andrew Mathau",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266054",
          "name": "Yun Ni",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266055",
          "name": "Lindsey Pavao",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266056",
          "name": "Wei Ping",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266057",
          "name": "David W. Romero",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266058",
          "name": "Misha Smelyanskiy",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266059",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605a",
          "name": "Lyne Tchapmi",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605b",
          "name": "Andrew Z. Wang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605c",
          "name": "Boxin Wang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605d",
          "name": "Haoxiang Wang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605e",
          "name": "Fangyin Wei",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605f",
          "name": "Jiashu Xu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266060",
          "name": "Yao Xu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266061",
          "name": "Xiaodong Yang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266062",
          "name": "Zhuolin Yang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266063",
          "name": "Xiaohui Zeng",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266064",
          "name": "Zhe Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T22:06:58.000Z",
      "submittedOnDailyAt": "2025-03-21T01:09:51.519Z",
      "title": "Cosmos-Reason1: 物理的常識から体験的推理へ",
      "submittedOnDailyBy": {
        "_id": "649f05367b57fab3a5b27c8b",
        "avatarUrl": "/avatars/749eca8ba898685c90c305f4e3549ba1.svg",
        "isPro": false,
        "fullname": "Yin Cui",
        "user": "richardaecn",
        "type": "user"
      },
      "summary": "物理的AIシステムは、物理的な世界で認識、理解し、複雑な行動を実行する必要があります。本論文では、物理的な世界を理解し、長いチャインオフコンシェンスの推理プロセスを通じて自然言語で適切な具象化された判断（例：次のステップの行動）を生成することができるCosmos-Reason1モデルを提出します。まず、物理的なAIの推理のためのキー能力を定義し、物理的な共通知識と具象化された推理に焦点を当てます。物理的な共通知識を表現するためには、空間、時間、物理学に関する基本的な知識を捉えるヒューリスティックなオントロジーを使用します。具象化された推理については、異なる物理的な具象化を拡張するための2次元オントロジーを依頼します。これらの能力に基づき、Cosmos-Reason1-8BとCosmos-Reason1-56Bの2つの多モーダル大語言モデルを開発します。データをカレーレードし、モデルを4段階で訓練します：視覚の予ちゅう、一般的なサブジェクト訓練（SFT）、物理的なAIのSFT、物理的なAIの強化学習（RL）を後チューニングとして。モデルの評価においては、我々のオントロジーに基づいた物理的な共通知識と具象化された推理の詳細なベンチマークを構築します。評価結果から、物理的なAIのSFTと強化学習は大幅な向上を示しています。物理的なAIの開発を促進するために、我々のコードと事前学習モデルをNVIDIA Open Model Licenseの下でフリーに提供します。",
      "upvotes": 14,
      "discussionId": "67dcadb1b2cd7d4f3a2660f4",
      "githubRepo": "https://github.com/nvidia-cosmos/cosmos-reason1",
      "ai_keywords": [
        "hierarchical ontology",
        "two-dimensional ontology",
        "multimodal large language models",
        "vision pre-training",
        "long chain-of-thought reasoning",
        "Physical AI SFT",
        "Physical AI reinforcement learning",
        "embodied reasoning",
        "physical common sense"
      ]
    },
    "publishedAt": "2025-03-18T18:06:58.000Z",
    "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning",
    "summary": "Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data\nand train our models in four stages: vision pre-training, general supervised\nfine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL)\nas the post-training. To evaluate our models, we build comprehensive benchmarks\nfor physical common sense and embodied reasoning according to our ontologies.\nEvaluation results show that Physical AI SFT and reinforcement learning bring\nsignificant improvements. To facilitate the development of Physical AI, we will\nmake our code and pre-trained models available under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-reason1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15558.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649f05367b57fab3a5b27c8b",
      "avatarUrl": "/avatars/749eca8ba898685c90c305f4e3549ba1.svg",
      "fullname": "Yin Cui",
      "name": "richardaecn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16212",
      "authors": [
        {
          "_id": "67dcd33626989570158ce8cf",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d0",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d1",
          "name": "Zhuoshi Pan",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d2",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d3",
          "name": "Honglin Lin",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d4",
          "name": "Chenlin Ming",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d5",
          "name": "Xin Gao",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d6",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d7",
          "name": "Rui Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T15:00:41.000Z",
      "submittedOnDailyAt": "2025-03-21T01:18:55.765Z",
      "title": "MathFusion: LLMの数学問題解決能力を向上させるためのインストラクション融合",
      "submittedOnDailyBy": {
        "_id": "6397f6081323f19c578f142e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
        "isPro": false,
        "fullname": "QizhiPei",
        "user": "QizhiPei",
        "type": "user"
      },
      "summary": "大型言語モデル（LLMs）は数学的論理に関する驚人的進歩を示しています。データ拡張は数学問題解決能力を向上させることができることが期待されていますが、現在のアプローチは主にインスタンスレベルの変更に限られており、これは数学知識の固有の関係的構造を捉えつつ利用することができない問題解決法の改良に失敗します。人間の学習プロセスによりもっとも、数学の熟練は連結した概念のシステマティックな接触によって進展します。そこで、MathFusionという新しいフレームワークを紹介します。これは、問題間の指導合成を通じて数学的論理を向上させることを目的としています。MathFusionは3つの融合戦略を実装しています。1. 順序的融合は、関連する問題を連鎖して解決法の依存関係をモデル化します。2. 平行融合は、類似な問題を結合して概念的理解を強化します。3. 条件付き融合は、コンテキストに関連付けされた選択的な問題を作成して論理の柔軟性を向上させます。これらの戦略を適用し、新しいデータセット（MathFusionQA）を生成し、モデル（DeepSeekMath-7B、Mistral-7B、Llama3-8B）をその上で微調節します。実験結果は、MathFusionは数学的論理において大幅な向上を達成し、多様なベンチマークでの精度が18.0点上がり、これにより効率的なデータ利用を保った同時に、45K追加シンテティックインストラクティングを必要としていることを示します。これは単一インストラクションアプローチに比べて大幅な向上を示しています。データセット、モデル、コードは公開的に利用可能です。https://github.com/QizhiPei/mathfusion",
      "upvotes": 13,
      "discussionId": "67dcd33726989570158ce90a",
      "githubRepo": "https://github.com/QizhiPei/MathFusion",
      "ai_keywords": [
        "MathFusion",
        "cross-problem instruction synthesis",
        "sequential fusion",
        "parallel fusion",
        "conditional fusion",
        "MathFusionQA",
        "DeepSeekMath-7B",
        "Mistral-7B",
        "Llama3-8B",
        "mathematical reasoning",
        "data efficiency"
      ]
    },
    "publishedAt": "2025-03-20T11:00:41.000Z",
    "title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion",
    "summary": "Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, MathFusionQA, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16212.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6397f6081323f19c578f142e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
      "fullname": "QizhiPei",
      "name": "QizhiPei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16257",
      "authors": [
        {
          "_id": "67dd2cbabf4c007db3bc0b76",
          "name": "Keda Tao",
          "hidden": false
        },
        {
          "_id": "67dd2cbabf4c007db3bc0b77",
          "name": "Haoxuan You",
          "hidden": false
        },
        {
          "_id": "67dd2cbabf4c007db3bc0b78",
          "name": "Yang Sui",
          "hidden": false
        },
        {
          "_id": "67dd2cbabf4c007db3bc0b79",
          "name": "Can Qin",
          "hidden": false
        },
        {
          "_id": "67dd2cbabf4c007db3bc0b7a",
          "name": "Huan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T15:52:43.000Z",
      "submittedOnDailyAt": "2025-03-21T07:41:12.399Z",
      "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language Models",
      "submittedOnDailyBy": {
        "_id": "62b624f3b52bef716e248fd7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b624f3b52bef716e248fd7/AllcccKH-eBWduA8KVnOQ.png",
        "isPro": false,
        "fullname": "Huan Wang",
        "user": "Huan-WhoRegisteredMyName",
        "type": "user"
      },
      "summary": "VideoLLMsは、長いビデオ入力を処理し、複雑な理由と分析を可能にする能力を示しています。しかし、ビデオフレームからの数千の可視トークンにより、Key-Value (KV) cacheがメモリ要求量を大幅に増加させ、推論速度とメモリ使用量にボトルネックとなります。KV cache キュアリゼーションはこの問題を解決するために広く使用されています。本論文では、2ビットのKV cache キュアリゼーションはモデルの性能を大きく損なわないことを示し、より低いビット数でのKV cache キュアリゼーションの限界が調査されていないことを見出しました。この空間を埋めるために、VidKVというポートアンドプレイバックのKV cache キュアリゼーション手法を導入します。特に、(1) キーに対して、チャネル方向での混合精度キュアリゼーション戦略を提案し、異常チャネルに対して2ビットキュアリゼーション、通常チャネルに対してFFTと組み合わせた1ビットキュアリゼーションを行います。 (2) バリューに対して、1.58ビットキュアリゼーションを実装し、語意的に重要な可視トークンを選択的にフィルタリングし、精度とモデル性能のバランスをより良く調整します。重要なことに、VidKVは、先行研究で提案されたLLMsのKV cache キュアリゼーション手法と異なり、VideoLLMsのバリューキャッシュをチャネルごとにキュアリゼーションする必要があることを示します。実験的に、LLaVA-OV-7BとQwen2.5-VL-7Bの6テストベンチマークでの検証結果から、VidKVはFP16コンターパートと比較しても、KV cacheを1.5ビットと1.58ビットの精度で圧縮でき、性能の低下は約してないことが明らかになりました。",
      "upvotes": 11,
      "discussionId": "67dd2cbebf4c007db3bc0cc4",
      "githubRepo": "https://github.com/KD-TAO/VidKV",
      "ai_keywords": [
        "large language models (LLMs)",
        "Video large language models (VideoLLMs)",
        "video frames",
        "key-value (KV) cache",
        "memory requirements",
        "inference speed",
        "KV cache quantization",
        "2-bit KV quantization",
        "VidKV",
        "mixed-precision quantization",
        "channel dimension",
        "anomalous channels",
        "1-bit quantization",
        "FFT",
        "1.58-bit quantization",
        "semantically salient visual tokens",
        "per-channel fashion",
        "per-token fashion",
        "LLaVA-OV-7B",
        "Qwen2.5-VL-7B",
        "benchmarks",
        "FP16 counterparts"
      ]
    },
    "publishedAt": "2025-03-20T11:52:43.000Z",
    "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
    "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16257.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b624f3b52bef716e248fd7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b624f3b52bef716e248fd7/AllcccKH-eBWduA8KVnOQ.png",
      "fullname": "Huan Wang",
      "name": "Huan-WhoRegisteredMyName",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16365",
      "authors": [
        {
          "_id": "67dcdc98e406e84ea880ccab",
          "name": "Muyao Li",
          "hidden": false
        },
        {
          "_id": "67dcdc98e406e84ea880ccac",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67dcdc98e406e84ea880ccad",
          "name": "Kaichen He",
          "hidden": false
        },
        {
          "_id": "67dcdc98e406e84ea880ccae",
          "name": "Xiaojian Ma",
          "hidden": false
        },
        {
          "_id": "67dcdc98e406e84ea880ccaf",
          "name": "Yitao Liang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/QK0LQhSbfUSqe9FuDmxJu.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/VHbOI8bWxJLDd1aOBjoRT.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/xBWzJPEtxSx-8agJJeIXc.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/y2QTnsahG2XEhyG35nhab.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/W_gV_JuPtKvZjdI9Wv1Jb.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/YDIIWH_7nTy1Xm2pTRt5B.mp4"
      ],
      "publishedAt": "2025-03-20T17:21:58.000Z",
      "submittedOnDailyAt": "2025-03-21T01:59:54.135Z",
      "title": "JARVIS-VLA: トレーニング後の大規模な視覚言語モデルを使ってキーボードとマウスでゲームをプレイする",
      "submittedOnDailyBy": {
        "_id": "642e8c99c1b0f8e4e76bcaab",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e8c99c1b0f8e4e76bcaab/BOs9r0P9KyT9pEba9v0H4.png",
        "isPro": false,
        "fullname": "Zihao Wang",
        "user": "zhwang4ai",
        "type": "user"
      },
      "summary": "最近、オープンワールド環境でのアクションベースの決定策が注目を集めました。ビジュアル・ラングジング・アクション（VLA）モデル、大規模なウェブデータセットで事前学習されたものは、決定策タスクにおいて望ましい結果を示しています。しかし、先行研究は主にアクションの後のトレーニングに焦点を当て、ベースモデル自身の向上に関する改良を多く見落としていました。これに対して、私たちは新しいアプローチ「ビジュアル・ラングジング後のトレーニングからアクション」を紹介し、ビジュアルと言語のガイドラインを用いた自動転教的な方法でビジュアル・ラングジングモデル（VLMs）を精進します。この向上は、オープンワールド環境での世界知識、ビジュアル認識、スペースアフィンディングの機能を向上させます。このような後のトレーニングパラダイムに基づいて、私たちはMinecraftで最初のVLAモデルを得ました。これらのモデルは、約1000種類の異なる原子的なタスクにおいて人間の指示を追うことができます。私たちの実験は、非タロイークスティックタスクに対する後のトレーニングは、多様な原子的なタスクの最良アガインベースに対して40%程度の大幅な向上を示しました。また、私たちのアプローチは、Minecraftでの伝統的なイマチャティング学習に基づくポリシーを超え、最先端の性能を達成しました。私たちはコード、モデル、データセットをオープンソース化し、進める研究のための基盤を提供しています。プロジェクトページは、https://craftjarvis.github.io/JarvisVLAに詳しくあります。",
      "upvotes": 10,
      "discussionId": "67dcdc9ce406e84ea880ce67",
      "projectPage": "https://craftjarvis.github.io/JarvisVLA/",
      "githubRepo": "https://github.com/CraftJarvis/JarvisVLA",
      "ai_keywords": [
        "Visual Language Action (VLA) models",
        "Visual Language Models (VLMs)",
        "self-supervised manner",
        "world knowledge",
        "visual recognition",
        "spatial grounding",
        "atomic tasks",
        "crafting",
        "smelting",
        "cooking",
        "mining",
        "killing",
        "non-trajectory tasks",
        "imitation learning-based policies"
      ]
    },
    "publishedAt": "2025-03-20T13:21:58.000Z",
    "title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play\n  Visual Games with Keyboards and Mouse",
    "summary": "Recently, action-based decision-making in open-world environments has gained\nsignificant attention. Visual Language Action (VLA) models, pretrained on\nlarge-scale web datasets, have shown promise in decision-making tasks. However,\nprevious work has primarily focused on action post-training, often neglecting\nenhancements to the foundational model itself. In response, we introduce a\nnovel approach, Act from Visual Language Post-Training, which refines Visual\nLanguage Models (VLMs) through visual and linguistic guidance in a\nself-supervised manner. This enhancement improves the models' capabilities in\nworld knowledge, visual recognition, and spatial grounding in open-world\nenvironments. Following the above post-training paradigms, we obtain the first\nVLA models in Minecraft that can follow human instructions on over 1k different\natomic tasks, including crafting, smelting, cooking, mining, and killing. Our\nexperiments demonstrate that post-training on non-trajectory tasks leads to a\nsignificant 40% improvement over the best agent baseline on a diverse set of\natomic tasks. Furthermore, we demonstrate that our approach surpasses\ntraditional imitation learning-based policies in Minecraft, achieving\nstate-of-the-art performance. We have open-sourced the code, models, and\ndatasets to foster further research. The project page can be found in\nhttps://craftjarvis.github.io/JarvisVLA.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/QK0LQhSbfUSqe9FuDmxJu.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/VHbOI8bWxJLDd1aOBjoRT.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/xBWzJPEtxSx-8agJJeIXc.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/y2QTnsahG2XEhyG35nhab.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/W_gV_JuPtKvZjdI9Wv1Jb.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/YDIIWH_7nTy1Xm2pTRt5B.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16365.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e8c99c1b0f8e4e76bcaab",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e8c99c1b0f8e4e76bcaab/BOs9r0P9KyT9pEba9v0H4.png",
      "fullname": "Zihao Wang",
      "name": "zhwang4ai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16356",
      "authors": [
        {
          "_id": "67dcd18ad2550735425351bf",
          "name": "Yunzhi Yao",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c0",
          "name": "Jizhan Fang",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c1",
          "name": "Jia-Chen Gu",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c2",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c3",
          "name": "Shumin Deng",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c4",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c5",
          "name": "Nanyun Peng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/aC9fMp8dvcRIzvms4CAu_.png"
      ],
      "publishedAt": "2025-03-20T17:14:34.000Z",
      "submittedOnDailyAt": "2025-03-21T01:12:07.189Z",
      "title": "CaKE: 回路に関する編集が一般的な知識の学習を可能にする",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "知識編集（KE）は、大規模言語モデル（LLMs）における過期や不正确な情報の修正を可能にします。既存のKEメソッドは、孤立した事実の更新を行うことができますが、これらの更新を多段階推論タスクに一般化することが難しいです。理由センター（neural pathways）を分析し、LLMsが使用する知識ベース推論のための神経パスウェイを観察することにより、現在の層間局在的KEアプローチ（例：MEMITとWISE）は、単一または少数のモデル層のみを編集することにより、これらの理由パスウェイに更新された情報を有効に統合することが難しいことが見出されました。この制限を解決するために、CaKE（Circuit-aware Knowledge Editing）という新しいメソッドを提案します。CaKEは、新しい知識の適切な理由パスウェイの開発を促すために、我々のセンターベース分析に基づいて戦略的にカスタマイズされたデータを使用し、LLMsにおける更新された情報の適切な統合を促します。実験結果によると、CaKEは関連する理由タスクにおける更新された情報の更正した使用を可能にし、MQuAKEデータセットでの多段階推論精度において現在のKEメソッドに比べて平均20%の向上を収めます。CaKEのコードとデータは、https://github.com/zjunlp/CaKEに公開されています。",
      "upvotes": 9,
      "discussionId": "67dcd18bd255073542535223",
      "githubRepo": "https://github.com/zjunlp/CaKE",
      "ai_keywords": [
        "Knowledge Editing (KE)",
        "large language models (LLMs)",
        "multi-hop reasoning tasks",
        "reasoning circuits",
        "neural pathways",
        "knowledge-based inference",
        "MEMIT",
        "WISE",
        "layer-localized KE approaches",
        "CaKE (Circuit-aware Knowledge Editing)",
        "strategically curated data",
        "circuits-based analysis",
        "MQuAKE dataset"
      ]
    },
    "publishedAt": "2025-03-20T13:14:34.000Z",
    "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
    "summary": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/aC9fMp8dvcRIzvms4CAu_.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16356.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16418",
      "authors": [
        {
          "_id": "67dcd0fe1f94b594ef4f3e8e",
          "name": "Liming Jiang",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e8f",
          "name": "Qing Yan",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e90",
          "name": "Yumin Jia",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e91",
          "name": "Zichuan Liu",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e92",
          "name": "Hao Kang",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e93",
          "name": "Xin Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:34.000Z",
      "submittedOnDailyAt": "2025-03-21T02:41:07.989Z",
      "title": "無限ユー: アイデンティティを保持しながらの柔軟な写真再構築",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "インフィニットユー（InfU）は、最先端のDiffusion Transformers（DiTs）のような機械学習技術を利用して、柔軟性と高品質で身份保護された画像生成を実現するために取り組む先進的なフレームワークの一つです。InfUは、既存の方法では見られる識別子の類似性の不足、文画と画像の対応性の悪いほか、生成質量と美術性の低い問題を解決することを目指しています。InfUの中心的な構成要素は、DiTベースモデルに識別子特徴を残差コネクションを通じて注入するInfuseNetです。これにより、識別子の類似性を向上させながら生成能力を維持します。また、多ステップの訓練戦略を実行し、合成された1人用の複数サンプル（SPMS）データを用いた予ち学習と観学調整（SFT）を含むことで、文画と画像の対応性を改善し、画像の質を向上させ、顔のコピーとペーストの問題を解決します。実験の結果から、InfUは最先端の性能を実現し、既存の基準を超えることを示しています。また、InfUのポートフォリングデザインは、現在の様々な方法との対応性を保ち、より広いコミュニティに貢献しています。",
      "upvotes": 8,
      "discussionId": "67dcd1001f94b594ef4f3f44",
      "ai_keywords": [
        "Diffusion Transformers (DiTs)",
        "FLUX",
        "InfiniteYou (InfU)",
        "InfuseNet",
        "residual connections",
        "synthetic single-person-multiple-sample (SPMS) data",
        "pretraining",
        "supervised fine-tuning (SFT)"
      ]
    },
    "publishedAt": "2025-03-20T13:59:34.000Z",
    "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
    "summary": "Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16418.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6415
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16322",
      "authors": [
        {
          "_id": "67dce4c10784200359ab2494",
          "name": "Ruonan Yu",
          "hidden": false
        },
        {
          "_id": "67dce4c10784200359ab2495",
          "name": "Songhua Liu",
          "hidden": false
        },
        {
          "_id": "67dce4c10784200359ab2496",
          "name": "Zhenxiong Tan",
          "hidden": false
        },
        {
          "_id": "67dce4c10784200359ab2497",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T16:44:43.000Z",
      "submittedOnDailyAt": "2025-03-21T02:36:06.925Z",
      "title": "超解像度アダプターンの簡単な変換",
      "submittedOnDailyBy": {
        "_id": "6486fb33570a419f41a882e4",
        "avatarUrl": "/avatars/860a42074439a23c629cd23851ae4da6.svg",
        "isPro": false,
        "fullname": "Ruonan Yu",
        "user": "roseannelexie",
        "type": "user"
      },
      "summary": "テキストから画像のディフュージョンモデルは最近数年にわたって驚異的な進歩を達しています。しかし、高解像度画像生成に対するモデルの訓練は、特に訓練データと計算リソースが限られている場合は難しいです。本論文では、この実用的な問題をデータとパラメータの効率性の2つのキー的な観点から調査し、超解像度アダプターと呼ばれるURAEのキーガイドラインを提案します。データの効率性について、理論的にも実験的に示しましたが、教師モデルが生成した合成データは訓練の収束を大幅に促進することができます。パラメータの効率性について、合成データが利用できない場合には、重み行列の少しの成分を調整することが広く使用されている低レンジアダプターよりもより良い性能を示すことがわかり、性能の大幅な向上を実現しながらも効率を維持します。また、ガイドライティングの煉熱を利用するモデル（例えばFLUX）については、クラスフレーゼフリーガイドランスを非効果化すること（アダプター時にガイドランススケールを1に設定すること）が満足する性能を得ることが重要であることを示しました。拡張的な実験は、URAEは、FLUX1.1[Pro] Ultraと同じレベルの2K生成性能を達成し、それに比べて3Kサンプルと2Kイテレーションで4Kレンジ生成の新たなベンチマークを設定していることを証明しました。コードは、https://github.com/Huage001/URAE{here}から利用可能です。",
      "upvotes": 7,
      "discussionId": "67dce4c50784200359ab25dc",
      "ai_keywords": [
        "text-to-image diffusion models",
        "high-resolution image generation",
        "training data",
        "computational resources",
        "data efficiency",
        "synthetic data",
        "teacher models",
        "training convergence",
        "parameter efficiency",
        "weight matrices",
        "low-rank adapters",
        "guidance distillation",
        "FLUX",
        "classifier-free guidance",
        "guidance scale",
        "2K-generation performance",
        "FLUX1.1",
        "4K-resolution generation"
      ]
    },
    "publishedAt": "2025-03-20T12:44:43.000Z",
    "title": "Ultra-Resolution Adaptation with Ease",
    "summary": "Text-to-image diffusion models have achieved remarkable progress in recent\nyears. However, training models for high-resolution image generation remains\nchallenging, particularly when training data and computational resources are\nlimited. In this paper, we explore this practical problem from two key\nperspectives: data and parameter efficiency, and propose a set of key\nguidelines for ultra-resolution adaptation termed URAE. For data\nefficiency, we theoretically and empirically demonstrate that synthetic data\ngenerated by some teacher models can significantly promote training\nconvergence. For parameter efficiency, we find that tuning minor components of\nthe weight matrices outperforms widely-used low-rank adapters when synthetic\ndata are unavailable, offering substantial performance gains while maintaining\nefficiency. Additionally, for models leveraging guidance distillation, such as\nFLUX, we show that disabling classifier-free guidance, i.e., setting\nthe guidance scale to 1 during adaptation, is crucial for satisfactory\nperformance. Extensive experiments validate that URAE achieves comparable\n2K-generation performance to state-of-the-art closed-source models like FLUX1.1\n[Pro] Ultra with only 3K samples and 2K iterations, while setting new\nbenchmarks for 4K-resolution generation. Codes are available\nhttps://github.com/Huage001/URAE{here}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16322.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6486fb33570a419f41a882e4",
      "avatarUrl": "/avatars/860a42074439a23c629cd23851ae4da6.svg",
      "fullname": "Ruonan Yu",
      "name": "roseannelexie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16057",
      "authors": [
        {
          "_id": "67dd04563b4c256a9809cc96",
          "name": "Yike Yuan",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc97",
          "name": "Ziyu Wang",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc98",
          "name": "Zihao Huang",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc99",
          "name": "Defa Zhu",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc9a",
          "name": "Xun Zhou",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc9b",
          "name": "Jingyi Yu",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc9c",
          "name": "Qiyang Min",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T11:45:08.000Z",
      "submittedOnDailyAt": "2025-03-21T04:50:45.905Z",
      "title": "専門家レース：デフォルトのスケーリングラインティング戦略によるミックスオブ専門家とトランジャーフォーマット",
      "submittedOnDailyBy": {
        "_id": "667505f4361b960c79e35486",
        "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
        "isPro": false,
        "fullname": "Defa Zhu",
        "user": "mathfinder",
        "type": "user"
      },
      "summary": "Diffusionモデルは、可視化生成の主流フレームワークとして立ち上がりました。この成功に基づき、Mixture of Experts (MoE) 方法の統合は、モデルのスケーラビリティと性能向上に望ましい効果を示しています。本論文では、Race-DiTという新しいMoEモデルを紹介します。これは、柔軟な路由戦略、Expert Raceを持つディフュージョントランスフォーマーです。トークンとエキスパーが一緒に競争し、最も有望な候補を選択することで、モデルは重要なトークンに対してエキスパーを動的に割り当てることを学ぶことができます。また、浅い層の学習における課題を解決するために、各層の正則化と、モード崩壊を防ぐためのルーター類似度損失を提案します。ImageNet上での拡張的な実験は、我々のアプローチの効果性を証明し、性能向上とスケーリング性能の期待であることを示しています。",
      "upvotes": 7,
      "discussionId": "67dd045a3b4c256a9809cdb1",
      "ai_keywords": [
        "diffusion models",
        "Mixture of Experts (MoE)",
        "Race-DiT",
        "diffusion transformers",
        "Expert Race",
        "tokens",
        "experts",
        "per-layer regularization",
        "router similarity loss",
        "mode collapse",
        "ImageNet"
      ]
    },
    "publishedAt": "2025-03-20T07:45:08.000Z",
    "title": "Expert Race: A Flexible Routing Strategy for Scaling Diffusion\n  Transformer with Mixture of Experts",
    "summary": "Diffusion models have emerged as mainstream framework in visual generation.\nBuilding upon this success, the integration of Mixture of Experts (MoE) methods\nhas shown promise in enhancing model scalability and performance. In this\npaper, we introduce Race-DiT, a novel MoE model for diffusion transformers with\na flexible routing strategy, Expert Race. By allowing tokens and experts to\ncompete together and select the top candidates, the model learns to dynamically\nassign experts to critical tokens. Additionally, we propose per-layer\nregularization to address challenges in shallow layer learning, and router\nsimilarity loss to prevent mode collapse, ensuring better expert utilization.\nExtensive experiments on ImageNet validate the effectiveness of our approach,\nshowcasing significant performance gains while promising scaling properties.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16057.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667505f4361b960c79e35486",
      "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
      "fullname": "Defa Zhu",
      "name": "mathfinder",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16421",
      "authors": [
        {
          "_id": "67dcd5913713a0e1da19bbe5",
          "name": "Quanhao Li",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbe6",
          "name": "Zhen Xing",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbe7",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbe8",
          "name": "Hui Zhang",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbe9",
          "name": "Qi Dai",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbea",
          "name": "Zuxuan Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:42.000Z",
      "submittedOnDailyAt": "2025-03-21T01:28:06.140Z",
      "title": "MagicMotion: 密かなトラジェクトガイドディングによる制御可能なビデオ生成",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "最近の映像生成の進歩は、可視性と時系列的連続性において驚異的な改善を収めています。この上で、軌道制御可能な映像生成が出現し、明確に定義された空間的パスを通じて物体の動きを精密に制御することができました。しかし、既存の方法は複雑な物体の動きと多物体の動きの制御に苦戦し、軌道の適応性が不精、物体の一貫性の低下、視覚的品質の損傷などの問題を伴います。また、これらの方法は軌道制御のフォーマットが一つしか持ち、多様なシナリオでの適用性が限定されています。また、軌道制御可能な映像生成に特に適した公開されているデータセットやベンチマークはありません、強固な訓練とシステマティックな評価を妨げています。これらの課題に対処するために、私たちはMagicMotionという新しい画像から映像生成フレームワークを介して軌道制御を可能にし、密かならびに稀薄な3つのレベルの条件から（マスク、バウンディングボックス、稀薄なボックス）軌道を通じて物体を流れ動かし、物体の一貫性と視覚的品質を維持することができるようにしました。入力画像と軌道を与えると、MagicMotionは定義された軌道に沿って物体を流れ動かし、物体の一貫性と視覚的品質を維持します。また、私たちはMagicDataという大規模な軌道制御可能な映像データセットを提供し、自動化されたアノテーションとフィルタリングプイプラインを提供します。また、MagicBenchという詳細なベンチマークを紹介し、物体の数によって変化する映像の品質と軌道制御の精度を評価することができます。拡大した実験は、MagicMotionは多様なメトリックで前の方法を超えていることを示しています。私たちのプロジェクトページは公開的に利用可能です https://quanhaol.github.io/magicmotion-site。",
      "upvotes": 6,
      "discussionId": "67dcd5953713a0e1da19bd51",
      "projectPage": "https://quanhaol.github.io/magicmotion-site",
      "ai_keywords": [
        "trajectory-controllable video generation",
        "dense conditions",
        "sparse conditions",
        "masks",
        "bounding boxes",
        "sparse boxes",
        "object consistency",
        "MagicMotion",
        "MagicData",
        "MagicBench"
      ]
    },
    "publishedAt": "2025-03-20T13:59:42.000Z",
    "title": "MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance",
    "summary": "Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16421.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6415
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16428",
      "authors": [
        {
          "_id": "67dcd7a53c21e084fe58c3a8",
          "name": "Ruyi Xu",
          "hidden": false
        },
        {
          "_id": "67dcd7a53c21e084fe58c3a9",
          "name": "Guangxuan Xiao",
          "hidden": false
        },
        {
          "_id": "67dcd7a53c21e084fe58c3aa",
          "user": {
            "_id": "63797f727df2fefdcaf3ff7e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668906853549-noauth.jpeg",
            "isPro": false,
            "fullname": "Song",
            "user": "songhan",
            "type": "user"
          },
          "name": "Haofeng Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-21T03:06:14.875Z",
          "hidden": false
        },
        {
          "_id": "67dcd7a53c21e084fe58c3ab",
          "name": "Junxian Guo",
          "hidden": false
        },
        {
          "_id": "67dcd7a53c21e084fe58c3ac",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:58.000Z",
      "submittedOnDailyAt": "2025-03-21T01:36:33.593Z",
      "title": "XAttention: ブロックスパースアテンションに反対対角線スコアリングを用いる",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "長文脈Transformerモデル（LCTMs）は実世界的なアプリケーションにおいて重要ですが、注意の二次元複雑性により高い計算コストを課せます。ブロックスパース注意は、重要な領域への計算を焦点にしてこれを軽減しますが、既存の方法はブロックの重要性の評価による高いコストにより精度と効率のバランスを保つに苦労しています。本論文では、XAttentionというプラグインとプレインプレイのフレームワークを紹介し、稀疏注意を使用してTransformerモデルの長文脈推論を大幅に加速します。XAttentionの主なイノベーションは、注意行列の反対対値の和（即、左下から右上まで）がブロックの重要性の強力な代理となることを理解しています。これにより、非重要なブロックの精確な識別と削除が可能で、高いスパース性と大幅に加速された推論が実現されます。RULERとLongBench（言語）、VideoMME（映像理解）、VBench（映像生成）などの複数の長文脈ベンチマークに対して詳細な評価を行い、XAttentionは全注意と同等の精度を達成しながら、大幅な計算コストの削減を実現します。XAttentionは注意計算において13.5倍の加速を示し、ブロックスパース注意の実用的な潜力を解放する能力を示し、実世界的なアプリケーションでの可換スキャラビリティと効率的な採用を可能にします。コードはhttps://github.com/mit-han-lab/x-attentionに提供されています。",
      "upvotes": 5,
      "discussionId": "67dcd7a63c21e084fe58c422",
      "ai_keywords": [
        "Long-Context Transformer Models (LCTMs)",
        "attention's quadratic complexity",
        "block-sparse attention",
        "block importance",
        "XAttention",
        "sparse attention",
        "attention matrix",
        "antidiagonal values",
        "block importance proxy",
        "precision identification",
        "block pruning",
        "high sparsity",
        "inference acceleration",
        "RULER benchmark",
        "LongBench benchmark",
        "VideoMME benchmark",
        "VBench benchmark",
        "video understanding",
        "video generation"
      ]
    },
    "publishedAt": "2025-03-20T13:59:58.000Z",
    "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
    "summary": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16428.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6415
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16422",
      "authors": [
        {
          "_id": "67dcd5da7f5c5665205b11c0",
          "name": "Yuheng Yuan",
          "hidden": false
        },
        {
          "_id": "67dcd5da7f5c5665205b11c1",
          "name": "Qiuhong Shen",
          "hidden": false
        },
        {
          "_id": "67dcd5da7f5c5665205b11c2",
          "name": "Xingyi Yang",
          "hidden": false
        },
        {
          "_id": "67dcd5da7f5c5665205b11c3",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/O7634-Dy0NBVM_UZzq5Nm.mp4"
      ],
      "publishedAt": "2025-03-20T17:59:44.000Z",
      "submittedOnDailyAt": "2025-03-21T01:29:12.177Z",
      "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
      "submittedOnDailyBy": {
        "_id": "634cfebc350bcee9bed20a4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
        "isPro": false,
        "fullname": "Xingyi Yang",
        "user": "adamdad",
        "type": "user"
      },
      "summary": "4D Gaussian Splatting (4DGS)は、動的なスペースを再現する方法として最近に相当の注目を集めています。しかし、高い質量を達成するには、大幅なストレージ要求と遅い描画速度の問題があります。本論文では、これらの問題について詳しく調査し、時間的な冗長性の2つの主な原因を特定しました。 (Q1) 短寿のGaussians：4DGSは、場の動きを表現するために、時間的なスパンが短いGaussiansを多く使用し、過多なGaussiansが生成されます。 (Q2) 不活躍なGaussians：描画時には、各フレームに対して貢献するGaussiansのみが少なく、すべてのGaussiansがレスタージェイションで処理され、冗長な計算オーバーヘッドが発生します。これらの冗長性に対処するために、4DGS-1Kを提案します。4DGS-1Kは現代のGPU上で1000FPS以上で動作します。 (Q1) に対して、スペクトラル-時間的な変化スコアを導入し、短寿のGaussiansを削減しながら、4DGSが長時間のスパンを持つGaussiansで場の動きを捉えるように促します。 (Q2) に対して、連続するフレームで活躍しているGaussiansのマスクを保存し、描画時の冗長な計算を大幅に減少させます。バージョン4DGSと比較して、複雑な動的な場に対しては、ストレージの41倍の削減とレスタージェイション速度の9倍の高速化を実現し、可視的な質量を維持します。詳細はプロジェクトページで確認してください (https://4DGS-1K.github.io)。",
      "upvotes": 5,
      "discussionId": "67dcd5e17f5c5665205b1422",
      "ai_keywords": [
        "4D Gaussian Splatting (4DGS)",
        "temporal redundancy",
        "Short-Lifespan Gaussians",
        "inactive Gaussians",
        "rasterization",
        "Spatial-Temporal Variation Score",
        "4DGS-1K",
        "FPS",
        "modern GPUs",
        "storage",
        "rasterization speed",
        "visual quality"
      ]
    },
    "publishedAt": "2025-03-20T13:59:44.000Z",
    "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
    "summary": "4D Gaussian Splatting (4DGS) has recently gained considerable attention as a\nmethod for reconstructing dynamic scenes. Despite achieving superior quality,\n4DGS typically requires substantial storage and suffers from slow rendering\nspeed. In this work, we delve into these issues and identify two key sources of\ntemporal redundancy. (Q1) Short-Lifespan Gaussians: 4DGS uses a large\nportion of Gaussians with short temporal span to represent scene dynamics,\nleading to an excessive number of Gaussians. (Q2) Inactive Gaussians:\nWhen rendering, only a small subset of Gaussians contributes to each frame.\nDespite this, all Gaussians are processed during rasterization, resulting in\nredundant computation overhead. To address these redundancies, we present\n4DGS-1K, which runs at over 1000 FPS on modern GPUs. For Q1, we\nintroduce the Spatial-Temporal Variation Score, a new pruning criterion that\neffectively removes short-lifespan Gaussians while encouraging 4DGS to capture\nscene dynamics using Gaussians with longer temporal spans. For Q2, we store a\nmask for active Gaussians across consecutive frames, significantly reducing\nredundant computations in rendering. Compared to vanilla 4DGS, our method\nachieves a 41times reduction in storage and 9times faster rasterization\nspeed on complex dynamic scenes, while maintaining comparable visual quality.\nPlease see our project page at https://4DGS-1K.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/O7634-Dy0NBVM_UZzq5Nm.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16422.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634cfebc350bcee9bed20a4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
      "fullname": "Xingyi Yang",
      "name": "adamdad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16413",
      "authors": [
        {
          "_id": "67dccb8ca33f11a56567bd61",
          "name": "Xueyan Zou",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd62",
          "name": "Yuchen Song",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd63",
          "name": "Ri-Zhao Qiu",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd64",
          "name": "Xuanbin Peng",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd65",
          "name": "Jianglong Ye",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd66",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd67",
          "name": "Xiaolong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:12.000Z",
      "submittedOnDailyAt": "2025-03-21T00:52:49.431Z",
      "title": "M3: 3Dスペクトル多モードルメモリ",
      "submittedOnDailyBy": {
        "_id": "62520988818a5dc29ab91d6f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671670066375-62520988818a5dc29ab91d6f.png",
        "isPro": false,
        "fullname": "Xueyan Zou",
        "user": "xueyanz",
        "type": "user"
      },
      "summary": "ここでは、3Dスペクトラル多モデルメモリ（M3）を紹介します。M3は、ビデオソースからの静的な中型スペクトラルの情報を保存し、視覚認識に役立つために設計された多モデルメモリシステムです。M3は、3Dガウススプレッティングテクニックと基盤モデルを統合し、構造規格の広い範囲の知識を含む特徴表現を渲染するための多モデルメモリを構築します。先行研究では、2つの重要な課題がありました：（1）各ガウスプリミティブの高次元特徴量の保存における計算制約、（2）結合された特徴量と基盤モデルの特徴量の調整または情報の失われ。これらの課題に対処するために、M3に主なスペクトラル成分とガウスメモリアタンションの要約成分を提案し、効率的な訓練と推論を可能にします。M3の有効性を証明するために、特徴量の類似性と下流タスクの評価を詳細に行い、ガウスメモリアタンションのピクセルトレースを可視化します。我々のアプローチは、視覚言語モデル（VLMs）、認識モデル、大規模多モデルおよび言語モデル（LMMs/LLMs）などの多様な基盤モデルを含みます。また、実世界的な応用を示すために、M3の特徴量フィールドをオートロボットの室内スペクトラルに採用します。特に、M3が3D特徴量の核心的な圧縮課題を解決する最初の研究として、この課題を解決することを主張します。",
      "upvotes": 5,
      "discussionId": "67dccb92a33f11a56567bf43",
      "ai_keywords": [
        "3D Spatial MultiModal Memory (M3)",
        "3D Gaussian Splatting",
        "feature representations",
        "granularities",
        "principal scene components",
        "Gaussian memory attention",
        "feature splatting",
        "computational constraints",
        "high-dimensional features",
        "Gaussian primitive",
        "misalignment",
        "information loss",
        "distilled features",
        "foundation models",
        "vision-language models (VLMs)",
        "perception models",
        "large multimodal and language models (LMMs/LLMs)",
        "feature field",
        "quadruped robot",
        "core compression challenges",
        "3D feature distillation"
      ]
    },
    "publishedAt": "2025-03-20T13:59:12.000Z",
    "title": "M3: 3D-Spatial MultiModal Memory",
    "summary": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16413.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62520988818a5dc29ab91d6f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671670066375-62520988818a5dc29ab91d6f.png",
      "fullname": "Xueyan Zou",
      "name": "xueyanz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16188",
      "authors": [
        {
          "_id": "67dce5afbabeda89ca6071c3",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67dce5afbabeda89ca6071c4",
          "name": "Shitian Zhao",
          "hidden": false
        },
        {
          "_id": "67dce5afbabeda89ca6071c5",
          "name": "Jike Zhong",
          "hidden": false
        },
        {
          "_id": "67dce5afbabeda89ca6071c6",
          "name": "Yuxiang Lai",
          "hidden": false
        },
        {
          "_id": "67dce5afbabeda89ca6071c7",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T14:37:45.000Z",
      "submittedOnDailyAt": "2025-03-21T03:47:03.083Z",
      "title": "CLS-RL: ルールベースの強化学習を用いた画像分類",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "クラス分類は機械学習の核心タスクである。最近の研究によると、モデルランド大語言モデル（MLLMs）は初期的に画像クラス分類には劣りがありましたが、適切なデータ量での微調節で性能を大幅に向上させ、最先端のクラス分類モデルと比較できるようになりました。しかし、大規模な標準化データの取得は高額です。本論文では、少ショットのMLLMクラス分類の微調節について調査します。私たちはSFT（シンプルなフィードバックティーンディング）が厳しい過学習問題を引き起こし、ゼロショットアプローチよりも性能が低下する可能性があることを見出しました。この挑戦に対して、ルールベースの強化学習の最近の成功をニュースとして、CLS-RL（クラス分類強化学習）を提案しました。CLS-RLは可証明的な信号を報酬としてMLLMを微調節します。私たちは、CLS-RLは多数のデータセットでSFTよりも優れてい、基礎から新しいデータセットへの学習もゼロショットモデルよりも平均精度が大幅に高くなりました。また、CLS-RLには「フリーランチャー現象」が見られました。特定のデータセットでの微調節によって、モデルの他の異なるデータセットへの性能もゼロショットモデルよりも向上し、データセットの分布やクラス名が異なる場合も同様の効果を示しました。これは、強化学習による方法がモデルにクラス分類の基本的な理解を教えることができることを示しています。最後に、推論時の思考に関する最近の研究をニュースとして、微調節中の「思考プロセス」を再検討し、強化学習による方法の重要な面です。このプロセスが必要かどうかを質疑し、これが実際に性能を低下させる可能性があることを提案しました。この前提に基づいて、等精度報酬を設定して思考プロセスを最小化するNo-Thinking-CLS-RLモジュールを提案しました。この方法は、大幅に短縮された微調節時間で、CLS-RLよりもモデル内の性能と一般化能力が上がりました。",
      "upvotes": 5,
      "discussionId": "67dce5b1babeda89ca607241",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "few-shot MLLM classification fine-tuning",
        "SFT",
        "overfitting issues",
        "zero-shot approach",
        "CLS-RL",
        "verifiable signals",
        "reward",
        "free-lunch phenomenon",
        "base-to-new",
        "few-shot learning",
        "RL-based methods",
        "inference time thinking",
        "thinking process",
        "No-Thinking-CLS-RL",
        "equality accuracy reward"
      ]
    },
    "publishedAt": "2025-03-20T10:37:45.000Z",
    "title": "CLS-RL: Image Classification with Rule-Based Reinforcement Learning",
    "summary": "Classification is a core task in machine learning. Recent research has shown\nthat although Multimodal Large Language Models (MLLMs) are initially poor at\nimage classification, fine-tuning them with an adequate amount of data can\nsignificantly enhance their performance, making them comparable to SOTA\nclassification models. However, acquiring large-scale labeled data is\nexpensive. In this paper, we explore few-shot MLLM classification fine-tuning.\nWe found that SFT can cause severe overfitting issues and may even degrade\nperformance over the zero-shot approach. To address this challenge, inspired by\nthe recent successes in rule-based reinforcement learning, we propose CLS-RL,\nwhich uses verifiable signals as reward to fine-tune MLLMs. We discovered that\nCLS-RL outperforms SFT in most datasets and has a much higher average accuracy\non both base-to-new and few-shot learning setting. Moreover, we observed a\nfree-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular\ndataset, their performance on other distinct datasets may also improve over\nzero-shot models, even if those datasets differ in distribution and class\nnames. This suggests that RL-based methods effectively teach models the\nfundamentals of classification. Lastly, inspired by recent works in inference\ntime thinking, we re-examine the `thinking process' during fine-tuning, a\ncritical aspect of RL-based methods, in the context of visual classification.\nWe question whether such tasks require extensive thinking process during\nfine-tuning, proposing that this may actually detract from performance. Based\non this premise, we introduce the No-Thinking-CLS-RL method, which minimizes\nthinking processes during training by setting an equality accuracy reward. Our\nfindings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL\nmethod achieves superior in-domain performance and generalization capabilities\nthan CLS-RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16188.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.13657",
      "authors": [
        {
          "_id": "67dc4391f618f3c7ba6a3b6d",
          "name": "Mert Cemri",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b6e",
          "name": "Melissa Z. Pan",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b6f",
          "name": "Shuyi Yang",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b70",
          "name": "Lakshya A. Agrawal",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b71",
          "name": "Bhavya Chopra",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b72",
          "name": "Rishabh Tiwari",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b73",
          "name": "Kurt Keutzer",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b74",
          "name": "Aditya Parameswaran",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b75",
          "name": "Dan Klein",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b76",
          "name": "Kannan Ramchandran",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b77",
          "name": "Matei Zaharia",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b78",
          "name": "Joseph E. Gonzalez",
          "hidden": false
        },
        {
          "_id": "67dc4391f618f3c7ba6a3b79",
          "name": "Ion Stoica",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-17T19:04:38.000Z",
      "submittedOnDailyAt": "2025-03-21T07:07:51.154Z",
      "title": "多Agent LLMシステムが失敗する理由",
      "submittedOnDailyBy": {
        "_id": "5ff5d596f244529b3ec0fb89",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png",
        "isPro": false,
        "fullname": "Philipp Schmid",
        "user": "philschmid",
        "type": "user"
      },
      "summary": "マルチアゲンシステム（MAS）では、複数のLLMアジェントがタスクを共同で実現することに対する熱意が高まっていますが、これらのアジェントがベンチマークでの性能向上は、単一アジェントフレームワークに比べて最小限なのである。この間違いは、MASの効果性における課題を分析する必要を明らかにしています。\n\nこの論文では、MASの課題に関する最初の詳細な研究を提供します。5つのベストランクのMASフレームワークを超150タスクにわたり、6人の専門家のアノテーターによる分析を行います。14種類の独自の失敗モードを識別し、それらを適用可能な詳細なタクロノミーを提案します。このタクロノミーは、3人の専門家のアノテーターが誰もが同意したもので、Cohen's Kappaスコアが0.88となります。これらの細かい失敗モードは、(i)規格とシステムデザインの失敗、(ii)アジェント間の不適切なアライメント、(iii)タスクの検証と終了に分類されています。スケーラブルな評価を支援するために、MASFTとLLM-as-a-Judgeを統合します。また、識別された失敗を簡単に予防することができるかどうかを調査するために、アジェントの役割の規格の改善とオーチャーストラテジーの向上を提案します。この論文では、識別された失敗は複雑な解決策が必要であることを示し、将来の研究に向けて明確なプログラムを示します。この論文では、データセットとLLMアノテーターを公開しています。",
      "upvotes": 5,
      "discussionId": "67dc4392f618f3c7ba6a3be9",
      "githubRepo": "https://github.com/multi-agent-systems-failure-taxonomy/MASFT",
      "ai_keywords": [
        "Multi-Agent Systems (MAS)",
        "LLM agents",
        "performance gains",
        "single-agent frameworks",
        "failure modes",
        "Cohen's Kappa score",
        "specification and system design failures",
        "inter-agent misalignment",
        "task verification and termination",
        "LLM-as-a-Judge",
        "orchestration strategies"
      ]
    },
    "publishedAt": "2025-03-17T15:04:38.000Z",
    "title": "Why Do Multi-Agent LLM Systems Fail?",
    "summary": "Despite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM\nagents collaborate to accomplish tasks, their performance gains across popular\nbenchmarks remain minimal compared to single-agent frameworks. This gap\nhighlights the need to analyze the challenges hindering MAS effectiveness.\n  In this paper, we present the first comprehensive study of MAS challenges. We\nanalyze five popular MAS frameworks across over 150 tasks, involving six expert\nhuman annotators. We identify 14 unique failure modes and propose a\ncomprehensive taxonomy applicable to various MAS frameworks. This taxonomy\nemerges iteratively from agreements among three expert annotators per study,\nachieving a Cohen's Kappa score of 0.88. These fine-grained failure modes are\norganized into 3 categories, (i) specification and system design failures, (ii)\ninter-agent misalignment, and (iii) task verification and termination. To\nsupport scalable evaluation, we integrate MASFT with LLM-as-a-Judge. We also\nexplore if identified failures could be easily prevented by proposing two\ninterventions: improved specification of agent roles and enhanced orchestration\nstrategies. Our findings reveal that identified failures require more complex\nsolutions, highlighting a clear roadmap for future research. We open-source our\ndataset and LLM annotator.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13657.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5ff5d596f244529b3ec0fb89",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png",
      "fullname": "Philipp Schmid",
      "name": "philschmid",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 811
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16397",
      "authors": [
        {
          "_id": "67dd1227046f2c38458e9588",
          "name": "Nikita Starodubcev",
          "hidden": false
        },
        {
          "_id": "67dd1227046f2c38458e9589",
          "name": "Denis Kuznedelev",
          "hidden": false
        },
        {
          "_id": "67dd1227046f2c38458e958a",
          "name": "Artem Babenko",
          "hidden": false
        },
        {
          "_id": "67dd1227046f2c38458e958b",
          "name": "Dmitry Baranchuk",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6410d3a4cfbe9c4400233d1e/lkzM32YzNPrhP9ESkaUpF.png"
      ],
      "publishedAt": "2025-03-20T17:54:02.000Z",
      "submittedOnDailyAt": "2025-03-21T08:05:39.256Z",
      "title": "スケールワイズ付きのディフューションモデルの煉熱",
      "submittedOnDailyBy": {
        "_id": "6410d3a4cfbe9c4400233d1e",
        "avatarUrl": "/avatars/9e1c4e0ea5f6964c90013f6e19c41db1.svg",
        "isPro": false,
        "fullname": "nikita",
        "user": "quickjkee",
        "type": "user"
      },
      "summary": "SwDは、ディフフェラスモデル（DMs）に対するスケールごとのディスティルダフォームです。SwDは、次のスケールの予測アイデアを効果的に活用して、ディフフェラスベースの少数ステップジェネレーターを実現します。詳しくは、SwDは最近のディフフェラスプロセスと隠れサピュラル自動帰り回りに関連する見解によって得られたものです。DMsは、データの低レンジュで生成を開始し、ノイズステップごとにサンプルを進段的にアップスケーリングし、性能の損失を最小化しながら計算コストを大幅に削減できることを仮定しています。SwDは、現在の分布マッチングに基づくディスティルダフォームに自然的にこのアイデアを統合し、また、新しいパッチ損失を紹介して分布マッチングアプローチの家族を豊富にします。SwDは、最先端のテキストから画像へのディフフェラスモデルに対して応用された場合、2ステップの全分辨率ステップの推論時間を近似し、同じ計算バジュート下では自動評価メトリックと人間の好み調査によって明らかに機械学習モデルの性能を大幅に向上させます。",
      "upvotes": 4,
      "discussionId": "67dd1229046f2c38458e9617",
      "projectPage": "https://yandex-research.github.io/swd/",
      "githubRepo": "https://github.com/yandex-research/swd",
      "ai_keywords": [
        "scale-wise distillation",
        "diffusion models",
        "next-scale prediction",
        "implicit spectral autoregression",
        "denoising",
        "computational costs",
        "distribution matching",
        "patch loss",
        "text-to-image diffusion models",
        "inference times",
        "computation budget",
        "automated metrics",
        "human preference studies"
      ]
    },
    "publishedAt": "2025-03-20T13:54:02.000Z",
    "title": "Scale-wise Distillation of Diffusion Models",
    "summary": "We present SwD, a scale-wise distillation framework for diffusion models\n(DMs), which effectively employs next-scale prediction ideas for\ndiffusion-based few-step generators. In more detail, SwD is inspired by the\nrecent insights relating diffusion processes to the implicit spectral\nautoregression. We suppose that DMs can initiate generation at lower data\nresolutions and gradually upscale the samples at each denoising step without\nloss in performance while significantly reducing computational costs. SwD\nnaturally integrates this idea into existing diffusion distillation methods\nbased on distribution matching. Also, we enrich the family of distribution\nmatching approaches by introducing a novel patch loss enforcing finer-grained\nsimilarity to the target distribution. When applied to state-of-the-art\ntext-to-image diffusion models, SwD approaches the inference times of two full\nresolution steps and significantly outperforms the counterparts under the same\ncomputation budget, as evidenced by automated metrics and human preference\nstudies.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6410d3a4cfbe9c4400233d1e/lkzM32YzNPrhP9ESkaUpF.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16397.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6410d3a4cfbe9c4400233d1e",
      "avatarUrl": "/avatars/9e1c4e0ea5f6964c90013f6e19c41db1.svg",
      "fullname": "nikita",
      "name": "quickjkee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16278",
      "authors": [
        {
          "_id": "67dcc98b54dcfdc1fd17d9b6",
          "name": "Shuqi Lu",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9b7",
          "name": "Haowei Lin",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9b8",
          "name": "Lin Yao",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9b9",
          "name": "Zhifeng Gao",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9ba",
          "name": "Xiaohong Ji",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9bb",
          "name": "Weinan E",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9bc",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9bd",
          "name": "Guolin Ke",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T16:07:04.000Z",
      "submittedOnDailyAt": "2025-03-21T00:36:39.769Z",
      "title": "Uni-3DAR: 圧縮シティートークン上の自動復元による統合ディメンショナル生成と理解",
      "submittedOnDailyBy": {
        "_id": "6348de0c62c668c7b48d83c9",
        "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
        "isPro": false,
        "fullname": "Guolin Ke",
        "user": "guolinke",
        "type": "user"
      },
      "summary": "最近の大語言モデルの進歩とその多モーダル拡張版は、自動単語予測による生成と理解の統一の効果性を示しています。しかし、科学のAIでの3次元構造の生成と理解（{3D GU}）は、自動単語法による方法が主に調査されていない状態で、独立に進化しています。この隙を埋めるために、我々はUni-3DARという統一フレームワークを紹介します。これは、自動単語予測を通じて{3D GU}タスクを無間に統合します。Uni-3DARの中心的な部分では、新しい階層的トークナリゼーションを用いて、オクトリーにより3次元空間を圧縮し、3次元構造の固有の稀疏性を活用します。その後、細かい構造の詳細を捉えるために追加のトークナリゼーションを適用し、原子の種類や微視的な3次元構造の正確な空間座標などのキー属性を捉えます。また、効率と効果性を向上させるために2つの最適化を提案します。1つは、オクトリートークンシーケンスを8倍まで減少させる2段階サブツリー圧縮戦略です。2つは、動的に変化するトークン位置に適したマスク付き次のトークン予測機構で、モデルの性能を大幅に向上させます。これらの戦略を組み合わせて、Uni-3DARは、単一の自動単語予測フレームワークで多様な{3D GU}タスクを統一します。分子、蛋白質、ポリマー、クリスタルなどの複数の微观{3D GU}タスクにおいて幅広く実験を行い、その効果性と広泛性を証明しました。特に、Uni-3DARは前の最先端の拡散モデルを大幅に超え、256%の相対的な改善を達成し、推論速度が21.8倍速くなります。コードは、https://github.com/dptech-corp/Uni-3DARで公開しています。",
      "upvotes": 4,
      "discussionId": "67dcc98c54dcfdc1fd17da0f",
      "githubRepo": "https://github.com/dptech-corp/Uni-3DAR",
      "ai_keywords": [
        "hierarchical tokenization",
        "octree",
        "two-level subtree compression strategy",
        "masked next-token prediction mechanism",
        "Uni-3DAR",
        "3D GU (3D generation and understanding)",
        "autoregressive prediction",
        "state-of-the-art diffusion models"
      ]
    },
    "publishedAt": "2025-03-20T12:07:04.000Z",
    "title": "Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens",
    "summary": "Recent advancements in large language models and their multi-modal extensions\nhave demonstrated the effectiveness of unifying generation and understanding\nthrough autoregressive next-token prediction. However, despite the critical\nrole of 3D structural generation and understanding ({3D GU}) in AI for science,\nthese tasks have largely evolved independently, with autoregressive methods\nremaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified\nframework that seamlessly integrates {3D GU} tasks via autoregressive\nprediction. At its core, Uni-3DAR employs a novel hierarchical tokenization\nthat compresses 3D space using an octree, leveraging the inherent sparsity of\n3D structures. It then applies an additional tokenization for fine-grained\nstructural details, capturing key attributes such as atom types and precise\nspatial coordinates in microscopic 3D structures. We further propose two\noptimizations to enhance efficiency and effectiveness. The first is a two-level\nsubtree compression strategy, which reduces the octree token sequence by up to\n8x. The second is a masked next-token prediction mechanism tailored for\ndynamically varying token positions, significantly boosting model performance.\nBy combining these strategies, Uni-3DAR successfully unifies diverse {3D GU}\ntasks within a single autoregressive framework. Extensive experiments across\nmultiple microscopic {3D GU} tasks, including molecules, proteins, polymers,\nand crystals, validate its effectiveness and versatility. Notably, Uni-3DAR\nsurpasses previous state-of-the-art diffusion models by a substantial margin,\nachieving up to 256\\% relative improvement while delivering inference speeds up\nto 21.8x faster. The code is publicly available at\nhttps://github.com/dptech-corp/Uni-3DAR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6348de0c62c668c7b48d83c9",
      "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
      "fullname": "Guolin Ke",
      "name": "guolinke",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15567",
      "authors": [
        {
          "_id": "67dcc734067589b43b19af7a",
          "name": "Yanchen Luo",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7b",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7c",
          "name": "Yi Zhao",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7d",
          "name": "Sihang Li",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7e",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7f",
          "name": "Tat-Seng Chua",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af80",
          "name": "Xiang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T08:56:13.000Z",
      "submittedOnDailyAt": "2025-03-21T00:33:21.187Z",
      "title": "「3D 分子潜在扩散建模のユニーク潜在空間への向け」",
      "submittedOnDailyBy": {
        "_id": "64f04a28f3cd962c21726459",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/MOTc7SWbzc4jdJbMcWMcK.jpeg",
        "isPro": false,
        "fullname": "LuoYanchen",
        "user": "lyc0930",
        "type": "user"
      },
      "summary": "3D分子生成は薬物発見と材料科学において重要であり、原子の種類、化学結合、3D座標などの複雑な多モーダリティを処理するモデルが必要とされる。主な課題は、異なる形状のモーダリティを統合しながら3D座標のSE(3)等対称性を維持することである。これを達成するために、現在のアプローチは通常、変換不変性と等対称性のモーダリティに対して別々の潜在空間を維持し、訓練およびサンプリングの両方での効率を低下させている。本研究では、Unified Variational Auto-Encoder for 3D Molecular Latent Diffusion Modeling (UAE-3D)を提案しています。UAE-3Dは、3D分子を統一的な潜在空間からの潜在シーケンスに圧縮し、近似0の再構成エラーを維持する多モーダリティのVAEです。この統一的な潜在空間は、潜在ディフュージョンモデリングを行う際に多モーダリティと等対称性の複雑さを排除します。Diffusion Transformer（一般的なディフュージョンモデルで、分子の導出バイアスを持たない）を使用して潜在ジェネレーションを行い、GEOM-DrugsとQM9データセットでの実験を通じて、我々の方法が新たなベンチマークを大幅に向上させ、新しい3D分子のデュノールと条件付き生成の両方で先駆力の効率と品質を達成していることを示しています。",
      "upvotes": 4,
      "discussionId": "67dcc735067589b43b19afd9",
      "ai_keywords": [
        "SE(3) equivariance",
        "latent spaces",
        "multi-modal VAE",
        "latent sequences",
        "unified latent space",
        "latent diffusion modeling",
        "Diffusion Transformer",
        "GEOM-Drugs dataset",
        "QM9 dataset",
        "de novo molecule generation",
        "conditional molecule generation"
      ]
    },
    "publishedAt": "2025-03-19T04:56:13.000Z",
    "title": "Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling",
    "summary": "3D molecule generation is crucial for drug discovery and material science,\nrequiring models to process complex multi-modalities, including atom types,\nchemical bonds, and 3D coordinates. A key challenge is integrating these\nmodalities of different shapes while maintaining SE(3) equivariance for 3D\ncoordinates. To achieve this, existing approaches typically maintain separate\nlatent spaces for invariant and equivariant modalities, reducing efficiency in\nboth training and sampling. In this work, we propose Unified\nVariational Auto-Encoder for 3D Molecular Latent\nDiffusion Modeling (UAE-3D), a multi-modal VAE that compresses 3D\nmolecules into latent sequences from a unified latent space, while maintaining\nnear-zero reconstruction error. This unified latent space eliminates the\ncomplexities of handling multi-modality and equivariance when performing latent\ndiffusion modeling. We demonstrate this by employing the Diffusion\nTransformer--a general-purpose diffusion model without any molecular inductive\nbias--for latent generation. Extensive experiments on GEOM-Drugs and QM9\ndatasets demonstrate that our method significantly establishes new benchmarks\nin both de novo and conditional 3D molecule generation, achieving\nleading efficiency and quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15567.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f04a28f3cd962c21726459",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/MOTc7SWbzc4jdJbMcWMcK.jpeg",
      "fullname": "LuoYanchen",
      "name": "lyc0930",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10625",
      "authors": [
        {
          "_id": "67dacb439c49701f604e4257",
          "name": "Lingteng Qiu",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e4258",
          "name": "Xiaodong Gu",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e4259",
          "name": "Peihao Li",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425a",
          "user": {
            "_id": "64d0d72e15b26cc7f704a60f",
            "avatarUrl": "/avatars/f450013fb2b204f1422e614629914248.svg",
            "isPro": true,
            "fullname": "Qi Zuo",
            "user": "DyrusQZ",
            "type": "user"
          },
          "name": "Qi Zuo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-20T10:46:05.121Z",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425b",
          "name": "Weichao Shen",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425c",
          "name": "Junfei Zhang",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425d",
          "name": "Kejie Qiu",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425e",
          "name": "Weihao Yuan",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425f",
          "name": "Guanying Chen",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e4260",
          "name": "Zilong Dong",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e4261",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64d0d72e15b26cc7f704a60f/tiDonOVoU7mFVv3w6qgcR.mp4"
      ],
      "publishedAt": "2025-03-13T17:59:21.000Z",
      "submittedOnDailyAt": "2025-03-21T05:52:51.662Z",
      "title": "LHM: 秒速の1枚の画像からの大きな可動人間再構築モデル",
      "submittedOnDailyBy": {
        "_id": "64d0d72e15b26cc7f704a60f",
        "avatarUrl": "/avatars/f450013fb2b204f1422e614629914248.svg",
        "isPro": true,
        "fullname": "Qi Zuo",
        "user": "DyrusQZ",
        "type": "user"
      },
      "summary": "動かせる3D人間再構築従一枚の写真からは、ジオメトリー、外観、変形の離れ方の不明確性により艱難な問題です。最近の3D人間再構築の進展は主に静的な人間モデリングに焦点を当てていますが、合成的な3Dスキャンを用いた訓練に依存していることにより、一般化能力が限られています。一方、最適化ベースのビデオメソッドはファイドエリティーが高く、制御された撮影条件と計算量の軽い精練プロセスが必要です。大規模な再構築モデルの出現をモチーフに、効率的な静的再構築を実現するために、LHM（Large Animatable Human Reconstruction Model）を提案します。LHMは、3Dガウススプレッティングで表現される高ファイドエリティーのアバターを一回の前向きパスで推論することができます。我々のモデルは、多モーダルトランスフォーマーアーキテクチャを拡張し、人間ボディの位置フィーチャフィーチャと画像フィーチャを注意力機構を用いて効果的にエンコードすることで、衣裳のジオメトリーとテクスチャの詳細保存を可能にします。また、顔の識別保持と細部の再複元を進めるために、頭の特徴フィーチャピラミッドエンコーディングスキームを提案します。広範囲の実験では、我々のLHMは、顔と手の後処理を除いた秒程度で証拠的な動かせる人間を生成し、既存の方法に比べて再構築精度と一般化能力にも優れていることが示されました。",
      "upvotes": 4,
      "discussionId": "67dacb499c49701f604e4454",
      "ai_keywords": [
        "3D Gaussian splatting",
        "multimodal transformer architecture",
        "attention mechanism",
        "head feature pyramid encoding scheme"
      ]
    },
    "publishedAt": "2025-03-13T13:59:21.000Z",
    "title": "LHM: Large Animatable Human Reconstruction Model from a Single Image in\n  Seconds",
    "summary": "Animatable 3D human reconstruction from a single image is a challenging\nproblem due to the ambiguity in decoupling geometry, appearance, and\ndeformation. Recent advances in 3D human reconstruction mainly focus on static\nhuman modeling, and the reliance of using synthetic 3D scans for training\nlimits their generalization ability. Conversely, optimization-based video\nmethods achieve higher fidelity but demand controlled capture conditions and\ncomputationally intensive refinement processes. Motivated by the emergence of\nlarge reconstruction models for efficient static reconstruction, we propose LHM\n(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars\nrepresented as 3D Gaussian splatting in a feed-forward pass. Our model\nleverages a multimodal transformer architecture to effectively encode the human\nbody positional features and image features with attention mechanism, enabling\ndetailed preservation of clothing geometry and texture. To further boost the\nface identity preservation and fine detail recovery, we propose a head feature\npyramid encoding scheme to aggregate multi-scale features of the head regions.\nExtensive experiments demonstrate that our LHM generates plausible animatable\nhuman in seconds without post-processing for face and hands, outperforming\nexisting methods in both reconstruction accuracy and generalization ability.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64d0d72e15b26cc7f704a60f/tiDonOVoU7mFVv3w6qgcR.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d0d72e15b26cc7f704a60f",
      "avatarUrl": "/avatars/f450013fb2b204f1422e614629914248.svg",
      "fullname": "Qi Zuo",
      "name": "DyrusQZ",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.16252",
      "authors": [
        {
          "_id": "67dcc7b29c17514cb9815abd",
          "name": "Zhaowei Liu",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815abe",
          "name": "Xin Guo",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815abf",
          "name": "Fangqi Lou",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac0",
          "name": "Lingfeng Zeng",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac1",
          "name": "Jinyi Niu",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac2",
          "name": "Zixuan Wang",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac3",
          "name": "Jiajie Xu",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac4",
          "name": "Weige Cai",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac5",
          "name": "Ziwei Yang",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac6",
          "name": "Xueqian Zhao",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac7",
          "name": "Chao Li",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac8",
          "name": "Sheng Xu",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac9",
          "name": "Dezhi Chen",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815aca",
          "name": "Yun Chen",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815acb",
          "name": "Zuo Bai",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815acc",
          "name": "Liwen Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T15:46:18.000Z",
      "submittedOnDailyAt": "2025-03-21T00:54:01.337Z",
      "title": "Fin-R1: 財務論理を通じたリニジュース学習による大規模言語モデル",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "大語言モデルは、複雑な金融タスクを処理する能力が深い探索が必要ですが、金融業界に特に設計された大語言モデルとしてFin-R1を紹介します。Fin-R1は、DeepSeek-R1に基づいて精製された金融論理データセットを利用して2段階アーキテクチャで構築されています。SFTとRLの訓練を通じて、7億パラメーターサイズで金融論理タスクの様々な範囲でDeepSeek-R1に近い性能を示します。FinQAとConvFinQAの評価で最先端の性能を達成し、他のモデルよりも優れた性能を示します。Fin-R1は、金融領域で現れる多様な問題を解決する強い論理と決策能力を示しています。コードは、https://github.com/SUFE-AIFLM-Lab/Fin-R1に公開されています。",
      "upvotes": 3,
      "discussionId": "67dcc7b69c17514cb9815c1d",
      "githubRepo": "https://github.com/SUFE-AIFLM-Lab/Fin-R1",
      "ai_keywords": [
        "reasoning large language models",
        "two-stage architecture",
        "financial reasoning dataset",
        "DeepSeek-R1",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "state-of-the-art (SOTA)",
        "FinQA",
        "ConvFinQA"
      ]
    },
    "publishedAt": "2025-03-20T11:46:18.000Z",
    "title": "Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning",
    "summary": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16252.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6415
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15851",
      "authors": [
        {
          "_id": "67dcff844aa37abf77ae7338",
          "name": "Zhou Zhenglin",
          "hidden": false
        },
        {
          "_id": "67dcff844aa37abf77ae7339",
          "name": "Ma Fan",
          "hidden": false
        },
        {
          "_id": "67dcff844aa37abf77ae733a",
          "name": "Fan Hehe",
          "hidden": false
        },
        {
          "_id": "67dcff844aa37abf77ae733b",
          "name": "Chua Tat-Seng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T05:07:46.000Z",
      "submittedOnDailyAt": "2025-03-21T04:27:49.029Z",
      "title": "Zero-1-to-A: 映像を用いたゼロショット1画像から動き出せる頭像アバターの生成",
      "submittedOnDailyBy": {
        "_id": "6425318d175bd2952281065e",
        "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
        "isPro": false,
        "fullname": "ZhenglinZhou",
        "user": "zhenglin",
        "type": "user"
      },
      "summary": "アニマティブな頭像モデルの生成は通常、複雑なデータを必要とする。データ要求を減らすための自然な解決策は、既存のデータ無し静的な頭像モデル生成手法を利用することである。これには、ディフュージョンモデルを事前学習したものやスコアディスティルサンプリング（SDS）を使用することが含まれる。しかし、4D頭像モデルをビデオディフュージョンから直接ディスタイルすると、生成されるビデオの空間的および時間的な不確実性により、オーバースムースな結果が発生する。この問題に対処するために、我々はZero-1-to-Aという強固な手法を提案しています。これは、ビデオディフュージョンモデルを用いて、4D頭像モデルの再構築に適切な空間的および時間的な一貫性のデータセットを合成することで、データ要求を減らすための解決策として提案されています。特に、Zero-1-to-Aは、進捗的な手順でビデオデータセットを構築し、進捗的にアニマティブな頭像モデルを最適化し、学習プロセス中にわからなくなるようにアニマティブな頭像モデルの質が平滑に増加し、一貫性を維持するようにしています。この進捗的な学習は2つのステップからなります：1）空間的な一貫性学習は、表情を固定し、前から横の視点から学習し、2）時間的な一貫性学習は、視点を固定し、放鬆したからっとの表情から学習し、簡単から複雑な方法で4D頭像モデルを生成します。拡張的な実験は、Zero-1-to-Aが現在のディフュージョンベースの手法と比較して、忠実度、アニメーションの質、渲染速度を向上させることを示し、生き生きとした頭像モデルの作成の解決策として提供しています。コードは公開的に利用可能です：https://github.com/ZhenglinZhou/Zero-1-to-A。",
      "upvotes": 3,
      "discussionId": "67dcff884aa37abf77ae7415",
      "ai_keywords": [
        "diffusion models",
        "score distillation sampling (SDS)",
        "pseudo ground-truth outputs",
        "video diffusion",
        "spatial consistency",
        "temporal consistency",
        "Zero-1-to-A",
        "front-to-side views",
        "progressive learning",
        "spatial consistency learning",
        "temporal consistency learning",
        "4D avatars",
        "avatar quality",
        "fidelity",
        "animation quality",
        "rendering speed"
      ]
    },
    "publishedAt": "2025-03-20T01:07:46.000Z",
    "title": "Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video\n  Diffusion",
    "summary": "Animatable head avatar generation typically requires extensive data for\ntraining. To reduce the data requirements, a natural solution is to leverage\nexisting data-free static avatar generation methods, such as pre-trained\ndiffusion models with score distillation sampling (SDS), which align avatars\nwith pseudo ground-truth outputs from the diffusion model. However, directly\ndistilling 4D avatars from video diffusion often leads to over-smooth results\ndue to spatial and temporal inconsistencies in the generated video. To address\nthis issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial\nand temporal consistency dataset for 4D avatar reconstruction using the video\ndiffusion model. Specifically, Zero-1-to-A iteratively constructs video\ndatasets and optimizes animatable avatars in a progressive manner, ensuring\nthat avatar quality increases smoothly and consistently throughout the learning\nprocess. This progressive learning involves two stages: (1) Spatial Consistency\nLearning fixes expressions and learns from front-to-side views, and (2)\nTemporal Consistency Learning fixes views and learns from relaxed to\nexaggerated expressions, generating 4D avatars in a simple-to-complex manner.\nExtensive experiments demonstrate that Zero-1-to-A improves fidelity, animation\nquality, and rendering speed compared to existing diffusion-based methods,\nproviding a solution for lifelike avatar creation. Code is publicly available\nat: https://github.com/ZhenglinZhou/Zero-1-to-A.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15851.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6425318d175bd2952281065e",
      "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
      "fullname": "ZhenglinZhou",
      "name": "zhenglin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16375",
      "authors": [
        {
          "_id": "67dd269625f9991caf94c667",
          "name": "Han-Hung Lee",
          "hidden": false
        },
        {
          "_id": "67dd269625f9991caf94c668",
          "name": "Qinghong Han",
          "hidden": false
        },
        {
          "_id": "67dd269625f9991caf94c669",
          "name": "Angel X. Chang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6313c7a754e6e5d9f0fa4d81/U4GisQW1kt0WXR-0HiBLv.mp4"
      ],
      "publishedAt": "2025-03-20T17:37:43.000Z",
      "submittedOnDailyAt": "2025-03-21T07:15:44.580Z",
      "title": "NuiScene: 無限の外景の効率的な生成についての研究",
      "submittedOnDailyBy": {
        "_id": "6313c7a754e6e5d9f0fa4d81",
        "avatarUrl": "/avatars/0b0e5f09171b87d61260225b2021aa98.svg",
        "isPro": true,
        "fullname": "HAN-HUNG LEE",
        "user": "rexleeppp",
        "type": "user"
      },
      "summary": "本論文では、城や高層ビルの範囲で幅広い外観の場所を生成するタスクを検討します。先行研究の主な焦点としていた室内場所の生成に比べ、外観場所の生成は場所の高さの広い変化や大規模な風景を迅速に生成する方法の必要性など、特有の課題を持っています。これに対して、我々は、先行手法で用いられていた空間的な構造化ラテンを比較してより良い圧縮と性能を提供するために、場所のチャンクを一貫したベクトルセットとしてエファクティブに収納する効率的なアプローチを提案します。また、無制限的な生成を実現するためには、明示的なオープンペイントモデルを訓練し、先行のリサンプリングベースのインペイントシナプスに比べてよりコンチューションが向上し、追加のディフュージョンステップを除去して生成速度を高速化します。このタスクを支援するために、我々は、小さなければならないが高品質の場所セットを選び出し、共同訓練に適した前処理を行います。特に、風格が異なる場所を訓練した場合、我々のモデルは、同じ場所内で郊外の家と都市のスクレーパーをブレンドし、我々の選び出しプロセスが異なる場所を共有訓練に利用できることを示します。",
      "upvotes": 2,
      "discussionId": "67dd269c25f9991caf94c87b",
      "projectPage": "https://3dlg-hcvc.github.io/NuiScene/",
      "githubRepo": "https://github.com/3dlg-hcvc/NuiScene",
      "ai_keywords": [
        "outpainting model",
        "explicit outpainting",
        "diffusion steps"
      ]
    },
    "publishedAt": "2025-03-20T13:37:43.000Z",
    "title": "NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes",
    "summary": "In this paper, we explore the task of generating expansive outdoor scenes,\nranging from castles to high-rises. Unlike indoor scene generation, which has\nbeen a primary focus of prior work, outdoor scene generation presents unique\nchallenges, including wide variations in scene heights and the need for a\nmethod capable of rapidly producing large landscapes. To address this, we\npropose an efficient approach that encodes scene chunks as uniform vector sets,\noffering better compression and performance than the spatially structured\nlatents used in prior methods. Furthermore, we train an explicit outpainting\nmodel for unbounded generation, which improves coherence compared to prior\nresampling-based inpainting schemes while also speeding up generation by\neliminating extra diffusion steps. To facilitate this task, we curate\nNuiScene43, a small but high-quality set of scenes, preprocessed for joint\ntraining. Notably, when trained on scenes of varying styles, our model can\nblend different environments, such as rural houses and city skyscrapers, within\nthe same scene, highlighting the potential of our curation process to leverage\nheterogeneous scenes for joint training.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313c7a754e6e5d9f0fa4d81/U4GisQW1kt0WXR-0HiBLv.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16375.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313c7a754e6e5d9f0fa4d81",
      "avatarUrl": "/avatars/0b0e5f09171b87d61260225b2021aa98.svg",
      "fullname": "HAN-HUNG LEE",
      "name": "rexleeppp",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16194",
      "authors": [
        {
          "_id": "67dcf6375fd14aedd3005237",
          "name": "Ziyao Guo",
          "hidden": false
        },
        {
          "_id": "67dcf6375fd14aedd3005238",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67dcf6375fd14aedd3005239",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T14:41:29.000Z",
      "submittedOnDailyAt": "2025-03-21T03:46:49.486Z",
      "title": "コアストライートからフィニットトークン予測を通じて画像生成の自動復元を向上させる",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "自動回帰モデルは、言語モデリングからの順序予測手法を適用して画像生成において驚異的な成功を示しました。しかし、これらの手法を画像に適用するには、VQ-VAEなどのベクトルクオンテーショナル化手法を用いて絶対値のピクセルデータを数値化する必要があります。VQ-VAEであったような数値化エラーを軽減するために、最近の研究はコードブックを大きくするようになりましたが、これは単語サイズを増やし、自動回帰モデリングの課題を複雑化します。本論文の目的は、大きなコードブックの利点を受けるようにしながら自動回帰モデリングを難しくすることを避ける方法を見つけることです。実験的な調査を通じて、同じコードワード表現を持つトークンが最終的に生成される画像に類似した効果を生じ、大きなコードブックにおける過度の冗長性を明らかにしました。この見解に基づき、同じ粗略なラベルを割り当てることで、コースからフィンエルダーに進むようなトークンの予測を提案しました。我々のフレームワークは2段階構成です: (1) 順序的にシーケンス内の各トークンに粗略なラベルを予測する自動回帰モデル、(2) 粗略なラベルに基づいて全トークンの細かいラベルを同時に予測する助手モデル。ImageNet上での実験により、我々の方法の上位性が示され、基準と比較して平均的な変化率が59点上がりました。特に、推論ステップを追加しながらも、我々のアプローチはより高速なサンプリングスピードを達成しました。",
      "upvotes": 2,
      "discussionId": "67dcf6375fd14aedd300527b",
      "ai_keywords": [
        "autoregressive models",
        "image generation",
        "sequential prediction",
        "language modeling",
        "VQ-VAE",
        "vector quantization",
        "codebooks",
        "token",
        "codeword representations",
        "coarse to fine (CTF)",
        "inference step",
        "Inception Score",
        "sampling speeds"
      ]
    },
    "publishedAt": "2025-03-20T10:41:29.000Z",
    "title": "Improving Autoregressive Image Generation through Coarse-to-Fine Token\n  Prediction",
    "summary": "Autoregressive models have shown remarkable success in image generation by\nadapting sequential prediction techniques from language modeling. However,\napplying these approaches to images requires discretizing continuous pixel data\nthrough vector quantization methods like VQ-VAE. To alleviate the quantization\nerrors that existed in VQ-VAE, recent works tend to use larger codebooks.\nHowever, this will accordingly expand vocabulary size, complicating the\nautoregressive modeling task. This paper aims to find a way to enjoy the\nbenefits of large codebooks without making autoregressive modeling more\ndifficult. Through empirical investigation, we discover that tokens with\nsimilar codeword representations produce similar effects on the final generated\nimage, revealing significant redundancy in large codebooks. Based on this\ninsight, we propose to predict tokens from coarse to fine (CTF), realized by\nassigning the same coarse label for similar tokens. Our framework consists of\ntwo stages: (1) an autoregressive model that sequentially predicts coarse\nlabels for each token in the sequence, and (2) an auxiliary model that\nsimultaneously predicts fine-grained labels for all tokens conditioned on their\ncoarse labels. Experiments on ImageNet demonstrate our method's superior\nperformance, achieving an average improvement of 59 points in Inception Score\ncompared to baselines. Notably, despite adding an inference step, our approach\nachieves faster sampling speeds.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16194.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16055",
      "authors": [
        {
          "_id": "67dd0d7c68dc6463747e6cac",
          "name": "Abdelrahman Elsayed",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6cad",
          "name": "Sarim Hashmi",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6cae",
          "name": "Mohammed Elseiagy",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6caf",
          "name": "Hu Wang",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6cb0",
          "name": "Mohammad Yaqub",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6cb1",
          "name": "Ibrahim Almakky",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T11:42:41.000Z",
      "submittedOnDailyAt": "2025-03-21T05:26:30.501Z",
      "title": "SALT: 低階数変換による特徴値の適応",
      "submittedOnDailyBy": {
        "_id": "62676a94dacab364889bb36c",
        "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
        "isPro": false,
        "fullname": "SARIM HASHMI",
        "user": "Sarim-Hash",
        "type": "user"
      },
      "summary": "医学画像分割の複雑な性質に対して、特に設計されてその詳細的なドメイン特有の特徴を捉えるモデルが必要となります。大きな基盤モデルはゆっくりと変更可能ですが、これらのモデルを微調節するためのコストは大きな障壁となります。Parameter-Efficient Fine-Tuning (PEFT) の方法である Low-Rank Adaptation (LoRA) などは、低次元行列でモデルの重みを効率的に更新できますが、選択された次元がドメイン特有の軽微な部分を捉えることができない場合に欠損しそうです。一方で、全次元 Singular Value Decomposition (SVD) に基づく方法は、すべての固有値を変更して詳細な更新を行いますが、柔軟性がなく、データセット間で性能が変わります。私たちは、Singular Value Adaptation with Low-Rank Transformation (SALT) を提案します。この方法は、学習可能なスケールとシフトパラメータを用いて最も影響力のある固有値を選択的に変更し、その他の次元に低次元更新を追加します。このハイブリッドアプローチは、LoRA と SVD の両方の優れた点を生かし、モデルのサイズや深さを増やす必要がなくても効果的な変更が可能になります。5つの難しい医学データセットに対して評価した結果、SALT は Dice で最先端の PEFT (LoRA と SVD) を 2% から 5% ほど超え、3.9% の学習可能なパラメータで低リソース環境でも強固な変更が可能です。SALT のコードは以下の URL から利用できます：https://github.com/BioMedIA-MBZUAI/SALT",
      "upvotes": 2,
      "discussionId": "67dd0d7e68dc6463747e6d03",
      "ai_keywords": [
        "SALT",
        "Singular Value Adaptation with Low-Rank Transformation",
        "Low-Rank Adaptation",
        "LoRA",
        "Singular Value Decomposition",
        "SVD",
        "Dice",
        "Parameter-Efficient Fine-Tuning",
        "trainable parameters"
      ]
    },
    "publishedAt": "2025-03-20T07:42:41.000Z",
    "title": "SALT: Singular Value Adaptation with Low-Rank Transformation",
    "summary": "The complex nature of medical image segmentation calls for models that are\nspecifically designed to capture detailed, domain-specific features. Large\nfoundation models offer considerable flexibility, yet the cost of fine-tuning\nthese models remains a significant barrier. Parameter-Efficient Fine-Tuning\n(PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model\nweights with low-rank matrices but may suffer from underfitting when the chosen\nrank is insufficient to capture domain-specific nuances. Conversely, full-rank\nSingular Value Decomposition (SVD) based methods provide comprehensive updates\nby modifying all singular values, yet they often lack flexibility and exhibit\nvariable performance across datasets. We propose SALT (Singular Value\nAdaptation with Low-Rank Transformation), a method that selectively adapts the\nmost influential singular values using trainable scale and shift parameters\nwhile complementing this with a low-rank update for the remaining subspace.\nThis hybrid approach harnesses the advantages of both LoRA and SVD, enabling\neffective adaptation without relying on increasing model size or depth.\nEvaluated on 5 challenging medical datasets, ranging from as few as 20 samples\nto 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in\nDice with only 3.9% trainable parameters, demonstrating robust adaptation even\nin low-resource settings. The code for SALT is available at:\nhttps://github.com/BioMedIA-MBZUAI/SALT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62676a94dacab364889bb36c",
      "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
      "fullname": "SARIM HASHMI",
      "name": "Sarim-Hash",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16031",
      "authors": [
        {
          "_id": "67dcc5e41f94b594ef4c0312",
          "user": {
            "_id": "651692d718f3a57f869a5a0a",
            "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
            "isPro": false,
            "fullname": "Sai Kartheek Reddy",
            "user": "UVSKKR",
            "type": "user"
          },
          "name": "Sai Kartheek Reddy Kasu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-21T01:51:18.039Z",
          "hidden": false
        },
        {
          "_id": "67dcc5e41f94b594ef4c0313",
          "name": "Shankar Biradar",
          "hidden": false
        },
        {
          "_id": "67dcc5e41f94b594ef4c0314",
          "name": "Sunil Saumya",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T10:58:02.000Z",
      "submittedOnDailyAt": "2025-03-21T00:28:13.365Z",
      "title": "Deceptive Humor: 誇張されたハモーニー、誇張された主張とハモーニーの内容を結ぶための合成的な多言語ベンチマークデータセット",
      "submittedOnDailyBy": {
        "_id": "651692d718f3a57f869a5a0a",
        "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
        "isPro": false,
        "fullname": "Sai Kartheek Reddy",
        "user": "UVSKKR",
        "type": "user"
      },
      "summary": "この論文では、誇張さられたハムードデータセット（DHD）を紹介します。これは、誇張さられたプロポーズと不正情報からのハムードを研究するための新しいリソースです。不正情報が氾濫している時代には、ハムードが騙偽にどのように関わっているかを理解することが重要です。DHDは、ChatGPT-4oモデルを使用して作成された誇張さられたノートから生成されたハムード付きコメントのセットです。各インスタンスは、軽微なサティアレベル（1）から高レベルのサティア（3）までのサティアレベルを付与され、暗黒ハムード、イラニー、ソシャルコメンタリー、ワードプレイ、そして荒誕な5つの異なるハムードカテゴリに分類されます。このデータセットは、英語、テルグラ、ヒンディ、カナダ、タミル、そしてそれらの言語混合版（Te-En、Hi-En、Ka-En、Ta-En）を含む複数の言語に拡張されています。これは、價値のある多言語ベンチマークになります。DHDの紹介により、ハムードの騙偽コンテキストでの分析のための構造化された基盤を確立し、ハムードが不正情報とはどのように相互作用しているかを、そしてその見解と広がりにどのように影響を与えるかを調査する新しい研究方向を開拓します。この提出されたデータセットに強いベースラインを確立し、将来の研究においてベンチマークとして使用できる基盤を提供します。",
      "upvotes": 2,
      "discussionId": "67dcc5e41f94b594ef4c0353"
    },
    "publishedAt": "2025-03-20T06:58:02.000Z",
    "title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging\n  Fabricated Claims with Humorous Content",
    "summary": "This paper presents the Deceptive Humor Dataset (DHD), a novel resource for\nstudying humor derived from fabricated claims and misinformation. In an era of\nrampant misinformation, understanding how humor intertwines with deception is\nessential. DHD consists of humor-infused comments generated from false\nnarratives, incorporating fabricated claims and manipulated information using\nthe ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging\nfrom 1 for subtle satire to 3 for high-level satire and classified into five\ndistinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and\nAbsurdity. The dataset spans multiple languages including English, Telugu,\nHindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En,\nTa-En), making it a valuable multilingual benchmark. By introducing DHD, we\nestablish a structured foundation for analyzing humor in deceptive contexts,\npaving the way for a new research direction that explores how humor not only\ninteracts with misinformation but also influences its perception and spread. We\nestablish strong baselines for the proposed dataset, providing a foundation for\nfuture research to benchmark and advance deceptive humor detection models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16031.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "651692d718f3a57f869a5a0a",
      "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
      "fullname": "Sai Kartheek Reddy",
      "name": "UVSKKR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.15855",
      "authors": [
        {
          "_id": "67dd072f1c182b6168eaa004",
          "name": "Hyojun Go",
          "hidden": false
        },
        {
          "_id": "67dd072f1c182b6168eaa005",
          "name": "Byeongjun Park",
          "hidden": false
        },
        {
          "_id": "67dd072f1c182b6168eaa006",
          "name": "Hyelin Nam",
          "hidden": false
        },
        {
          "_id": "67dd072f1c182b6168eaa007",
          "name": "Byung-Hoon Kim",
          "hidden": false
        },
        {
          "_id": "67dd072f1c182b6168eaa008",
          "name": "Hyungjin Chung",
          "hidden": false
        },
        {
          "_id": "67dd072f1c182b6168eaa009",
          "name": "Changick Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T05:26:09.000Z",
      "submittedOnDailyAt": "2025-03-21T07:39:17.497Z",
      "title": "VideoRFSplat: 直接スペースレベルのテキストから3Dガウススプラット生成\nフレックスポーズと多角度共変記述\n\nVideoRFSplat: スケーンレベルの直接テキストから3Dガウススプラット生成\n柔軟な姿勢と多角度共変記述",
      "submittedOnDailyBy": {
        "_id": "649f65a4ca03a1a35e3dac14",
        "avatarUrl": "/avatars/b0dcd8ad795b1e666ee247b2ac024d53.svg",
        "isPro": false,
        "fullname": "Hyojun GO",
        "user": "HJGO",
        "type": "user"
      },
      "summary": "ビデオRFSplatを提案します。これは、ビデオ生成モデルを利用して、無制限のリアルウォールスケーンでの写実的な3次元ガウススプラッティング（3DGS）を生成する直接的文字から3次元モデルです。リアルウォールスケーンの多様なカメラ姿勢と無制限の空間拡大を生成し、任意の文字プロンプトに対する一般化を確保するため、以前の方法は2次元生成モデルを共同モデリングするために調整されます。しかし、これらの方法は2次元生成モデルを共同モデリングする際にモデル間のギャップによる不穩定さを受けます。これにより、追加のモデルが訓練と推論の安定化に必要となります。本稿では、ビデオ生成モデルを調整する際に多様なカメラ姿勢と空間拡大を共同モデリングするアーキテクチャとサンプリングスタテジストを提案します。核心のアイデアは、プレイントされたビデオ生成モデルと専用の姿勢生成モデルを通信ブロックでつなげたダブルストリームアーキテクチャです。この設計では、姿勢と画像のモデル間の干渉を減少します。また、アンシンクスケープリングスタテジストを提案し、カメラ姿勢のデノイズ化を多様なカメラ画像よりも速く行うことで、急速にデノイズ化された姿勢が多様なカメラ生成に条件付けられ、相互の不明確性を減少し、クロスモードの一致性を向上させます。RealEstate10K、MVImgNet、DL3DV-10K、ACIDなどの複数の大規模なリアルウォールデータセットで訓練されたビデオRFSplatは、スコアディスティルサンプリングに依存した既存の文字から3次元の直接生成方法を上回り、そのような補正を不要にして優れた結果を実現します。",
      "upvotes": 2,
      "discussionId": "67dd07351c182b6168eaa1fc",
      "ai_keywords": [
        "VideoRFSplat",
        "text-to-3D model",
        "video generation model",
        "realistic 3D Gaussian Splatting (3DGS)",
        "camera poses",
        "unbounded real-world scenes",
        "multi-view images",
        "dual-stream architecture",
        "pose generation model",
        "communication blocks",
        "asynchronous sampling strategy",
        "denoising",
        "mutual ambiguity",
        "cross-modal consistency",
        "RealEstate10K",
        "MVImgNet",
        "DL3DV-10K",
        "ACID",
        "score distillation sampling"
      ]
    },
    "publishedAt": "2025-03-20T01:26:09.000Z",
    "title": "VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting\n  Generation with Flexible Pose and Multi-View Joint Modeling",
    "summary": "We propose VideoRFSplat, a direct text-to-3D model leveraging a video\ngeneration model to generate realistic 3D Gaussian Splatting (3DGS) for\nunbounded real-world scenes. To generate diverse camera poses and unbounded\nspatial extent of real-world scenes, while ensuring generalization to arbitrary\ntext prompts, previous methods fine-tune 2D generative models to jointly model\ncamera poses and multi-view images. However, these methods suffer from\ninstability when extending 2D generative models to joint modeling due to the\nmodality gap, which necessitates additional models to stabilize training and\ninference. In this work, we propose an architecture and a sampling strategy to\njointly model multi-view images and camera poses when fine-tuning a video\ngeneration model. Our core idea is a dual-stream architecture that attaches a\ndedicated pose generation model alongside a pre-trained video generation model\nvia communication blocks, generating multi-view images and camera poses through\nseparate streams. This design reduces interference between the pose and image\nmodalities. Additionally, we propose an asynchronous sampling strategy that\ndenoises camera poses faster than multi-view images, allowing rapidly denoised\nposes to condition multi-view generation, reducing mutual ambiguity and\nenhancing cross-modal consistency. Trained on multiple large-scale real-world\ndatasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms\nexisting text-to-3D direct generation methods that heavily depend on post-hoc\nrefinement via score distillation sampling, achieving superior results without\nsuch refinement.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15855.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649f65a4ca03a1a35e3dac14",
      "avatarUrl": "/avatars/b0dcd8ad795b1e666ee247b2ac024d53.svg",
      "fullname": "Hyojun GO",
      "name": "HJGO",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15451",
      "authors": [
        {
          "_id": "67dd03fb2672aa3643252e8c",
          "user": {
            "_id": "65220fedc709aaca9aa63061",
            "avatarUrl": "/avatars/36e56199fa8c98ff3430636567a0ed62.svg",
            "isPro": false,
            "fullname": "Lixing Xiao",
            "user": "lxxiao",
            "type": "user"
          },
          "name": "Lixing Xiao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-21T06:19:01.806Z",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e8d",
          "name": "Shunlin Lu",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e8e",
          "name": "Huaijin Pi",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e8f",
          "name": "Ke Fan",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e90",
          "name": "Liang Pan",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e91",
          "name": "Yueer Zhou",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e92",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e93",
          "name": "Xiaowei Zhou",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e94",
          "name": "Sida Peng",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e95",
          "name": "Jingbo Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T17:32:24.000Z",
      "submittedOnDailyAt": "2025-03-21T05:04:41.437Z",
      "title": "MotionStreamer: 拡散基底での因果潜在空間内の自動回帰モデルを用いた動き生成ストリーミング",
      "submittedOnDailyBy": {
        "_id": "65220fedc709aaca9aa63061",
        "avatarUrl": "/avatars/36e56199fa8c98ff3430636567a0ed62.svg",
        "isPro": false,
        "fullname": "Lixing Xiao",
        "user": "lxxiao",
        "type": "user"
      },
      "summary": "この論文は、変長の歴史的な動きと入力されたテキストに基づいて次のステップの人間の姿勢を予測することを要求しているテキスト条件付きストリーミング動作生成の挑戦を解決することを目的としています。現在の方法は、ストリーミング動作生成を達成することが難しい問題を抱えています。例えば、ディフュージョンモデルは事前定義された動作長に制限され、GPTベースの方法は、非因果的なトークナイゼーションによる離散化による情報損失と、長期の自動協議回生中のエラーの蓄積問題を抱えています。これらの問題を解決するために、私たちは、確率的な自動協議回生モデルに連続的な因果的な潜在空間を統合した新しいフレームワークを提案しています。連続的な潜在変数は、離散化による情報損失を軽減し、長期の自動協議回生中のエラーの蓄積を効果的に減少することができます。また、現在のと歴史的な動きの潜在変数の間に時間的因果的依存関係を確立し、私たちのモデルは、利用できる情報を最大限に活用し、正確なオンライン動作解釈を実現することができます。実験は、私たちの方法は現在のアプローチを上回り、多回の生成、長期の生成、および動的な動作組み合わせを含むより多くのアプリケーションを提供していることを示しています。プロジェクトページ：https://zju3dv.github.io/MotionStreamer/",
      "upvotes": 2,
      "discussionId": "67dd03fd2672aa3643252f2a",
      "projectPage": "https://zju3dv.github.io/MotionStreamer/",
      "ai_keywords": [
        "diffusion models",
        "streaming motion generation",
        "human pose prediction",
        "historical motions",
        "incoming texts",
        "GPT-based methods",
        "continuous causal latent space",
        "probabilistic autoregressive model",
        "information loss",
        "error accumulation",
        "temporal causal dependencies",
        "online motion decoding",
        "multi-round generation",
        "long-term generation",
        "dynamic motion composition"
      ]
    },
    "publishedAt": "2025-03-19T13:32:24.000Z",
    "title": "MotionStreamer: Streaming Motion Generation via Diffusion-based\n  Autoregressive Model in Causal Latent Space",
    "summary": "This paper addresses the challenge of text-conditioned streaming motion\ngeneration, which requires us to predict the next-step human pose based on\nvariable-length historical motions and incoming texts. Existing methods\nstruggle to achieve streaming motion generation, e.g., diffusion models are\nconstrained by pre-defined motion lengths, while GPT-based methods suffer from\ndelayed response and error accumulation problem due to discretized non-causal\ntokenization. To solve these problems, we propose MotionStreamer, a novel\nframework that incorporates a continuous causal latent space into a\nprobabilistic autoregressive model. The continuous latents mitigate information\nloss caused by discretization and effectively reduce error accumulation during\nlong-term autoregressive generation. In addition, by establishing temporal\ncausal dependencies between current and historical motion latents, our model\nfully utilizes the available information to achieve accurate online motion\ndecoding. Experiments show that our method outperforms existing approaches\nwhile offering more applications, including multi-round generation, long-term\ngeneration, and dynamic motion composition. Project Page:\nhttps://zju3dv.github.io/MotionStreamer/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15451.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65220fedc709aaca9aa63061",
      "avatarUrl": "/avatars/36e56199fa8c98ff3430636567a0ed62.svg",
      "fullname": "Lixing Xiao",
      "name": "lxxiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14237",
      "authors": [
        {
          "_id": "67db7a33b1d42828a18fd0c8",
          "name": "Chenting Wang",
          "hidden": false
        },
        {
          "_id": "67db7a33b1d42828a18fd0c9",
          "name": "Kunchang Li",
          "hidden": false
        },
        {
          "_id": "67db7a33b1d42828a18fd0ca",
          "name": "Tianxiang Jiang",
          "hidden": false
        },
        {
          "_id": "67db7a33b1d42828a18fd0cb",
          "name": "Xiangyu Zeng",
          "hidden": false
        },
        {
          "_id": "67db7a33b1d42828a18fd0cc",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "67db7a33b1d42828a18fd0cd",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T13:15:58.000Z",
      "submittedOnDailyAt": "2025-03-21T07:23:35.664Z",
      "title": "テキストの日本語翻訳は以下の通りです。\n\n「訓練を柔軟にする：部署効率的なビデオモデルへの向け」",
      "submittedOnDailyBy": {
        "_id": "62aafa49f29ff279b51f0182",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62aafa49f29ff279b51f0182/rQx8QFQGOY2qIhqJ8zSRj.jpeg",
        "isPro": false,
        "fullname": "yinanhe",
        "user": "ynhe",
        "type": "user"
      },
      "summary": "ビデオトークントレーニングの人気手法は、一定数のトークンを、事前に決めた空間時間グリッドからサンプリングし、ビデオの固有の冗長性により、精度と計算量の最適な調整が難しくなる。そして、後続タスクの変動する計算バジュームに適応することができないため、最も強力なモデルの実世界の応用が妨害される。そこで、我々は、ビデオの入力情報をバジューム全体で最適化するための新しいテスト設定「トークン最適化」を提案します。これは、適切なサンプリングされたビデオからトークンを選択して、入力トークンのサイズ制限されたセットを最適化することで、バジューム全体で最適な入力情報を得ることを目指しています。ここで、我々は新しいアフェレーションツール「Flux」を提案します。これは、サンプリンググリッドを柔軟にし、トークン選択を活用して、多数のビデオトレーニングフレームワークに容易に採用できるように設計されています。このように、モデルの強固性を大幅に向上させることができることにより、追加コストがほとんどないようにします。フラックスを大規模なビデオ予約トレーニングに統合し、その結果、FluxViTは標準コストで様々なタスクで最新のレートを設定しました。特に、1/4のトークンしか使用しないときにも、Token Optimizationを用いて、前回の最新のモデルの性能と比較しても、近似90%のコスト削減が実現されます。すべてのモデルとデータは、https://github.com/OpenGVLab/FluxViT に公開されています。",
      "upvotes": 2,
      "discussionId": "67db7a35b1d42828a18fd11f",
      "ai_keywords": [
        "token optimization",
        "spatiotemporal grid",
        "token selection",
        "Flux augmentation tool",
        "FluxViT",
        "video pre-training"
      ]
    },
    "publishedAt": "2025-03-18T09:15:58.000Z",
    "title": "Make Your Training Flexible: Towards Deployment-Efficient Video Models",
    "summary": "Popular video training methods mainly operate on a fixed number of tokens\nsampled from a predetermined spatiotemporal grid, resulting in sub-optimal\naccuracy-computation trade-offs due to inherent video redundancy. They also\nlack adaptability to varying computational budgets for downstream tasks,\nhindering applications of the most competitive model in real-world scenes. We\nthus propose a new test setting, Token Optimization, for maximized input\ninformation across budgets, which optimizes the size-limited set of input\ntokens through token selection from more suitably sampled videos. To this end,\nwe propose a novel augmentation tool termed Flux. By making the sampling grid\nflexible and leveraging token selection, it is easily adopted in most popular\nvideo training frameworks, boosting model robustness with nearly no additional\ncost. We integrate Flux in large-scale video pre-training, and the resulting\nFluxViT establishes new state-of-the-art results across extensive tasks at\nstandard costs. Notably, with 1/4 tokens only, it can still match the\nperformance of previous state-of-the-art models with Token Optimization,\nyielding nearly 90\\% savings. All models and data are available at\nhttps://github.com/OpenGVLab/FluxViT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14237.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62aafa49f29ff279b51f0182",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62aafa49f29ff279b51f0182/rQx8QFQGOY2qIhqJ8zSRj.jpeg",
      "fullname": "yinanhe",
      "name": "ynhe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.12689",
      "authors": [
        {
          "_id": "67dcc2e10df3501c657ef478",
          "name": "Hengjia Li",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef479",
          "name": "Lifan Jiang",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47a",
          "name": "Xi Xiao",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47b",
          "name": "Tianyang Wang",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47c",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47d",
          "name": "Boxi Wu",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47e",
          "name": "Deng Cai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T23:15:09.000Z",
      "submittedOnDailyAt": "2025-03-21T02:25:48.893Z",
      "title": "MagicID: ハイブリッド偏好最適化 - アイド一致と動的保存のビデオカスタマイズ",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "ビデオアイデンティティーカスタマイズは、ユーザーのリファレンス画像に基づいて、高精度のビデオを生成し、一致したアイデンティティーと運動性を維持することを目的としています。しかし、現在のアプローチには2つの主要な課題があります：長いビデオの長さによるアイデンティティーの減衰と、トレーニング中の運動性の低下、これらは、静的画像を基にした伝統的な自己再構成トレーニングによる依存関係によって主な原因となっています。これらの問題を解決するために、我々は、ユーザーの好みに合わせたアイデンティティーの一致と運動性の豊富なビデオの生成を直接促成するための新しいフレームワーク「MagicID」を紹介します。特に、我々は、傾好学習において明示的なアイデンティティーと動的な報酬を含むペアワイズプロフェアビデオデータを構築することを提案し、伝統的な自己再構成を追うことを離れます。また、ユーザーの好みデータの制約を解決するために、我々は、静的画像から得られるビデオを利用してアイデンティティーの保存を優先し、生成されたビデオの動的な動作の品質をフロンティアベースのサンプリング方法を用いて向上させるハイブリッドサンプリングステージを導入します。これらのハイブリッド傾好ペアを利用して、モデルを報酬の差異に合わせることで最適化します。拡大的な実験は、MagicIDが一致したアイデンティティーと自然な運動性を成功して達成し、現在の方法を超えることを示しています。",
      "upvotes": 2,
      "discussionId": "67dcc2e50df3501c657ef56b",
      "projectPage": "https://echopluto.github.io/MagicID-project/",
      "githubRepo": "https://github.com/EchoPluto/MagicID",
      "ai_keywords": [
        "pairwise preference video data",
        "identity and dynamic rewards",
        "preference learning",
        "hybrid sampling strategy",
        "static videos",
        "Frontier-based sampling method",
        "reward differences",
        "customized preferences",
        "identity-preserving",
        "dynamic motion quality",
        "high-fidelity videos",
        "consistent identity",
        "natural dynamics"
      ]
    },
    "publishedAt": "2025-03-16T19:15:09.000Z",
    "title": "MagicID: Hybrid Preference Optimization for ID-Consistent and\n  Dynamic-Preserved Video Customization",
    "summary": "Video identity customization seeks to produce high-fidelity videos that\nmaintain consistent identity and exhibit significant dynamics based on users'\nreference images. However, existing approaches face two key challenges:\nidentity degradation over extended video length and reduced dynamics during\ntraining, primarily due to their reliance on traditional self-reconstruction\ntraining with static images. To address these issues, we introduce\nMagicID, a novel framework designed to directly promote the\ngeneration of identity-consistent and dynamically rich videos tailored to user\npreferences. Specifically, we propose constructing pairwise preference video\ndata with explicit identity and dynamic rewards for preference learning,\ninstead of sticking to the traditional self-reconstruction. To address the\nconstraints of customized preference data, we introduce a hybrid sampling\nstrategy. This approach first prioritizes identity preservation by leveraging\nstatic videos derived from reference images, then enhances dynamic motion\nquality in the generated videos using a Frontier-based sampling method. By\nutilizing these hybrid preference pairs, we optimize the model to align with\nthe reward differences between pairs of customized preferences. Extensive\nexperiments show that MagicID successfully achieves consistent identity and\nnatural dynamics, surpassing existing methods across various metrics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12689.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 35
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16219",
      "authors": [
        {
          "_id": "67dd1a9cfa598c90d14e9b47",
          "user": {
            "_id": "645b663eca5d8a297712f2e1",
            "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
            "isPro": false,
            "fullname": "Quy-Anh Dang",
            "user": "quyanh",
            "type": "user"
          },
          "name": "Quy-Anh Dang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-21T07:52:47.629Z",
          "hidden": false
        },
        {
          "_id": "67dd1a9cfa598c90d14e9b48",
          "name": "Chris Ngo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/EIt1JuoKkqWkGY1Pied64.png"
      ],
      "publishedAt": "2025-03-20T15:13:23.000Z",
      "submittedOnDailyAt": "2025-03-21T07:03:47.115Z",
      "title": "強化学習による小さなLLMの論理推理における効果と無効性：どれが効果的で、どれが効果的でないか",
      "submittedOnDailyBy": {
        "_id": "645b663eca5d8a297712f2e1",
        "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
        "isPro": false,
        "fullname": "Quy-Anh Dang",
        "user": "quyanh",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）の推理能力を向上させるためには、巨大な計算リソースと幅広いデータセットが必要となり、リソース制限された環境でのアクセスが限られています。本研究では、強化学習（RL）が小規模なLLMsの推理を改善する可能性について検討し、特に150億パラメータのモデル、DeepSeek-R1-Distill-Qwen-1.5Bを対象として、4ノードのNVIDIA A40 GPU（各ノード48GB VRAM）で24時間以内に学習する厳格な制約を設定しました。Group Relative Policy Optimization（GRPO）アルゴリズムを適用し、組織的な数学的推理データセットを作成し、3つの実験を行いました。これらの結果から、AMC23の正解率が63%から80%に上がり、AIME24の正解率が46.7%に達し、基準モデルの数千ドルのコストを超えるための7,000サンプルを使用した効果的な推理の向上が示されました。しかし、長期的な学習に伴う最適化不穩定と長さ制約の問題が出てきました。これらの発見は、小規模なLLMsに対するRL基礎的な微調節の効果性を示し、大規模なアプローチへのコスト効率的な代替としての可能性を示します。本研究のコードとデータセットを公開ソースリソースとしてリリースし、補損を見つけるための見解を提供し、リソース制限された環境でのスケーラブルな推理能力のあるLLMsの基盤を作成します。すべては、https://github.com/knoveleng/open-rsから利用できます。",
      "upvotes": 1,
      "discussionId": "67dd1a9dfa598c90d14e9ba4",
      "githubRepo": "https://github.com/knoveleng/open-rs",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "Group Relative Policy Optimization (GRPO)",
        "mathematical reasoning dataset",
        "AMC23",
        "AIME24",
        "optimization instability",
        "RL-based fine-tuning",
        "scalable",
        "reasoning-capable LLMs"
      ]
    },
    "publishedAt": "2025-03-20T11:13:23.000Z",
    "title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't",
    "summary": "Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/645b663eca5d8a297712f2e1/EIt1JuoKkqWkGY1Pied64.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16219.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b663eca5d8a297712f2e1",
      "avatarUrl": "/avatars/c61dcba43feb879088b15b525e441cb9.svg",
      "fullname": "Quy-Anh Dang",
      "name": "quyanh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13834",
      "authors": [
        {
          "_id": "67dcf6456b575dc3179e05a2",
          "name": "JuneHyoung Kwon",
          "hidden": false
        },
        {
          "_id": "67dcf6456b575dc3179e05a3",
          "name": "MiHyeon Kim",
          "hidden": false
        },
        {
          "_id": "67dcf6456b575dc3179e05a4",
          "name": "Eunju Lee",
          "hidden": false
        },
        {
          "_id": "67dcf6456b575dc3179e05a5",
          "name": "Juhwan Choi",
          "hidden": false
        },
        {
          "_id": "67dcf6456b575dc3179e05a6",
          "name": "YoungBin Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T02:17:41.000Z",
      "submittedOnDailyAt": "2025-03-21T03:48:06.030Z",
      "title": "シーソーモディアバランス：シージェンダー、そして、視覚言語の不均衡を織り合わせて主導モディアバイアスを軽減する",
      "submittedOnDailyBy": {
        "_id": "65646b22ac9d3c2bd7b14788",
        "avatarUrl": "/avatars/0bf19dcfa568a694361fb3a63b999997.svg",
        "isPro": false,
        "fullname": "Juhwan Choi",
        "user": "c-juhwan",
        "type": "user"
      },
      "summary": "ビジョン言語（VL）モデルは、様々なタスクで強力な性能を示しています。しかし、これらのモデルは通常、特定のモデルダイレクトリーを使用して予測を行うため、「優位モデルバイアス」と呼ばれる偏りを生み出します。この偏りは、一つのモデルが損傷された場合に特に性能を悪くします。本研究では、優位モデルバイアスの影響を分析し、理論的には、不適切な勾配または勾配の大きさの差が損失の調和平衡な収束を妨げることを示しました。これらの発見に基づいて、私たちは新しいフレームワーク、BalGradを提案し、優位モデルバイアスを軽減します。私たちのアプローチは、モデル間の勾配の再重み付け、各モデルの貢献に基づいたKL分散の勾配の調整、モデル間の勾配プロジェクションを含みます。UPMC Food-101、Hateful Memes、MM-IMDbデータセットの実験は、BalGradが予測時に特定のモデルダイレクトリーに過度に依存していることを効果的に軽減することを確認しました。",
      "upvotes": 1,
      "discussionId": "67dcf64a6b575dc3179e0754",
      "ai_keywords": [
        "dominant modality bias",
        "unaligned gradients",
        "gradient magnitudes",
        "inter-modality gradient reweighting",
        "KL divergence",
        "inter-task gradient projection"
      ]
    },
    "publishedAt": "2025-03-17T22:17:41.000Z",
    "title": "See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language\n  Balance to Mitigate Dominant Modality Bias",
    "summary": "Vision-language (VL) models have demonstrated strong performance across\nvarious tasks. However, these models often rely on a specific modality for\npredictions, leading to \"dominant modality bias.'' This bias significantly\nhurts performance, especially when one modality is impaired. In this study, we\nanalyze model behavior under dominant modality bias and theoretically show that\nunaligned gradients or differences in gradient magnitudes prevent balanced\nconvergence of the loss. Based on these findings, we propose a novel framework,\nBalGrad to mitigate dominant modality bias. Our approach includes\ninter-modality gradient reweighting, adjusting the gradient of KL divergence\nbased on each modality's contribution, and inter-task gradient projection to\nalign task directions in a non-conflicting manner. Experiments on UPMC\nFood-101, Hateful Memes, and MM-IMDb datasets confirm that BalGrad effectively\nalleviates over-reliance on specific modalities when making predictions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13834.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65646b22ac9d3c2bd7b14788",
      "avatarUrl": "/avatars/0bf19dcfa568a694361fb3a63b999997.svg",
      "fullname": "Juhwan Choi",
      "name": "c-juhwan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]