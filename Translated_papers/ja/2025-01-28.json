[
  {
    "paper": {
      "id": "2501.15368",
      "authors": [
        {
          "_id": "67986c6822990ae89bb71fb9",
          "name": "Yadong Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fba",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbb",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbc",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbd",
          "name": "Song Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbe",
          "name": "Tianpeng Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fbf",
          "name": "Zehuan Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc0",
          "name": "Lijun Liu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc1",
          "name": "Lingfeng Ming",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc2",
          "name": "Guosheng Dong",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc3",
          "name": "Da Pan",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc4",
          "name": "Chong Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc5",
          "name": "Yuanbo Fang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc6",
          "name": "Dongdong Kuang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc7",
          "name": "Mingrui Wang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc8",
          "name": "Chenglin Zhu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fc9",
          "name": "Youwei Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fca",
          "name": "Hongyu Guo",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fcb",
          "name": "Fengyu Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fcc",
          "name": "Yuran Wang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fcd",
          "name": "Bowen Ding",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fce",
          "name": "Wei Song",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fcf",
          "name": "Xu Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd0",
          "name": "Yuqi Huo",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd1",
          "name": "Zheng Liang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd2",
          "name": "Shusen Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd3",
          "name": "Xin Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd4",
          "name": "Shuai Zhao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd5",
          "name": "Linchu Xiong",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd6",
          "name": "Yozhen Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd7",
          "name": "Jiahui Ye",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd8",
          "name": "Wenhao Lu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fd9",
          "name": "Bowen Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fda",
          "name": "Yan Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fdb",
          "name": "Yaqi Zhou",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fdc",
          "name": "Xin Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fdd",
          "name": "Lei Su",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fde",
          "name": "Hongda Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fdf",
          "name": "Fuzhong Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe0",
          "name": "Xuezhen Dong",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe1",
          "name": "Na Nie",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe2",
          "name": "Zhiying Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe3",
          "name": "Bin Xiao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe4",
          "name": "Ting Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe5",
          "name": "Shunya Dang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe6",
          "name": "Ping Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe7",
          "name": "Yijia Sun",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe8",
          "name": "Jincheng Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fe9",
          "name": "Jinjie Yang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fea",
          "name": "Xionghai Lin",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71feb",
          "name": "Zhi Ma",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fec",
          "name": "Kegeng Wu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fed",
          "name": "Jia li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fee",
          "name": "Aiyuan Yang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fef",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff0",
          "name": "Jianqiang Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff1",
          "name": "Xiaoxi Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff2",
          "name": "Guangwei Ai",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff3",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff4",
          "name": "Yicong Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff5",
          "name": "Xiaoqin Huang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff6",
          "name": "Kun Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff7",
          "name": "Wenjing Luo",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff8",
          "name": "Yifei Duan",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ff9",
          "name": "Lingling Zhu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffa",
          "name": "Ran Xiao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffb",
          "name": "Zhe Su",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffc",
          "name": "Jiani Pu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffd",
          "name": "Dian Wang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71ffe",
          "name": "Xu Jia",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb71fff",
          "name": "Tianyu Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72000",
          "name": "Mengyu Ai",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72001",
          "name": "Mang Wang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72002",
          "name": "Yujing Qiao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72003",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72004",
          "name": "Yanjun Shen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72005",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72006",
          "name": "Miao Zhen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72007",
          "name": "Yijie Zhou",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72008",
          "name": "Mingyang Chen",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72009",
          "name": "Fei Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200a",
          "name": "Chenzheng Zhu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200b",
          "name": "Keer Lu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200c",
          "name": "Yaqi Zhao",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200d",
          "name": "Hao Liang",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200e",
          "name": "Youquan Li",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb7200f",
          "name": "Yanzhao Qin",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72010",
          "name": "Linzhuang Sun",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72011",
          "name": "Jianhua Xu",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72012",
          "name": "Haoze Sun",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72013",
          "name": "Mingan Lin",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72014",
          "name": "Zenan Zhou",
          "hidden": false
        },
        {
          "_id": "67986c6822990ae89bb72015",
          "name": "Weipeng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-26T02:19:03.000Z",
      "title": "Baichuan-Omni-1.5 技術報告",
      "summary": "Baichuan-Omni-1.5 は、多様的なモデルであり、モデルが複数のモデルを理解する能力を持つのに加え、終始一致の音声生成能力を提供します。モデルのどのモデルの能力を失わずに、モデル間の流れ通りの高品質なインタラクションを実現するために、3つの重要な面で最適化を優先しました。まず、多様データのコンテンツ整形と合成のパイプラインを構築し、約500Bの高品質データ（テキスト、音声、視覚）を取得しました。次に、音声テキナイザー（Baichuan-Audio-Tokenizer）を設計し、音声から語義的および音響的な情報を捉え、MLLMとの無間違いのよい統合と拡張性を実現しました。最後に、多ステージの訓練戦略を設計し、進段的に多様モデルの対応と多タスクの微調節を統合し、すべてのモデル間で効果的な調和を確保しました。Baichuan-Omni-1.5 は、現代のモデル（GPT4o-mini や MiniCPM-o 2.6 を含む）を凌駕し、複数のモデルの詳細な評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に、複数のモデルの評価を行いました。特に",
      "upvotes": 18,
      "discussionId": "67986c6b22990ae89bb720aa"
    },
    "publishedAt": "2025-01-28T00:34:49.721Z",
    "title": "Baichuan-Omni-1.5 Technical Report",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15368.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.15383",
      "authors": [
        {
          "_id": "67986c83b5e71350993d28eb",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ec",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ed",
          "name": "Chengyuan Li",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ee",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ef",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f0",
          "name": "Haoyan Huang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f1",
          "name": "Jiandong Jiang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f2",
          "name": "Jianhong Tu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f3",
          "name": "Jianwei Zhang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f4",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f5",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f6",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f7",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f8",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28f9",
          "name": "Mei Li",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fa",
          "name": "Minmin Sun",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fb",
          "name": "Qin Zhu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fc",
          "name": "Rui Men",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fd",
          "name": "Tao He",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28fe",
          "name": "Weijia Xu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d28ff",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2900",
          "name": "Wenyuan Yu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2901",
          "name": "Xiafei Qiu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2902",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2903",
          "name": "Xinlong Yang",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2904",
          "name": "Yong Li",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2905",
          "name": "Zhiying Xu",
          "hidden": false
        },
        {
          "_id": "67986c83b5e71350993d2906",
          "name": "Zipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-26T03:47:25.000Z",
      "title": "Qwen2.5-1M 技術報告",
      "summary": "Qwen2.5-1M モデルシリーズを紹介します。これらのモデルは、コンテキスト長を1,000,000トークンに拡張しました。前の128Kバージョンに比して、長コンテキスト能力が大幅に向上しました。長コンテキスト予ち学習と後ほどの学習を通じて、長コンテキスト性能を効果的に向上させ、学習コストを減らしました。長データ合成、進歩的予ち学習、多段階のマニュアルフィードバック調整などのキーテクニックを使用して、長コンテキスト性能を向上させ、同時に学習コストを削減しました。\n\nブロードワールドのユーザーベースに長コンテキストモデルの利用を促進するために、我々の推論フレームワークを提供し、そのソースコードを公開します。このフレームワークには、追加の学習を除けばモデルのコンテキスト長を4倍以上拡大できる長コンテキスト推論法が含まれています。推論コストを削減するために、スパースアタション法とチャンク付きのフィルドプリフィルオプティマイズを実装し、部署シナリオでのスパース性の改善フィードバックを実装しました。また、推論エンジンの最適化について詳細に説明し、カーネル最適化、パイプラインパラレリズム、スケジューリング最適化などが実施され、全体の推論性能を大幅に向上させました。このフレームワークを活用することで、1,000,000トークンのコンテキストの場合には、3倍から7倍のフィルドプリフィルドスピードアップを実現できます。このフレームワークは、長コンテキスト処理を必要とするアプリケーションの開発において、効率的かつ強力的な解決策を提供します。\n\n現在、Qwen2.5-1M シリーズには、開放ソースモデル Qwen2.5-7B-Instruct-1M と Qwen2.5-14B-Instruct-1M が含まれており、APIアクセス可能なモデル Qwen2.5-Turbo も含まれています。評価によると、Qwen2.5-1M モデルは長コンテキストタスクで大幅に向上し、短コンテキストシナリオでの性能を落としません。特に、Qwen2.5-14B-Instruct-1M モデルは長コンテキストタスクで GPT-4o-mini よりも显著に優位を取り、8倍の長コンテキストをサポートしています。",
      "upvotes": 7,
      "discussionId": "67986c84b5e71350993d2974"
    },
    "publishedAt": "2025-01-28T00:35:46.871Z",
    "title": "Qwen2.5-1M Technical Report",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15383.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.15570",
      "authors": [
        {
          "_id": "679843ae7d7b7f8196c61ab7",
          "name": "Lin Yueyu",
          "hidden": false
        },
        {
          "_id": "679843ae7d7b7f8196c61ab8",
          "name": "Li Zhiyuan",
          "hidden": false
        },
        {
          "_id": "679843ae7d7b7f8196c61ab9",
          "name": "Peter Yue",
          "hidden": false
        },
        {
          "_id": "679843ae7d7b7f8196c61aba",
          "user": {
            "_id": "6176b32847ee6431f632981e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg",
            "isPro": false,
            "fullname": "IvanD",
            "user": "xiaol",
            "type": "user"
          },
          "name": "Liu Xiao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-01-28T02:44:02.658Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-26T15:56:56.000Z",
      "title": "ARWKV: 予っ学習は必要ない、Transformerから生まれたRNN-Attentionベースの言語モデル",
      "summary": "これまで知られているように、多ヘッドアーキテクチャでの組み合わせされたギャバライドキュバービックおよびサブキュバービックアテンションモデルは、TransformerおよびLinear RNNモデルを超えて、KV複雑性の削減と効率向上に焦点を当てています。さらに表現力についての研究を進めるために、Qwen 2.5からの純粋な原生RWKV-7アテンションに基づくモデルシリーズを紹介します。これらのモデルはRNNの表現力を高め、Transformerを超える状態追跡能力を示すことを目指しています。また、QRWK 32BはRWKV-6アーキテクチャに基づいて、16タンパクミ300XGPUを使用して知識処理時間を8時間に抑え、Qwen 2.5の性能を維持することで、知識処理時間を大幅に短縮します。実際には、ディスティルーションプロセスはLLMの種類に制限されず、大きなLLMから小さなLLMへの知識伝達を可能にします。これらのプロセスの詳細および強力な基盤モデルの構築についてのフィードバックを共有します。これは継続的に更新されている進行中のワークです。モデルチェックポイントとソースコードは以下のURLで利用できます。",
      "upvotes": 3,
      "discussionId": "679843af7d7b7f8196c61b21"
    },
    "publishedAt": "2025-01-28T03:02:56.062Z",
    "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15570.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6176b32847ee6431f632981e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg",
      "fullname": "IvanD",
      "name": "xiaol",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 81
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.16142",
      "authors": [
        {
          "_id": "67986cbc7dbf69e4e38539b7",
          "name": "Scott Fujimoto",
          "hidden": false
        },
        {
          "_id": "67986cbc7dbf69e4e38539b8",
          "name": "Pierluca D'Oro",
          "hidden": false
        },
        {
          "_id": "67986cbc7dbf69e4e38539b9",
          "name": "Amy Zhang",
          "hidden": false
        },
        {
          "_id": "67986cbc7dbf69e4e38539ba",
          "name": "Yuandong Tian",
          "hidden": false
        },
        {
          "_id": "67986cbc7dbf69e4e38539bb",
          "name": "Michael Rabbat",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T15:36:37.000Z",
      "title": "Towards General-Purpose Model-Free Reinforcement Learning",
      "summary": "強化学習（RL）は、近沢粛けの問題解決のフレームワークを提供することを承諾しています。実際には、RLアルゴリズムは特定のベンチマークにタイルドされ、調整された超パラメータとアルゴリズムの選択に依存しています。最近、強力なモデルベースのRLメソッドは、複雑性と遅い実行時間により、ベンチマーク全体で驚異的な結果を示していますが、これは計画や計算ロジックに関連するコストを軽減するために、モデルベースのタスクオブジェクトを利用することによって実現されています。本論文では、広範囲のドメインと問題設定を扱うための統一的なモデル無しの深層RLアルゴリズムを探求しようとしています。これを達成するために、モデルベースの表現を利用し、価値関数を近似線形化し、モデルベースのRLで使用されるより詳細なタスクオブジェクトを利用しながら、計画や計算ロジックに関連するコストを軽減します。我々のアルゴリズム、MR.Qを、一つの超パラメーターセットで多様な通常のRLベンチマークに対して評価し、領域特化されたベースラインと一般的なベースラインとの比較で競争的な性能を示し、一般的なモデル無しの深層RLアルゴリズムの構築に向けて具象的なステップを提供します。",
      "upvotes": 2,
      "discussionId": "67986cbf7dbf69e4e3853a89"
    },
    "publishedAt": "2025-01-28T00:36:09.186Z",
    "title": "Towards General-Purpose Model-Free Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.15369",
      "authors": [
        {
          "_id": "6798706dabdc35456a92212d",
          "name": "Chuanyang Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-26T02:34:58.000Z",
      "title": "iFormer: モバイルアプリケーション向けのConvNetとTransformerの統合",
      "summary": "We present a new family of mobile hybrid vision networks, called iFormer,\nwith a focus on optimizing latency and accuracy on mobile applications. iFormer\neffectively integrates the fast local representation capacity of convolution\nwith the efficient global modeling ability of self-attention. The local\ninteractions are derived from transforming a standard convolutional network,\ni.e., ConvNeXt, to design a more lightweight mobile network. Our newly\nintroduced mobile modulation attention removes memory-intensive operations in\nMHA and employs an efficient modulation mechanism to boost dynamic global\nrepresentational capacity. We conduct comprehensive experiments demonstrating\nthat iFormer outperforms existing lightweight networks across various tasks.\nNotably, iFormer achieves an impressive Top-1 accuracy of 80.4\\% on ImageNet-1k\nwith a latency of only 1.10 ms on an iPhone 13, surpassing the recently\nproposed MobileNetV4 under similar latency constraints. Additionally, our\nmethod shows significant improvements in downstream tasks, including COCO\nobject detection, instance segmentation, and ADE20k semantic segmentation,\nwhile still maintaining low latency on mobile devices for high-resolution\ninputs in these scenarios.",
      "upvotes": 1,
      "discussionId": "6798706eabdc35456a92215a"
    },
    "publishedAt": "2025-01-28T00:51:51.263Z",
    "title": "iFormer: Integrating ConvNet and Transformer for Mobile Application",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15369.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.16295",
      "authors": [
        {
          "_id": "67986cd6bdc99911a989b0a5",
          "name": "Weixin Liang",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0a6",
          "name": "Junhong Shen",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0a7",
          "name": "Genghan Zhang",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0a8",
          "name": "Ning Dong",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0a9",
          "name": "Luke Zettlemoyer",
          "hidden": false
        },
        {
          "_id": "67986cd6bdc99911a989b0aa",
          "name": "Lili Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-27T18:35:05.000Z",
      "title": "Mixture-of-Mamba: モダルティーに関するスパース性を持つ多モデルステートスペースモデルを強化する",
      "summary": "ステートスペースモデル（SSMs）は、シーケンスモデリングに対してTransformersの効率的な代替として登場しましたが、モデル毎の特徴を活用できないことにより、多モデル予ち学習での性能が限られています。ここでは、モデル毎のパラメータ化によるモデル毎の隠れ性を導入する新しいSSMアーキテクチャ「Mixture-of-Mamba」を提案します。Mixture-of-Transformers（W. Liang et al. arXiv:2411.04996; 2024）に基づいて、SSMにモデル毎の隠れ性の利点を延長し、計算効率を維持することを目指します。Mixture-of-Mambaは、Transfusion（交差したテキストと連続画像トークン）、Chameleon（交差したテキストと離散画像トークン）、および音声を含む3モデルフレームワークの3つの多モデル予ち学習設定で評価されました。Mixture-of-Mambaは、同じ損失値を早期の学習ステップで達成し、大幅に計算コストを削減しました。Transfusion設定では、1.4Bスケールで34.76%のトレーニングFLOPを使用して等価な画像損失を達成しました。Chameleon設定では、1.4Bスケールで42.50%のFLOPで類似の画像損失を達成し、65.40%のFLOPで類似のテキスト損失を達成しました。3モデル設定では、1.4Bスケールで24.80%のFLOPで音声損失を達成しました。ディセイブルスタディにおけるモデル毎の隠れ性の効果を明らかにします。これらの結果は、モデル毎の隠れ性を効果的な設計原則として、TransformersからSSMへと拡張し、多モデル予ち学習の新しいベンチマークを設定します。コードは、https://github.com/Weixin-Liang/Mixture-of-Mamba からアクセスできます。",
      "upvotes": 1,
      "discussionId": "67986cd7bdc99911a989b0ea"
    },
    "publishedAt": "2025-01-28T00:36:31.841Z",
    "title": "Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16295.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.14912",
      "authors": [
        {
          "_id": "67986d764fccd4b95149db0b",
          "name": "Juan Ramirez",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db0c",
          "name": "Ignacio Hounie",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db0d",
          "name": "Juan Elenter",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db0e",
          "name": "Jose Gallego-Posada",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db0f",
          "name": "Meraj Hashemizadeh",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db10",
          "name": "Alejandro Ribeiro",
          "hidden": false
        },
        {
          "_id": "67986d764fccd4b95149db11",
          "name": "Simon Lacoste-Julien",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-24T20:39:38.000Z",
      "title": "Feasible Learning\n\nこれは、実用的な学習の意味です。",
      "summary": "Feasible Learning (FL) を介して、サンプルセンタリックな学習パラダイムを紹介します。このパラダイムでは、モデルは各トレーニングサンプルの損失を制限する可能性問題を解くことでトレーニングされます。普遍的な経験リスク最小化 (ERM) フレームワークと比較して、ERMは平均的な性能を最適化しているのに対して、FLはすべての個々のデータポイントに満足した性能を求めます。どのモデルが指定された性能スロープを満たすものは、FLの正当な解として認められるため、最適化アルゴリズムの選択およびその動態が結果の解の特性を形成する重要な役割を果たします。特に、実際のトレーニング中に各サンプルの重要性を動的に再評価する原則と対応するアプローチについて研究します。実際の設定での有意なスロープの設定についての挑戦を解決するために、最小ノルムのスラック変数を含むFLの放題を導入します。画像分類、年齢推定、大規模言語モデルの好み最適化を含む実験的な分析は、ERMに比べて平均的な性能には少しだけ影響を及ぼしながら、FLによってトレーニングされたモデルがデータから学習し、タイルバイエクシズを改善することを示します。",
      "upvotes": 0,
      "discussionId": "67986d784fccd4b95149db6b"
    },
    "publishedAt": "2025-01-28T00:39:11.423Z",
    "title": "Feasible Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.14912.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5828
    },
    "isAuthorParticipating": false
  }
]