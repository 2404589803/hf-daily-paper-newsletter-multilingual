[
  {
    "paper": {
      "id": "2505.09388",
      "authors": [
        {
          "_id": "68299e3128752b51372d31ea",
          "user": {
            "_id": "62088594a5943c8a8fc94560",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png",
            "isPro": false,
            "fullname": "An Yang",
            "user": "yangapku",
            "type": "user"
          },
          "name": "An Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:43:00.733Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31eb",
          "user": {
            "_id": "6799128b9da39716ab1ebd95",
            "avatarUrl": "/avatars/677d8ae2087137134c3f0e58f4cf769f.svg",
            "isPro": false,
            "fullname": "Anfeng Li",
            "user": "laf070810",
            "type": "user"
          },
          "name": "Anfeng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:15:44.771Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ec",
          "user": {
            "_id": "64b0a77df12b47366663884c",
            "avatarUrl": "/avatars/a212ea862abb5966060e439dd0e7656f.svg",
            "isPro": false,
            "fullname": "Baosong Yang",
            "user": "Baosong",
            "type": "user"
          },
          "name": "Baosong Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:15:37.853Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ed",
          "user": {
            "_id": "64b93578ee257c3a4cfceed1",
            "avatarUrl": "/avatars/e6188562254f75a09b4048b800860016.svg",
            "isPro": false,
            "fullname": "Beichen Zhang",
            "user": "BeichenZhang",
            "type": "user"
          },
          "name": "Beichen Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:16:13.672Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ee",
          "user": {
            "_id": "61e4c4ca1ab24785ac11ba69",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg",
            "isPro": false,
            "fullname": "Binyuan Hui",
            "user": "huybery",
            "type": "user"
          },
          "name": "Binyuan Hui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:16:22.151Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ef",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f0",
          "user": {
            "_id": "6583ab7983a9e1460c67d876",
            "avatarUrl": "/avatars/74400bc448c3f07e23a4cd53d68a6af7.svg",
            "isPro": false,
            "fullname": "bowen",
            "user": "bowenYu",
            "type": "user"
          },
          "name": "Bowen Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:16:31.453Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f1",
          "name": "Chang Gao",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f2",
          "name": "Chengen Huang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f3",
          "name": "Chenxu Lv",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f4",
          "user": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "isPro": false,
            "fullname": "Chujie Zheng",
            "user": "chujiezheng",
            "type": "user"
          },
          "name": "Chujie Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:43:04.798Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f5",
          "user": {
            "_id": "6434d4989bd5a84b5dd0b0f5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg",
            "isPro": false,
            "fullname": "Dayiheng Liu",
            "user": "Losin94",
            "type": "user"
          },
          "name": "Dayiheng Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:17:32.677Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f6",
          "name": "Fan Zhou",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f7",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f8",
          "name": "Feng Hu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31f9",
          "name": "Hao Ge",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fa",
          "user": {
            "_id": "6436618aeef1f55654a9f458",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6436618aeef1f55654a9f458/OvxGtuDg2GAFG9As-2hzW.jpeg",
            "isPro": false,
            "fullname": "Haoran Wei",
            "user": "HaoranWei",
            "type": "user"
          },
          "name": "Haoran Wei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:17:56.110Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fb",
          "name": "Huan Lin",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fc",
          "user": {
            "_id": "63281d05ac205d01918b5fc7",
            "avatarUrl": "/avatars/fc3e0f7285bb2869a92670f764dfc535.svg",
            "isPro": false,
            "fullname": "Jialong Tang",
            "user": "Jialong",
            "type": "user"
          },
          "name": "Jialong Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:18:16.959Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fd",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31fe",
          "user": {
            "_id": "654bead777401b47e6424f88",
            "avatarUrl": "/avatars/7bcbdbb051c93b004f0dc3ad36c4a0ce.svg",
            "isPro": false,
            "fullname": "Jianhong Tu",
            "user": "ToviTu",
            "type": "user"
          },
          "name": "Jianhong Tu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:18:30.045Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d31ff",
          "name": "Jianwei Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3200",
          "name": "Jianxin Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3201",
          "name": "Jiaxi Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3202",
          "name": "Jing Zhou",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3203",
          "user": {
            "_id": "602f88f5e8149a962412a667",
            "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
            "isPro": false,
            "fullname": "Zhou",
            "user": "Jingren",
            "type": "user"
          },
          "name": "Jingren Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:20:51.253Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3204",
          "name": "Junyang Lin",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3205",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3206",
          "name": "Keqin Bao",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3207",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3208",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3209",
          "name": "Lianghao Deng",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320a",
          "name": "Mei Li",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320b",
          "user": {
            "_id": "5f8946925d083370c711f296",
            "avatarUrl": "/avatars/14246aae3b1f8b7ad050f8ff2c8b260e.svg",
            "isPro": false,
            "fullname": "Mingfeng Xue",
            "user": "mingfengxue",
            "type": "user"
          },
          "name": "Mingfeng Xue",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:21:56.048Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320c",
          "name": "Mingze Li",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320d",
          "name": "Pei Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320e",
          "user": {
            "_id": "62f220ccee7d7af44979efc7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f220ccee7d7af44979efc7/RImNglMumGCpAKB5gin6k.jpeg",
            "isPro": false,
            "fullname": "Peng Wang",
            "user": "ZJUPeng",
            "type": "user"
          },
          "name": "Peng Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:43:02.813Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d320f",
          "name": "Qin Zhu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3210",
          "name": "Rui Men",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3211",
          "user": {
            "_id": "6629ed94aabce1b25c3db90c",
            "avatarUrl": "/avatars/cbc39db81c8e8f950d3bd2c2e03f71c8.svg",
            "isPro": false,
            "fullname": "Ruize Gao",
            "user": "gaoruize",
            "type": "user"
          },
          "name": "Ruize Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:21:46.295Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3212",
          "name": "Shixuan Liu",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3213",
          "name": "Shuang Luo",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3214",
          "name": "Tianhao Li",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3215",
          "name": "Tianyi Tang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3216",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3217",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3218",
          "name": "Xinyu Wang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3219",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321a",
          "name": "Xuancheng Ren",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321b",
          "name": "Yang Fan",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321c",
          "name": "Yang Su",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321d",
          "name": "Yichang Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321e",
          "name": "Yinger Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d321f",
          "name": "Yu Wan",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3220",
          "user": {
            "_id": "666aacfb918ba11c7c598194",
            "avatarUrl": "/avatars/45bee8f1fdbdd256ee47d25e4bf01a7a.svg",
            "isPro": false,
            "fullname": "Yuqiong Liu",
            "user": "lyq333",
            "type": "user"
          },
          "name": "Yuqiong Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:20:06.363Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3221",
          "name": "Zekun Wang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3222",
          "user": {
            "_id": "672c25ca8cfb61188128eb6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FJWy9Tt7UQmu9KcTOx3Rt.png",
            "isPro": false,
            "fullname": "Zeyu Cui",
            "user": "misakamage",
            "type": "user"
          },
          "name": "Zeyu Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:19:43.843Z",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3223",
          "name": "Zhenru Zhang",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3224",
          "name": "Zhipeng Zhou",
          "hidden": false
        },
        {
          "_id": "68299e3128752b51372d3225",
          "user": {
            "_id": "647ccbd6e07cf9bb2d485244",
            "avatarUrl": "/avatars/e8915abaff04f6762247e196b7cf84df.svg",
            "isPro": false,
            "fullname": "Zihan Qiu",
            "user": "QwQZh",
            "type": "user"
          },
          "name": "Zihan Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:18:58.545Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T13:41:34.000Z",
      "submittedOnDailyAt": "2025-05-19T01:23:20.310Z",
      "title": "Qwen3 技術報告",
      "submittedOnDailyBy": {
        "_id": "610b70452719facd4ea85e28",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
        "isPro": false,
        "fullname": "Chujie Zheng",
        "user": "chujiezheng",
        "type": "user"
      },
      "summary": "この作品では、Qwen3、Qwenモデルフamilyの最新版を紹介します。Qwen3は、性能向上、効率化と多言語能力を進めるために設計された数多くの大規模言語モデル（LLMs）のシリーズです。Qwen3シリーズは、デンスとMixture-of-Expert（MoE）アーキテクチャの両方のモデルを含み、パラメーターサイズが0.6から235億をまたぐらいです。Qwen3の重要な革新点として、複雑な多ステップ推理のための思い出されるモードと、コンテキストを基に迅速な応答のための非思い出されるモードを一つの統合フレームワークに統合したことがあります。これにより、チャット最適化モデル（例：GPT-4o）と専用の推理モデル（例：QwQ-32B）のような異なるモデルの切り替えが必要とならなくなり、ユーザーのクエリやチャットテンプレートに基づいて動的なモード切り替えが可能になります。一方で、Qwen3は、推論中に計算ネットワークリソースの適応的分配を可能にし、タスクの複雑さに応じた遅延と性能のバランスを保ちます。また、旗艦モデルの知識を活用して、小規模モデルの構築に必要な計算ネットワークリソースを大幅に削減し、その性能を高く保ちます。実験的な評価により、Qwen3は、コード生成、数学的推理、アガントタスクなどの多様なベンチマークで最先端の結果を収め、より大きなMoEモデルや専有モデルと競争的です。Qwen3は、前作のQwen2.5と比較して、言語サポートを29言語から119言語へ拡張し、改善されたクロスラングラジュ理解と生成能力によってグローバルなアクセス性を向上させます。Qwen3のすべてのモデルは、Apache 2.0のフリーソftware許諾の下で公開にされ、再現性とコミュニティ駆動の研究開発を促進することを目的としています。",
      "upvotes": 70,
      "discussionId": "68299e3228752b51372d325f",
      "projectPage": "https://qwenlm.github.io/blog/qwen3/",
      "githubRepo": "https://github.com/QwenLM/Qwen3",
      "ai_keywords": [
        "large language models (LLMs)",
        "Mixture-of-Expert (MoE) architectures",
        "thinking mode",
        "non-thinking mode",
        "chat-optimized models",
        "dedicated reasoning models",
        "thinking budget mechanism",
        "computational resources adaptively",
        "inference",
        "latency",
        "performance",
        "code generation",
        "mathematical reasoning",
        "agent tasks",
        "multilingual support",
        "cross-lingual understanding",
        "generation capabilities"
      ]
    },
    "publishedAt": "2025-05-14T09:41:34.000Z",
    "title": "Qwen3 Technical Report",
    "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09388.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "610b70452719facd4ea85e28",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
      "fullname": "Chujie Zheng",
      "name": "chujiezheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 37
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10610",
      "authors": [
        {
          "_id": "682adaf581c740ab4aabc5a3",
          "name": "Zhaowei Wang",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a4",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a5",
          "name": "Xiyu Ren",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a6",
          "name": "Jipeng Zhang",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a7",
          "name": "Yu Zhao",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a8",
          "name": "Rohit Saxena",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5a9",
          "name": "Liang Cheng",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5aa",
          "name": "Ginny Wong",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5ab",
          "name": "Simon See",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5ac",
          "name": "Pasquale Minervini",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5ad",
          "name": "Yangqiu Song",
          "hidden": false
        },
        {
          "_id": "682adaf581c740ab4aabc5ae",
          "name": "Mark Steedman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:52:54.000Z",
      "submittedOnDailyAt": "2025-05-19T08:37:50.522Z",
      "title": "MMLongBench: 長文脈ビジョン言語モデルの評価指標\n長文脈ビジョン言語モデルを効果的かつ詳細に評価する指標",
      "submittedOnDailyBy": {
        "_id": "657ccbf2869d5bb0e53b482f",
        "avatarUrl": "/avatars/2eae5a10bdc14814a04d9f255f16de6b.svg",
        "isPro": false,
        "fullname": "Rohit Saxena",
        "user": "rohitsaxena",
        "type": "user"
      },
      "summary": "大観様ビジョン言語モデルの急速なコンテキストウィンドウ拡大により、長コンテキストビジョン言語モデル（LCVLMs）が発展し、1回の進行計算で数百枚の画像と間に入れ替わったテキストトークンを処理することができるようになりました。本稿では、最初の長コンテキストビジョン言語タスクの多様なセットをカバーするMMLongBenchを紹介し、LCVLMsの効果的かつ透覧的に評価するために設計しました。MMLongBenchは、Visual RAGやMany-Shot ICLなどの5種類の下流タスクを含む13,331例を構成し、自然画像や合成画像の幅広い範囲をカバーしています。また、入力長さに対するモデルの響きを評価するために、5種類の標準化された入力長さ（8K-128Kトークン）でのコスモードトークナイゼーションプログラムを用いて、ビジョンパッチとテキストトークンの組み合わせを使用してサンプルを提供しています。46つの閉源モデルと開源モデルのLCVLMsを透覧的に評価し、現在のモデルの長コンテキスト能力について全面的な分析を行いました。結果として、以下のことが示されました：i) 1つのタスクの性能は全体の長コンテキスト能力の弱い代理となります；ii) 閉源モデルと開源モデルは長コンテキストビジョン言語タスクにおいて課題を抱え、将来の改善の余地があります；iii) 理由能力が強いモデルは長コンテキスト性能がより良くなります。タスクの幅広いカバー、画像の種類の多様性、そして厳格な長さ制御を提供することで、MMLongBenchは次世代のLCVLMsの診断と進歩に欠かせない基盤を提供しています。",
      "upvotes": 14,
      "discussionId": "682adaf681c740ab4aabc5e2",
      "ai_keywords": [
        "long-context vision-language models (LCVLMs)",
        "MMLongBench",
        "Visual RAG",
        "Many-Shot ICL",
        "vision patches",
        "cross-modal tokenization scheme",
        "long-context vision-language tasks",
        "reasoning ability"
      ]
    },
    "publishedAt": "2025-05-15T13:52:54.000Z",
    "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models\n  Effectively and Thoroughly",
    "summary": "The rapid extension of context windows in large vision-language models has\ngiven rise to long-context vision-language models (LCVLMs), which are capable\nof handling hundreds of images with interleaved text tokens in a single forward\npass. In this work, we introduce MMLongBench, the first benchmark covering a\ndiverse set of long-context vision-language tasks, to evaluate LCVLMs\neffectively and thoroughly. MMLongBench is composed of 13,331 examples spanning\nfive different categories of downstream tasks, such as Visual RAG and Many-Shot\nICL. It also provides broad coverage of image types, including various natural\nand synthetic images. To assess the robustness of the models to different input\nlengths, all examples are delivered at five standardized input lengths (8K-128K\ntokens) via a cross-modal tokenization scheme that combines vision patches and\ntext tokens. Through a thorough benchmarking of 46 closed-source and\nopen-source LCVLMs, we provide a comprehensive analysis of the current models'\nvision-language long-context ability. Our results show that: i) performance on\na single task is a weak proxy for overall long-context capability; ii) both\nclosed-source and open-source models face challenges in long-context\nvision-language tasks, indicating substantial room for future improvement; iii)\nmodels with stronger reasoning ability tend to exhibit better long-context\nperformance. By offering wide task coverage, various image types, and rigorous\nlength control, MMLongBench provides the missing foundation for diagnosing and\nadvancing the next generation of LCVLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10610.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657ccbf2869d5bb0e53b482f",
      "avatarUrl": "/avatars/2eae5a10bdc14814a04d9f255f16de6b.svg",
      "fullname": "Rohit Saxena",
      "name": "rohitsaxena",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11409",
      "authors": [
        {
          "_id": "682abb7984695084c1a48eab",
          "name": "Yi Xu",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eac",
          "name": "Chengzu Li",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48ead",
          "user": {
            "_id": "62b279e92375526ae51a537b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
            "isPro": false,
            "fullname": "Han Zhou",
            "user": "hzhouml",
            "type": "user"
          },
          "name": "Han Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:42:16.276Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eae",
          "user": {
            "_id": "65bf213f8467e2a3d6374d4b",
            "avatarUrl": "/avatars/0194cdba95d7a4c01fbbdd505e384a3d.svg",
            "isPro": false,
            "fullname": "X Wan",
            "user": "masonxw",
            "type": "user"
          },
          "name": "Xingchen Wan",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-19T05:02:52.536Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eaf",
          "user": {
            "_id": "63920dfac47e36ddeb8f1864",
            "avatarUrl": "/avatars/c36cbf7b084d62368312e5c9292e4260.svg",
            "isPro": false,
            "fullname": "Caiqi Zhang",
            "user": "caiqizh",
            "type": "user"
          },
          "name": "Caiqi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:23:48.005Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eb0",
          "user": {
            "_id": "617a6284941993035fbaf299",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635410461794-noauth.jpeg",
            "isPro": false,
            "fullname": "Anna Korhonen",
            "user": "akorhonen",
            "type": "user"
          },
          "name": "Anna Korhonen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:23:42.059Z",
          "hidden": false
        },
        {
          "_id": "682abb7984695084c1a48eb1",
          "user": {
            "_id": "6273e70dc8d55dd434bd8e52",
            "avatarUrl": "/avatars/3483eeda218e95b1eb00c3dc63c7d000.svg",
            "isPro": false,
            "fullname": "Ivan Vulić",
            "user": "ivulic",
            "type": "user"
          },
          "name": "Ivan Vulić",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:23:36.111Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62b279e92375526ae51a537b/VYeWx-h6G2brVuuu-Wg5i.png"
      ],
      "publishedAt": "2025-05-16T16:17:22.000Z",
      "submittedOnDailyAt": "2025-05-19T03:37:48.826Z",
      "title": "Visual Planning: Let's Think Only with Images\n\nビジュアルプランニング：みずから画像でみるだけで考えてください",
      "submittedOnDailyBy": {
        "_id": "62b279e92375526ae51a537b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
        "isPro": false,
        "fullname": "Han Zhou",
        "user": "hzhouml",
        "type": "user"
      },
      "summary": "最近の大語言モデル（LLMs）とその多タイプ化拡張（MLLMs）の進歩は、多様なタスクでの機械認知を大幅に向上させました。しかし、これらのモデルは、ビジュアル情報がある場合もあまりにも、理由の表現と構造化において主に純粋なテキストを依存しています。本稿では、言語が理由の表現と構造化の最も自然かつ効果的なモデルではないことを主張し、特に空間的および幾何的な情報を含むタスクでは特に重要です。このモテイブに基づいて、我々は新しいパラダイム「Visual Planning」を提案します。このパラダイムでは、テキストに依存しない純粋なビジュアル表現を用いて計画を行うことができます。このパラダイムでは、計画は、ステップごとの推論をビジュアル領域で表現する画像の列で実行され、人間が未来の行動をスケッチまたは可視化するようにしています。我々は、後学の大視覚モデルにブラウザープログラミング（GRPO）を基に新しい強化学習フレームワーク「Visual Planning via Reinforcement Learning (VPRL)」を紹介します。このフレームワークは、代表的な可視化ナビゲーションタスク（FrozenLake、Maze、MiniBehavior）での計画において大幅な向上を実現します。我々の可視化計画パラダイムは、テキストだけの空間での理由の論理を行うすべての計画の変体よりも優れています。我々の結果は、Visual Planningが言語ベースの理由の論理の代替として可能であることを示し、直感的な画像ベースの推論を受けたタスクにおける新たな道を開きます。",
      "upvotes": 10,
      "discussionId": "682abb7c84695084c1a48fb4",
      "githubRepo": "https://github.com/yix8/VisualPlanning",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "multimodal extensions (MLLMs)",
        "machine reasoning",
        "visual information",
        "Visual Planning",
        "purely visual representations",
        "sequences of images",
        "step-by-step inference",
        "Visual Planning via Reinforcement Learning (VPRL)",
        "GRPO",
        "post-training large vision models",
        "planning",
        "visual navigation tasks",
        "FrozenLake",
        "Maze",
        "MiniBehavior",
        "text-only space",
        "intuitive, image-based inference"
      ]
    },
    "publishedAt": "2025-05-16T12:17:22.000Z",
    "title": "Visual Planning: Let's Think Only with Images",
    "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62b279e92375526ae51a537b/VYeWx-h6G2brVuuu-Wg5i.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11409.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b279e92375526ae51a537b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b279e92375526ae51a537b/U2DxDscDjQ6kWh-jMn0IG.jpeg",
      "fullname": "Han Zhou",
      "name": "hzhouml",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.07675",
      "authors": [
        {
          "_id": "6829dcab0daa5ccc817e6ec8",
          "name": "Seongjae Kang",
          "hidden": false
        },
        {
          "_id": "6829dcab0daa5ccc817e6ec9",
          "user": {
            "_id": "64f000769e7770db74d44bba",
            "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
            "isPro": false,
            "fullname": "Dong-Bok Lee",
            "user": "dongboklee",
            "type": "user"
          },
          "name": "Dong Bok Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:42:58.152Z",
          "hidden": false
        },
        {
          "_id": "6829dcab0daa5ccc817e6eca",
          "name": "Hyungjoon Jang",
          "hidden": false
        },
        {
          "_id": "6829dcab0daa5ccc817e6ecb",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T15:39:51.000Z",
      "submittedOnDailyAt": "2025-05-19T06:17:24.942Z",
      "title": "ビジョン・ラベルモデルによる簡単な半監督知識収納を、ダブルヘッド最適化を通じて実現する",
      "submittedOnDailyBy": {
        "_id": "64f000769e7770db74d44bba",
        "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
        "isPro": false,
        "fullname": "Dong-Bok Lee",
        "user": "dongboklee",
        "type": "user"
      },
      "summary": "Vision-language models (VLMs)は、豊富な文脈情報と最小限の標準化データを利用して多様なタスクで驚異的な成功を達成しました。しかし、これらの大規模なモデルの実装は、特にリソース制限された環境では難しいです。Knowledge distillation (KD)はこの問題に対して既に定着した解決策ですが、最近のVLMsからのKDアプローチは多段階的なトレーニングまたは追加の調整を含むことで計算オーバーヘッドと最適化複雑性を増加させています。本論文では、シンプルで効果的なKDフレームワークとして、VLMsからコンパクトなタスク専門モデルへの知識の伝達を実現するtexttt{D}ual-texttt{H}ead\ntexttt{O}ptimization (texttt{DHO})を提案します。特に、ラベル付けデータと教師の予測から独立に学習するダブル予測ヘッドを導入し、推論時にその出力を線形結合することを提案します。DHOは、規範的と結合信号の勾配の衝突を抑え、単一ヘッドのKDベースラインに比べてより効果的な特徴学習を可能にします。その結果、拡張された実験は、DHOは複数のデータセットでベースラインを続けています。特に、ImageNetでは状態の最先端の性能を達成し、1%と10%のラベル付けデータを使用してそれぞれ3%と0.1%の精度を向上させ、少ないパラメータを使用します。",
      "upvotes": 8,
      "discussionId": "6829dcad0daa5ccc817e6f40",
      "ai_keywords": [
        "Vision-language models (VLMs)",
        "knowledge distillation (KD)",
        "dual prediction heads",
        "gradient conflicts",
        "feature learning",
        "semi-supervised settings",
        "state-of-the-art performance",
        "ImageNet",
        "accuracy"
      ]
    },
    "publishedAt": "2025-05-12T11:39:51.000Z",
    "title": "Simple Semi-supervised Knowledge Distillation from Vision-Language\n  Models via texttt{D}ual-texttt{H}ead\n  texttt{O}ptimization",
    "summary": "Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\ntexttt{D}ual-texttt{H}ead\ntexttt{O}ptimization (texttt{DHO}) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that DHO mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that DHO\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07675.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64f000769e7770db74d44bba",
      "avatarUrl": "/avatars/d015820380ffb823b1b35df64dcd3457.svg",
      "fullname": "Dong-Bok Lee",
      "name": "dongboklee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11107",
      "authors": [
        {
          "_id": "682ad96cdc6d7453624831b9",
          "user": {
            "_id": "6213410828005421265b27d3",
            "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
            "isPro": false,
            "fullname": "許湛然",
            "user": "Splend1dchan",
            "type": "user"
          },
          "name": "Chan-Jan Hsu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:23:15.798Z",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831ba",
          "name": "Davide Buffelli",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bb",
          "name": "Jamie McGowan",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bc",
          "name": "Feng-Ting Liao",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bd",
          "name": "Yi-Chang Chen",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831be",
          "name": "Sattar Vakili",
          "hidden": false
        },
        {
          "_id": "682ad96cdc6d7453624831bf",
          "name": "Da-shan Shiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T10:40:35.000Z",
      "submittedOnDailyAt": "2025-05-19T05:58:53.531Z",
      "title": "グループサインキング：トークンレベルでの並列理由論理アガントの協力",
      "submittedOnDailyBy": {
        "_id": "6213410828005421265b27d3",
        "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
        "isPro": false,
        "fullname": "許湛然",
        "user": "Splend1dchan",
        "type": "user"
      },
      "summary": "最近の大規模言語モデル（LLMs）の進展は、自己生成された思いつきの連鎖を通じて理由論を行う力を示しています。多数の理由論アガントは、個人の結果よりも共通の理由論の質を高めることができます。しかし、これらのアガントは通常、ターンバーステップのモードで相互作用し、後退性を増やしながら品質を向上させます。本論文では、Group ThinkというシングルのLLMを提案します。これは複数の並行理由論アガント（シンカー）として機能します。他のアガントの部分生成進捗を共有することで、Group Thinkはトークンレベルで動的に相互に理由論のトラジェクトを適応し、新しい並行理由論パラダイムを導入します。例えば、理由論のスレッドは、他のスレッドがより適切な位置で続けることが認識された場合、中間で生成を変更することができます。このトークンレベルのコラボレーションにより、Group Thinkは冗長な理由論を減らし、品質を向上させ、同時に非常に低い後退性を達成できます。また、その並行性により、空き計算コンピュータリズムリソースを効率的に利用でき、エッジ推論に特に適しています。ここでは、現存するすべてのLLMがローカルGPUでGroup Thinkを行うことを可能にする簡単で一般化可能な変更を提案します。また、理由論の後退性をベンチマークする評価戦略を提出し、Group Thinkを明示的に訓練されていないオープンソースLLMを使用して後退性の向上を実験的に示します。この研究は、将来のLLMがより複雑でより効率的な協調行動を示すことで、より高品質な生成に向けていくことを望むことを示しています。",
      "upvotes": 7,
      "discussionId": "682ad96ddc6d7453624831f3",
      "ai_keywords": [
        "large language models (LLMs)",
        "reasoning through self-generated chains of thought",
        "reasoning agents",
        "turn-based manner",
        "Group Think",
        "concurrent reasoning agents",
        "think ers",
        "shared visibility",
        "reasoning trajectories",
        "token level",
        "reasoning thread",
        "fine-grained, token-level collaboration",
        "redundant reasoning",
        "edge inference",
        "modification",
        "LLMs",
        "local GPU",
        "evaluation strategy",
        "reasoning latency"
      ]
    },
    "publishedAt": "2025-05-16T06:40:35.000Z",
    "title": "Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity",
    "summary": "Recent advances in large language models (LLMs) have demonstrated the power\nof reasoning through self-generated chains of thought. Multiple reasoning\nagents can collaborate to raise joint reasoning quality above individual\noutcomes. However, such agents typically interact in a turn-based manner,\ntrading increased latency for improved quality. In this paper, we propose Group\nThink--a single LLM that acts as multiple concurrent reasoning agents, or\nthinkers. With shared visibility into each other's partial generation progress,\nGroup Think introduces a new concurrent-reasoning paradigm in which multiple\nreasoning trajectories adapt dynamically to one another at the token level. For\nexample, a reasoning thread may shift its generation mid-sentence upon\ndetecting that another thread is better positioned to continue. This\nfine-grained, token-level collaboration enables Group Think to reduce redundant\nreasoning and improve quality while achieving significantly lower latency.\nMoreover, its concurrent nature allows for efficient utilization of idle\ncomputational resources, making it especially suitable for edge inference,\nwhere very small batch size often underutilizes local~GPUs. We give a simple\nand generalizable modification that enables any existing LLM to perform Group\nThink on a local GPU. We also present an evaluation strategy to benchmark\nreasoning latency and empirically demonstrate latency improvements using\nopen-source LLMs that were not explicitly trained for Group Think. We hope this\nwork paves the way for future LLMs to exhibit more sophisticated and more\nefficient collaborative behavior for higher quality generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11107.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6213410828005421265b27d3",
      "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
      "fullname": "許湛然",
      "name": "Splend1dchan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11427",
      "authors": [
        {
          "_id": "682ad9809506a7e45a93be00",
          "user": {
            "_id": "6318e7a2acffc70bd4e057ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6318e7a2acffc70bd4e057ec/2m3XSbNLwv7Kmo8qfWq3L.jpeg",
            "isPro": false,
            "fullname": "Adrian Robert Minut",
            "user": "adrianrob",
            "type": "user"
          },
          "name": "Adrian Robert Minut",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:22.059Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be01",
          "user": {
            "_id": "63ab16a6d7ee953f604ecd52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ab16a6d7ee953f604ecd52/ujylOpczHKxU6Kfr-jGVr.png",
            "isPro": false,
            "fullname": "Tommaso Mencattini",
            "user": "tmencatt",
            "type": "user"
          },
          "name": "Tommaso Mencattini",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:22:21.898Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be02",
          "user": {
            "_id": "5e8ef1f14957053f606489e6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
            "isPro": false,
            "fullname": "Andrea Santilli",
            "user": "teelinsan",
            "type": "user"
          },
          "name": "Andrea Santilli",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:22:26.518Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be03",
          "user": {
            "_id": "64256584daa3502ee3570b86",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64256584daa3502ee3570b86/kui0eb59S5aTUeZIjawUj.jpeg",
            "isPro": false,
            "fullname": "Donato Crisostomi",
            "user": "crisostomi",
            "type": "user"
          },
          "name": "Donato Crisostomi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:28.737Z",
          "hidden": false
        },
        {
          "_id": "682ad9809506a7e45a93be04",
          "user": {
            "_id": "652681664e066bf73f8e2bd1",
            "avatarUrl": "/avatars/084dec4765d9996d74901b8df95ec35f.svg",
            "isPro": false,
            "fullname": "Emanuele Rodola'",
            "user": "erodola",
            "type": "user"
          },
          "name": "Emanuele Rodolà",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:35.566Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T16:43:23.000Z",
      "submittedOnDailyAt": "2025-05-19T05:45:27.421Z",
      "title": "メルジネット：簡単な進化モデルメリジングライブラリ",
      "submittedOnDailyBy": {
        "_id": "5e8ef1f14957053f606489e6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
        "isPro": false,
        "fullname": "Andrea Santilli",
        "user": "teelinsan",
        "type": "user"
      },
      "summary": "モデルの統合できることができ、既存のモデルの機能を新しいモデルに統合できることで、追加のトレーニングを必要としない。これは、コストの低さと消費者向けのグラフィックプロセッサでの統合をサポートするライブラリの存在により、これを増加してプロパティティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティポラリティを高めることができることで、これは増加してプロパティ",
      "upvotes": 6,
      "discussionId": "682ad9819506a7e45a93be38",
      "githubRepo": "https://github.com/tommasomncttn/mergenetic",
      "ai_keywords": [
        "model merging",
        "evolutionary algorithms",
        "Mergenetic",
        "fitness estimators",
        "evaluation costs"
      ]
    },
    "publishedAt": "2025-05-16T12:43:23.000Z",
    "title": "Mergenetic: a Simple Evolutionary Model Merging Library",
    "summary": "Model merging allows combining the capabilities of existing models into a new\none - post hoc, without additional training. This has made it increasingly\npopular thanks to its low cost and the availability of libraries that support\nmerging on consumer GPUs. Recent work shows that pairing merging with\nevolutionary algorithms can boost performance, but no framework currently\nsupports flexible experimentation with such strategies in language models. We\nintroduce Mergenetic, an open-source library for evolutionary model merging.\nMergenetic enables easy composition of merging methods and evolutionary\nalgorithms while incorporating lightweight fitness estimators to reduce\nevaluation costs. We describe its design and demonstrate that Mergenetic\nproduces competitive results across tasks and languages using modest hardware.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e8ef1f14957053f606489e6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1635502086699-5e8ef1f14957053f606489e6.jpeg",
      "fullname": "Andrea Santilli",
      "name": "teelinsan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10962",
      "authors": [
        {
          "_id": "682ab4fe7a9f1a7ec9779dd6",
          "user": {
            "_id": "62ffa3f8311cad266f9af236",
            "avatarUrl": "/avatars/4c88cb518e000a475f8381573f21aa7f.svg",
            "isPro": false,
            "fullname": "Zhenwen Liang",
            "user": "invokerliang",
            "type": "user"
          },
          "name": "Zhenwen Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:24:26.692Z",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dd7",
          "user": {
            "_id": "64c94eddcb2f1bf0e7db5a4d",
            "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
            "isPro": false,
            "fullname": "Linfeng Song",
            "user": "freesunshine0316",
            "type": "user"
          },
          "name": "Linfeng Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:24:45.999Z",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dd8",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dd9",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779dda",
          "name": "Feng Zhang",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779ddb",
          "user": {
            "_id": "65147a1426fbd558dbd08f1b",
            "avatarUrl": "/avatars/86574ee2d5c22e940be1c4e50be88675.svg",
            "isPro": false,
            "fullname": "Haitao Mi",
            "user": "haitaominlp",
            "type": "user"
          },
          "name": "Haitao Mi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:24:56.781Z",
          "hidden": false
        },
        {
          "_id": "682ab4fe7a9f1a7ec9779ddc",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T07:56:03.000Z",
      "submittedOnDailyAt": "2025-05-19T03:06:11.065Z",
      "title": "MPS-Prover: マルチパーソンショップの検索とデータカレーティングによるステップごとの定理証明の進展",
      "submittedOnDailyBy": {
        "_id": "64c94eddcb2f1bf0e7db5a4d",
        "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
        "isPro": false,
        "fullname": "Linfeng Song",
        "user": "freesunshine0316",
        "type": "user"
      },
      "summary": "自動定理証明（ATP）は形式言語では、AIにとって難しい挑戦であり、厳密な論理的推論と巨大な探索空間の歩みを通る必要がある。この他、大規模言語モデル（LLMs）は顕著な性能を示しているが、現在のステップごとの証明器は偏りをもった探索ガイドにより、不適切な証明戦略を導くことがあり、これにより効率が低下します。本論文では、これらの制限を克服するために新しいステップごとのATPシステムである多角度探索証明器（MPS-Prover）を紹介します。MPS-Proverは2つのキーイノベーションを採用しています：高い効率性を維持する限りに約40%の冗長なトレーニングデータを削減する後学習データ編集戦略と、多角度の木探索機構です。この探索は学習された批判モデルと戦略的に設計されたヒューリスティックルールを組み合わせ、戦略の選択を多様化し、無産業的な状態に陥らず、探索の強固性を向上させることを目指しています。拡張評価により、MPS-ProverはminiF2FとProofNetなどの複数の難しいベンチマークで最先端の性能を達成し、先行の7Bパラメータモデルを超えています。また、分析により、MPS-Proverは現在のステップごとのおよび全体の証明方法に比べて、より短いおよび多様な証明を生成し、これにより効率と効果性を特徴としています。我々の研究は、LLMベースの形式論理の能力を進め、より強力な定理証明器の開発に向けて強固なフレームワークと詳細な分析を提供しています。",
      "upvotes": 5,
      "discussionId": "682ab4ff7a9f1a7ec9779e71",
      "ai_keywords": [
        "Automated Theorem Proving (ATP)",
        "large language models (LLMs)",
        "biased search guidance",
        "Multi-Perspective Search Prover (MPS-Prover)",
        "post-training data curation strategy",
        "multi-perspective tree search mechanism",
        "learned critic model",
        "heuristic rules",
        "tactic selection",
        "search robustness",
        "miniF2F",
        "ProofNet",
        "state-of-the-art performance",
        "formal reasoning"
      ]
    },
    "publishedAt": "2025-05-16T03:56:03.000Z",
    "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective\n  Search and Data Curation",
    "summary": "Automated Theorem Proving (ATP) in formal languages remains a formidable\nchallenge in AI, demanding rigorous logical deduction and navigating vast\nsearch spaces. While large language models (LLMs) have shown promising\nperformance, existing stepwise provers often suffer from biased search\nguidance, leading to inefficiencies and suboptimal proof strategies. This paper\nintroduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise\nATP system designed to overcome these limitations. MPS-Prover incorporates two\nkey innovations: a highly effective post-training data curation strategy that\nprunes approximately 40% of redundant training data without sacrificing\nperformance, and a multi-perspective tree search mechanism. This search\nintegrates a learned critic model with strategically designed heuristic rules\nto diversify tactic selection, prevent getting trapped in unproductive states,\nand enhance search robustness. Extensive evaluations demonstrate that\nMPS-Prover achieves state-of-the-art performance on multiple challenging\nbenchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter\nmodels. Furthermore, our analyses reveal that MPS-Prover generates\nsignificantly shorter and more diverse proofs compared to existing stepwise and\nwhole-proof methods, highlighting its efficiency and efficacy. Our work\nadvances the capabilities of LLM-based formal reasoning and offers a robust\nframework and a comprehensive analysis for developing more powerful theorem\nprovers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10962.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c94eddcb2f1bf0e7db5a4d",
      "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
      "fullname": "Linfeng Song",
      "name": "freesunshine0316",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10518",
      "authors": [
        {
          "_id": "68271c682f2e31ef0667bfaf",
          "user": {
            "_id": "668e4d1b446c8736208d99e1",
            "avatarUrl": "/avatars/dbe10f3b181e789d98b9b6bde4f711b2.svg",
            "isPro": false,
            "fullname": "Anastasios Gerontopoulos",
            "user": "nasos10",
            "type": "user"
          },
          "name": "Anastasios Gerontopoulos",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-18T19:39:25.735Z",
          "hidden": false
        },
        {
          "_id": "68271c682f2e31ef0667bfb0",
          "name": "Spyros Gidaris",
          "hidden": false
        },
        {
          "_id": "68271c682f2e31ef0667bfb1",
          "name": "Nikos Komodakis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/668e4d1b446c8736208d99e1/VMNrdGj9BjgRO8fMsagaH.png"
      ],
      "publishedAt": "2025-05-15T17:25:03.000Z",
      "submittedOnDailyAt": "2025-05-19T07:50:27.978Z",
      "title": "Multi-Token Prediction Needs Registers",
      "submittedOnDailyBy": {
        "_id": "668e4d1b446c8736208d99e1",
        "avatarUrl": "/avatars/dbe10f3b181e789d98b9b6bde4f711b2.svg",
        "isPro": false,
        "fullname": "Anastasios Gerontopoulos",
        "user": "nasos10",
        "type": "user"
      },
      "summary": "多トーク予測は、言語モデルの事前学習を改善するための有望な目標として登場しましたが、その利益はファイナルチューニングなどの他の設定に一貫して一般化されていません。本論文では、学習可能なレジスタトークンを入力シーケンスに間違って入れることで、各トークンは将来のターゲットを予測するための簡単で効果的な多トーク予測アプローチを提案します。既存の方法と比較して、MuToRは以下のような主要な優点を提供します：追加するパラメータ数は微視的に数えられます、構造的変更は必要ありません—オフショールの事前学習済み言語モデルとの相性を確保します—次のトークンの事前学習目標に沿っていますので、特にサブジェクトフィードバックファイナルチューニングに適しています。また、スケーラブルな予測ホリゾンを自然にサポートします。多様なケースにおいて、言語と視覚領域の難しい生成タスクに対して、サブジェクトフィードバックファイナルチューニング、パラメータ効率的なファイナルチューニング（PEFT）、事前学習を含む、MuToRの効果および多様性を示します。コードは以下のURLで公開されます：https://github.com/nasosger/MuToR。",
      "upvotes": 3,
      "discussionId": "68271c692f2e31ef0667bff6",
      "githubRepo": "https://github.com/nasosger/MuToR",
      "ai_keywords": [
        "register tokens",
        "multi-token prediction",
        "next-token pretraining",
        "parameter-efficient fine-tuning (PEFT)",
        "generative tasks"
      ]
    },
    "publishedAt": "2025-05-15T13:25:03.000Z",
    "title": "Multi-Token Prediction Needs Registers",
    "summary": "Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/668e4d1b446c8736208d99e1/VMNrdGj9BjgRO8fMsagaH.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10518.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668e4d1b446c8736208d99e1",
      "avatarUrl": "/avatars/dbe10f3b181e789d98b9b6bde4f711b2.svg",
      "fullname": "Anastasios Gerontopoulos",
      "name": "nasos10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11152",
      "authors": [
        {
          "_id": "682a9a3f5e6f0c59f4d8a0e5",
          "user": {
            "_id": "65601c6ee23401f82005e361",
            "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
            "isPro": false,
            "fullname": "Daniel Sungho Jung",
            "user": "dqj5182",
            "type": "user"
          },
          "name": "Daniel Sungho Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T06:42:43.502Z",
          "hidden": false
        },
        {
          "_id": "682a9a3f5e6f0c59f4d8a0e6",
          "user": {
            "_id": "656056b21392aa3beb5de0bd",
            "avatarUrl": "/avatars/07f25b750ef308d65f2e6c82506e7816.svg",
            "isPro": false,
            "fullname": "Kyoung Mu  Lee ",
            "user": "kyoungmu",
            "type": "user"
          },
          "name": "Kyoung Mu Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:48.640Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T11:54:25.000Z",
      "submittedOnDailyAt": "2025-05-19T01:11:35.713Z",
      "title": "デンシットハンド接点推定従って不均衡データから学習する",
      "submittedOnDailyBy": {
        "_id": "65601c6ee23401f82005e361",
        "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
        "isPro": false,
        "fullname": "Daniel Sungho Jung",
        "user": "dqj5182",
        "type": "user"
      },
      "summary": "手は人間の相互作用に重要な役割を果たし、手と世界との接触を理解することは、手の機能を全面的に理解するために重要である。最近、物体、他の手、スケーン、および身体との相互作用をカバーする手の相互作用データセットの数が増加している。このタスクの重要性と高品質なデータの増加にもとづいても、手の接触を密度的に推定するためによく学習できる方法はまだ大きく探索されていない。手の接触を密度的に推定するために学習するには、2つの主要な課題がある。最初に、手の接触データセットには、多数のサンプルが接触していないクラス不均衡問題がある。次に、手の接触データセットには、多数の手の接触が指先に現れ、他の手の領域での接触の一般化に課題があるスペクトル不均衡問題がある。これらの問題を解決するために、我々は不均衡なデータから密度的なHAnd COntact推定(HACO)を学習するフレームワークを提案する。クラス不均衡問題を解決するために、我々は平衡された接触サンプリングを導入し、接触と非接触のサンプルの多様な接触統計を公平に表現するために、複数のサンプリンググループからサンプリングすることを行う。また、スペクトル不均衡問題を解決するために、我々は頂点レベルクラスバランス(VCB)損失を提案する。これは、頂点の接触頻度をデータセット全体で別々に重み付けし、接触の空間的な分布を考慮することで、損失の貢献を変更する。その結果、我々は大規模な手の接触データを使用して、クラスとスペクトル不均衡問題による影響を受けずに、密度的な手の接触推定を学習することができる。コードは公開される予定である。",
      "upvotes": 2,
      "discussionId": "682a9a405e6f0c59f4d8a125",
      "projectPage": "https://haco-release.github.io/",
      "githubRepo": "https://github.com/dqj5182/HACO_RELEASE",
      "ai_keywords": [
        "dense hand contact estimation",
        "class imbalance issue",
        "spatial imbalance issue",
        "finger tips",
        "balanced contact sampling",
        "vertex-level class-balanced (VCB) loss",
        "contact distribution",
        "contact frequency"
      ]
    },
    "publishedAt": "2025-05-16T07:54:25.000Z",
    "title": "Learning Dense Hand Contact Estimation from Imbalanced Data",
    "summary": "Hands are essential to human interaction, and understanding contact between\nhands and the world can promote comprehensive understanding of their function.\nRecently, there have been growing number of hand interaction datasets that\ncover interaction with object, other hand, scene, and body. Despite the\nsignificance of the task and increasing high-quality data, how to effectively\nlearn dense hand contact estimation remains largely underexplored. There are\ntwo major challenges for learning dense hand contact estimation. First, there\nexists class imbalance issue from hand contact datasets where majority of\nsamples are not in contact. Second, hand contact datasets contain spatial\nimbalance issue with most of hand contact exhibited in finger tips, resulting\nin challenges for generalization towards contacts in other hand regions. To\ntackle these issues, we present a framework that learns dense HAnd COntact\nestimation (HACO) from imbalanced data. To resolve the class imbalance issue,\nwe introduce balanced contact sampling, which builds and samples from multiple\nsampling groups that fairly represent diverse contact statistics for both\ncontact and non-contact samples. Moreover, to address the spatial imbalance\nissue, we propose vertex-level class-balanced (VCB) loss, which incorporates\nspatially varying contact distribution by separately reweighting loss\ncontribution of each vertex based on its contact frequency across dataset. As a\nresult, we effectively learn to predict dense hand contact estimation with\nlarge-scale hand contact data without suffering from class and spatial\nimbalance issue. The codes will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11152.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65601c6ee23401f82005e361",
      "avatarUrl": "/avatars/e9fc24bd8c5afd8b07a2f42765d44a7d.svg",
      "fullname": "Daniel Sungho Jung",
      "name": "dqj5182",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11049",
      "authors": [
        {
          "_id": "682af4241286a7273c5bfd09",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0a",
          "name": "Shengfang Zhai",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0b",
          "name": "Mingzhe Du",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0c",
          "name": "Yulin Chen",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0d",
          "name": "Tri Cao",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0e",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd0f",
          "name": "Cheng Wang",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd10",
          "name": "Xinfeng Li",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd11",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd12",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd13",
          "name": "Jiaheng Zhang",
          "hidden": false
        },
        {
          "_id": "682af4241286a7273c5bfd14",
          "name": "Bryan Hooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T09:46:10.000Z",
      "submittedOnDailyAt": "2025-05-19T07:36:35.140Z",
      "title": "GuardReasoner-VL: VLMsを守るための強化ロジック",
      "submittedOnDailyBy": {
        "_id": "6650c77a74664a42ddfb9187",
        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
        "isPro": false,
        "fullname": "yueliu1999",
        "user": "yueliu1999",
        "type": "user"
      },
      "summary": "ビデオラボリティーモデル（VLM）の安全性を向上させるために、本論文では、新しい理由基盤をもつVLMガードモデル「GuardReasoner-VL」を紹介します。核心のアイデアは、オンラインRLを通じて、ガードモデルが決断をする前に説明を行うことを奨励することです。まず、123Kサンプルと631K理由ステップを持つ理由コーパス「GuardReasoner-VLTrain」を構築します。その後、これに基づいてSFTを通じてモデルの理由能力を初期化します。また、オンラインRLを通じて、モデレーションの理由を進めることで理由能力を進化させます。具體には、サンプルの多様性と難易度を向上させるために、提出された安全意識のあるデータ結合による拒否サンプリングとデータアフィーンメントを行います。また、早期階で探索を促すために動的なクリップパラメータを使用し、後期階で開発を促すことでバランスを取ります。性能とトークンの効率性をバランスとするために、精度、フォーマット、トークンコストを統合した長さによる安全リベンバルを設計します。拡張された実験は、モデルの上位性を示し、平均で19.27%のF1スコアでオーバーライドします。GuardReasoner-VLのデータ、コード、モデル（3B/7B）は、https://github.com/yueliu1999/GuardReasoner-VL/ からリリースされています。",
      "upvotes": 2,
      "discussionId": "682af42c1286a7273c5bfed9",
      "ai_keywords": [
        "GuardReasoner-VL",
        "online RL",
        "GuardReasoner-VLTrain",
        "reasoning corpus",
        "SFT",
        "rejection sampling",
        "data augmentation",
        "safety-aware data concatenation",
        "dynamic clipping parameter",
        "length-aware safety reward",
        "F1 score"
      ]
    },
    "publishedAt": "2025-05-16T05:46:10.000Z",
    "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning",
    "summary": "To enhance the safety of VLMs, this paper introduces a novel reasoning-based\nVLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the\nguard model to deliberatively reason before making moderation decisions via\nonline RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with\n123K samples and 631K reasoning steps, spanning text, image, and text-image\ninputs. Then, based on it, we cold-start our model's reasoning ability via SFT.\nIn addition, we further enhance reasoning regarding moderation through online\nRL. Concretely, to enhance diversity and difficulty of samples, we conduct\nrejection sampling followed by data augmentation via the proposed safety-aware\ndata concatenation. Besides, we use a dynamic clipping parameter to encourage\nexploration in early stages and exploitation in later stages. To balance\nperformance and token efficiency, we design a length-aware safety reward that\nintegrates accuracy, format, and token cost. Extensive experiments demonstrate\nthe superiority of our model. Remarkably, it surpasses the runner-up by 19.27%\nF1 score on average. We release data, code, and models (3B/7B) of\nGuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11049.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6650c77a74664a42ddfb9187",
      "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
      "fullname": "yueliu1999",
      "name": "yueliu1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11140",
      "authors": [
        {
          "_id": "682ad417500638b80a43471d",
          "user": {
            "_id": "60d33fbbd7b174177faabd4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
            "isPro": true,
            "fullname": "Mike Zhang",
            "user": "jjzha",
            "type": "user"
          },
          "name": "Mike Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-19T07:23:04.053Z",
          "hidden": false
        },
        {
          "_id": "682ad417500638b80a43471e",
          "user": {
            "_id": "678fa79005ae7fe48d03ba47",
            "avatarUrl": "/avatars/a78ab2b37fa3e18ace783f6f71f5a361.svg",
            "isPro": false,
            "fullname": "Johannes Bjerva",
            "user": "bjerva",
            "type": "user"
          },
          "name": "Johannes Bjerva",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:25:58.827Z",
          "hidden": false
        },
        {
          "_id": "682ad417500638b80a43471f",
          "user": {
            "_id": "60ed4c56abab3c2620df8ac8",
            "avatarUrl": "/avatars/ad5508c1c94a96f6d1290e4735e81b73.svg",
            "isPro": false,
            "fullname": "Russa Biswas",
            "user": "rubis",
            "type": "user"
          },
          "name": "Russa Biswas",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-19T07:26:04.749Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T11:39:33.000Z",
      "submittedOnDailyAt": "2025-05-19T05:25:57.460Z",
      "title": "スケーリング・レジニングは、大規模言語モデルの事実性を改善することができます。",
      "submittedOnDailyBy": {
        "_id": "60d33fbbd7b174177faabd4f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
        "isPro": true,
        "fullname": "Mike Zhang",
        "user": "jjzha",
        "type": "user"
      },
      "summary": "最近の大規模言語モデル（LLM）の理由論能力に関する研究は、数学的な理由論タスクにおいて長い思考過程と追加的な計算コンピューティングリソースを使用してモデルの性能に望ましい向上を示していることが明らかになっています（Muennighoff et al., 2025）。しかし、長い理由論ケースが事実の正確性を自動的に向上させることがわからない状況が残っています、特に数学的なコンテキストを超えた場合は。本研究では、複雑な開放ドメインの質問回答（QA）シナリオ内でのLLMの理由論を検討します。最初に、進捗な大規模の理由論モデル（QwQ-32BとDeepSeek-R1-671B）から理由論トレースを精約し、Qwen2.5に基づく小さなモデルから大きなアーキテクチャまでの多様なモデルを微調校します。理由論トレースを豊富にするために、知識グラフからのパスを理由論トレースに組み込みます。実験設定は、4つのベースラインアプローチと6つの異なる指示調整モデルを含むベンチマークの6つのデータセットで行われ、22.6K以上の質問を含みます。全体として、168回の実験を行い、約170万の理由論トレースを分析します。我々の発見は、1回の実験で、小さな理由論モデルが事実の正確性に顕著な向上を達成し、その指示調整モデルと比較して見られます。また、我々の分析は、テスト時の計算量とトークンバジュードを追加して事実の正確性が積極的に向上し、2-8%程度で、開放ドメインのQAタスクでの理由論の正確性を向上させるためのテスト時スケーリングの効果を進めることを顕著にします。すべての実験アーティファクトを進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進めるための進",
      "upvotes": 1,
      "discussionId": "682ad418500638b80a434770",
      "githubRepo": "https://github.com/jjzha/fs1",
      "ai_keywords": [
        "large language model (LLM)",
        "reasoning capabilities",
        "mathematical reasoning",
        "length thinking process",
        "computational resources",
        "inference",
        "complex open-domain question-answering (QA)",
        "reasoning traces",
        "reasoning models",
        "QwQ-32B",
        "DeepSeek-R1-671B",
        "instruction-tuned variants",
        "Qwen2.5",
        "knowledge graphs",
        "paths",
        "reasoning traces",
        "baseline approaches",
        "instruction-tuned models",
        "benchmark",
        "datasets",
        "experimental runs",
        "factual accuracy",
        "test-time compute",
        "token budgets",
        "test-time scaling",
        "reasoning accuracy"
      ]
    },
    "publishedAt": "2025-05-16T07:39:33.000Z",
    "title": "Scaling Reasoning can Improve Factuality in Large Language Models",
    "summary": "Recent studies on large language model (LLM) reasoning capabilities have\ndemonstrated promising improvements in model performance by leveraging a\nlengthy thinking process and additional computational resources during\ninference, primarily in tasks involving mathematical reasoning (Muennighoff et\nal., 2025). However, it remains uncertain if longer reasoning chains inherently\nenhance factual accuracy, particularly beyond mathematical contexts. In this\nwork, we thoroughly examine LLM reasoning within complex open-domain\nquestion-answering (QA) scenarios. We initially distill reasoning traces from\nadvanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then\nfine-tune a variety of models ranging from smaller, instruction-tuned variants\nto larger architectures based on Qwen2.5. To enrich reasoning traces, we\nintroduce factual information from knowledge graphs in the form of paths into\nour reasoning traces. Our experimental setup includes four baseline approaches\nand six different instruction-tuned models evaluated across a benchmark of six\ndatasets, encompassing over 22.6K questions. Overall, we carry out 168\nexperimental runs and analyze approximately 1.7 million reasoning traces. Our\nfindings indicate that, within a single run, smaller reasoning models achieve\nnoticeable improvements in factual accuracy compared to their original\ninstruction-tuned counterparts. Moreover, our analysis demonstrates that adding\ntest-time compute and token budgets factual accuracy consistently improves by\n2-8%, further confirming the effectiveness of test-time scaling for enhancing\nperformance and consequently improving reasoning accuracy in open-domain QA\ntasks. We release all the experimental artifacts for further research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11140.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d33fbbd7b174177faabd4f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d33fbbd7b174177faabd4f/pfyv_xj2B2m2N4F4sT9zJ.jpeg",
      "fullname": "Mike Zhang",
      "name": "jjzha",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.11011",
      "authors": [
        {
          "_id": "682ae098730bd40a0755f87c",
          "name": "Darija Barak",
          "hidden": false
        },
        {
          "_id": "682ae098730bd40a0755f87d",
          "name": "Miguel Costa-Gomes",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T09:01:09.000Z",
      "submittedOnDailyAt": "2025-05-19T06:12:42.874Z",
      "title": "人間は戦略ゲームのLLM相手から理性と協力を期待しています。",
      "submittedOnDailyBy": {
        "_id": "6475c2794766357252e69e9f",
        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
        "isPro": false,
        "fullname": "i",
        "user": "iliashum",
        "type": "user"
      },
      "summary": "LLMsが社会と経済的インタラクションに統合していく中、LLMsの敵対者として人間がどのように対応するかを深く理解する必要があります。また、人間とLLMsとの多様者の美しさのコンテストでの行動の差異を見るための最初の制御的な費用奨励実験の結果を紹介します。実験は個人レベルでの行動を比較するために、同じ受験者を使用しています。この環境では、LLMsとの対戦では、人間と比べて非常に低い数字を選択します。この変化は、`zero' Nash-equilibrium選択の増加により主な原因となります。この変化は、高い戦略的な理由能力を持つ受験者によって主導されています。`zero' Nash-equilibrium選択を選択する受験者は、LLMの理由能力と、驚くべきに、協調性の傾向をよみがえり、その戦略を説明しています。我々の発見は、同時選択ゲームでの人間とLLMsの相互作用における基盤的なエンドネスを提供し、人間の行動と信念の不均一性を明らかにし、混合ハイプロン人間LLMsシステムの機構設計に重要な意味を示しています。",
      "upvotes": 1,
      "discussionId": "682ae099730bd40a0755f8b9",
      "ai_keywords": [
        "p-beauty contest",
        "Nash-equilibrium choices",
        "strategic reasoning ability",
        "reasoning ability",
        "propensity towards cooperation",
        "mechanism design"
      ]
    },
    "publishedAt": "2025-05-16T05:01:09.000Z",
    "title": "Humans expect rationality and cooperation from LLM opponents in\n  strategic games",
    "summary": "As Large Language Models (LLMs) integrate into our social and economic\ninteractions, we need to deepen our understanding of how humans respond to LLMs\nopponents in strategic settings. We present the results of the first controlled\nmonetarily-incentivised laboratory experiment looking at differences in human\nbehaviour in a multi-player p-beauty contest against other humans and LLMs. We\nuse a within-subject design in order to compare behaviour at the individual\nlevel. We show that, in this environment, human subjects choose significantly\nlower numbers when playing against LLMs than humans, which is mainly driven by\nthe increased prevalence of `zero' Nash-equilibrium choices. This shift is\nmainly driven by subjects with high strategic reasoning ability. Subjects who\nplay the zero Nash-equilibrium choice motivate their strategy by appealing to\nperceived LLM's reasoning ability and, unexpectedly, propensity towards\ncooperation. Our findings provide foundational insights into the multi-player\nhuman-LLM interaction in simultaneous choice games, uncover heterogeneities in\nboth subjects' behaviour and beliefs about LLM's play when playing against\nthem, and suggest important implications for mechanism design in mixed\nhuman-LLM systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11011.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6475c2794766357252e69e9f",
      "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
      "fullname": "i",
      "name": "iliashum",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  }
]