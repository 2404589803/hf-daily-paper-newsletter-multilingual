[
  {
    "paper": {
      "id": "2507.01853",
      "authors": [
        {
          "_id": "686b4e69213f123a1f88bd76",
          "name": "Samridhi Raj Sinha",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd77",
          "name": "Rajvee Sheth",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd78",
          "name": "Abhishek Upperwal",
          "hidden": false
        },
        {
          "_id": "686b4e69213f123a1f88bd79",
          "name": "Mayank Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T16:07:54.000Z",
      "submittedOnDailyAt": "2025-07-07T03:06:37.666Z",
      "title": "エカ-エバル : インド語の大規模言語モデルの詳細な評価フレームワーク",
      "submittedOnDailyBy": {
        "_id": "66e1425c919f283fbd7dfb5e",
        "avatarUrl": "/avatars/b43ee9b9042486b5faf91472b4c49c5d.svg",
        "isPro": false,
        "fullname": "Rajvee Sheth",
        "user": "RajveeSheth",
        "type": "user"
      },
      "summary": "LLMの急速な進歩は、英語中心的ベンチマークを超えた評価フレームワークの必要性を強め、ディバリジョニスト語言多様な地域への要求を満たすことに必要とします。私たちは、EKA-EVALという統一したプロダクション準備された評価フレームワークを紹介します。これは35より多くのベンチマークを統合し、10より多くのIndic特有データセットを含み、理由、数学、ツール使用、長コンテキスト理解、読解評価などのカテゴリを収めています。現在のインド語評価ツールと比較して、EKA-EVALはディストリビューテッド推論、ポタシュアル化、多GPU使用の機能を機能し、ベンチマークカバーを広めます。我々のシステム的な比較は、EKA-EVALが全球的およびIndic LLM向けの最初の端末からの拡張可能な評価システムとして位置づけます。多言語ベンチマークの壁を大幅に下げます。フレームワークはオープンソースで、https://github.com/lingo-iitgn/eka-evalで公開されています。EKAイニシアティブの一部であり、https://eka.soket.aiによって進行中で、100以上のベンチマークを拡張し、強固な多言語評価エコシステムを構築することを目的としています。",
      "upvotes": 2,
      "discussionId": "686b4e69213f123a1f88bd7a",
      "ai_summary": "EKA-EVAL is a comprehensive multilingual evaluation framework for large language models, supporting diverse benchmarks and features for efficient distributed inference and GPU usage.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "unified evaluation framework",
        "production-ready",
        "Indic-specific datasets",
        "reasoning",
        "mathematics",
        "tool use",
        "long-context understanding",
        "reading comprehension",
        "distributed inference",
        "quantization",
        "multi-GPU usage",
        "end-to-end",
        "extensible evaluation suite",
        "multilingual benchmarking",
        "open-source"
      ]
    },
    "publishedAt": "2025-07-02T12:07:54.000Z",
    "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages",
    "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e1425c919f283fbd7dfb5e",
      "avatarUrl": "/avatars/b43ee9b9042486b5faf91472b4c49c5d.svg",
      "fullname": "Rajvee Sheth",
      "name": "RajveeSheth",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01955",
      "authors": [
        {
          "_id": "686b8347213f123a1f88bdc8",
          "name": "Rahul Ramachandran",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdc9",
          "name": "Ali Garjani",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdca",
          "name": "Roman Bachmann",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcb",
          "name": "Andrei Atanov",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcc",
          "name": "Oğuzhan Fatih Kar",
          "hidden": false
        },
        {
          "_id": "686b8347213f123a1f88bdcd",
          "name": "Amir Zamir",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T17:59:07.000Z",
      "submittedOnDailyAt": "2025-07-07T06:51:04.452Z",
      "title": "GPT-4oはどの程度視覚を理解しますか？標準的なコンピュータビジョンタスクにおける多モデル基盤モデルの評価",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": true,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "多モデル基盤モデル（例：GPT-4o）は最近驚異的な進歩を遂げていますが、これらのモデルが視覚を理解するどの程度に立ちはじめてはわかりません。本論文では、標準的な計算機視覚タスク（セマンティックセグメンテーション、物体検出、画像分類、深さと表面正規線予測）において、人気の多モデル基盤モデル（GPT-4o、o4-mini、Gemini 1.5 Pro、Gemini 2.0 Flash、Claude 3.5 Sonnet、Qwen2-VL、Llama 3.2）の性能を評価します。使用される既定のデータセット（例：COCO、ImageNetおよびその変体など）。\n\nこの評価を行うための主な課題は、1）多くのモデルはテキストを出力することを学習しているため、段落や3Dジェネリックなドメインを自然に表現できないということです。2）多くの先進モデルは専有軟体であり、APIレベルでのアクセスしか可能で、重みアクセスでの適応はできません。これらの課題を解決するために、標準的な視覚タスクを等価なテキストプロンプトバイアンスとAPI兼容なタスクに翻訳し、プロンプトチェーニングを用いて標準化された評価フレームワークを作成します。\n\nこれらのモデルの性能について、以下の点が見られます。1）どのタスクでも最先端の専門モデルに近い性能を示していませんが、2）それらは認められる一般的なモデルであり、これは主に画像テキストベースのタスクで学習されていることから見えるようです。3）セマンティックタスクの性能は幾つかのジェネリックタスクよりも特に良く、4）プロンプトチェーニングの技術は性能に影響を与えますが、ベターのモデルはプロンプトの変化に対して敏感性が低いと見られます。5）GPT-4oは推理モデルではないモデルの中で最も優れています、6つのタスクのうち4つのタスクで最も高い位に立ちました。6）推理モデル（例：o3）はジェネリックタスクでの改善を示し、7）最新のGPT-4oのような原生の画像生成機能を持つモデルの初步的な分析は、幻想や空間的な調整誤差などの特徴を示しているということが分かります。",
      "upvotes": 0,
      "discussionId": "686b8348213f123a1f88bdce",
      "ai_summary": "Multimodal foundation models, despite being primarily trained on image-text tasks, demonstrate respectable performance across various vision tasks when adapted through prompt chaining, though they fall short compared to specialized models.",
      "ai_keywords": [
        "GPT-4o",
        "o4-mini",
        "Gemini 1.5 Pro",
        "Gemini 2.0 Flash",
        "Claude 3.5 Sonnet",
        "Qwen2-VL",
        "Llama 3.2",
        "semantic segmentation",
        "object detection",
        "image classification",
        "depth prediction",
        "surface normal prediction",
        "COCO",
        "ImageNet",
        "prompt chaining",
        "reasoning models",
        "hallucinations",
        "spatial misalignments"
      ]
    },
    "publishedAt": "2025-07-02T13:59:07.000Z",
    "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation\n  Models on Standard Computer Vision Tasks",
    "summary": "Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01955.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 908
    },
    "isAuthorParticipating": false
  }
]