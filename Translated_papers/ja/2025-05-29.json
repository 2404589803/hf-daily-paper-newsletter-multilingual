[
  {
    "paper": {
      "id": "2505.22617",
      "authors": [
        {
          "_id": "6837cd8fc537d91527323667",
          "user": {
            "_id": "650eba9555dc1e841746f132",
            "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
            "isPro": false,
            "fullname": "Ganqu Cui",
            "user": "ganqu",
            "type": "user"
          },
          "name": "Ganqu Cui",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:09.467Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323668",
          "user": {
            "_id": "66e3f8fb5d97b5bb46923444",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DW806I00-00oQAYvD4ocQ.png",
            "isPro": false,
            "fullname": "Yuchen Zhang",
            "user": "YucZhang2003",
            "type": "user"
          },
          "name": "Yuchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:14.207Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323669",
          "user": {
            "_id": "65352acb7139c5dd8d9a8590",
            "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
            "isPro": false,
            "fullname": "JiachengChen",
            "user": "JC-Chen",
            "type": "user"
          },
          "name": "Jiacheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:20.736Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366a",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366b",
          "name": "Zhi Wang",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366c",
          "user": {
            "_id": "622474f38dc6b0b64f5e903d",
            "avatarUrl": "/avatars/d6b60a014277a8ec7d564163c5f644aa.svg",
            "isPro": false,
            "fullname": "Yuxin Zuo",
            "user": "yuxinzuo",
            "type": "user"
          },
          "name": "Yuxin Zuo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:12.022Z",
          "hidden": true
        },
        {
          "_id": "6837cd8fc537d9152732366d",
          "user": {
            "_id": "662f638ba9891e43cc4c5125",
            "avatarUrl": "/avatars/77c22de5511f9b85d98ec75fb0b5e9be.svg",
            "isPro": false,
            "fullname": "Li Haozhan",
            "user": "Haozhan72",
            "type": "user"
          },
          "name": "Haozhan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:22.720Z",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366e",
          "name": "Yuchen Fan",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d9152732366f",
          "name": "Huayu Chen",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323670",
          "name": "Weize Chen",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323671",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323672",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323673",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323674",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323675",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323676",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "6837cd8fc537d91527323677",
          "user": {
            "_id": "60cf4bcb1ce3775ebb86e5d5",
            "avatarUrl": "/avatars/12bcd18d215abf91f297f93007733148.svg",
            "isPro": false,
            "fullname": "Ning Ding",
            "user": "stingning",
            "type": "user"
          },
          "name": "Ning Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:16.809Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:38:45.000Z",
      "submittedOnDailyAt": "2025-05-29T01:38:57.501Z",
      "title": "強化学習のエントロピー機構による語言モデルの推論",
      "submittedOnDailyBy": {
        "_id": "650eba9555dc1e841746f132",
        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
        "isPro": false,
        "fullname": "Ganqu Cui",
        "user": "ganqu",
        "type": "user"
      },
      "summary": "この論文は、LLMsとの理由論を行うためのRLのスケーリングの大きな障害としての政策エントロピーの崩壊を克服することを目的としています。この現象は、大規模なRL実験でエントロピー対策がない場合に一貫して見られ、政策エントロピーは早期の訓練ステージで急激に低下し、減少した探索能力は政策性能の飽和と共に見られます。実際には、エントロピーHと下流性能Rの間に変換方程式R = -a * e^H + bを構築しました。この実験的法則は強く示唆しています、政策性能は政策エントロピーから買い替えされ、その枯渇によってバックレッドされ、上限は完全に予測可能です。H=0、R=-a+b。我々の発見は、エントロピー管理を実現するために、計算量のスケーリングに向けて継続的な探索を行う必要があることを示しています。これによって、理論的にも実験的にエントロピーの動態を調査しました。我々の計算は、行動確率とロジットの変化の共分散が、Policy Gradientようなアルゴリズムを使用する場合にその優位に比例して政策エントロピーの変化を駆動していることを主張しています。実験的な研究は、共分散項とエントロピーの差が完全に一致し、理論的な結論を支えています。また、共分散項は学習の間にほとんど全て正として保持され、これが政策エントロピーが単調に減少する理由をより深く解釈します。エントロピーの動態の機構を理解することによって、高共分散トークンの更新を制限することでエントロピーを制御することを考えました。特に、Clip-CovとKL-Covの2つの簡単で効果的な技術を提案しました。Clip-Covは高共分散のトークンにコピーを適用し、KLペナルティを適用します。実験は、これらの方法が探索を促進し、政策がエントロピー崩壊から逃げ、より良い下流性能を達成することを示しました。",
      "upvotes": 64,
      "discussionId": "6837cd90c537d9152732369d",
      "githubRepo": "https://github.com/PRIME-RL/Entropy-Mechanism-of-RL",
      "ai_summary": "Entropy dynamics in reinforcement learning with large language models are investigated to prevent policy entropy collapse and improve exploration.",
      "ai_keywords": [
        "policy entropy",
        "reinforcement learning",
        "LLMs",
        "entropy intervention",
        "transformation equation",
        "policy performance",
        "entropy dynamics",
        "covariance",
        "action probability",
        "logits",
        "advantage",
        "Policy Gradient",
        "Clip-Cov",
        "KL-Cov"
      ]
    },
    "publishedAt": "2025-05-28T13:38:45.000Z",
    "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models",
    "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22617.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650eba9555dc1e841746f132",
      "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
      "fullname": "Ganqu Cui",
      "name": "ganqu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21600",
      "authors": [
        {
          "_id": "6837bc9c9937bcb69885799c",
          "user": {
            "_id": "6445fd9ba56444c355dcbcba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
            "isPro": false,
            "fullname": "Tianyu Fu",
            "user": "fuvty",
            "type": "user"
          },
          "name": "Tianyu Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:43.123Z",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799d",
          "name": "Yi Ge",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799e",
          "user": {
            "_id": "66954ebfbcd81f395e9dca37",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66954ebfbcd81f395e9dca37/0C3m5YdxyXuK7dJBu4AdL.png",
            "isPro": false,
            "fullname": "Yichen You",
            "user": "youyc22",
            "type": "user"
          },
          "name": "Yichen You",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:40.790Z",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb69885799f",
          "name": "Enshu Liu",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a0",
          "name": "Zhihang Yuan",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a1",
          "name": "Guohao Dai",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a2",
          "name": "Shengen Yan",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a3",
          "name": "Huazhong Yang",
          "hidden": false
        },
        {
          "_id": "6837bc9c9937bcb6988579a4",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6445fd9ba56444c355dcbcba/edz42EmKaJTdax2mDUmhh.mp4"
      ],
      "publishedAt": "2025-05-27T16:57:20.000Z",
      "submittedOnDailyAt": "2025-05-29T00:18:54.118Z",
      "title": "R2R: 小大モデルトークンルーティングでの効率的な異なる理由のパスの検索",
      "submittedOnDailyBy": {
        "_id": "6445fd9ba56444c355dcbcba",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
        "isPro": false,
        "fullname": "Tianyu Fu",
        "user": "fuvty",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、推論オーバーヘッドの大幅なコストを負担しながら、驚異的な理由能力を達成し、採用の大きな課題を抱えています。しかし、結晶化された小語言モデル（SLMs）は、LLMsの理由パスを追うことができないため、性能が低下します。幸い、LLMsとSLMsの理由パスの違いが生じるのは、その中の少ない部分のトークンだけです。多数の生成されたトークンは、同じであるか、縮略語や表現の微妙な違いなどの中性な違いを示すことが多いです。この見解を活用し、**Roads to Rome (R2R)** というニューラルトークルーティングメソッドを導入しました。これは、重要な、パスの違いを示すトークンのみにLLMsを選択的に利用し、トークン生成の多くをSLMに委ねるようにします。また、自動データ生成プイルプリンは、違いのトークンを識別し、トークンレベルのルーティングラベルを生成し、軽量ルーターを訓練するためのデータを生成します。R2Rは、DeepSeekフamilyのR1-1.5BとR1-32Bモデルを組み合わせ、難しい数学、コーディング、QAベンチマークで評価しました。平均活性化パラメータサイズが5.6Bで、R2RはR1-7Bの平均精度を1.6倍に達し、R1-14Bモデルを上回りました。R1-32Bと比較しては、性能相当の中で2.8倍のウォールクロックスピードアップを提供し、テストタイムスケーリングエフィシェンスのパロトフォームを進めました。コードは、https://github.com/thu-nics/R2R から利用できます。",
      "upvotes": 47,
      "discussionId": "6837bc9d9937bcb6988579d1",
      "projectPage": "https://fuvty.github.io/R2R_Project_Page/",
      "githubRepo": "https://github.com/thu-nics/R2R",
      "ai_summary": "Roads to Rome (R2R) selectively utilizes large language models for critical reasoning tasks to enhance efficiency and performance in lightweight models.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Small Language Models (SLMs)",
        "token routing",
        "neural token routing",
        "token divergence",
        "token generation",
        "automatic data generation pipeline",
        "token-level routing labels",
        "R1-1.5B",
        "R1-32B",
        "math benchmarks",
        "coding benchmarks",
        "QA benchmarks",
        "parameter size",
        "activated parameters",
        "test-time scaling efficiency",
        "pareto frontier"
      ]
    },
    "publishedAt": "2025-05-27T12:57:20.000Z",
    "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing",
    "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6445fd9ba56444c355dcbcba/edz42EmKaJTdax2mDUmhh.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6445fd9ba56444c355dcbcba",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6445fd9ba56444c355dcbcba/NCyRCD-MK-yA0_qY6I2y0.png",
      "fullname": "Tianyu Fu",
      "name": "fuvty",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22312",
      "authors": [
        {
          "_id": "6837c342cd1601f5bd670255",
          "name": "Jujie He",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670256",
          "user": {
            "_id": "65643645b09c0b9ece1b8f0e",
            "avatarUrl": "/avatars/d5197103b6e92f765bfda7ed2cc8d53e.svg",
            "isPro": false,
            "fullname": "Jiacai Liu",
            "user": "skydownacai",
            "type": "user"
          },
          "name": "Jiacai Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:28.696Z",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670257",
          "user": {
            "_id": "658229ef5f6d83438257fce5",
            "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
            "isPro": false,
            "fullname": "Chris (Yuhao) Liu",
            "user": "chrisliu298",
            "type": "user"
          },
          "name": "Chris Yuhao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:32.117Z",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670258",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670259",
          "name": "Chaojie Wang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025a",
          "name": "Peng Cheng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025b",
          "name": "Xiaoyu Zhang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025c",
          "name": "Fuxiang Zhang",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025d",
          "name": "Jiacheng Xu",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025e",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd67025f",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670260",
          "name": "Liang Zeng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670261",
          "name": "Tianwen Wei",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670262",
          "name": "Cheng Cheng",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670263",
          "name": "Bo An",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670264",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6837c342cd1601f5bd670265",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T12:56:04.000Z",
      "submittedOnDailyAt": "2025-05-29T00:48:05.741Z",
      "title": "Skywork Open Reasoner 1 テクニカルレポート",
      "submittedOnDailyBy": {
        "_id": "658229ef5f6d83438257fce5",
        "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
        "isPro": false,
        "fullname": "Chris (Yuhao) Liu",
        "user": "chrisliu298",
        "type": "user"
      },
      "summary": "DeepSeek-R1の成功は、強化学習（RL）が大規模言語モデル（LLMs）の理由能力を向上させるために重要な役割を果たしていることを強調します。本論文では、長期Chain-of-Thought（CoT）モデルに対する効果的かつスケーラブルなRL実装を提案します。DeepSeek-R1-Distillモデルシリーズに基づいて、我々のRLアプローチは顕著な性能向上を実現し、AIME24、AIME25、LiveCodeBenchの平均正確率を32Bモデルでは57.8%から72.8%（+15.0%）、7Bモデルでは43.6%から57.5%（+13.9%）に上げました。Skywork-OR1-32BモデルはAIME24とAIME25ベンチマークでDeepSeek-R1とQwen3-32Bを超え、LiveCodeBenchで比較的結果を収めました。Skywork-OR1-7BとSkywork-OR1-Math-7Bモデルは類似サイズのモデルの中で競争的な理由能力を示しました。我々は訓練パイプラインの核心成分について詳細な消去試験を行い、その効果性を証明しました。また、熵崩壊現象を詳細に調査し、熵の動態に影響する要因を特定し、過早な熵崩壊を抑制することが検証性能向上に重要であることを示しました。コミュニティ研究のサポートを提供するため、我々はモデル重み、訓練コード、訓練データセットを完全にオープンソース化します。",
      "upvotes": 39,
      "discussionId": "6837c344cd1601f5bd6702dd",
      "githubRepo": "https://github.com/SkyworkAI/Skywork-OR1",
      "ai_summary": "Skywork-OR1 is a reinforcement learning approach for long Chain-of-Thought models that improves accuracy over DeepSeek-R1 across various benchmarks by addressing entropy collapse.",
      "ai_keywords": [
        "reinforcement learning",
        "LLMs",
        "Chain-of-Thought",
        "Skywork-OR1",
        "DeepSeek-R1-Distill",
        "AIME24",
        "AIME25",
        "LiveCodeBench",
        "entropy collapse",
        "ablation studies"
      ]
    },
    "publishedAt": "2025-05-28T08:56:04.000Z",
    "title": "Skywork Open Reasoner 1 Technical Report",
    "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22312.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "658229ef5f6d83438257fce5",
      "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
      "fullname": "Chris (Yuhao) Liu",
      "name": "chrisliu298",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22651",
      "authors": [
        {
          "_id": "6837ffdd1bfb4a669ad6de09",
          "user": {
            "_id": "662678dfdd43e904ef1dcd03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
            "isPro": false,
            "fullname": "Yi Ding",
            "user": "Tuwhy",
            "type": "user"
          },
          "name": "Yi Ding",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:19.811Z",
          "hidden": false
        },
        {
          "_id": "6837ffdd1bfb4a669ad6de0a",
          "name": "Ruqi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:58:03.000Z",
      "submittedOnDailyAt": "2025-05-29T06:18:44.515Z",
      "title": "シェルロック: 視覚言語モデルの自己補正論理",
      "submittedOnDailyBy": {
        "_id": "662678dfdd43e904ef1dcd03",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
        "isPro": false,
        "fullname": "Yi Ding",
        "user": "Tuwhy",
        "type": "user"
      },
      "summary": "推理ビジョン・ラングワードモデル（VLMs）は複雑なマルチモーダルタスクにおいて有望な性能を示しています。しかし、それらは以下の重要な課題に直面しています：理由論誤りに高度に敏感であり、大量の標準化データまたは正確なバリデータを必要とし、特定の領域を超えた一般化が難しい。これらの制限を解決するために、私たちは理由論VLMsの自動調整を戦略として採用し、その性能を向上させることを試みました。まず、理由論VLMsの自動調整能力についての詳細な分析を行い、キー的な欠陥を特定しました。この分析の基に、私たちはSherlockという自動調整と自動改善のトレーニングフレームワークを導入しました。Sherlockは、軌道レベルの自動調整オブジェクティブ、可視的なパーバラントに基づくデータ構築方法、および好み調整の動的なbetaを導入しました。モデルは、20kのランダムにサンプリングされた標準化データをだけ用いて自動調整能力を獲得した後、外部のスーパーバイオンを除き自動改善を続けます。Llama3.2-Vision-11BモデルにビルドされたSherlockは、8バンクマーク標準で驚異的な結果を収め、直接生成での平均正確率は64.1で、自動調整後は65.4となります。LLaVA-CoT（63.2）、Mulberry（63.9）、LlamaV-o1（63.4）を上回り、それらを使用したデータの20%を下回ります。",
      "upvotes": 38,
      "discussionId": "6837ffdf1bfb4a669ad6de71",
      "projectPage": "https://dripnowhy.github.io/Sherlock/",
      "githubRepo": "https://github.com/DripNowhy/Sherlock",
      "ai_summary": "Sherlock, a self-correction and self-improvement framework for reasoning vision-language models, enhances accuracy across benchmarks using limited annotated data.",
      "ai_keywords": [
        "vision-language models",
        "self-correction",
        "trajectory-level self-correction",
        "preference data",
        "visual perturbation",
        "dynamic beta",
        "Llama3.2-Vision-11B",
        "LLaVA-CoT",
        "Mulberry",
        "LlamaV-o1"
      ]
    },
    "publishedAt": "2025-05-28T13:58:03.000Z",
    "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models",
    "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\nbeta for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22651.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662678dfdd43e904ef1dcd03",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
      "fullname": "Yi Ding",
      "name": "Tuwhy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22453",
      "authors": [
        {
          "_id": "6837c318a4e378954486e45d",
          "user": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "WaltonFuture",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:35.145Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e45e",
          "name": "Yuting Li",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e45f",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e460",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Yue Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:14:48.838Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e461",
          "name": "Linghe Kong",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e462",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Weiran Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:14:48.838Z",
          "hidden": false
        },
        {
          "_id": "6837c318a4e378954486e463",
          "name": "Lichao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T15:11:16.000Z",
      "submittedOnDailyAt": "2025-05-29T00:45:28.570Z",
      "title": "GRPOによる無チェックポイント学習を用いた多モーダルLLMの推論の無チェックポイント学習",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "後ディレインステージでの多モーダル大語言モデル（MLLMs）の向上は、通常、規範的微調（SFT）または強化学習（RL）によるものである。しかし、これらの規範的な方法は、高額で手動でアノテーションされた多モーダルデータを必要とするため、最終的には持続可能でないリソースである。一方、最近の努力は無視規範的な後ディレインを検討しているが、その方法は複雑でイテレーションが難しい。本稿では、GRPO（穩定してスケーラブルなオンラインRLアルゴリズム）を用いて、外部の規範を不要にした継続的な自己向上を可能にすることを最初に調査している。MM-UPT（簡単で効果的な無視規範的な後ディレインのフレームワーク）を提案し、GRPOを基に、従来の報酬信号を多数決に基づく自己報酬機構に置き換えている。実験結果によると、MM-UPTはQwen2.5-VL-7Bの理由能力を大幅に向上させることができる（例えば、MathVistaで66.3%→72.9%、We-Mathで62.9%→68.7%）、標準データセットを使用しても真のラベルがない状況でも。MM-UPTは先週の無視規範的なベースラインを超え、規範的なGRPOの結果に近づくことも示している。また、MLLM自身で生成された合成クエスチョンを採用することで性能を向上させることができることも示し、スケーラブルな自己向上の有望なアプローチを示している。全体として、MM-UPTは外部の規範を不要にしたMLLMの継続的な、自律的な向上の新しいパラダイムを提供している。コードはhttps://github.com/waltonfuture/MM-UPTにアクセスできる。",
      "upvotes": 29,
      "discussionId": "6837c318a4e378954486e48a",
      "projectPage": "https://github.com/waltonfuture/MM-UPT",
      "githubRepo": "https://github.com/waltonfuture/MM-UPT",
      "ai_summary": "MM-UPT, a framework employing GRPO and self-rewarding, enhances multi-modal LLMs through unsupervised continual learning, showing performance improvements without manual annotations.",
      "ai_keywords": [
        "GRPO",
        "MM-UPT",
        "reinforcement learning",
        "unsupervised post-training",
        "multi-modal large language models",
        "self-rewarding mechanism",
        "majority voting",
        "synthetic questions",
        "MathVista",
        "We-Math"
      ]
    },
    "publishedAt": "2025-05-28T11:11:16.000Z",
    "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO",
    "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %rightarrow72.9 % on MathVista, 62.9\n%rightarrow68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22453.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21136",
      "authors": [
        {
          "_id": "6837c91ec790885f338b8f27",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:22.525Z",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f28",
          "name": "Xiaoming Xu",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f29",
          "name": "Jia Wei",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2a",
          "name": "Haofeng Huang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2b",
          "name": "Pengle Zhang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2c",
          "name": "Chendong Xiang",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2d",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "6837c91ec790885f338b8f2e",
          "name": "Jianfei Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/e7t1XOditivMjvbyyouoc.png"
      ],
      "publishedAt": "2025-05-27T12:50:36.000Z",
      "submittedOnDailyAt": "2025-05-29T01:12:49.349Z",
      "title": "SageAttention2++: 更エフケイシブルな実装のSageAttention2",
      "submittedOnDailyBy": {
        "_id": "66c0a08bac74db25de8427ec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
        "isPro": false,
        "fullname": "Jintao Zhang",
        "user": "jt-zhang",
        "type": "user"
      },
      "summary": "アツションの効率性は重要であり、それは順序長さに二次的に時間計算量が増加するからです。SageAttention2はこれを解決するために、マトミル（Matmul）の加速を目的としてカウンティングを利用しています。また、さらにSageAttention2を高速化するために、FP16で累積されたFP8 Matmulの速いインストラクションを利用することを提案しています。このインストラクションはSageAttention2で使用されているFP8 Matmulより2倍速く、実験結果によると、SageAttention2++はFlashAttentionより3.9倍速くなり、同様のアツション精度を維持します。これは、言語、画像、映像生成モデルなどの多様なモデルを効果的に高速化することを意味し、終端からのメトリック損失は可視ではないことを示しています。コードは、https://github.com/thu-ml/SageAttentionに公開されます。",
      "upvotes": 28,
      "discussionId": "6837c923c790885f338b90e5",
      "projectPage": "https://github.com/thu-ml/SageAttention",
      "githubRepo": "https://github.com/thu-ml/SageAttention",
      "ai_summary": "SageAttention2++ improves attention efficiency by using FP8 Matmul in FP16, achieving a 3.9x speedup over FlashAttention without losing accuracy.",
      "ai_keywords": [
        "attention",
        "time complexity",
        "sequence length",
        "quantization",
        "matrix multiplications",
        "Matmul",
        "FP8",
        "FP16",
        "SageAttention2",
        "SageAttention2++",
        "FlashAttention",
        "image generation",
        "video generation"
      ]
    },
    "publishedAt": "2025-05-27T08:50:36.000Z",
    "title": "SageAttention2++: A More Efficient Implementation of SageAttention2",
    "summary": "The efficiency of attention is critical because its time complexity grows\nquadratically with sequence length. SageAttention2 addresses this by utilizing\nquantization to accelerate matrix multiplications (Matmul) in attention. To\nfurther accelerate SageAttention2, we propose to utilize the faster instruction\nof FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8\nMatmul used in SageAttention2. Our experiments show that SageAttention2++\nachieves a 3.9x speedup over FlashAttention while maintaining the same\nattention accuracy as SageAttention2. This means SageAttention2++ effectively\naccelerates various models, including those for language, image, and video\ngeneration, with negligible end-to-end metrics loss. The code will be available\nat https://github.com/thu-ml/SageAttention.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/e7t1XOditivMjvbyyouoc.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21136.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22334",
      "authors": [
        {
          "_id": "6837c360b127cae8a0b36e85",
          "user": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "isPro": false,
            "fullname": "Lai Wei",
            "user": "WaltonFuture",
            "type": "user"
          },
          "name": "Lai Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:26.705Z",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e86",
          "name": "Yuting Li",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e87",
          "name": "Kaipeng Zheng",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e88",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e89",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Yue Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:16:03.908Z",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8a",
          "name": "Linghe Kong",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8b",
          "name": "Lichao Sun",
          "hidden": false
        },
        {
          "_id": "6837c360b127cae8a0b36e8c",
          "user": {
            "_id": "65e095da35ad8b2fe8c80e71",
            "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
            "isPro": false,
            "fullname": "Weiran Huang",
            "user": "weiranhuang",
            "type": "user"
          },
          "name": "Weiran Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:16:03.908Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T13:21:38.000Z",
      "submittedOnDailyAt": "2025-05-29T00:46:20.111Z",
      "title": "冷スタートでの強化学習を用いた多モデル論理の進展",
      "submittedOnDailyBy": {
        "_id": "64a16b1aeacb4b50ba1c889d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
        "isPro": false,
        "fullname": "Lai Wei",
        "user": "WaltonFuture",
        "type": "user"
      },
      "summary": "最近の大規模言語モデル（LLMs）の進展は、印象的な連鎖的思考能力を示し、強化学習（RL）がこの進歩に重要な役割を果たしていることが明らかになっています。「気がついた瞬間」パターンであるモデルが自ら調整を行うことを示すものは、通常RLからのエピーフォリック性質によって説明されますが、これらのパターンは、RL訓練前に多めモデル（MLLMs）でも存在し、理由論の改善とは必ずしも関連しないことを最初に示します。これらの洞察を基に、多めモデルの理由論を向上させるための一連の詳細な研究を提案します：1. 監督的調整（SFT）を冷やかなスタートとして構造化された連鎖的思考パターンを、2. GRPOを用いた強化学習でさらに能力を精進する。様々な難しい多めモデルの理由論ベンチマークでの詳細な実験は、SFTだけやRLだけの方法を超えることを確認しました。結果としてのモデルは、開放ソースのMLLMsで最先端の性能を達成し、3Bと7Bスケールでそれぞれ最先端となっています。特に、7Bモデルは基礎モデルより大幅に向上しています（例えば、MathVistaでは66.3%→73.4%、We-Mathでは62.9%→70.4%）、3Bモデルは数々の7Bモデルとの性能と比べて競争的です。この研究は、先進的な多めモデルの構築に実用的なガイドラインを提供します。コードは、https://github.com/waltonfuture/RL-with-Cold-Start に公開されています。",
      "upvotes": 25,
      "discussionId": "6837c363b127cae8a0b36f6f",
      "projectPage": "https://github.com/waltonfuture/RL-with-Cold-Start",
      "githubRepo": "https://github.com/waltonfuture/RL-with-Cold-Start",
      "ai_summary": "A two-stage approach combining supervised fine-tuning and reinforcement learning enhances multimodal reasoning in large language models, achieving state-of-the-art performance on benchmarks.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "multimodal LLMs",
        "MLLMs",
        "reinforcement learning",
        "RL",
        "chain-of-thought reasoning",
        "GRPO",
        "supervised fine-tuning",
        "SFT",
        "multimodal reasoning",
        "MathVista",
        "We-Math"
      ]
    },
    "publishedAt": "2025-05-28T09:21:38.000Z",
    "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start",
    "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %rightarrow73.4 % on\nMathVista, 62.9 %rightarrow70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22334.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a16b1aeacb4b50ba1c889d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
      "fullname": "Lai Wei",
      "name": "WaltonFuture",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22457",
      "authors": [
        {
          "_id": "68380912e9c1608de91e23f3",
          "name": "Haonan Wang",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f4",
          "name": "Hongfu Liu",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f5",
          "name": "Xiangyan Liu",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f6",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f7",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f8",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "68380912e9c1608de91e23f9",
          "name": "Tianyu Pang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T15:13:34.000Z",
      "submittedOnDailyAt": "2025-05-29T05:44:17.072Z",
      "title": "ビデオ理由論を育成するための次のイベント予測",
      "submittedOnDailyBy": {
        "_id": "63d91b6d255ef6add20e1b38",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
        "isPro": false,
        "fullname": "Tianyu Pang",
        "user": "P2333",
        "type": "user"
      },
      "summary": "次のトーク予測は、LLMでの推理を可能にする基盤的な学習任務です。しかし、テンポラル推理能力をビデオ入力に対して持つMLLMに向けた学習任務は何にしようか？現在のタスクでは、ビデオクライアント回答などは、人間からの注釈または強力なMLLMからの注釈を依存していることが多いのに対し、ビデオキャプチャは時間的推理と空間的情報を組み合わせていることが多い。この空間を解決するために、次のイベント予測（NEP）を提案します。NEPは、未来のビデオセグメントを豊富な、自動観測された信号として利用し、時間的推理を促成する学習任務です。ビデオを過去と未来のフレームに分け、MLLMは過去のフレームを入力として、未来のフレームから得られるイベントの要約を予測することで、モデルが時間的に推理してタスクを完了させるように促します。このタスクを支援するために、V1-33Kデータセットを作成します。このデータセットは、33,000個の自動的に抜粋されたビデオセグメントからなり、多様なリアルウェアスキーチャーを収録しています。また、時間的推理の効果について調査するために、ビデオインストラクションチューニング戦略の範囲を拡張します。進捗を評価するために、FutureBenchを導入し、見たことのない未来のイベントの予測の一致性を評価します。実験は、NEPがMLLMの時間的推理を促成するための可換性と効果的な学習パラダイムを提供していることを証明します。",
      "upvotes": 23,
      "discussionId": "68380913e9c1608de91e2430",
      "githubRepo": "https://github.com/sail-sg/Video-Next-Event-Prediction",
      "ai_summary": "Next-event prediction (NEP) is proposed as a learning task to enable MLLMs to reason temporally over video inputs, using future video segments as a self-supervised signal.",
      "ai_keywords": [
        "next-token prediction",
        "next-event prediction (NEP)",
        "LLMs",
        "MLLMs",
        "video question answering",
        "video captioning",
        "temporal reasoning",
        "future video segments",
        "past frames",
        "video segments",
        "V1-33K",
        "video instruction-tuning strategies",
        "FutureBench"
      ]
    },
    "publishedAt": "2025-05-28T11:13:34.000Z",
    "title": "Fostering Video Reasoning via Next-Event Prediction",
    "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22457.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d91b6d255ef6add20e1b38",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
      "fullname": "Tianyu Pang",
      "name": "P2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19253",
      "authors": [
        {
          "_id": "6837bc8d0b39c9653de4d06f",
          "user": {
            "_id": "6244451c9fdefb55a0b900cc",
            "avatarUrl": "/avatars/ca2b46ddb5d905501d827920582b5438.svg",
            "isPro": false,
            "fullname": "Joao Coelho",
            "user": "jmvcoelho",
            "type": "user"
          },
          "name": "João Coelho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:45.049Z",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d070",
          "name": "Jingjie Ning",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d071",
          "name": "Jingyuan He",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d072",
          "name": "Kangrui Mao",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d073",
          "name": "Abhijay Paladugu",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d074",
          "name": "Pranav Setlur",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d075",
          "name": "Jiahe Jin",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d076",
          "name": "Jamie Callan",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d077",
          "name": "João Magalhães",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d078",
          "name": "Bruno Martins",
          "hidden": false
        },
        {
          "_id": "6837bc8d0b39c9653de4d079",
          "name": "Chenyan Xiong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T18:16:13.000Z",
      "submittedOnDailyAt": "2025-05-29T00:28:31.472Z",
      "title": "DeepResearchGym: 無料で、透明で、再現性のある評価サンドボックスでの深層研究",
      "submittedOnDailyBy": {
        "_id": "6135eeeb5bc6ecdf86b60f0d",
        "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
        "isPro": false,
        "fullname": "Shi Yu",
        "user": "yushi",
        "type": "user"
      },
      "summary": "Deep research systemsは、複雑なクエリに対して詳細でより証拠があるレポートを生成する新興の情報検索方法のクラスである。しかし、現在の多数のフレームワークは動的なコマーシャルサーチAPIを依存し、再現性と透明性の課題に加えてコストが高い問題を抱えている。これらの制限を解決するために、DeepResearchGymというオープンソースのサンドボックスを紹介し、再現性のあるサーチAPIと厳密な評価プロトコルを組み合わせて深層研究システムのベンチマークを行う。APIは、最新の密集検索器とディスクANNを用いて、ClueWeb22とFineWebという大規模な公開ウェブコRPORAをインデックス化し、人気のコマーシャルAPIよりも低カットレンタルを実現し、運行間で穩定なドキュメントのランキングを保証し、研究用に無料で利用可能である。深層研究システムの出力を評価するために、Researchy QuestionsベンチマークをLLM-as-a-judge評価に通じて自動評価メトリックを拡張し、ユーザーの情報需要に合わせた対応性、検索の忠実性、レポートの質を評価する。実験結果によると、DeepResearchGymを組み合わせたシステムは、コマーシャルAPIを使用したものと同等の性能を達成し、評価メトリックによる性能ランキングは一致している。また、人間評価研究は、自動評価プロトコルが人間の好みに合わせていることを確認し、フレームワークが深層研究システムの制御された評価に役立つことを証明した。コードとAPI文書は、https://www.deepresearchgym.ai から利用可能である。",
      "upvotes": 17,
      "discussionId": "6837bc8d0b39c9653de4d0a6",
      "projectPage": "https://www.deepresearchgym.ai",
      "ai_summary": "DeepResearchGym provides an open-source evaluation framework for deep research systems using a reproducible search API and LLM-as-a-judge assessments.",
      "ai_keywords": [
        "agentic information retrieval",
        "deep research systems",
        "search API",
        "reproducibility",
        "transparency",
        "open-source sandbox",
        "ClueWeb22",
        "FineWeb",
        "dense retriever",
        "approximate nearest neighbor search",
        "DiskANN",
        "Researchy Questions benchmark",
        "LLM-as-a-judge",
        "retrieval faithfulness",
        "report quality",
        "human evaluation"
      ]
    },
    "publishedAt": "2025-05-25T14:16:13.000Z",
    "title": "DeepResearchGym: A Free, Transparent, and Reproducible Evaluation\n  Sandbox for Deep Research",
    "summary": "Deep research systems represent an emerging class of agentic information\nretrieval methods that generate comprehensive and well-supported reports to\ncomplex queries. However, most existing frameworks rely on dynamic commercial\nsearch APIs, which pose reproducibility and transparency challenges in addition\nto their cost. To address these limitations, we introduce DeepResearchGym, an\nopen-source sandbox that combines a reproducible search API with a rigorous\nevaluation protocol for benchmarking deep research systems. The API indexes\nlarge-scale public web corpora, namely ClueWeb22 and FineWeb, using a\nstate-of-the-art dense retriever and approximate nearest neighbor search via\nDiskANN. It achieves lower latency than popular commercial APIs while ensuring\nstable document rankings across runs, and is freely available for research use.\nTo evaluate deep research systems' outputs, we extend the Researchy Questions\nbenchmark with automatic metrics through LLM-as-a-judge assessments to measure\nalignment with users' information needs, retrieval faithfulness, and report\nquality. Experimental results show that systems integrated with DeepResearchGym\nachieve performance comparable to those using commercial APIs, with performance\nrankings remaining consistent across evaluation metrics. A human evaluation\nstudy further confirms that our automatic protocol aligns with human\npreferences, validating the framework's ability to help support controlled\nassessment of deep research systems. Our code and API documentation are\navailable at https://www.deepresearchgym.ai.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19253.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6135eeeb5bc6ecdf86b60f0d",
      "avatarUrl": "/avatars/43cedcf20ab6b0801a662787400e1384.svg",
      "fullname": "Shi Yu",
      "name": "yushi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21925",
      "authors": [
        {
          "_id": "6837c23acce400abe6f18790",
          "user": {
            "_id": "60747cbf3ea03830676542b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
            "isPro": false,
            "fullname": "Chong Zeng",
            "user": "NCJ",
            "type": "user"
          },
          "name": "Chong Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:38.730Z",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18791",
          "user": {
            "_id": "63299011bdb6242b42b77f57",
            "avatarUrl": "/avatars/056aec97eb3c10d3b63eb13238e1d2a4.svg",
            "isPro": false,
            "fullname": "doyleconan",
            "user": "doyleconan",
            "type": "user"
          },
          "name": "Yue Dong",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:11:08.981Z",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18792",
          "name": "Pieter Peers",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18793",
          "name": "Hongzhi Wu",
          "hidden": false
        },
        {
          "_id": "6837c23acce400abe6f18794",
          "name": "Xin Tong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/60747cbf3ea03830676542b5/DuGYODs8k7DhVHb78BmCZ.jpeg"
      ],
      "publishedAt": "2025-05-28T03:20:46.000Z",
      "submittedOnDailyAt": "2025-05-29T00:45:14.960Z",
      "title": "RenderFormer: 三角形マッチングのTransformerベースニューラルレンダリングとグローバルイルミネーション",
      "submittedOnDailyBy": {
        "_id": "60747cbf3ea03830676542b5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
        "isPro": false,
        "fullname": "Chong Zeng",
        "user": "NCJ",
        "type": "user"
      },
      "summary": "RenderFormerは、三角形ベースのスケーン表現から画像を直接渲染するニューラルレンダリングパイプラインです。これは完全なグローバル照明効果を含むことで、スケーン毎のトレーニングや微調節が不要です。物理的なアプローチを採用しない代わりに、レンダリングをトークンの列から出力トークンの列への変換として構成します。これは、反射性を持つ三角形を表すトークンの列を、小さなピクセルパッチを表す出力トークンの列に変換します。RenderFormerは2ステップパイプラインを採用します：視点無依存ステップでは、三角形から三角形への光の伝播をモデル化し、視点依存ステップでは、視点無依存ステップから得られる三角形列により、ライトバンドルを表すトークンをピクセル値に変換します。両ステップはトランジフォーターアーキテクチャに基づき、最小限の事前制約を持って学習されます。形状と光の伝播の複雑性が変化するスケーンでRenderFormerの機能と性能を示して評価します。",
      "upvotes": 15,
      "discussionId": "6837c23ccce400abe6f18812",
      "projectPage": "https://microsoft.github.io/renderformer/",
      "githubRepo": "https://github.com/microsoft/renderformer",
      "ai_summary": "RenderFormer is a transformer-based neural rendering pipeline that renders images from triangle representations without per-scene training and with full global illumination effects.",
      "ai_keywords": [
        "neural rendering pipeline",
        "global illumination effects",
        "sequence-to-sequence transformation",
        "tokens",
        "reflectance properties",
        "pixel patches",
        "transformer architecture",
        "view-independent stage",
        "view-dependent stage",
        "triangle-to-triangle light transport",
        "ray bundles"
      ]
    },
    "publishedAt": "2025-05-27T23:20:46.000Z",
    "title": "RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with\n  Global Illumination",
    "summary": "We present RenderFormer, a neural rendering pipeline that directly renders an\nimage from a triangle-based representation of a scene with full global\nillumination effects and that does not require per-scene training or\nfine-tuning. Instead of taking a physics-centric approach to rendering, we\nformulate rendering as a sequence-to-sequence transformation where a sequence\nof tokens representing triangles with reflectance properties is converted to a\nsequence of output tokens representing small patches of pixels. RenderFormer\nfollows a two stage pipeline: a view-independent stage that models\ntriangle-to-triangle light transport, and a view-dependent stage that\ntransforms a token representing a bundle of rays to the corresponding pixel\nvalues guided by the triangle-sequence from the view-independent stage. Both\nstages are based on the transformer architecture and are learned with minimal\nprior constraints. We demonstrate and evaluate RenderFormer on scenes with\nvarying complexity in shape and light transport.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/60747cbf3ea03830676542b5/DuGYODs8k7DhVHb78BmCZ.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60747cbf3ea03830676542b5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60747cbf3ea03830676542b5/wGr1Jzz520JM9nZ-UcLyb.png",
      "fullname": "Chong Zeng",
      "name": "NCJ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18600",
      "authors": [
        {
          "_id": "6837fe7664391bba7e477747",
          "name": "Bryan Sangwoo Kim",
          "hidden": false
        },
        {
          "_id": "6837fe7664391bba7e477748",
          "name": "Jeongsol Kim",
          "hidden": false
        },
        {
          "_id": "6837fe7664391bba7e477749",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T08:50:08.000Z",
      "submittedOnDailyAt": "2025-05-29T05:07:53.112Z",
      "title": "Chain-of-Zoom: 極大の超解像度を達成するスケール自動回帰と好み調整",
      "submittedOnDailyBy": {
        "_id": "6628efe14e1fa854f48d3a28",
        "avatarUrl": "/avatars/aa5421149a07a82b5c2a25978f9b6926.svg",
        "isPro": false,
        "fullname": "Sangwoo Kim",
        "user": "bryanswkim",
        "type": "user"
      },
      "summary": "現代の単一画像超解像度（SISR）モデルは、訓練されたスケール因子の範囲で写実的な結果を提供しますが、それよりも大きなスケールに拡大させられると崩壊します。このスケーラビリティブロックを解決するために、Chain-of-Zoom（CoZ）を用います。CoZは、多スケール情報を含むプロンプトを持つ中間スケール状態の自動回帰連鎖によりSISRを分解します。CoZは、主干SRモデルを再利用し、条件付き確率を計算可能なサブ問題に分解し、追加の訓練を必要とさせない極端な解像度を達成します。高い拡大率で視覚的キープは減少するため、各拡大ステップには、視覚言語モデル（VLM）で生成される多スケール情報を含むプロンプトを追加します。このプロンプト抽出器は、Generalized Reward Policy Optimization（GRPO）とコーナー VLMを用いて微調節され、人間の好みに合わせたテキストガイドダイナミクスに向けられます。実験は、標準の4倍拡大ディフュージョンSRモデルをCoZで包み込むことで、高い視覚質量と忠実度を伴う256倍以上の拡大を実現できることを示します。プロジェクトページ：https://bryanswkim.github.io/chain-of-zoom/ 。",
      "upvotes": 15,
      "discussionId": "6837fe7864391bba7e47779b",
      "projectPage": "https://bryanswkim.github.io/chain-of-zoom/",
      "githubRepo": "https://github.com/bryanswkim/Chain-of-Zoom",
      "ai_summary": "Chain-of-Zoom (CoZ) enhances single-image super-resolution models by using an autoregressive chain of intermediate scale-states and multi-scale-aware prompts to achieve extreme magnifications with high quality.",
      "ai_keywords": [
        "single-image super-resolution",
        "Chain-of-Zoom",
        "autoregressive chain",
        "multi-scale-aware prompts",
        "backbone SR model",
        "diffusion SR model",
        "prompt extractor",
        "Generalized Reward Policy Optimization",
        "critic VLM"
      ]
    },
    "publishedAt": "2025-05-24T04:50:08.000Z",
    "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and\n  Preference Alignment",
    "summary": "Modern single-image super-resolution (SISR) models deliver photo-realistic\nresults at the scale factors on which they are trained, but collapse when asked\nto magnify far beyond that regime. We address this scalability bottleneck with\nChain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an\nautoregressive chain of intermediate scale-states with multi-scale-aware\nprompts. CoZ repeatedly re-uses a backbone SR model, decomposing the\nconditional probability into tractable sub-problems to achieve extreme\nresolutions without additional training. Because visual cues diminish at high\nmagnifications, we augment each zoom step with multi-scale-aware text prompts\ngenerated by a vision-language model (VLM). The prompt extractor itself is\nfine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic\nVLM, aligning text guidance towards human preference. Experiments show that a\nstandard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement\nwith high perceptual quality and fidelity. Project Page:\nhttps://bryanswkim.github.io/chain-of-zoom/ .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18600.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6628efe14e1fa854f48d3a28",
      "avatarUrl": "/avatars/aa5421149a07a82b5c2a25978f9b6926.svg",
      "fullname": "Sangwoo Kim",
      "name": "bryanswkim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.19075",
      "authors": [
        {
          "_id": "6837fc484d14d7c8800e8b9c",
          "user": {
            "_id": "64c3732de6c3860fba66ceb0",
            "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
            "isPro": false,
            "fullname": "JaeminKim",
            "user": "kjm981995",
            "type": "user"
          },
          "name": "Jaemin Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:35.589Z",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9d",
          "name": "Hangeol Chang",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9e",
          "name": "Hyunmin Hwang",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8b9f",
          "name": "Choonghan Kim",
          "hidden": false
        },
        {
          "_id": "6837fc484d14d7c8800e8ba0",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T10:19:10.000Z",
      "submittedOnDailyAt": "2025-05-29T04:49:39.731Z",
      "title": "Universal Reasoner: 1つの、組み合わせ可能なPlug-and-PlayのReasonerを、Frozen LLMsに対して使用する",
      "submittedOnDailyBy": {
        "_id": "64c3732de6c3860fba66ceb0",
        "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
        "isPro": false,
        "fullname": "JaeminKim",
        "user": "kjm981995",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、卓越した一般的な能力を示していますが、理由論へのスキルの向上は、計算コンピューティングリソースの大量の負担を伴い、その一般化を補損することもあります。Parameter-Efficient Fine-Tuning（PEFT）メソッドは、コンピューティングリソースに親しみのある代替策として提案されていますが、アーキテクチャの依存関係により、各LLMバックボーンに対して再学習が必要となります。これらの課題を解決するために、ここではUniversal Reasoner（UniR）を提案します。UniRは、一つのライトウェイト、可組み合わせ可能、プラグインとプレイング可能な理由論モジュールです。このモジュールは、任意のフリーズンドLLMと組み合わせて、専門的な理由論能力を付与することができます。特に、UniRは、専用の理由論モジュールとして、予約定義の報酬を用いて独立に学習され、プロセスレベルの信号をトークンレベルのガイドに変換します。学習後、UniRは、推論時に、LLMバックボーンの出力ロジットに簡単に追加されることで、任意のフリーズンドLLMと組み合わせることができます。この加算構造は、モジュールの組み合わせを自然に可能にします：異なるタスクにおいて学習された複数のUniRモジュールを、ロジットの和で共に適用することで、複雑な理由論を可能にします。数学的な理由論と機械翻訳タスクにおける実験結果は、UniRはLlama3.2モデルを用いた既存のベースラインの再学習メソッドに比べて显著に優れていることを示しています。また、UniRは強めから強めの一般化を示し、小さなモデルで学習された理由論モジュールは、それらがもっと大きなLLMにも効果的に指導することができることを示しています。これは、LLMの理由論を向上させるためのコスト効率の高い、適応性のある、強固な解決策として、その核心能力を補損することを避けることができます。コードは、https://github.com/hangeol/UniR で公開されています。",
      "upvotes": 14,
      "discussionId": "6837fc494d14d7c8800e8be6",
      "ai_summary": "UniR, a lightweight reasoning module, enhances Large Language Models with specialized reasoning abilities through modular composition, improving performance and generalization at lower computational costs.",
      "ai_keywords": [
        "Large Language Models",
        "Parameter-Efficient Fine-Tuning",
        "Universal Reasoner",
        "trajectory-level signals",
        "token-level guidance",
        "additive structure",
        "modular composition",
        "Llama3.2",
        "mathematical reasoning",
        "machine translation",
        "cost-efficient",
        "adaptable",
        "robust"
      ]
    },
    "publishedAt": "2025-05-25T06:19:10.000Z",
    "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for\n  Frozen LLMs",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms existing baseline fine-tuning methods using the\nLlama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19075.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c3732de6c3860fba66ceb0",
      "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
      "fullname": "JaeminKim",
      "name": "kjm981995",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21887",
      "authors": [
        {
          "_id": "6837e3400aa18c6f96fe4876",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:49.456Z",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4877",
          "name": "Yahia Salaheldin Shaaban",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4878",
          "name": "Martin Takac",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe4879",
          "name": "Salem Lahlou",
          "hidden": false
        },
        {
          "_id": "6837e3400aa18c6f96fe487a",
          "name": "Zangir Iklassov",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Xa51WJrOIV-xl7A-Ry4t2.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/eS_bEDm2o2WAbn3YPS2XR.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/jaG4WahYyrQGaalZObNew.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mMKY5jn0SCJPxxR8_mssx.png"
      ],
      "publishedAt": "2025-05-28T02:03:31.000Z",
      "submittedOnDailyAt": "2025-05-29T03:02:06.399Z",
      "title": "SVRPBench: 確率的ビークルルーティング問題の実用的なベンチマーク",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "確実な路線処理は実世界的な物流の中心となっていますが、ほとんどのベンチマークは静的、理想的な設定を前提としています。私たちは、都市サイズでの運転路線処理の高精度の確率的な動きを捉える最初の開放ベンチマーク、SVRPBenchを紹介します。500以上のインスタンスを経験し、1000以上のクレーンを含む、時間依存性の交通渋滞、ログノーマルのデライ延び、確率的な事故、住宅と商業のクレーンの実験的に基づく時間ウィンドウを模倣します。私たちのパイプラインは、多デポと多ビークの構成や制約を豊富に含む多様なスケーナーを生成します。ベンチマークによる結果は、分布的な変動によって最先端のRLソルバーこそれもPOMOとAMが20%以上の降級を見出しますが、古典的なおよびメタハイュリスティックな方法は強固です。実験的な研究を可能にするために、データセットと評価システムをリリースします。SVRPBenchは、合成的な前提を超えて一般化可能なソルバーを設計するよう、コミュニティに挑戦します。",
      "upvotes": 13,
      "discussionId": "6837e3410aa18c6f96fe48b8",
      "githubRepo": "https://github.com/yehias21/vrp-benchmarks",
      "ai_summary": "SVRPBench introduces a new benchmark for vehicle routing under uncertainty, simulating realistic urban conditions and highlighting the limitations of state-of-the-art RL solvers.",
      "ai_keywords": [
        "vehicle routing",
        "SVRPBench",
        "time-dependent congestion",
        "log-normal delays",
        "probabilistic accidents",
        "multi-depot",
        "multi-vehicle",
        "state-of-the-art RL solvers",
        "POMO",
        "AM",
        "distributional shift",
        "classical methods",
        "metaheuristic methods"
      ]
    },
    "publishedAt": "2025-05-27T22:03:31.000Z",
    "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem",
    "summary": "Robust routing under uncertainty is central to real-world logistics, yet most\nbenchmarks assume static, idealized settings. We present SVRPBench, the first\nopen benchmark to capture high-fidelity stochastic dynamics in vehicle routing\nat urban scale. Spanning more than 500 instances with up to 1000 customers, it\nsimulates realistic delivery conditions: time-dependent congestion, log-normal\ndelays, probabilistic accidents, and empirically grounded time windows for\nresidential and commercial clients. Our pipeline generates diverse,\nconstraint-rich scenarios, including multi-depot and multi-vehicle setups.\nBenchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade\nby over 20% under distributional shift, while classical and metaheuristic\nmethods remain robust. To enable reproducible research, we release the dataset\nand evaluation suite. SVRPBench challenges the community to design solvers that\ngeneralize beyond synthetic assumptions and adapt to real-world uncertainty.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Xa51WJrOIV-xl7A-Ry4t2.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/eS_bEDm2o2WAbn3YPS2XR.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/jaG4WahYyrQGaalZObNew.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mMKY5jn0SCJPxxR8_mssx.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21887.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 39
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22129",
      "authors": [
        {
          "_id": "6837dae6e9b21653755a05d4",
          "user": {
            "_id": "64c71a5647418a0a59e5c7cb",
            "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
            "isPro": false,
            "fullname": "Jinhong Ni",
            "user": "mcleanie",
            "type": "user"
          },
          "name": "Jinhong Ni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:03.476Z",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d5",
          "user": {
            "_id": "65434daa5a36a8774d0e2271",
            "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
            "isPro": false,
            "fullname": "Allen Zhang",
            "user": "allencbzhang",
            "type": "user"
          },
          "name": "Chang-Bin Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:59.930Z",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d6",
          "name": "Qiang Zhang",
          "hidden": false
        },
        {
          "_id": "6837dae6e9b21653755a05d7",
          "name": "Jing Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T08:54:04.000Z",
      "submittedOnDailyAt": "2025-05-29T02:38:27.338Z",
      "title": "テキストから360度パナラマ生成を安定して行うための要因は何ですか？",
      "submittedOnDailyBy": {
        "_id": "65434daa5a36a8774d0e2271",
        "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
        "isPro": false,
        "fullname": "Allen Zhang",
        "user": "allencbzhang",
        "type": "user"
      },
      "summary": "最近、テキストから画像のディフュージョンモデル（例：Stable Diffusion）の豊かさにより、360度パナラマ画像生成に適用する研究が進み始めています。先行研究は、予っちゃったディフュージョンモデルを用いてパナラマ画像を生成するために通常の低レンキャンアダプターテクニックの有効性を示しています。しかし、撮影角度とパナラマ画像の間の大きな領域間違いが、この実験的な成功における裏技術を質疑しています。私たちは、学習可能なコンペナントがパナラマデータにファイナルチューニングされると異なる行動を示すことを仮定し、このアダプターは予っちゃったディフュージョンモデル内の先週知識を利用する内在的な機能を隠していることを調査しています。私たちの分析は以下のことを示しています：1）注意モジュールのクエリとキーマトリックスは、パナラマと撮影角度の領域間で共有できる共通情報を責任に持っているため、パナラマ生成に関係しないようである；2）値と出力重みマトリックスは、予っちゃった知識をパナラマ領域に適用することを特化し、パナラマ生成のファイナルチューニング期間においてより重要な役割を果たしている。私たちは、これらの洞察を実験的に証明するために、簡単なフレームワークを提案しています。これをUniPanoと呼び、将来の研究に優れた基準を作成することを目的としています。UniPanoは、既存の方法を超えているだけでなく、先行のダブルブランチアプローチに比べてメモリ使用量と訓練時間を大幅に減少させ、高解像度のパナラマ生成にも拡張可能です。コードはリリースされます。",
      "upvotes": 12,
      "discussionId": "6837daece9b21653755a0791",
      "ai_summary": "Analysis of fine-tuning diffusion models for panoramic image generation reveals distinct roles of attention module matrices and introduces UniPano, a memory-efficient and speed-enhanced baseline framework.",
      "ai_keywords": [
        "text-to-image diffusion models",
        "Stable Diffusion",
        "low-rank adaptation",
        "pre-trained diffusion models",
        "attention modules",
        "query matrices",
        "key matrices",
        "value matrices",
        "output weight matrices",
        "panoramic image generation",
        "domain gap",
        "common information",
        "pre-trained knowledge",
        "UniPano",
        "end-to-end panorama generation",
        "memory usage",
        "training time"
      ]
    },
    "publishedAt": "2025-05-28T04:54:04.000Z",
    "title": "What Makes for Text to 360-degree Panorama Generation with Stable\n  Diffusion?",
    "summary": "Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,\nhas stimulated research to adapt them to 360-degree panorama generation. Prior\nwork has demonstrated the feasibility of using conventional low-rank adaptation\ntechniques on pre-trained diffusion models to generate panoramic images.\nHowever, the substantial domain gap between perspective and panoramic images\nraises questions about the underlying mechanisms enabling this empirical\nsuccess. We hypothesize and examine that the trainable counterparts exhibit\ndistinct behaviors when fine-tuned on panoramic data, and such an adaptation\nconceals some intrinsic mechanism to leverage the prior knowledge within the\npre-trained diffusion models. Our analysis reveals the following: 1) the query\nand key matrices in the attention modules are responsible for common\ninformation that can be shared between the panoramic and perspective domains,\nthus are less relevant to panorama generation; and 2) the value and output\nweight matrices specialize in adapting pre-trained knowledge to the panoramic\ndomain, playing a more critical role during fine-tuning for panorama\ngeneration. We empirically verify these insights by introducing a simple\nframework called UniPano, with the objective of establishing an elegant\nbaseline for future research. UniPano not only outperforms existing methods but\nalso significantly reduces memory usage and training time compared to prior\ndual-branch approaches, making it scalable for end-to-end panorama generation\nwith higher resolution. The code will be released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22129.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65434daa5a36a8774d0e2271",
      "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
      "fullname": "Allen Zhang",
      "name": "allencbzhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20411",
      "authors": [
        {
          "_id": "683735aff42cc8a1d260e677",
          "user": {
            "_id": "654e5e094319c75e3e1b6cbc",
            "avatarUrl": "/avatars/a8889036fa38f80f2d45aea8d1471395.svg",
            "isPro": false,
            "fullname": "Ibragim",
            "user": "ibragim-bad",
            "type": "user"
          },
          "name": "Ibragim Badertdinov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:24.705Z",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e678",
          "user": {
            "_id": "644e9ffcd6001776ed77d874",
            "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
            "isPro": false,
            "fullname": "Alexander",
            "user": "djalexj",
            "type": "user"
          },
          "name": "Alexander Golubev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T19:13:38.030Z",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e679",
          "name": "Maksim Nekrashevich",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67a",
          "name": "Anton Shevtsov",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67b",
          "user": {
            "_id": "65e48cb3a4e46e644ec1277d",
            "avatarUrl": "/avatars/dd2bf04a6f81bf0a0892080af5d485b2.svg",
            "isPro": false,
            "fullname": "Simon Karasik",
            "user": "sbkarasik",
            "type": "user"
          },
          "name": "Simon Karasik",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:26.644Z",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67c",
          "name": "Andrei Andriushchenko",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67d",
          "name": "Maria Trofimova",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67e",
          "name": "Daria Litvintseva",
          "hidden": false
        },
        {
          "_id": "683735aff42cc8a1d260e67f",
          "name": "Boris Yangel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-26T18:01:00.000Z",
      "submittedOnDailyAt": "2025-05-29T06:59:04.261Z",
      "title": "SWE-rebench: ソフトウェア工学アグリーンのタスク集計とデコンテミニティング評価の自動化パイプライン",
      "submittedOnDailyBy": {
        "_id": "644e9ffcd6001776ed77d874",
        "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
        "isPro": false,
        "fullname": "Alexander",
        "user": "djalexj",
        "type": "user"
      },
      "summary": "LLMベースのアガントは、増えている範囲のソフトウェア開発（SWE）タスクにおいて、望ましい能力を示しています。しかし、この分野の進歩には2つの重要な課題があります。1つ目の課題は、高品質の訓練データが稀少で、特に、アガントが開発環境と相互作用し、コードを実行し、その行動の結果に基づいて行動を変更する実世界的なSWEシナリオを反映するデータが稀少です。現在のデータセットは、1回のコード生成または小さな手動で編集された相互作用タスクの集合で限定されていて、どちらもスケールと多様性が不足しています。2つ目の課題は、新しい相互作用タスクの不足が、急速に改善しているモデルの評価に影響を及ぼしています。静的ベンチマークは、コンテナー化問題により迅速に過期しやすくなります。これらの制限を解決するために、私たちは、多様なGitHubリポジトリからの実世界的な相互作用タスクを継続的に抜き出す新しい、自動的でスケーラブルなパイプラインを導入します。このパイプラインを使用して、SWE-rebenchという公開データセットを構築しました。このデータセットは、21,000以上のインタラクティブなPythonベースのSWEタスクを含み、スケールでのSWEアガントの強化学習に適しています。また、SWE-rebenchの方法を用いて採集された新しいタスクの継続的な供給を使用して、コンテナー化問題を避けたベンチマークを構築しました。このベンチマークでの各種LLMの結果を、SWE-bench Verifiedでの結果と比較し、そのエンジニアリングモデルの性能がコンテナー化問題によって膨脹している可能性を示しています。",
      "upvotes": 11,
      "discussionId": "683735b0f42cc8a1d260e69f",
      "ai_summary": "A novel pipeline extracts real-world, interactive software engineering tasks from GitHub to create SWE-rebench, improving the evaluation of reinforcement learning models in SWE.",
      "ai_keywords": [
        "LLM-based agents",
        "reinforcement learning",
        "software engineering tasks",
        "GitHub repositories",
        "SWE-rebench",
        "contamination-free benchmark",
        "SWE-bench Verified"
      ]
    },
    "publishedAt": "2025-05-26T14:01:00.000Z",
    "title": "SWE-rebench: An Automated Pipeline for Task Collection and\n  Decontaminated Evaluation of Software Engineering Agents",
    "summary": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20411.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644e9ffcd6001776ed77d874",
      "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
      "fullname": "Alexander",
      "name": "djalexj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22648",
      "authors": [
        {
          "_id": "6837c03cbbee677da73e6034",
          "user": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "isPro": false,
            "fullname": "Jialong Wu",
            "user": "callanwu",
            "type": "user"
          },
          "name": "Jialong Wu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T02:02:37.069Z",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6035",
          "name": "Baixuan Li",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6036",
          "name": "Runnan Fang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6037",
          "name": "Wenbiao Yin",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6038",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e6039",
          "name": "Zhengwei Tao",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603a",
          "name": "Dingchu Zhang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603b",
          "name": "Zekun Xi",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603c",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603d",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603e",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837c03cbbee677da73e603f",
          "name": "Jingren Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/vhAmZAlJqekE6vLcVjWtO.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/RXQVwE9PRmBzxCURRiKAU.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/woFRNvRqdUXKnAHpRH2eM.mp4"
      ],
      "publishedAt": "2025-05-28T17:57:07.000Z",
      "submittedOnDailyAt": "2025-05-29T00:34:30.750Z",
      "title": "WebDancer: 自動化情報探求機関への向け方",
      "submittedOnDailyBy": {
        "_id": "644a4fbc2166258fccc664bc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
        "isPro": false,
        "fullname": "Jialong Wu",
        "user": "callanwu",
        "type": "user"
      },
      "summary": "実際の複雑な問題を解決するには、詳細な情報の探求と多段階的推理が必要です。Deep Researchを例にしているエージェントシステムの最近の進歩は、自動化された多段階的調査の可能性を強調しています。本稿では、データセンタリックと訓練ステージの観点から、端末からのエージェント的な情報探求アグエントの構築の一連のパラダイムを提案します。我々のアプローチは4つのキーステージから成る：1）データブラウザの構築、2）タライクスのサンプリング、3）冷やかなスタートの効果的な訓練、4）拡張性の向上を図る強化学習。ReActに基づくWebDancerのWebアガントでこのフレームワークを実装しました。GAIAとWebWalkerQAの難しい情報探求ベンチマークでの実験評価は、WebDancerの強力な性能を示し、我々の訓練パラダイムの効果を明らかにしました。アガエントの訓練の進一歩の分析は、より有能なエージェントモデルの開発に役立つ有効なアイデアと行動的な、システム的なパスワードを提供します。コードとデモは、https://github.com/Alibaba-NLP/WebAgentに公開されます。",
      "upvotes": 10,
      "discussionId": "6837c03dbbee677da73e607f",
      "githubRepo": "https://github.com/Alibaba-NLP/WebAgent",
      "ai_summary": "The paper proposes a framework for building end-to-end agentic information seeking agents through a combination of data construction, trajectory sampling, supervised fine-tuning, and reinforcement learning, showcasing its effectiveness on information seeking benchmarks.",
      "ai_keywords": [
        "browsing data construction",
        "trajectories sampling",
        "supervised fine-tuning",
        "reinforcement learning",
        "WebDancer",
        "GAIA",
        "WebWalkerQA"
      ]
    },
    "publishedAt": "2025-05-28T13:57:07.000Z",
    "title": "WebDancer: Towards Autonomous Information Seeking Agency",
    "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/vhAmZAlJqekE6vLcVjWtO.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/RXQVwE9PRmBzxCURRiKAU.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/644a4fbc2166258fccc664bc/woFRNvRqdUXKnAHpRH2eM.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22648.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "644a4fbc2166258fccc664bc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
      "fullname": "Jialong Wu",
      "name": "callanwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.19187",
      "authors": [
        {
          "_id": "6837210455e9bab4e9c302b1",
          "user": {
            "_id": "6002c316698168af3bb9f4a6",
            "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
            "isPro": false,
            "fullname": "yangxiao",
            "user": "YangXiao-nlp",
            "type": "user"
          },
          "name": "Yang Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T19:13:52.090Z",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b2",
          "name": "Jiashuo Wang",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b3",
          "name": "Ruifeng Yuan",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b4",
          "name": "Chunpu Xu",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b5",
          "name": "Kaishuai Xu",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b6",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "6837210455e9bab4e9c302b7",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-25T15:17:57.000Z",
      "submittedOnDailyAt": "2025-05-29T01:05:08.605Z",
      "title": "LIMOPro: 検査時間の効率的かつ有効なスケーリングのための理由論の精進",
      "submittedOnDailyBy": {
        "_id": "6002c316698168af3bb9f4a6",
        "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
        "isPro": false,
        "fullname": "yangxiao",
        "user": "YangXiao-nlp",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、検証時スケーリングアプローチにより、特にチェーンオブコンショート（CoT）データを用いてファイナルチューニングされた場合に、驚異的な理由論的計算能力を示している。しかし、これらの理由論的チェーンは、進歩的な理由論（本質的な解決策開発パス）と機能的要素（検証プロセス、代替解決策アプローチ、エラー修正）に分類されるような、長文の要素を含んでいることが多い。進歩的な理由論は重要であるが、機能的要素は検証時推論時の計算負荷を大幅に増加させる。\n\n我々は、答え予測の信頼度における影響を基準に、各理由論ステップの重要性を定量的に評価するPrincipled Frameworkを導入しています。これは、PIR（Perplexity-based Importance Refinement）と呼ばれ、進歩的な理由論のコンポーネントを保持しながら、低重要性の機能的ステップを選択的に削除することで、最適化されたトレーニングデータを作成します。PIRによりトレーニングされたモデルは、検証時スケーリングの性能が向上し、計算量の削減（-3% ～ -41%）と精度の向上（+0.9% ～ +6.6%）を収得します。特に、AIME、AMC、GPQA Diamondなどの難しい理由論ベンチマークでも効果が見られます。我々のアプローチは、異なるモデルサイズ、データ源、トークンバッジに対して強い一般化性能を示し、理由論的なLLMsの実用化のための実用的な解決策を提供します。",
      "upvotes": 10,
      "discussionId": "6837210555e9bab4e9c302f2",
      "ai_summary": "A framework called PIR refines the importance of reasoning steps in large language models by pruning low-importance functional elements, leading to more concise reasoning chains with improved accuracy and reduced computational demands.",
      "ai_keywords": [
        "large language models",
        "chain-of-thought",
        "large reasoning models",
        "progressive reasoning",
        "functional elements",
        "perplexity-based importance refinement",
        "token usage",
        "reasoning benchmarks",
        "AIME",
        "AMC",
        "GPQA Diamond"
      ]
    },
    "publishedAt": "2025-05-25T11:17:57.000Z",
    "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time\n  Scaling",
    "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities through test-time scaling approaches, particularly when fine-tuned\nwith chain-of-thought (CoT) data distilled from more powerful large reasoning\nmodels (LRMs). However, these reasoning chains often contain verbose elements\nthat mirror human problem-solving, categorized as progressive reasoning (the\nessential solution development path) and functional elements (verification\nprocesses, alternative solution approaches, and error corrections). While\nprogressive reasoning is crucial, the functional elements significantly\nincrease computational demands during test-time inference. We introduce PIR\n(Perplexity-based Importance Refinement), a principled framework that\nquantitatively evaluates the importance of each reasoning step based on its\nimpact on answer prediction confidence. PIR systematically identifies and\nselectively prunes only low-importance functional steps while preserving\nprogressive reasoning components, creating optimized training data that\nmaintains the integrity of the core solution path while reducing verbosity.\nModels fine-tuned on PIR-optimized data exhibit superior test-time scaling\nproperties, generating more concise reasoning chains while achieving improved\naccuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to\n-41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).\nOur approach demonstrates strong generalizability across different model sizes,\ndata sources, and token budgets, offering a practical solution for deploying\nreasoning-capable LLMs in scenarios where efficient test-time scaling, response\ntime, and computational efficiency are valuable constraints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19187.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6002c316698168af3bb9f4a6",
      "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
      "fullname": "yangxiao",
      "name": "YangXiao-nlp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17663",
      "authors": [
        {
          "_id": "6833c6ff97966d18e7b995b0",
          "user": {
            "_id": "6002c316698168af3bb9f4a6",
            "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
            "isPro": false,
            "fullname": "yangxiao",
            "user": "YangXiao-nlp",
            "type": "user"
          },
          "name": "Yang Xiao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T19:14:41.639Z",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b1",
          "name": "Jiashuo Wang",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b2",
          "name": "Qiancheng Xu",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b3",
          "name": "Changhe Song",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b4",
          "name": "Chunpu Xu",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b5",
          "name": "Yi Cheng",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b6",
          "name": "Wenjie Li",
          "hidden": false
        },
        {
          "_id": "6833c6ff97966d18e7b995b7",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T09:27:40.000Z",
      "submittedOnDailyAt": "2025-05-29T01:05:57.079Z",
      "title": "ダイナミックな心の理論への向け方：人間の状態の時系列的進化に対するLLMの適応を評価する",
      "submittedOnDailyBy": {
        "_id": "6002c316698168af3bb9f4a6",
        "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
        "isPro": false,
        "fullname": "yangxiao",
        "user": "YangXiao-nlp",
        "type": "user"
      },
      "summary": "LLMが人間とAIのインタラクションについて日増しに参加することに伴い、Theory of Mind (ToM)の能力を評価することが重要になります。特に、心の状態を追跡する能力を評価することが重要です。現在のベンチマークは基本的なToM能力を評価しますが、主に静的な心の状態のスニペットを焦点にしていて、実世界的な社会的なインタラクションの時系列的な進行を見落としています。DynToMという新しいベンチマークを提案します。これはLLMが心の状態の時系列的な進行を理解し、連結したシナリオの中で追跡する能力を評価するために特に設計されています。システマティックな4段階のフレームワークを通じて、1,100のソシャルコンテキストを生成し、それに5,500のシナリオと78,100の質問を含みます。これらはリアリズムと品質にバリデーションされています。10つの最先端のLLMの詳細な評価により、平均的な性能は人間より44.7%低いことが明らかになり、心の状態の変化を追跡して理由を与える場合には性能が显著に低下します。この性能の差は、現在のLLMが人間の心の状態の動的な性質をモデル化する能力における基本的な制限を明らかにしています。",
      "upvotes": 10,
      "discussionId": "6833c70097966d18e7b99616",
      "ai_summary": "The DynToM benchmark evaluates LLMs' ability to track and understand the temporal progression of mental states, revealing significant gaps compared to human performance.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Theory of Mind (ToM)",
        "DynToM",
        "social contexts",
        "mental states",
        "temporal progression",
        "evaluation framework",
        "state-of-the-art LLMs",
        "performance gap",
        "dynamic mental states"
      ]
    },
    "publishedAt": "2025-05-23T05:27:40.000Z",
    "title": "Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal\n  Evolution of Human States",
    "summary": "As Large Language Models (LLMs) increasingly participate in human-AI\ninteractions, evaluating their Theory of Mind (ToM) capabilities - particularly\ntheir ability to track dynamic mental states - becomes crucial. While existing\nbenchmarks assess basic ToM abilities, they predominantly focus on static\nsnapshots of mental states, overlooking the temporal evolution that\ncharacterizes real-world social interactions. We present DynToM, a\nnovel benchmark specifically designed to evaluate LLMs' ability to understand\nand track the temporal progression of mental states across interconnected\nscenarios. Through a systematic four-step framework, we generate 1,100 social\ncontexts encompassing 5,500 scenarios and 78,100 questions, each validated for\nrealism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs\nreveals that their average performance underperforms humans by 44.7\\%, with\nperformance degrading significantly when tracking and reasoning about the shift\nof mental states. This performance gap highlights fundamental limitations in\ncurrent LLMs' ability to model the dynamic nature of human mental states.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17663.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6002c316698168af3bb9f4a6",
      "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
      "fullname": "yangxiao",
      "name": "YangXiao-nlp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22019",
      "authors": [
        {
          "_id": "6837ed297d00cf0a04677bc1",
          "user": {
            "_id": "657429d833e5a4bf5b278615",
            "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
            "isPro": false,
            "fullname": "QiuchenWang",
            "user": "autumncc",
            "type": "user"
          },
          "name": "Qiuchen Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:19.039Z",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc2",
          "name": "Ruixue Ding",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc3",
          "user": {
            "_id": "665d652e0f35c005de892108",
            "avatarUrl": "/avatars/240bebdc7fdc6d50719c65de0e3cf1cd.svg",
            "isPro": false,
            "fullname": "Yu Zeng",
            "user": "YuZeng260",
            "type": "user"
          },
          "name": "Yu Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:42.906Z",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc4",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc5",
          "user": {
            "_id": "64b02ec0e5000ae8a572ced5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
            "isPro": false,
            "fullname": "Lin Chen",
            "user": "Lin-Chen",
            "type": "user"
          },
          "name": "Lin Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:44.983Z",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc6",
          "name": "Shihang Wang",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc7",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc8",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837ed297d00cf0a04677bc9",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T06:30:51.000Z",
      "submittedOnDailyAt": "2025-05-29T03:46:05.539Z",
      "title": "VRAG-RL: ビジョンベースのRAGを強化学習による反復的推論で強化する視覚豊富な情報理解",
      "submittedOnDailyBy": {
        "_id": "64b02ec0e5000ae8a572ced5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
        "isPro": false,
        "fullname": "Lin Chen",
        "user": "Lin-Chen",
        "type": "user"
      },
      "summary": "広義的画像付き情報の効果的な検索、論理、理解は、RAG手法にとって難題です。伝統的なテキストベースの方法は、画像関連の情報を処理できません。一方、現在の画像ベースのRAGアプローチは、固定プイプラインに制限され、モデルの基本的な能力の不十分な活性化により、論理を効果的に行うことが難しくなります。RLがモデルの論理にベニフィシャルな影響を与えることが証明されているため、私たちは、複雑な論理を行うための新しいRLフレームワークVRAG-RLを紹介します。このフレームワークを用いて、VLMsは検索エンジンと相互作用し、画像認識トークンの助けを受けて、一回だけの論理タライトや複数回の論理タライトを自動的にサンプリングし、これらのサンプルに基づいて継続的に最適化されます。私たちのアプローチは、RAG領域でのRLの主な制限を特徴的にします：(i) 先行の多モデルRAGアプローチは、画像をコンテキストに収めるだけで、論理トークンの割り当てが不足し、画像特有の認識を飛ばしています；(ii) モデルが検索エンジンと相互作用すると、そのクエリは、要求を明確に表現することができないため、関連情報を検索しきれなく、最適な性能を獲得できません。これらの挑戦を解決するために、画像付き入力にタイドマップされた行動空間を定義し、切り取りとスケーリングなどの行動を含むことで、モデルがコアーストライフトからフィンストライフトの視点から情報を集めることができます。また、ユーザーの元の質問と検索モデルの間の隙間を埋めるために、簡単で効果的な報酬を使用し、クエリの再書きと検索性能をモデルベースの報酬と統合します。VRAG-RLは、特別に設計されたRL戦略を用いてRAGタスクに最適化され、モデルが実世界的なアプリケーションに合わせることを目指しています。コードは、https://github.com/Alibaba-NLP/VRAG{https://github.com/Alibaba-NLP/VRAG}に提供されています。",
      "upvotes": 8,
      "discussionId": "6837ed297d00cf0a04677bf5",
      "githubRepo": "https://github.com/Alibaba-NLP/VRAG",
      "ai_summary": "VRAG-RL, a reinforcement learning framework, enhances reasoning and visual information handling in RAG methods by integrating visual perception tokens and employing specialized action spaces and rewards.",
      "ai_keywords": [
        "reinforcement learning",
        "VRAG-RL",
        "VLMs",
        "search engines",
        "visually rich information",
        "reasoning trajectories",
        "visual perception tokens",
        "action space",
        "query rewriting",
        "retrieval performance",
        "model-based reward"
      ]
    },
    "publishedAt": "2025-05-28T02:30:51.000Z",
    "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich\n  Information Understanding via Iterative Reasoning with Reinforcement Learning",
    "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\nhttps://github.com/Alibaba-NLP/VRAG{https://github.com/Alibaba-NLP/VRAG}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22019.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64b02ec0e5000ae8a572ced5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
      "fullname": "Lin Chen",
      "name": "Lin-Chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 89
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22232",
      "authors": [
        {
          "_id": "683815574d9866c160e88670",
          "name": "Mehdi Ali",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88671",
          "user": {
            "_id": "62fa1d95e8c9c532aa75331c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
            "isPro": false,
            "fullname": "Manuel Brack",
            "user": "mbrack",
            "type": "user"
          },
          "name": "Manuel Brack",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T09:40:14.826Z",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88672",
          "name": "Max Lübbering",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88673",
          "name": "Elias Wendt",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88674",
          "name": "Abbas Goher Khan",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88675",
          "name": "Richard Rutmann",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88676",
          "name": "Alex Jude",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88677",
          "name": "Maurice Kraus",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88678",
          "name": "Alexander Arno Weber",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88679",
          "name": "Felix Stollenwerk",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867a",
          "name": "David Kaczér",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867b",
          "name": "Florian Mai",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867c",
          "name": "Lucie Flek",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867d",
          "name": "Rafet Sifa",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867e",
          "name": "Nicolas Flores-Herr",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e8867f",
          "name": "Joachim Köhler",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88680",
          "name": "Patrick Schramowski",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88681",
          "name": "Michael Fromm",
          "hidden": false
        },
        {
          "_id": "683815574d9866c160e88682",
          "name": "Kristian Kersting",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T11:06:54.000Z",
      "submittedOnDailyAt": "2025-05-29T07:38:07.888Z",
      "title": "記述質量の多言語アプローチ：予備学習のための多言語アプローチ\n言語モデルを用いたデータフィルタリング",
      "submittedOnDailyBy": {
        "_id": "62fa1d95e8c9c532aa75331c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
        "isPro": false,
        "fullname": "Manuel Brack",
        "user": "mbrack",
        "type": "user"
      },
      "summary": "高品質の多言語訓練データは、大規模言語モデル（LLMs）の効果的な事前訓練に不可欠です。しかし、適切な開放ソースの多言語データセットの可利用性は限られています。現在の最先端のデータセットは主にヒューリスティックフィルタリングメソッドを基にしているため、両言語間のコンバージョン性とスケーラビリティが制限されています。ここで、私たちはJQLというシステマティックなアプローチを紹介します。これは、スケール上で多言語データを効率的にディバーシティーと高品質にカレードし、計算負担を大幅に減少させることを目的としています。JQLは、LLMsの注釈能力を基礎にした軽量ラベリングモデルにエノイゼートします。これらのモデルは、訓練中に見ぬ言語や文字系でも強力な多言語と両言語間の性能を示します。35言語を拡張して実験的に評価した結果、現在のヒューリスティックフィルタリングメソッドのようなFineweb2を大幅に超えます。JQLは、下流モデルの訓練品質を向上させ、データの保存率を上げることを特に顕著にします。私たちの研究は、実用的なインサイトと有價値なリソースを提供し、多言語データカレードの標準を高めることを目指しています。",
      "upvotes": 7,
      "discussionId": "683815594d9866c160e88708",
      "projectPage": "https://huggingface.co/spaces/Jackal-AI/JQL",
      "githubRepo": "https://github.com/JQL-AI/JQL-Annotation-Pipeline",
      "ai_summary": "JQL systematically curates high-quality multilingual training data using pretrained multilingual embeddings, outperforming heuristic methods and improving downstream model training across diverse languages.",
      "ai_keywords": [
        "pretraining",
        "large language models",
        "multilingual datasets",
        "heuristic filtering methods",
        "JQL",
        "lightweight annotators",
        "multilingual embeddings",
        "cross-lingual transferability",
        "annotation pipeline",
        "data retention rates",
        "multilingual data curation"
      ]
    },
    "publishedAt": "2025-05-28T07:06:54.000Z",
    "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining\n  Data Filtering with Language Models",
    "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22232.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fa1d95e8c9c532aa75331c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
      "fullname": "Manuel Brack",
      "name": "mbrack",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22613",
      "authors": [
        {
          "_id": "6837d79d4d9866c160d8f43b",
          "name": "Yuchi Wang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43c",
          "name": "Yishuo Cai",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43d",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43e",
          "name": "Sihan Yang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f43f",
          "name": "Linli Yao",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f440",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f441",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f442",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6837d79d4d9866c160d8f443",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:29:34.000Z",
      "submittedOnDailyAt": "2025-05-29T02:20:00.209Z",
      "title": "リコ: 画像再キャプチャの精度と完全性向上において、可視的再構築を通じて",
      "submittedOnDailyBy": {
        "_id": "622842e296588dd1a2594746",
        "avatarUrl": "/avatars/b96d0b49e4cff25d83fefc67b7cd1076.svg",
        "isPro": false,
        "fullname": "wangyuchi",
        "user": "YuchiWang",
        "type": "user"
      },
      "summary": "画像リカプチングは、多様なモデルタスクに対して、質を向上させたトレーニングデータセットを生成するために広く使用されています。現在のリカプチング方法は通常、強力な多モデル大語言モデル（MLLM）をもとに、文脈を向上させるために依存していますが、欠陥のない細かい詳細を欠けていることによって、幻想と不完全性による不正確性を見受けます。これらの制限を解決するために、私たちは、画像重建をもとにしたリカプチングを改良する新しいフレームワークを提案しています。特に、文脈を画像に再構成するための文脈モデルを利用し、元の画像と再構成画像の間の差異をMLLMで識別させ、リカプチンを改良することを目指しています。このプロセスは反復的に行われ、忠実かつ詳細な説明の生成を進めます。複数回のプロセスによる追加計算コストを軽減するために、私たちは、DPOを用いてリカポーチコのようにキャプションを生成することを学習させるリカポーチコ-Flashを導入しています。拡張された実験は、私たちのアプローチがリカプチンの正確性と詳細性を大幅に向上させ、CapsBenchとCompreCapの両方で基準となるモデルより約10%以上の性能を上げることを示しています。コードは以下のURLで公開されています。\nhttps://github.com/wangyuchi369/RICO",
      "upvotes": 5,
      "discussionId": "6837d79e4d9866c160d8f471",
      "githubRepo": "https://github.com/wangyuchi369/RICO",
      "ai_summary": "A novel iterative framework, RICO, improves image caption accuracy by using visual reconstruction and a text-to-image model to refine discrepancies, while RICO-Flash enhances efficiency using DPO.",
      "ai_keywords": [
        "multimodal large language models",
        "text-to-image model",
        "DPO",
        "CapsBench",
        "CompreCap"
      ]
    },
    "publishedAt": "2025-05-28T13:29:34.000Z",
    "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via\n  Visual Reconstruction",
    "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22613.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "622842e296588dd1a2594746",
      "avatarUrl": "/avatars/b96d0b49e4cff25d83fefc67b7cd1076.svg",
      "fullname": "wangyuchi",
      "name": "YuchiWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22525",
      "authors": [
        {
          "_id": "6837da438f680552f7b86b28",
          "user": {
            "_id": "64bb5f9d8e051085bace4d1e",
            "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
            "isPro": true,
            "fullname": "Ethan Chern",
            "user": "ethanchern",
            "type": "user"
          },
          "name": "Ethan Chern",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:05.613Z",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b29",
          "name": "Zhulin Hu",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2a",
          "name": "Steffi Chern",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2b",
          "name": "Siqi Kou",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2c",
          "name": "Jiadi Su",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2d",
          "name": "Yan Ma",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2e",
          "name": "Zhijie Deng",
          "hidden": false
        },
        {
          "_id": "6837da438f680552f7b86b2f",
          "name": "Pengfei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T16:12:45.000Z",
      "submittedOnDailyAt": "2025-05-29T02:24:51.876Z",
      "title": "生成画像を使って考える",
      "submittedOnDailyBy": {
        "_id": "64bb5f9d8e051085bace4d1e",
        "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
        "isPro": true,
        "fullname": "Ethan Chern",
        "user": "ethanchern",
        "type": "user"
      },
      "summary": "We present Thinking with Generated Images, a novel paradigm that fundamentally transforms how large multimodal models (LMMs) engage with visual reasoning by enabling them to natively think across text and vision modalities through spontaneous generation of intermediate visual thinking steps. Current visual reasoning with LMMs is constrained to either processing fixed user-provided images or reasoning solely through text-based chain-of-thought (CoT). Thinking with Generated Images unlocks a new dimension of cognitive capability where models can actively construct intermediate visual thoughts, critique their own visual hypotheses, and refine them as integral components of their reasoning process. We demonstrate the effectiveness of our approach through two complementary mechanisms: (1) vision generation with intermediate visual subgoals, where models decompose complex visual tasks into manageable components that are generated and integrated progressively, and (2) vision generation with self-critique, where models generate an initial visual hypothesis, analyze its shortcomings through textual reasoning, and produce refined outputs based on their own critiques. Our experiments on vision generation benchmarks show substantial improvements over baseline approaches, with our models achieving up to 50% (from 38% to 57%) relative improvement in handling complex multi-object scenarios. From biochemists exploring novel protein structures, and architects iterating on spatial designs, to forensic analysts reconstructing crime scenes, and basketball players envisioning strategic plays, our approach enables AI models to engage in the kind of visual imagination and iterative refinement that characterizes human creative, analytical, and strategic thinking. We release our open-source suite at https://github.com/GAIR-NLP/thinking-with-generated-images.",
      "upvotes": 5,
      "discussionId": "6837da468f680552f7b86bb2",
      "githubRepo": "https://github.com/GAIR-NLP/thinking-with-generated-images",
      "ai_summary": "Thinking with Generated Images allows large multimodal models to generate and critique intermediate visual steps, enhancing visual reasoning capabilities and achieving significant improvements in complex scenarios.",
      "ai_keywords": [
        "LMMs",
        "visual reasoning",
        "chain-of-thought",
        "vision generation",
        "intermediate visual subgoals",
        "self-critique",
        "multis-object scenarios",
        "biochemists",
        "architects",
        "forensic analysts",
        "basketball players"
      ]
    },
    "publishedAt": "2025-05-28T12:12:45.000Z",
    "title": "Thinking with Generated Images",
    "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22525.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64bb5f9d8e051085bace4d1e",
      "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
      "fullname": "Ethan Chern",
      "name": "ethanchern",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22523",
      "authors": [
        {
          "_id": "6837c1cb80fc90ca2d9e8153",
          "name": "Junwen Chen",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8154",
          "name": "Heyang Jiang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8155",
          "name": "Yanbin Wang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8156",
          "name": "Keming Wu",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8157",
          "name": "Ji Li",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8158",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e8159",
          "name": "Keiji Yanai",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e815a",
          "name": "Dong Chen",
          "hidden": false
        },
        {
          "_id": "6837c1cb80fc90ca2d9e815b",
          "name": "Yuhui Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T16:09:33.000Z",
      "submittedOnDailyAt": "2025-05-29T00:40:52.849Z",
      "title": "PrismLayers: 高品質の多層透明画像の開放データを用いた生成モデル",
      "submittedOnDailyBy": {
        "_id": "631f108bb45367a05fe74260",
        "avatarUrl": "/avatars/c20da6800b643728062712d9a2771648.svg",
        "isPro": false,
        "fullname": "Researcher",
        "user": "YuanYuhui",
        "type": "user"
      },
      "summary": "生成高质量的多层透明图像从文本提示中，可以解锁新的创意控制水平，使用户能够像编辑LLM的文本输出一样轻松地编辑每一层。然而，由于缺乏大型高质量的多层透明数据集，多层生成模型的发展落后于传统的文本到图像模型。在本文中，我们通过以下方式解决这一基本挑战：（i）发布第一个开放的、超高保真度的PrismLayers（PrismLayersPro）数据集，包含200K（20K）多层透明图像和准确的alpha mattes，（ii）引入一个无需训练的合成管道，使用现成的扩散模型按需生成此类数据，（iii）提供一个强大的、开源的多层生成模型ART+，其美学与现代文本到图像生成模型相匹配。关键技术贡献包括：LayerFLUX，擅长生成高质量的单层透明图像，带有准确的alpha mattes，以及MultiLayerFLUX，根据人类标注的语义布局将多个LayerFLUX输出组合成完整的图像。为了确保更高的质量，我们应用了严格的过滤阶段以去除伪影和语义不匹配，随后进行人工选择。在合成的PrismLayersPro上微调最先进的ART模型，得到ART+，在60%的头对头用户研究比较中优于原始的ART，甚至匹配FLUX.1-[dev]模型生成的图像的视觉质量。我们预计，我们的工作将为多层透明图像生成任务建立坚实的数据集基础，促进需要精确、可编辑且视觉引人注目的分层图像的研究和应用。",
      "upvotes": 4,
      "discussionId": "6837c1d180fc90ca2d9e82bc",
      "ai_summary": "The work introduces a dataset and model for generating high-quality, multi-layer transparent images using diffusion models and a novel synthesis pipeline.",
      "ai_keywords": [
        "PrismLayers",
        "diffusion models",
        "LayerFLUX",
        "MultiLayerFLUX",
        "alpha mattes",
        "semantic layout",
        "user study",
        "ART model",
        "FLUX.1-[dev]"
      ]
    },
    "publishedAt": "2025-05-28T12:09:33.000Z",
    "title": "PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image\n  Generative Models",
    "summary": "Generating high-quality, multi-layer transparent images from text prompts can\nunlock a new level of creative control, allowing users to edit each layer as\neffortlessly as editing text outputs from LLMs. However, the development of\nmulti-layer generative models lags behind that of conventional text-to-image\nmodels due to the absence of a large, high-quality corpus of multi-layer\ntransparent data. In this paper, we address this fundamental challenge by: (i)\nreleasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)\ndataset of 200K (20K) multilayer transparent images with accurate alpha mattes,\n(ii) introducing a trainingfree synthesis pipeline that generates such data on\ndemand using off-the-shelf diffusion models, and (iii) delivering a strong,\nopen-source multi-layer generation model, ART+, which matches the aesthetics of\nmodern text-to-image generation models. The key technical contributions\ninclude: LayerFLUX, which excels at generating high-quality single transparent\nlayers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple\nLayerFLUX outputs into complete images, guided by human-annotated semantic\nlayout. To ensure higher quality, we apply a rigorous filtering stage to remove\nartifacts and semantic mismatches, followed by human selection. Fine-tuning the\nstate-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which\noutperforms the original ART in 60% of head-to-head user study comparisons and\neven matches the visual quality of images generated by the FLUX.1-[dev] model.\nWe anticipate that our work will establish a solid dataset foundation for the\nmulti-layer transparent image generation task, enabling research and\napplications that require precise, editable, and visually compelling layered\nimagery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22523.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631f108bb45367a05fe74260",
      "avatarUrl": "/avatars/c20da6800b643728062712d9a2771648.svg",
      "fullname": "Researcher",
      "name": "YuanYuhui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22338",
      "authors": [
        {
          "_id": "6837c79576eac3fa930de19b",
          "name": "Hanyang Wang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19c",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19d",
          "name": "Chaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19e",
          "name": "Tianjun Mao",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de19f",
          "name": "Si Qin",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a0",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a1",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "6837c79576eac3fa930de1a2",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T13:23:49.000Z",
      "submittedOnDailyAt": "2025-05-29T01:04:31.053Z",
      "title": "Text2Grad: ネイティブ言語のフィードバックからの強化学習",
      "submittedOnDailyBy": {
        "_id": "654dbac9938fbf1e696be8aa",
        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
        "isPro": false,
        "fullname": "Chaoyun Zhang",
        "user": "vyokky",
        "type": "user"
      },
      "summary": "傳統のRLHFは、粗略なスカラーレベルの報酬を用いて言語モデルを最適化し、成功または失敗の詳細な理由を隠し、徐々に学習を進めない。最近の研究は、プロンプトや反省による記述的な評価をRLに追加し、解釈性を向上させるが、モデルパラメーターを触れない。私たちは、Text2Gradという言語的なフィードバックをスパンレベルの勾配に変換するRLパラダイムを介して、モデルの政策の誤り部分を直接改善する。これにより、フィードバックによる調整が精確であり、グローバル的なネッドがなくなる。Text2Gradは、(1)高品質なフィードバック注釈パイプライン、(2)詳細な報酬モデル、(3)スパンレベルの政策最適化器の3つの構成要素で実現される。要約、コード生成、問題解答の各領域で、Text2Gradはスカラーレベルの報酬のRLとプロンプトだけの基準と比べて、より高いタスクメトリックと詳細な解釈性を提供する。私たちの結果は、自然言語のフィードバックを勾配に変換した場合、詳細な政策最適化の強力な信号としての力を示している。私たちの方法のコードは、https://github.com/microsoft/Text2Gradに提供されている。",
      "upvotes": 4,
      "discussionId": "6837c79576eac3fa930de1dd",
      "ai_summary": "Text2Grad converts human textual feedback into span-level gradients to optimize language models precisely and efficiently.",
      "ai_keywords": [
        "RLHF",
        "reinforcement-learning",
        "free-form textual feedback",
        "span-level gradients",
        "token spans",
        "differentiable reward signals",
        "gradient updates",
        "span-level policy optimizer",
        "fine-grained reward model",
        "feedback-annotation pipeline"
      ]
    },
    "publishedAt": "2025-05-28T09:23:49.000Z",
    "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback",
    "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22338.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654dbac9938fbf1e696be8aa",
      "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
      "fullname": "Chaoyun Zhang",
      "name": "vyokky",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.22203",
      "authors": [
        {
          "_id": "6837dcc41448b8bf0c91fa30",
          "user": {
            "_id": "6462def82a83863b97c0611e",
            "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
            "isPro": false,
            "fullname": "Yuzhen Huang",
            "user": "yuzhen17",
            "type": "user"
          },
          "name": "Yuzhen Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:53.737Z",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa31",
          "name": "Weihao Zeng",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa32",
          "name": "Xingshan Zeng",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa33",
          "name": "Qi Zhu",
          "hidden": false
        },
        {
          "_id": "6837dcc41448b8bf0c91fa34",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T10:28:41.000Z",
      "submittedOnDailyAt": "2025-05-29T02:37:12.694Z",
      "title": "ルールやモデルベースのバリデーターの課題 -- 数学的推理における実例研究",
      "submittedOnDailyBy": {
        "_id": "6462def82a83863b97c0611e",
        "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
        "isPro": false,
        "fullname": "Yuzhen Huang",
        "user": "yuzhen17",
        "type": "user"
      },
      "summary": "信頼性の高いバリデーターは、可証明可能な報酬を持つ強化学習（RLVR）の成功には欠かせない。これは、DeepSeek-R1などの大規模な理由論モデルの核心的なメソッドである。数学的な理由論のような複雑な領域では、前の研究では、基準を基にしたバリデーターが強い理由論モデルの訓練に広く採用されていた。しかし、これらのバリデーターの信頼性と、その強化学習訓練プロセスに与える影響は、理解されていません。本研究では、数学的な理由論を場合研究として扱い、静的評価と強化学習訓練の両方で様々なバリデーターを詳細に分析します。まず、現在の開放ソースの基準バリデーターが、複数の通常の数学データセットでの異なるフォーマットでの等価な回答を認識しようとすることが難しいことを見出し、可視性の高いバリデーターが発生している。この制限は、RL訓練の性能に不利な影響を与え、政策モデルが強くなるにつれてその影響が明らかになります。次に、モデルバージンバリデーターを潜在的な解決策として検討します。静的評価では、モデルバージンバリデーターが顕著に高い認識精度を達成しますが、進める分析とRL訓練の結果からは、それが特定のパターンを正しいものとして間違って分類すること（すなわち、フォルシブルプオジャイド）に高度に脆弱であることが示されます。この脆弱性は、政策モデルの最適化の際に利用され、人工的に報酬を上昇させることになります。我々の発見は、基準バリデーターやモデルバージンバリデーターにおける固有のリスクに焦点を当て、強化学習の報酬システムの開発に有價値なヒントを提供することを目的としています。",
      "upvotes": 4,
      "discussionId": "6837dcc51448b8bf0c91fa54",
      "ai_summary": "The study examines the effectiveness and reliability of rule-based and model-based verifiers in reinforcement learning with verifiable reward, highlighting limitations and vulnerabilities in their use for mathematical reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable reward",
        "DeepSeek-R1",
        "mathematical reasoning",
        "rule-based verifiers",
        "reward systems",
        "model-based verifiers",
        "false negatives",
        "false positives"
      ]
    },
    "publishedAt": "2025-05-28T06:28:41.000Z",
    "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on\n  Mathematical Reasoning",
    "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22203.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6462def82a83863b97c0611e",
      "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
      "fullname": "Yuzhen Huang",
      "name": "yuzhen17",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22202",
      "authors": [
        {
          "_id": "6837eff312d1f7a138bd09b3",
          "name": "Hyeonbin Hwang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b4",
          "name": "Byeongguk Jeon",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b5",
          "name": "Seungone Kim",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b6",
          "name": "Jiyeon Kim",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b7",
          "name": "Hoyeon Chang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b8",
          "name": "Sohee Yang",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09b9",
          "name": "Seungpil Won",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09ba",
          "name": "Dohaeng Lee",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09bb",
          "name": "Youbin Ahn",
          "hidden": false
        },
        {
          "_id": "6837eff312d1f7a138bd09bc",
          "name": "Minjoon Seo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T10:28:35.000Z",
      "submittedOnDailyAt": "2025-05-29T04:00:37.847Z",
      "title": "Let's 逐句预测句子\n\nこれを返してください。",
      "submittedOnDailyBy": {
        "_id": "647eaaf61a1fcad2fdc5d1ef",
        "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
        "isPro": false,
        "fullname": "Hyeonbin Hwang ",
        "user": "hbin0701",
        "type": "user"
      },
      "summary": "自動回帰言語モデル（LM）は一つのトークンずつ生成しますが、人間の理由論は文、命題、および概念のより高レベルの抽象化を使用しています。この対照は中心的な問題を引き起こします：LMはそのように、構造化された意味ユニットで理由論を行うことができるでしょうか。本研究では、学習済みの表現を元に、この抽象的な理由論空間にライフタイプすることができるかどうかを調査します。ここでは、予測する次の文の継続的な埋め込みを自動回帰的に予測することで文の空間で動作する予測済みトークンレベルのLMを適応するフレームワークを提出します。古典的な表現学習にヒントを得た2つの埋め込みパラダイムを調査します：1）セマンティック埋め込み、自動エンコーディングにより表面意味を保存するもの；2）コンテキスト埋め込み、次の文予測により予測的構造をエンコードするもの。2つの推論モードで評価します：ディスクリティナイズされたモードでは、予測された埋め込みを文字列に解釈し再エンコードする；コンティニュースモードでは、埋め込み空間で完全に理由論を行い、効率的な効果を得ます。数学、ロジック、一般知識、プランニングの4つの領域で、コンティニュース推論でのコンテキスト埋め込みはChain-of-Thought（CoT）と競争的な性能を示し、平均で推論時のFLOPを半分に減らします。また、スケーラビリティとモジュール化アダプタションの早期の跡象を示します。最後に、潜在的なトラジェクトを可視化するために、SentenceLensという診断ツールを導入し、中間のモデル状態を読解可能な文に解釈することです。これらの結果から、学習済みのLMは潜在的な埋め込み空間内で抽象的、構造化された理由論により効果的に変換できることが示されます。",
      "upvotes": 4,
      "discussionId": "6837eff412d1f7a138bd0a3e",
      "ai_summary": "Pretrained language models can adapt to operate in sentence space, reason over structured semantic units, and achieve competitive performance with Chain-of-Thought while reducing inference-time FLOPs.",
      "ai_keywords": [
        "autoregressive language models",
        "semantic embeddings",
        "contextual embeddings",
        "next-sentence prediction",
        "Chain-of-Thought",
        "SentenceLens"
      ]
    },
    "publishedAt": "2025-05-28T06:28:35.000Z",
    "title": "Let's Predict Sentence by Sentence",
    "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22202.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647eaaf61a1fcad2fdc5d1ef",
      "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
      "fullname": "Hyeonbin Hwang ",
      "name": "hbin0701",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21876",
      "authors": [
        {
          "_id": "6837d80bf42b2aacfc26c460",
          "name": "Zun Wang",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c461",
          "name": "Jaemin Cho",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c462",
          "name": "Jialu Li",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c463",
          "name": "Han Lin",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c464",
          "user": {
            "_id": "652066649004117947e46ed6",
            "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg",
            "isPro": false,
            "fullname": "Jaehong Yoon",
            "user": "jaehong31",
            "type": "user"
          },
          "name": "Jaehong Yoon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:42:07.590Z",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c465",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "6837d80bf42b2aacfc26c466",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T01:45:26.000Z",
      "submittedOnDailyAt": "2025-05-29T02:15:48.194Z",
      "title": "EPiC: 適切なアンコール・ビデオガイドラインを用いた効率的なビデオカメラ制御学習",
      "submittedOnDailyBy": {
        "_id": "5ffe32d8942cf3533d364449",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
        "isPro": false,
        "fullname": "Jaemin Cho",
        "user": "j-min",
        "type": "user"
      },
      "summary": "最近の3Dカメラ制御手法は、ビデオディフュージョンモデル（VDM）において、アノテーションされたカメラトラジェクトリーに従って推定された点群から渲染されたアンカービデオを作成し、構造化された先驅としてディフュージョンモデルをガイドすることが多い。しかし、点群推定における固有の誤差は、アンカービデオの不正確性を招く。また、詳細なカメラトラジェクトリーのアノテーションの必要性は資源の要求を増加させる。これらの制限を解決するために、私たちは、費用の高いカメラトラジェクトリーのアノテーションを必要とせずに高品質のアンカービデオを自動的に構築する効率的かつ精密なカメラ制御学習フレームワークEPiCを紹介する。具體には、ビデオの最初のフレームの視覚性に基づいてソースビデオをマスクすることで、高度なアンカービデオを作成し、このアプローチは高度なアラインメントを確保し、カメラトラジェクトリーのアノテーションが不要となり、これは、どのような野生のビデオにも対して画像からビデオ（I2V）のトレーニングペアを生成することができる。また、私たちは、軽量の条件付きモジュールAnchor-ControlNetを紹介する。これは、事前学習されたVDMにアンカービデオガイドを統合するための、主干モデルパラメーターの1%以下を使用する。このフレームワークは、通常のディフュージョンモデルの主干に変更を加える必要がなく、渲染の不正確性を軽減するための必要なパラメーター、トレーニングステップ、データの使用量を大幅に減少することで、効率的なトレーニングを実現する。そのため、EPiCは、I2Vカメラ制御タスクにおいてRealEstate10KとMiraDataで最先端の性能を達成し、定量的および定性的に精密かつ強固なカメラ制御能力を示す。特に、EPiCは、推論時に点群を使用して作成されたアンカービデオに対しても強固に一般化し、精密な3D情報に基づくカメラ制御を可能にし、また、ビデオからビデオのシーンにおいても強力なゼロシート一般化を示す。",
      "upvotes": 4,
      "discussionId": "6837d810f42b2aacfc26c5ec",
      "projectPage": "https://zunwang1.github.io/Epic",
      "githubRepo": "https://github.com/wz0919/EPiC",
      "ai_summary": "EPiC is a framework for efficient 3D camera control in video diffusion models that constructs high-quality anchor videos through first-frame visibility masking, integrates them using a lightweight ControlNet module, and achieves state-of-the-art performance on I2V tasks with minimal resources.",
      "ai_keywords": [
        "anchor videos",
        "point cloud estimation",
        "camera trajectories",
        "diffusion models",
        "first-frame visibility",
        "EPiC",
        "ControlNet",
        "I2V training pairs",
        "rendering misalignments",
        "RealEstate10K",
        "MiraData"
      ]
    },
    "publishedAt": "2025-05-27T21:45:26.000Z",
    "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video\n  Guidance",
    "summary": "Recent approaches on 3D camera control in video diffusion models (VDMs) often\ncreate anchor videos to guide diffusion models as a structured prior by\nrendering from estimated point clouds following annotated camera trajectories.\nHowever, errors inherent in point cloud estimation often lead to inaccurate\nanchor videos. Moreover, the requirement for extensive camera trajectory\nannotations further increases resource demands. To address these limitations,\nwe introduce EPiC, an efficient and precise camera control learning framework\nthat automatically constructs high-quality anchor videos without expensive\ncamera trajectory annotations. Concretely, we create highly precise anchor\nvideos for training by masking source videos based on first-frame visibility.\nThis approach ensures high alignment, eliminates the need for camera trajectory\nannotations, and thus can be readily applied to any in-the-wild video to\ngenerate image-to-video (I2V) training pairs. Furthermore, we introduce\nAnchor-ControlNet, a lightweight conditioning module that integrates anchor\nvideo guidance in visible regions to pretrained VDMs, with less than 1% of\nbackbone model parameters. By combining the proposed anchor video data and\nControlNet module, EPiC achieves efficient training with substantially fewer\nparameters, training steps, and less data, without requiring modifications to\nthe diffusion model backbone typically needed to mitigate rendering\nmisalignments. Although being trained on masking-based anchor videos, our\nmethod generalizes robustly to anchor videos made with point clouds during\ninference, enabling precise 3D-informed camera control. EPiC achieves SOTA\nperformance on RealEstate10K and MiraData for I2V camera control task,\ndemonstrating precise and robust camera control ability both quantitatively and\nqualitatively. Notably, EPiC also exhibits strong zero-shot generalization to\nvideo-to-video scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21876.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5ffe32d8942cf3533d364449",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
      "fullname": "Jaemin Cho",
      "name": "j-min",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.18700",
      "authors": [
        {
          "_id": "6837cc29bbee677da741aba7",
          "name": "Chun Wang",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741aba8",
          "name": "Xiaoran Pan",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741aba9",
          "name": "Zihao Pan",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741abaa",
          "name": "Haofan Wang",
          "hidden": false
        },
        {
          "_id": "6837cc29bbee677da741abab",
          "name": "Yiren Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-24T13:48:57.000Z",
      "submittedOnDailyAt": "2025-05-29T01:25:51.194Z",
      "title": "GRE Suite: 地域位置推定による地理的推定インフェリクションを行うための視覚言語モデルの微調校と強化された理由鏈",
      "submittedOnDailyBy": {
        "_id": "64311a95034ecbefddd141ef",
        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
        "isPro": true,
        "fullname": "Yiren Song",
        "user": "yiren98",
        "type": "user"
      },
      "summary": "最近のビジュアル・ラジオ言語モデル（VLMs）の進歩は、ビジュアル論理任務での卓越した性能を示しています。しかし、地理位置識別は特徴的な課題を持っています。画像から多粒度のビジュアルコープを抽出し、外部の世界知識と統合してシステム的な論理を行う必要があります。現在の地理位置識別タスクのアプローチは、強力的な論理機構と説明性を欠いています。これらの制限を解決するために、ビジュアル・ラジオ言語モデルを構造化された論理鍵鏈で拡張する新しいフレームワークを提案します。これは、正確かつ解釈可能な位置推論を実現するために、VLMsを強化します。GREシートは、データセット、モデル、ベンチマークの3つのキーディメンションでシステマティックに開発されています。まず、GRE30Kを紹介します。これは、ビジュアルとコンテキストの細かい分析を促進するために設計された高品質の地理位置識別論理データセットです。次に、GREモデルを紹介します。これは、多段階の論理戦略を用いて、スケーンの属性、局所の詳細、セマンティックの特徴を進歩的に推論し、精度の高い潜在的な地理的領域を特定することを通じて、地理位置識別の精度を向上させます。最後に、Geo Reason Evaluation Benchmark（GREval-Bench）を構築します。これは、VLMsが都市、自然、ランマークの多様なスケーンでの地理位置識別性能を評価するための詳細な評価フレームワークです。実験結果は、GREがすべての地理位置識別タスクの粒度で現在の方法を大幅に超えることを示し、複雑な地理的推論での理由付きのVLMsの効果を強調しています。コードとデータは、https://github.com/Thorin215/GREで公開されます。",
      "upvotes": 3,
      "discussionId": "6837cc2abbee677da741abf5",
      "ai_summary": "The GRE Suite enhances Visual Language Models with structured reasoning chains, improving geo-localization tasks through a multi-stage strategy and comprehensive evaluation benchmark.",
      "ai_keywords": [
        "Visual Language Models",
        "geo-localization",
        "reasoning chains",
        "GRE30K",
        "GRE model",
        "GREval-Bench",
        "multi-stage reasoning",
        "scene attributes",
        "local details",
        "semantic features",
        "coarse-grained localization",
        "fine-grained localization"
      ]
    },
    "publishedAt": "2025-05-24T09:48:57.000Z",
    "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language\n  Models and Enhanced Reasoning Chains",
    "summary": "Recent advances in Visual Language Models (VLMs) have demonstrated\nexceptional performance in visual reasoning tasks. However, geo-localization\npresents unique challenges, requiring the extraction of multigranular visual\ncues from images and their integration with external world knowledge for\nsystematic reasoning. Current approaches to geo-localization tasks often lack\nrobust reasoning mechanisms and explainability, limiting their effectiveness.\nTo address these limitations, we propose the Geo Reason Enhancement (GRE)\nSuite, a novel framework that augments VLMs with structured reasoning chains\nfor accurate and interpretable location inference. The GRE Suite is\nsystematically developed across three key dimensions: dataset, model, and\nbenchmark. First, we introduce GRE30K, a high-quality geo-localization\nreasoning dataset designed to facilitate fine-grained visual and contextual\nanalysis. Next, we present the GRE model, which employs a multi-stage reasoning\nstrategy to progressively infer scene attributes, local details, and semantic\nfeatures, thereby narrowing down potential geographic regions with enhanced\nprecision. Finally, we construct the Geo Reason Evaluation Benchmark\n(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across\ndiverse urban, natural, and landmark scenes to measure both coarse-grained\n(e.g., country, continent) and fine-grained (e.g., city, street) localization\nperformance. Experimental results demonstrate that GRE significantly\noutperforms existing methods across all granularities of geo-localization\ntasks, underscoring the efficacy of reasoning-augmented VLMs in complex\ngeographic inference. Code and data will be released at\nhttps://github.com/Thorin215/GRE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18700.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64311a95034ecbefddd141ef",
      "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
      "fullname": "Yiren Song",
      "name": "yiren98",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17870",
      "authors": [
        {
          "_id": "6837f049023315653be65a88",
          "user": {
            "_id": "640f32f6ef5c6dcac8b094bd",
            "avatarUrl": "/avatars/89b95837666ad696fe1f10808e4619b0.svg",
            "isPro": false,
            "fullname": "Shaina Raza",
            "user": "Shainarazavi",
            "type": "user"
          },
          "name": "Shaina Raza",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-29T05:27:38.248Z",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a89",
          "name": "Rizwan Qureshi",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8a",
          "name": "Marcelo Lotif",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8b",
          "user": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "isPro": false,
            "fullname": "Aman Chadha",
            "user": "amanchadha",
            "type": "user"
          },
          "name": "Aman Chadha",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:40.928Z",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8c",
          "name": "Deval Pandya",
          "hidden": false
        },
        {
          "_id": "6837f049023315653be65a8d",
          "name": "Christos Emmanouilidis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T13:20:23.000Z",
      "submittedOnDailyAt": "2025-05-29T04:08:04.627Z",
      "title": "モデルも人間のようにビキンスが必要です：モデルインミュニシティマニューティングで偽りを撃破する",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "生成型AIモデルは、訓練コーパスに含まれる不実の情報を学習し、再現することが多い。この論文は、生物学的な免疫化と同様に、弱化された病原体に対する制御的な接触が免疫力を築くことを実現することを前提として、AIモデルは明示的にラベルされた不実事実の小さな、隔離されたセットに基礎的調整を行うことを「ワクチン」として提唱しています。これらのカレーレッドされた不実例は、調整の際に定期的に注入され、真実の入力に対する精度を保持する同時に、誤導的な主張を識別し、拒否するモデルの能力を強化します。具体的なケーススタディは、免疫化されたモデルが基準よりも大幅に少ない不実情報を生成することを示しています。私たちの知識によると、これは最初のトレーニングフレームワークで、事実検証された不実事実自身を規範的なワクチンとして扱い、将来の不実情報に対してモデルを強化するために、入力の変形や一般的な人間のフィードバックシグナルを依存しないものであることを示しています。また、私たちは、不実データの安全な使用を確保するための倫理的なガードラインとガバナンス制御を記述しています。モデルの免疫化は、AIシステムとの事実性に対応するための前積的なパラダイムを提供しています。",
      "upvotes": 3,
      "discussionId": "6837f04a023315653be65ac6",
      "ai_summary": "A generative AI model is fine-tuned with labeled falsehoods to reduce misinformation generation, analogous to biological immunization.",
      "ai_keywords": [
        "generative AI models",
        "misinformation",
        "fine-tuning",
        "labeled falsehoods",
        "immunization",
        "fact-checked falsehoods",
        "supervised vaccine"
      ]
    },
    "publishedAt": "2025-05-23T09:20:23.000Z",
    "title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat\n  Falsehoods",
    "summary": "Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.21191",
      "authors": [
        {
          "_id": "6837eb38f09a146728a4b80f",
          "name": "Junyan Zhang",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b810",
          "name": "Yubo Gao",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b811",
          "name": "Yibo Yan",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b812",
          "name": "Jungang Li",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b813",
          "name": "Zhaorui Hou",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b814",
          "name": "Sicheng Tao",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b815",
          "name": "Shuliang Liu",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b816",
          "name": "Song Dai",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b817",
          "name": "Yonghua Hei",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b818",
          "name": "Junzhuo Li",
          "hidden": false
        },
        {
          "_id": "6837eb38f09a146728a4b819",
          "name": "Xuming Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T13:40:28.000Z",
      "submittedOnDailyAt": "2025-05-29T03:36:16.801Z",
      "title": "インストラクション対応ニューロンとエクスパートの公開：LLMのインストラクション従い能力の分析的フレームワーク",
      "submittedOnDailyBy": {
        "_id": "64b76528fdb702b3d8641514",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/wCbEtpPEHRjS4sZSVK1gK.png",
        "isPro": false,
        "fullname": "Jungang Li",
        "user": "Jungang",
        "type": "user"
      },
      "summary": "LLMの微調節は、指示に従う能力を大幅に向上させたが、これらの向上を駆動する計算機構は理解されていません。本研究は、指示に特化された稀疏成分（ニューロンやMixture-of-Experts（MoE）アーキテクチャのエキスパート）を孤立し、分析して、LLMの計算をどのように再構成されているかをシステマティックに調査します。特に、HexaInstという、6つの異なるカテゴリを含むよりもよく調整された指示データセットを紹介し、SPARCOMという新しい解析フレームワークを提案します。このフレームワークは3つの主な貢献を含む：1）これらの稀疏成分を特定する方法、2）その機能的普遍性と独自性の評価、3）その変化のシステマティック比較。実験では、機能的普遍性、独自性、そして指示実行におけるこれらの成分の重要な役割を示します。微調節による変化と稀疏計算機構との関係を明らかにすることで、LLMが指示に従う行動を内部化することをより深いエンシングにすることを、信頼できるLLMのコミュニティに提供します。",
      "upvotes": 2,
      "discussionId": "6837eb39f09a146728a4b872",
      "ai_summary": "The study investigates the role of sparse computational components in the instruction-following capabilities of Large Language Models through systematic analysis and introduces HexaInst and SPARCOM for better understanding.",
      "ai_keywords": [
        "Large Language Models",
        "fine-tuning",
        "instruction-following",
        "HexaInst",
        "SPARCOM",
        "sparse components",
        "neurons",
        "Mixture-of-Experts",
        "instruction execution",
        "computational adaptations"
      ]
    },
    "publishedAt": "2025-05-27T09:40:28.000Z",
    "title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical\n  Framework for LLM's Instruction-Following Capabilities",
    "summary": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21191.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "64b76528fdb702b3d8641514",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/wCbEtpPEHRjS4sZSVK1gK.png",
      "fullname": "Jungang Li",
      "name": "Jungang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17507",
      "authors": [
        {
          "_id": "6833f24ed5c438959f7decf9",
          "user": {
            "_id": "619ef3f253061ce00477b09e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
            "isPro": false,
            "fullname": "Qiaosheng Chen",
            "user": "cqsss",
            "type": "user"
          },
          "name": "Qiaosheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-28T09:01:00.198Z",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfa",
          "name": "Kaijia Huang",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfb",
          "name": "Xiao Zhou",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfc",
          "name": "Weiqing Luo",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfd",
          "name": "Yuanning Cui",
          "hidden": false
        },
        {
          "_id": "6833f24ed5c438959f7decfe",
          "name": "Gong Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T06:00:20.000Z",
      "submittedOnDailyAt": "2025-05-29T00:53:38.693Z",
      "title": "ハウギングフェースキノロジーベースの推薦、分類、チェーンポイント基準評価",
      "submittedOnDailyBy": {
        "_id": "619ef3f253061ce00477b09e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
        "isPro": false,
        "fullname": "Qiaosheng Chen",
        "user": "cqsss",
        "type": "user"
      },
      "summary": "オープンソース機械学習（ML）リソースの急速な成長、モデルやデータセットなど、IR研究の加速に寄与しています。しかし、Hugging Faceやそれらのような既存のプラットフォームは、構造化された表現を明記的に利用していません、モデルの進化を追跡するや、関連するデータセットを推薦するなどの先進的なクエリと分析を制限しています。この空間を埋めるために、HuggingKG、Hugging Faceコミュニティから構築された最初の大規模な知識グラフを構築しました。260万ノードと620万エッジを持ち、領域特有の関係と豊富な文字的属性を捉えています。これにより、HuggingBenchという多タスクベンチマークを進めることができます。これは、リソース推薦、クラス分類、モデルの進化を追跡するなどのIRタスクに含まれます。私たちの実験は、HuggingKGの特徴と派生されたタスクの特徴を明らかにしました。両方のリソースは公開的に利用でき、オープンソースリソース共有と管理における研究の進歩を期待しています。",
      "upvotes": 2,
      "discussionId": "6833f24ed5c438959f7ded31",
      "githubRepo": "https://github.com/nju-websoft/HuggingBench",
      "ai_summary": "HuggingKG, a large-scale knowledge graph, enhances open source ML resource management by enabling advanced queries and analyses via HuggingBench.",
      "ai_keywords": [
        "knowledge graph",
        "resource recommendation",
        "classification",
        "tracing",
        "multi-task benchmark"
      ]
    },
    "publishedAt": "2025-05-23T02:00:20.000Z",
    "title": "Benchmarking Recommendation, Classification, and Tracing Based on\n  Hugging Face Knowledge Graph",
    "summary": "The rapid growth of open source machine learning (ML) resources, such as\nmodels and datasets, has accelerated IR research. However, existing platforms\nlike Hugging Face do not explicitly utilize structured representations,\nlimiting advanced queries and analyses such as tracing model evolution and\nrecommending relevant datasets. To fill the gap, we construct HuggingKG, the\nfirst large-scale knowledge graph built from the Hugging Face community for ML\nresource management. With 2.6 million nodes and 6.2 million edges, HuggingKG\ncaptures domain-specific relations and rich textual attributes. It enables us\nto further present HuggingBench, a multi-task benchmark with three novel test\ncollections for IR tasks including resource recommendation, classification, and\ntracing. Our experiments reveal unique characteristics of HuggingKG and the\nderived tasks. Both resources are publicly available, expected to advance\nresearch in open source resource sharing and management.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17507.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619ef3f253061ce00477b09e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
      "fullname": "Qiaosheng Chen",
      "name": "cqsss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15813",
      "authors": [
        {
          "_id": "68380974717461677df17514",
          "name": "Muquan Yu",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17515",
          "name": "Mu Nan",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17516",
          "name": "Hossein Adeli",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17517",
          "name": "Jacob S. Prince",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17518",
          "name": "John A. Pyles",
          "hidden": false
        },
        {
          "_id": "68380974717461677df17519",
          "name": "Leila Wehbe",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751a",
          "name": "Margaret M. Henderson",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751b",
          "name": "Michael J. Tarr",
          "hidden": false
        },
        {
          "_id": "68380974717461677df1751c",
          "user": {
            "_id": "64b6ce23dbbd1f2cdb624d56",
            "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
            "isPro": false,
            "fullname": "Andrew Luo",
            "user": "aluo-x",
            "type": "user"
          },
          "name": "Andrew F. Luo",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-29T07:16:44.495Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b6ce23dbbd1f2cdb624d56/vCXz4Xv4AgHNNIC1S8aYf.png"
      ],
      "publishedAt": "2025-05-21T17:59:41.000Z",
      "submittedOnDailyAt": "2025-05-29T05:45:43.922Z",
      "title": "メタ学習を用いた人間の高階視覚コーテックスのIn-Context Transformerモデルの学習",
      "submittedOnDailyBy": {
        "_id": "64b6ce23dbbd1f2cdb624d56",
        "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
        "isPro": false,
        "fullname": "Andrew Luo",
        "user": "aluo-x",
        "type": "user"
      },
      "summary": "ファインドバージョンのデータセットにプレトレーンされた人工神経ネットワークは、人間の神経反応との表現的な一致を驚くことがあるが、視覚皮質の画像計算モデルの学習は、個々レベルの大規模なfMRIデータセットを基にしている。高価な、時間的に強いされることで、新しい受験者や刺激に対するエンコーダーの一般化能力が限定されている。BraInCoRLは、新しい受験者や刺激に対しても追加の微調節を行わずに、記述的学習を用いてボクセルごとの神経反応を予測する。Transformerアーキテクチャを利用して、変数の数の画像刺激によって変形可能な条件を付与することで、複数の受験者に対するインデックスバイアスを学習する。訓練期間には、記述的学習に専念的にモデルを最適化し、画像特徴とボクセル活性化を共に条件付けることで、ボクセルごとの視覚皮質のモデルを直接生成することで、より優れた性能を示すことができる。BraInCoRLは、完全に新しい画像に対しても、既存のボクセルごとのエンコーダー設計を一致しているが、強いテスト時スケーリングバイアスを示す。モデルは、異なる受験者とfMRIデータの取得パラメータを使用した完全に新しい視覚fMRIデータセットにも一般化可能である。また、BraInCoRLは、視覚皮質の神経信号の解釈性を高めることを促進する。最後に、自然言語クエリからボクセル選択性への解釈的なマッピングを可能にしている。",
      "upvotes": 2,
      "discussionId": "68380977717461677df17638",
      "ai_summary": "BraInCoRL employs a transformer-based in-context learning approach to model higher visual cortex neural responses with few-shot examples, demonstrating superior performance and generalizability across new subjects, stimuli, and datasets.",
      "ai_keywords": [
        "functional representations",
        "higher visual cortex",
        "artificial neural networks",
        "fMRI datasets",
        "in-context learning",
        "transformer architecture",
        "inductive bias",
        "voxelwise neural responses",
        "image features",
        "voxel activations",
        "test-time scaling",
        "interpretability",
        "natural language queries",
        "voxel selectivity"
      ]
    },
    "publishedAt": "2025-05-21T13:59:41.000Z",
    "title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual\n  Cortex",
    "summary": "Understanding functional representations within higher visual cortex is a\nfundamental question in computational neuroscience. While artificial neural\nnetworks pretrained on large-scale datasets exhibit striking representational\nalignment with human neural responses, learning image-computable models of\nvisual cortex relies on individual-level, large-scale fMRI datasets. The\nnecessity for expensive, time-intensive, and often impractical data acquisition\nlimits the generalizability of encoders to new subjects and stimuli. BraInCoRL\nuses in-context learning to predict voxelwise neural responses from few-shot\nexamples without any additional finetuning for novel subjects and stimuli. We\nleverage a transformer architecture that can flexibly condition on a variable\nnumber of in-context image stimuli, learning an inductive bias over multiple\nsubjects. During training, we explicitly optimize the model for in-context\nlearning. By jointly conditioning on image features and voxel activations, our\nmodel learns to directly generate better performing voxelwise models of higher\nvisual cortex. We demonstrate that BraInCoRL consistently outperforms existing\nvoxelwise encoder designs in a low-data regime when evaluated on entirely novel\nimages, while also exhibiting strong test-time scaling behavior. The model also\ngeneralizes to an entirely new visual fMRI dataset, which uses different\nsubjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates\nbetter interpretability of neural signals in higher visual cortex by attending\nto semantically relevant stimuli. Finally, we show that our framework enables\ninterpretable mappings from natural language queries to voxel selectivity.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b6ce23dbbd1f2cdb624d56/vCXz4Xv4AgHNNIC1S8aYf.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15813.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b6ce23dbbd1f2cdb624d56",
      "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
      "fullname": "Andrew Luo",
      "name": "aluo-x",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.12667",
      "authors": [
        {
          "_id": "682dd41740c6417d995087de",
          "user": {
            "_id": "648dca31385b84261811505d",
            "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
            "isPro": false,
            "fullname": "Zihan Su",
            "user": "Sugewud",
            "type": "user"
          },
          "name": "Zihan Su",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-28T16:17:54.970Z",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087df",
          "name": "Xuerui Qiu",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e0",
          "name": "Hongbin Xu",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e1",
          "name": "Tangyu Jiang",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e2",
          "name": "Junhao Zhuang",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e3",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e4",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e5",
          "name": "Shengfeng He",
          "hidden": false
        },
        {
          "_id": "682dd41740c6417d995087e6",
          "name": "Fei Richard Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-19T03:31:31.000Z",
      "submittedOnDailyAt": "2025-05-29T01:30:41.124Z",
      "title": "Safe-Sora: 安全なグラフィックワーターマーキングによるテキストからビデオの生成",
      "submittedOnDailyBy": {
        "_id": "648dca31385b84261811505d",
        "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
        "isPro": false,
        "fullname": "Zihan Su",
        "user": "Sugewud",
        "type": "user"
      },
      "summary": "生成ビデオモデルの爆発的な成長は、AI生成内容の信頼性のある著作権保存の要求を増加させています。画像合成での人気があるにも関わらず、不可視的な生成ワーマーク付けはビデオ生成には大きく検討されていません。この空間を填えるために、私たちはSafe-Soraを提案します。これは、ビデオ生成プロセスに直接グラフィカルなワーマークを埋め込む最初のフレームワークです。水印とカバー内容の視覚的な類似性による水印性能の関係を観察に基づいて、私たちはヒューリスティックな粗から細への適応的なマッチング機構を導入します。特に、ワーマーク画像はパッチに分割され、最も視覚的に類似したビデオフレームに割り当てられ、フレーム内の最適な空間的な領域にさらに局在化されます。ビデオフレーム間のワーマークパッチの空間時間的なフュージョンを可能にするために、私たちは新しい空間時間的な局在的なサンプリング戦略を備えた3Dワブレット変換拡張マンバーアーキテクチャを開発します。これは、水印の埋め込みと取り出しにおける長距離依存関係を有効にモデル化することで、効率的かつ強固な水印プロテクションの新しい道を開きます。私たちの知られている限り、状態空間モデルを水印に適用する最初の試みであり、効率的かつ強固な水印プロテクションの新しい道を開きます。詳細な実験は、ビデオ品質、水印の忠実性、および強固性について最先端の性能を達成したことを示し、これは私たちの提案に大きく貢献されています。公開に伴い、私たちのコードをリリースします。",
      "upvotes": 2,
      "discussionId": "682dd41840c6417d99508847",
      "projectPage": "https://sugewud.github.io/Safe-Sora-project/",
      "githubRepo": "https://github.com/Sugewud/Safe-Sora",
      "ai_summary": "Safe-Sora embeds invisible watermarks into AI-generated videos using a hierarchical adaptive matching mechanism and a 3D wavelet transform-enhanced Mamba architecture, achieving top performance in video quality, watermark fidelity, and robustness.",
      "ai_keywords": [
        "generative watermarking",
        "hierarchical coarse-to-fine adaptive matching",
        "3D wavelet transform",
        "Mamba architecture",
        "spatiotemporal local scanning",
        "state space models",
        "watermark embedding",
        "watermark retrieval"
      ]
    },
    "publishedAt": "2025-05-18T23:31:31.000Z",
    "title": "Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking",
    "summary": "The explosive growth of generative video models has amplified the demand for\nreliable copyright preservation of AI-generated content. Despite its popularity\nin image synthesis, invisible generative watermarking remains largely\nunderexplored in video generation. To address this gap, we propose Safe-Sora,\nthe first framework to embed graphical watermarks directly into the video\ngeneration process. Motivated by the observation that watermarking performance\nis closely tied to the visual similarity between the watermark and cover\ncontent, we introduce a hierarchical coarse-to-fine adaptive matching\nmechanism. Specifically, the watermark image is divided into patches, each\nassigned to the most visually similar video frame, and further localized to the\noptimal spatial region for seamless embedding. To enable spatiotemporal fusion\nof watermark patches across video frames, we develop a 3D wavelet\ntransform-enhanced Mamba architecture with a novel spatiotemporal local\nscanning strategy, effectively modeling long-range dependencies during\nwatermark embedding and retrieval. To the best of our knowledge, this is the\nfirst attempt to apply state space models to watermarking, opening new avenues\nfor efficient and robust watermark protection. Extensive experiments\ndemonstrate that Safe-Sora achieves state-of-the-art performance in terms of\nvideo quality, watermark fidelity, and robustness, which is largely attributed\nto our proposals. We will release our code upon publication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12667.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648dca31385b84261811505d",
      "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
      "fullname": "Zihan Su",
      "name": "Sugewud",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.22645",
      "authors": [
        {
          "_id": "6837dbed1233747046da00f5",
          "name": "Hanjia Lyu",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f6",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f7",
          "name": "Jian Kang",
          "hidden": false
        },
        {
          "_id": "6837dbed1233747046da00f8",
          "name": "Allison Koenecke",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T17:56:49.000Z",
      "submittedOnDailyAt": "2025-05-29T02:33:57.876Z",
      "title": "バイアスの特徴化：簡体字と傳統中国語での大規模言語モデルのベンチマーク",
      "submittedOnDailyBy": {
        "_id": "64c939307dba66c3a7e4d215",
        "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
        "isPro": false,
        "fullname": "BruceLyu",
        "user": "brucelyu",
        "type": "user"
      },
      "summary": "ラージェット・ラングジュージャーモデル（LLMs）の能力は、シンプルカタカナと標準カタカナの両方で研究されていますが、LLMsがこれらの書き言葉の両方で異なる性能を示すかどうかはまだ不明です。この理解は重要です、LLMsの回答の質の差異が、シンプルカタカナと標準カタカナの文化背景を無視して象徴的な被害を続け、教育や雇用などの領域でLLMsを利用した決定を悪化させることによって、下流の被害を拡大することがあります。LLMsの性能の差異を調査するために、私たちは実世界的なスケーナリオを反映する2つのベンチマークタスクを設計します：地域的な用語選択（LLMsをシンプルカタカナと標準カタカナで異なるものを命名するよう促す）と地域的な名前選択（LLMsをシンプルカタカナと標準カタカナの名前のリストから雇用する人を選択するよう促す）。両タスクでは、11つの先進的な商用LLMsサービスとオープンソースモデルの性能を評価します。これらのモデルは主に英語、シンプルカタカナ、または標準カタカナで訓練されています。私たちの分析は、LLMsの回答に存在するバイアスは、仕事と促す言語に依存していることを示しています。地域的な用語選択タスクでは、ほとんどのLLMsはシンプルカタカナの回答を偏り、一方で地域的な名前選択タスクでは、標準カタカナの名前を偏りました。これらの差異は、訓練データの表現、書き言葉の好み、シンプルカタカナと標準カタカナのトークナリゼーションの違いによって起こることがあると考えられます。これらの発見は、LLMsのバイアスに関する進めた分析が必要であることを示し、私たちは、将来のLLMsの行動を中国語の変体に跨いだ可複製的な評価を促すためのオープンソースデータセットを提供します（https://github.com/brucelyu17/SC-TC-Bench）。",
      "upvotes": 1,
      "discussionId": "6837dbee1233747046da0125",
      "githubRepo": "https://github.com/brucelyu17/SC-TC-Bench",
      "ai_summary": "Research examines LLM performance biases between Simplified and Traditional Chinese in regional term and name choice tasks, attributing differences to training data and tokenization.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "LLM-facilitated decision-making",
        "regional term choice",
        "regional name choice",
        "open-sourced benchmark dataset",
        "SC-TC-Bench"
      ]
    },
    "publishedAt": "2025-05-28T13:56:49.000Z",
    "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese",
    "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22645.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c939307dba66c3a7e4d215",
      "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
      "fullname": "BruceLyu",
      "name": "brucelyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21960",
      "authors": [
        {
          "_id": "6837dd74ec10479b9605da15",
          "user": {
            "_id": "637e1cf4f09bf2498c543a73",
            "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
            "isPro": false,
            "fullname": "Senmao Li",
            "user": "senmaonk",
            "type": "user"
          },
          "name": "Senmao Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:51.659Z",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da16",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da17",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da18",
          "name": "Tao Liu",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da19",
          "name": "Jiehang Xie",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1a",
          "name": "Joost van de Weijer",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1b",
          "name": "Fahad Shahbaz Khan",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1c",
          "name": "Shiqi Yang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1d",
          "name": "Yaxing Wang",
          "hidden": false
        },
        {
          "_id": "6837dd74ec10479b9605da1e",
          "name": "Jian Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-28T04:23:22.000Z",
      "submittedOnDailyAt": "2025-05-29T02:44:09.323Z",
      "title": "ワンワイズチケット：時間無依存な統一エンコーダーを用いたテキストから画像への拡散モデルの収納",
      "submittedOnDailyBy": {
        "_id": "637e1cf4f09bf2498c543a73",
        "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
        "isPro": false,
        "fullname": "Senmao Li",
        "user": "senmaonk",
        "type": "user"
      },
      "summary": "Text-to-Image (T2I) 拡散モデルは生成モデリングにおいて驚異的な進展を遂げていますが、推論速度と画像質の補減関係があり、効率的な機械の扱いにおいて課題があります。既存の結合された T2I モデルは少ないサンプリングステップで高品質の画像を生成できますが、特に一ステップモデルでは多様性と質に欠点があります。私たちの分析から、UNet エンコーダーに冗長な計算があることが見出されました。私たちの発見は、T2I 拡散モデルではデコーダーが豊富なより明確な語意情報を捉えるのに優れていることを示し、エンコーダーは異なる時間ステップからの多様なデコーダー間で共有できることができることを示しています。これらの見つけに基づき、私たちは学生モデルの UNet アーキテクチャに対して最初の時間依存性なしの統一エンコーダー TiUE を導入します。TiUE はループなしの画像生成アプローチであり、拡散モデルの結合を行うための一ステップシナプスを使用して、複数のデコーダー時間ステップでエンコーダーの特徴を共有し、並列サンプリングを可能にし、推論時間複雑さを大幅に削減します。また、KL ディバリジエンス項を追加して、ノイズ予測を正規化し、生成画像の視覚的写実性と多様性を向上させます。実験結果は、TiUE が最先端の方法を超え、LCM、SD-Turbo、SwiftBrushv2 を含む、より多様かつ写実な結果を生成しながら計算効率を維持することを示しています。",
      "upvotes": 1,
      "discussionId": "6837dd78ec10479b9605db06",
      "githubRepo": "https://github.com/sen-mao/Loopfree",
      "ai_summary": "Time-independent Unified Encoder TiUE reduces inference time and improves diversity and quality in Text-to-Image diffusion models by sharing encoder features across decoder time steps.",
      "ai_keywords": [
        "Text-to-Image diffusion models",
        "inference speed",
        "image quality",
        "distilled T2I models",
        "UNet encoders",
        "decoders",
        "semantic information",
        "Time-independent Unified Encoder TiUE",
        "loop-free image generation",
        "one-pass scheme",
        "parallel sampling",
        "KL divergence",
        "perceptual realism",
        "LCM",
        "SD-Turbo",
        "SwiftBrushv2"
      ]
    },
    "publishedAt": "2025-05-28T00:23:22.000Z",
    "title": "One-Way Ticket:Time-Independent Unified Encoder for Distilling\n  Text-to-Image Diffusion Models",
    "summary": "Text-to-Image (T2I) diffusion models have made remarkable advancements in\ngenerative modeling; however, they face a trade-off between inference speed and\nimage quality, posing challenges for efficient deployment. Existing distilled\nT2I models can generate high-fidelity images with fewer sampling steps, but\noften struggle with diversity and quality, especially in one-step models. From\nour analysis, we observe redundant computations in the UNet encoders. Our\nfindings suggest that, for T2I diffusion models, decoders are more adept at\ncapturing richer and more explicit semantic information, while encoders can be\neffectively shared across decoders from diverse time steps. Based on these\nobservations, we introduce the first Time-independent Unified Encoder TiUE for\nthe student model UNet architecture, which is a loop-free image generation\napproach for distilling T2I diffusion models. Using a one-pass scheme, TiUE\nshares encoder features across multiple decoder time steps, enabling parallel\nsampling and significantly reducing inference time complexity. In addition, we\nincorporate a KL divergence term to regularize noise prediction, which enhances\nthe perceptual realism and diversity of the generated images. Experimental\nresults demonstrate that TiUE outperforms state-of-the-art methods, including\nLCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results\nwhile maintaining the computational efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21960.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637e1cf4f09bf2498c543a73",
      "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
      "fullname": "Senmao Li",
      "name": "senmaonk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.20715",
      "authors": [
        {
          "_id": "6837eef0e237f02cd3c87963",
          "name": "Fuwen Luo",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87964",
          "name": "Shengfeng Lou",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87965",
          "name": "Chi Chen",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87966",
          "name": "Ziyue Wang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87967",
          "name": "Chenliang Li",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87968",
          "name": "Weizhou Shen",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c87969",
          "name": "Jiyue Guo",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796a",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796b",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796c",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796d",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "6837eef0e237f02cd3c8796e",
          "name": "Yang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T04:50:07.000Z",
      "submittedOnDailyAt": "2025-05-29T03:57:26.118Z",
      "title": "MUSEG: 時間スタンプに関心を持つマルチセグメントギャラピングを通じての映像の時間的な理解を強化する",
      "submittedOnDailyBy": {
        "_id": "642086ed290342c5df85662d",
        "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
        "isPro": false,
        "fullname": "Chi Chen",
        "user": "carboncoo",
        "type": "user"
      },
      "summary": "映像の時系列理解は、多モーダル大語言モデル（MLLMs）で映像内のイベントを理由にするために重要です。最近の映像理解の進歩にもかかわらず、現在のMLLMsは微妙な時系列理由について難しいことがあります。強化学習（RL）はこの問題を解決するために最近に検討されていますが、現在のRLアプローチは効果が限られています。本研究では、時系列理解を高めるために時間ステップに関心を持つ多段階ギャラフローディングを導入する新しいRLベースの方法、MUSEGを提案します。MUSEGは、MLLMsが複数の関連する映像セグメントとクエリをアラインすることで、より詳細な時系列理由を促進します。効果的な学習を促進するために、時系列ギャラフローディングに向けて段階的な報酬を与えるカスタマイズされたRL学習レシピを設計しました。時系列ギャラフローと時間適応的映像QAタスクにおいて極めて広範囲で実験を行い、MUSEGは現在の方法よりも显著に優れていて、多様な時系列理解シナリオでも一般化性能が良く、このプロジェクトはhttps://github.com/THUNLP-MT/MUSEGで確認できます。",
      "upvotes": 1,
      "discussionId": "6837eef1e237f02cd3c8798d",
      "githubRepo": "https://github.com/THUNLP-MT/MUSEG",
      "ai_summary": "MUSEG, an RL-based method with timestamp-aware multi-segment grounding, significantly enhances the temporal understanding of large language models by improving alignment with video segments and demonstrating superior performance in temporal reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "timestamp-aware multi-segment grounding",
        "temporal understanding",
        "MUSEG",
        "phased rewards",
        "temporal grounding",
        "time-sensitive video QA"
      ]
    },
    "publishedAt": "2025-05-27T00:50:07.000Z",
    "title": "MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware\n  Multi-Segment Grounding",
    "summary": "Video temporal understanding is crucial for multimodal large language models\n(MLLMs) to reason over events in videos. Despite recent advances in general\nvideo understanding, current MLLMs still struggle with fine-grained temporal\nreasoning. While reinforcement learning (RL) has been explored to address this\nissue recently, existing RL approaches remain limited in effectiveness. In this\nwork, we propose MUSEG, a novel RL-based method that enhances temporal\nunderstanding by introducing timestamp-aware multi-segment grounding. MUSEG\nenables MLLMs to align queries with multiple relevant video segments, promoting\nmore comprehensive temporal reasoning. To facilitate effective learning, we\ndesign a customized RL training recipe with phased rewards that progressively\nguides the model toward temporally grounded reasoning. Extensive experiments on\ntemporal grounding and time-sensitive video QA tasks demonstrate that MUSEG\nsignificantly outperforms existing methods and generalizes well across diverse\ntemporal understanding scenarios. View our project at\nhttps://github.com/THUNLP-MT/MUSEG.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20715.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642086ed290342c5df85662d",
      "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
      "fullname": "Chi Chen",
      "name": "carboncoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21060",
      "authors": [
        {
          "_id": "683818841902f641cc669774",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "683818841902f641cc669775",
          "name": "Xiang Liu",
          "hidden": false
        },
        {
          "_id": "683818841902f641cc669776",
          "name": "Peidong Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T11:47:15.000Z",
      "submittedOnDailyAt": "2025-05-29T06:49:42.295Z",
      "title": "スタイル3R: 任意シーンとスタイルに対するインスタント3Dスタイリズム再構築",
      "submittedOnDailyBy": {
        "_id": "5f1158120c833276f61f1a84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
        "isPro": false,
        "fullname": "Niels Rogge",
        "user": "nielsr",
        "type": "user"
      },
      "summary": "スタイリングされた3Dスケーンを瞬時に作成し、多点一致性を維持し、スタイリング画像に忠実に似ていることは重要な課題です。現在の最先端の3Dスタイリング方法は、計算量の多いテストタイム最適化を使用して、芸術的な特徴を学習済みの3D表現に転送することが一般的です。これは、密集ポーズされた入力画像が必要となります。対照的に、最近の前向き再構成モデルの進歩を活用し、無ポーズされたスプースビューのスケーン画像と任意のスタイル画像を使用して、1秒以内に直接の3Dスタイリングを実現する新しいアプローチを示します。再構成とスタイリングの固有の連結を解決するために、構造モデリングと外観シェーディングを分離する枝分けアーキテクチャを導入し、スタイリングの傾向が3Dスケーンの構造を歪めることを防ぎます。また、識別度の損失を適用し、新視点合成タスクを通じてスタイリングモデルの予え学習を促進します。この戦略は、スタイリングに専門化されることを許可しながら、元の再構成能力を維持することができます。データセットのフィールド内とフィールド外での詳細な評価は、我々のアプローチがスタイルとスケーンの外観のよい統合を達成し、多点一致性と効率性において現在の方法を上回ることを示しています。",
      "upvotes": 0,
      "discussionId": "6838188b1902f641cc669947",
      "ai_summary": "A novel feed-forward model achieves fast 3D stylization using sparse view images, maintaining multi-view consistency and high-quality style transfer while retaining reconstruction accuracy.",
      "ai_keywords": [
        "feed-forward reconstruction models",
        "3D stylization",
        "dense posed input images",
        "unposed sparse-view scene images",
        "branched architecture",
        "structure modeling",
        "appearance shading",
        "identity loss",
        "novel view synthesis",
        "in-domain datasets",
        "out-of-domain datasets",
        "multi-view consistency"
      ]
    },
    "publishedAt": "2025-05-27T07:47:15.000Z",
    "title": "Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and\n  Styles",
    "summary": "Stylizing 3D scenes instantly while maintaining multi-view consistency and\nfaithfully resembling a style image remains a significant challenge. Current\nstate-of-the-art 3D stylization methods typically involve computationally\nintensive test-time optimization to transfer artistic features into a\npretrained 3D representation, often requiring dense posed input images. In\ncontrast, leveraging recent advances in feed-forward reconstruction models, we\ndemonstrate a novel approach to achieve direct 3D stylization in less than a\nsecond using unposed sparse-view scene images and an arbitrary style image. To\naddress the inherent decoupling between reconstruction and stylization, we\nintroduce a branched architecture that separates structure modeling and\nappearance shading, effectively preventing stylistic transfer from distorting\nthe underlying 3D scene structure. Furthermore, we adapt an identity loss to\nfacilitate pre-training our stylization model through the novel view synthesis\ntask. This strategy also allows our model to retain its original reconstruction\ncapabilities while being fine-tuned for stylization. Comprehensive evaluations,\nusing both in-domain and out-of-domain datasets, demonstrate that our approach\nproduces high-quality stylized 3D content that achieve a superior blend of\nstyle and scene appearance, while also outperforming existing methods in terms\nof multi-view consistency and efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21060.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 874
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.21582",
      "authors": [
        {
          "_id": "68380c860fb1ddbe91ba0bf9",
          "user": {
            "_id": "631f5035c6b20f03c823c4ba",
            "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
            "isPro": false,
            "fullname": "Christopher Knievel",
            "user": "CKnievel",
            "type": "user"
          },
          "name": "Christopher Knievel",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:12.986Z",
          "hidden": false
        },
        {
          "_id": "68380c860fb1ddbe91ba0bfa",
          "name": "Alexander Bernhardt",
          "hidden": false
        },
        {
          "_id": "68380c860fb1ddbe91ba0bfb",
          "name": "Christian Bernhardt",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-27T10:07:05.000Z",
      "submittedOnDailyAt": "2025-05-29T06:18:15.842Z",
      "title": "AITEE -- 電気工学向けアウトロードターマー",
      "submittedOnDailyBy": {
        "_id": "631f5035c6b20f03c823c4ba",
        "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
        "isPro": false,
        "fullname": "Christopher Knievel",
        "user": "CKnievel",
        "type": "user"
      },
      "summary": "智能チューターシステムと大規模な言語モデルの組み合わせは、学生の多様な需要を満たし、自立学習を促進する可能性のあるアプローチです。大規模な言語モデルは電気工学の基本知識についてはよく知っていますが、電気回路に関する特定の質問を解決する能力は十分ではありません。本論文では、学習プロセスを通じて学生をそばにいって、個別的なサポートを提供し、自立学習を促進するための電気工学向けのアガントベースチューターシステムAITEEを紹介します。AITEEは手書きやデジタル回路をサポートし、自然的な学生との相互作用を可能にします。新しいグラフベースの類似性測定法は、レクチャーマテリアルから適切なコンテキストを特定するための検索アウガンド生成アプローチを用います。また、並列スパイスシミュレーションは解決方法論の適用精度を向上させます。システムはソクラティックディアロギーを実装し、ガイドドブリングの質問によって学習者の自立性を育成します。実験的評価により、AITEEは領域専門的な知識の適用において基準的なアプローチよりも显著に優れていることが示され、中サイズのLLMモデルでも可愛い性能を示しています。我々の結果は、電気工学教育におけるスケーラブルで、個別的で、効果的な学習環境を提供するアガントチューターの可能性を高めます。",
      "upvotes": 0,
      "discussionId": "68380c870fb1ddbe91ba0c55",
      "ai_summary": "An agent-based tutoring system for electrical engineering enhances learning through natural circuit interaction, context-retrieving generation, and guided questioning, demonstrating superior performance compared to baseline methods.",
      "ai_keywords": [
        "agent-based tutoring system",
        "large language models",
        "electrical circuits",
        "adapted circuit reconstruction",
        "graph-based similarity measure",
        "retrieval augmented generation",
        "parallel Spice simulation",
        "Socratic dialogue"
      ]
    },
    "publishedAt": "2025-05-27T06:07:05.000Z",
    "title": "AITEE -- Agentic Tutor for Electrical Engineering",
    "summary": "Intelligent tutoring systems combined with large language models offer a\npromising approach to address students' diverse needs and promote\nself-efficacious learning. While large language models possess good\nfoundational knowledge of electrical engineering basics, they remain\ninsufficiently capable of addressing specific questions about electrical\ncircuits. In this paper, we present AITEE, an agent-based tutoring system for\nelectrical engineering designed to accompany students throughout their learning\nprocess, offer individualized support, and promote self-directed learning.\nAITEE supports both hand-drawn and digital circuits through an adapted circuit\nreconstruction process, enabling natural interaction with students. Our novel\ngraph-based similarity measure identifies relevant context from lecture\nmaterials through a retrieval augmented generation approach, while parallel\nSpice simulation further enhances accuracy in applying solution methodologies.\nThe system implements a Socratic dialogue to foster learner autonomy through\nguided questioning. Experimental evaluations demonstrate that AITEE\nsignificantly outperforms baseline approaches in domain-specific knowledge\napplication, with even medium-sized LLM models showing acceptable performance.\nOur results highlight the potential of agentic tutors to deliver scalable,\npersonalized, and effective learning environments for electrical engineering\neducation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21582.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631f5035c6b20f03c823c4ba",
      "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
      "fullname": "Christopher Knievel",
      "name": "CKnievel",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.18149",
      "authors": [
        {
          "_id": "6837e51d05c81fd7d7d1962e",
          "user": {
            "_id": "63ca499104c97982831127ec",
            "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
            "isPro": false,
            "fullname": "Aradhye Agarwal",
            "user": "aradhye",
            "type": "user"
          },
          "name": "Aradhye Agarwal",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-29T07:41:47.337Z",
          "hidden": false
        },
        {
          "_id": "6837e51d05c81fd7d7d1962f",
          "name": "Ayan Sengupta",
          "hidden": false
        },
        {
          "_id": "6837e51d05c81fd7d7d19630",
          "name": "Tanmoy Chakraborty",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-23T17:57:43.000Z",
      "submittedOnDailyAt": "2025-05-29T06:15:54.434Z",
      "title": "初期終了検索：大規模言語モデルの効率的なテスト時スケーリング",
      "submittedOnDailyBy": {
        "_id": "63ca499104c97982831127ec",
        "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
        "isPro": false,
        "fullname": "Aradhye Agarwal",
        "user": "aradhye",
        "type": "user"
      },
      "summary": "テスト時スケーリング（TTS）は、推論時に計算量の動的割り当てを行うことで、大規模言語モデルの推論性能を向上させる有望な方法です。既存のTTS手法は効果的ですが、長い解码パスを必要とすることや、大量のサンプルを生成する必要があり、トークン使用量と推論時間が増加します。我々は、理由任務では短いトレースが長いトレースよりも正確なことが非常に多いことを見出しました。これに基づき、トレーニング不要な並行解码戦略であるFirst Finish Search（FFS）を導入しました。FFSはn個の独立サンプルを開始し、その中のどれかが完成したらすぐに返します。FFSは、DeepSeek-R1、R1-Distill-Qwen-32B、QwQ-32B、Phi-4-Reasoning-Plusの4つの理由モデルとAIME24、AIME25-I、AIME25-II、GPQA Diamondの4つのデータセットで、簡単な解码、ビームサーチ、多数決、バジュート迫強りと同時に評価しました。DeepSeek-R1では、FFSはAIMEデータセットで82.23%の正確率を達成し、DeepSeek-R1の単独正確率に対して15%の向上を収め、OpenAIのo4-miniと近似した性能を示しました。理論的な分析では、最短トレースで停止することが正しい答えを得ることがあることを説明し、早期停止が最適でない条件を特定しました。FFSのエレガンスと簡単は、簡単なTTS戦略が驚異的な性能を示すことを示し、推論時に発揮できる簡単なアプローチの潜力を明らかにしました。",
      "upvotes": 0,
      "discussionId": "6837e51d05c81fd7d7d19659",
      "ai_summary": "First Finish Search improves accuracy in large language models by stopping inference at the first completed sample, significantly outperforming other decoding strategies in reasoning tasks.",
      "ai_keywords": [
        "Test-time scaling",
        "TTS",
        "dynamic allocation",
        "inference",
        "reasoning tasks",
        "First Finish Search",
        "FFS",
        "parallel decoding",
        "decoding strategies",
        "beam search",
        "majority voting",
        "budget forcing",
        "accuracy",
        "performance",
        "inference latency",
        "early stopping"
      ]
    },
    "publishedAt": "2025-05-23T13:57:43.000Z",
    "title": "First Finish Search: Efficient Test-Time Scaling in Large Language\n  Models",
    "summary": "Test-time scaling (TTS), which involves dynamic allocation of compute during\ninference, offers a promising way to improve reasoning in large language\nmodels. While existing TTS methods work well, they often rely on long decoding\npaths or require a large number of samples to be generated, increasing the\ntoken usage and inference latency. We observe the surprising fact that for\nreasoning tasks, shorter traces are much more likely to be correct than longer\nones. Motivated by this, we introduce First Finish Search (FFS), a\ntraining-free parallel decoding strategy that launches n independent samples\nand returns as soon as any one completes. We evaluate FFS alongside simple\ndecoding, beam search, majority voting, and budget forcing on four reasoning\nmodels (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and\nacross four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With\nDeepSeek-R1, FFS achieves 82.23% accuracy on the AIME datasets, a 15%\nimprovement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's\no4-mini performance. Our theoretical analysis explains why stopping at the\nshortest trace is likely to yield a correct answer and identifies the\nconditions under which early stopping may be suboptimal. The elegance and\nsimplicity of FFS demonstrate that straightforward TTS strategies can perform\nremarkably well, revealing the untapped potential of simple approaches at\ninference time.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18149.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ca499104c97982831127ec",
      "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
      "fullname": "Aradhye Agarwal",
      "name": "aradhye",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]