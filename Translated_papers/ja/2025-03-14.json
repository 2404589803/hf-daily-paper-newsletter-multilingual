[
  {
    "paper": {
      "id": "2503.10613",
      "authors": [
        {
          "_id": "67d393ca336d57afb21bbf63",
          "user": {
            "_id": "67a99ec47b754f038d110926",
            "avatarUrl": "/avatars/e1ff318a42ccb75b094bbe7dae0cabec.svg",
            "isPro": false,
            "fullname": "Advait Gupta",
            "user": "advaitgupta",
            "type": "user"
          },
          "name": "Advait Gupta",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:36.855Z",
          "hidden": false
        },
        {
          "_id": "67d393ca336d57afb21bbf64",
          "user": {
            "_id": "672f89e6d7f4171f374dacea",
            "avatarUrl": "/avatars/4a8b378e13e862586bb428fdf000b3cc.svg",
            "isPro": false,
            "fullname": "NandaKiran Velaga",
            "user": "nandakiran09",
            "type": "user"
          },
          "name": "NandaKiran Velaga",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:34.327Z",
          "hidden": false
        },
        {
          "_id": "67d393ca336d57afb21bbf65",
          "name": "Dang Nguyen",
          "hidden": false
        },
        {
          "_id": "67d393ca336d57afb21bbf66",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:39.157Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:55:45.000Z",
      "submittedOnDailyAt": "2025-03-14T01:33:20.201Z",
      "title": "CoSTAast: コストシンシェージティーターゲットアェントである、多ターン画像編集用ツールパスアェント",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "テキストから画像へのモデル（例：Stable Diffusion および DALLE-3）は、多段階画像編集に関してまだ困難を見出しています。このタスクを効果的なアグェントワークフロー（パス）として分解し、AI ツールの組み合わせを利用して順番に複数のサブタスクを解決することを考えます。伝統的な探索アルゴリズムは、ツールパスを見つけるために費用の高い探索が必要です。一方、大規模言語モデル（LLMs）は、サブタスクの計画に関する先行知識を持っていますが、各サブタスクで使用するツールの能力と費用の正確な評価を行うことができることはないかもしれません。LLMs とグラフ探索の強みを統合して、費用効率的なツールパスを見つけることはできるかどうか？「CoSTA*」という三段階アプローチを提案し、LLMs を利用してサブタスクの木を構築し、タスクに対する AI ツールのグラフを削減し、その後小さなサブグラフに対して A* 探索を行います。タスク全体の費用と質のバランスをより良くするために、CoSTA* は各サブタスクのツールの両方の評価基準を統合し、A* 探索をガイドします。各サブタスクの出力は、視覚言語モデル（VLM）で評価され、失敗が起きた場合は、サブタスクのツールの費用と質を更新します。そのため、A* 探索は失敗から迅速に回復し、他のパスを探索することができます。また、CoSTA* は、サブタスク間でモデライズを自動的に切り替え、より良い費用と質のバランスを実現することができます。新しい難しい多段階画像編集ベンチマークを構築し、CoSTA* はこのベンチマークで、両方費用と質において最先端の画像編集モデルやアグェントを上回り、ユーザーの好みに応じた多様なバランスを実現します。",
      "upvotes": 32,
      "discussionId": "67d393cf336d57afb21bc0db",
      "githubRepo": "https://github.com/tianyi-lab/CoSTAR",
      "ai_keywords": [
        "text-to-image models",
        "stable diffusion",
        "DALLE-3",
        "multi-turn image editing",
        "agentic workflow",
        "tool use",
        "subtasks",
        "AI tools",
        "cost-efficient",
        "large language models (LLMs)",
        "subtask planning",
        "graph search",
        "three-stage approach",
        "CoSTA*",
        "subtask tree",
        "pruning",
        "A* search",
        "subgraph",
        "cost-quality trade-off",
        "vision-language model (VLM)",
        "failure",
        "total cost",
        "quality",
        "modality switching",
        "benchmark",
        "state-of-the-art image-editing models",
        "user preference"
      ]
    },
    "publishedAt": "2025-03-13T13:55:45.000Z",
    "title": "CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing",
    "summary": "Text-to-image models like stable diffusion and DALLE-3 still struggle with\nmulti-turn image editing. We decompose such a task as an agentic workflow\n(path) of tool use that addresses a sequence of subtasks by AI tools of varying\ncosts. Conventional search algorithms require expensive exploration to find\ntool paths. While large language models (LLMs) possess prior knowledge of\nsubtask planning, they may lack accurate estimations of capabilities and costs\nof tools to determine which to apply in each subtask. Can we combine the\nstrengths of both LLMs and graph search to find cost-efficient tool paths? We\npropose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask\ntree, which helps prune a graph of AI tools for the given task, and then\nconducts A* search on the small subgraph to find a tool path. To better balance\nthe total cost and quality, CoSTA* combines both metrics of each tool on every\nsubtask to guide the A* search. Each subtask's output is then evaluated by a\nvision-language model (VLM), where a failure will trigger an update of the\ntool's cost and quality on the subtask. Hence, the A* search can recover from\nfailures quickly to explore other paths. Moreover, CoSTA* can automatically\nswitch between modalities across subtasks for a better cost-quality trade-off.\nWe build a novel benchmark of challenging multi-turn image editing, on which\nCoSTA* outperforms state-of-the-art image-editing models or agents in terms of\nboth cost and quality, and performs versatile trade-offs upon user preference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10613.png",
    "numComments": 7,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10480",
      "authors": [
        {
          "_id": "67d38a42d3d16e1166d81bed",
          "user": {
            "_id": "64c3c631e77ea9f28111172a",
            "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
            "isPro": false,
            "fullname": "Siyin Wang",
            "user": "sinwang",
            "type": "user"
          },
          "name": "Siyin Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:44.686Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bee",
          "user": {
            "_id": "629ef8544313a7c1dd671130",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png",
            "isPro": false,
            "fullname": "Zhaoye Fei",
            "user": "ngc7293",
            "type": "user"
          },
          "name": "Zhaoye Fei",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:04.607Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bef",
          "name": "Qinyuan Cheng",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf0",
          "user": {
            "_id": "64196e45060a651c415d5cf7",
            "avatarUrl": "/avatars/71a43232a7bae851eca252782387a63d.svg",
            "isPro": false,
            "fullname": "Shiduo Zhang",
            "user": "CyberDJ",
            "type": "user"
          },
          "name": "Shiduo Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:20.440Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf1",
          "name": "Panpan Cai",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf2",
          "user": {
            "_id": "618497ea8aaadc9253c2dfa9",
            "avatarUrl": "/avatars/2eb3954a99f5aede6f31b8ae49b8c910.svg",
            "isPro": false,
            "fullname": "Fu Jinlan",
            "user": "Jinlan",
            "type": "user"
          },
          "name": "Jinlan Fu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:38.981Z",
          "hidden": false
        },
        {
          "_id": "67d38a42d3d16e1166d81bf3",
          "user": {
            "_id": "61457b8deff2c9fdb4de4988",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg",
            "isPro": false,
            "fullname": "Xipeng Qiu",
            "user": "xpqiu",
            "type": "user"
          },
          "name": "Xipeng Qiu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:46.041Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T15:49:56.000Z",
      "submittedOnDailyAt": "2025-03-14T01:42:40.120Z",
      "title": "世界モデリングはより良いプランナーを作る：構体化タスクプランニングの二重偏好最適化",
      "submittedOnDailyBy": {
        "_id": "64c3c631e77ea9f28111172a",
        "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
        "isPro": false,
        "fullname": "Siyin Wang",
        "user": "sinwang",
        "type": "user"
      },
      "summary": "最近の大規模な視覚言語モデル（LVLMs）の進展は、具象化タスク計画に望ましい可能性を示していますが、依存関係制約と効率性の問題に苦戦しています。既存のアプローチは、行動選択のみを最適化しているか、推論時に世界モデルを利用しているが、世界をモデル化することで計画能力を向上させることのメリットを見落としています。私たちは、状態予測と行動選択を共に偏好学習によって最適化する新しい学習フレームワーク「Dual Preference Optimization (D^2PO)」を提案します。これにより、LVLMsが環境の動力学を理解し、より良い計画を可能にします。人間の注釈を除いて軌跡とステップごとの偏好データを自動的に収集するために、試みとエラーを通じた拡大探索のための木探索機構を導入します。VoTa-Benchでの拡大的な実験は、D^2POに基づく方法がQwen2-VL (7B)、LLaVA-1.6 (7B)、LLaMA-3.2 (11B)に対して現在の方法やGPT-4oを大幅に超え、効率的な実行パスとともに上位のタスク成功率を達成することを示しています。",
      "upvotes": 25,
      "discussionId": "67d38a44d3d16e1166d81c54",
      "ai_keywords": [
        "Dual Preference Optimization (D$^2$PO)",
        "preference learning",
        "state prediction",
        "action selection",
        "environment dynamics",
        "tree search mechanism",
        "VoTa-Bench",
        "Qwen2-VL",
        "LLaVA-1.6",
        "LLaMA-3.2",
        "task success rates",
        "efficient execution paths"
      ]
    },
    "publishedAt": "2025-03-13T11:49:56.000Z",
    "title": "World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning",
    "summary": "Recent advances in large vision-language models (LVLMs) have shown promise\nfor embodied task planning, yet they struggle with fundamental challenges like\ndependency constraints and efficiency. Existing approaches either solely\noptimize action selection or leverage world models during inference,\noverlooking the benefits of learning to model the world as a way to enhance\nplanning capabilities. We propose Dual Preference Optimization (D^2PO), a new\nlearning framework that jointly optimizes state prediction and action selection\nthrough preference learning, enabling LVLMs to understand environment dynamics\nfor better planning. To automatically collect trajectories and stepwise\npreference data without human annotation, we introduce a tree search mechanism\nfor extensive exploration via trial-and-error. Extensive experiments on\nVoTa-Bench demonstrate that our D^2PO-based method significantly outperforms\nexisting methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and\nLLaMA-3.2 (11B), achieving superior task success rates with more efficient\nexecution paths.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10480.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "64c3c631e77ea9f28111172a",
      "avatarUrl": "/avatars/495dbb73b69c399bae780da3118e332f.svg",
      "fullname": "Siyin Wang",
      "name": "sinwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09669",
      "authors": [
        {
          "_id": "67d37754e07f664c7325f236",
          "user": {
            "_id": "63bbf972d8d676a2299cdb44",
            "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
            "isPro": false,
            "fullname": "Sangwon",
            "user": "agwmon",
            "type": "user"
          },
          "name": "Sangwon Jang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:49.038Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f237",
          "user": {
            "_id": "66c6edcc91dced946471bc13",
            "avatarUrl": "/avatars/55cc8593da6540e1566e1de9d7133f9f.svg",
            "isPro": false,
            "fullname": "June Suk Choi",
            "user": "wchoi403",
            "type": "user"
          },
          "name": "June Suk Choi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:05:59.636Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f238",
          "user": {
            "_id": "65e5bd4568234ef5d6decadc",
            "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
            "isPro": false,
            "fullname": "Jaehyeong Jo",
            "user": "harryjo97",
            "type": "user"
          },
          "name": "Jaehyeong Jo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:06:19.773Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f239",
          "user": {
            "_id": "635097ec59bfa9a85d4207b2",
            "avatarUrl": "/avatars/787085894e9e6538b6b3e3051efe9eea.svg",
            "isPro": false,
            "fullname": "Kimin Lee",
            "user": "kiminle2",
            "type": "user"
          },
          "name": "Kimin Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:06:26.649Z",
          "hidden": false
        },
        {
          "_id": "67d37754e07f664c7325f23a",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T17:21:57.000Z",
      "submittedOnDailyAt": "2025-03-14T02:05:42.787Z",
      "title": "サイレントブランドリングアタック：トリガー無しデータポイズニングアタックとしてのテキストから画像への拡散モデルに対する攻撃",
      "submittedOnDailyBy": {
        "_id": "63bbf972d8d676a2299cdb44",
        "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
        "isPro": false,
        "fullname": "Sangwon",
        "user": "agwmon",
        "type": "user"
      },
      "summary": "テキストから画像への拡散モデルは、テキストプライムから高品質な内容を生成するために驚異的な成功を収めています。しかし、公開で利用可能なデータの依存関係と、ファインチューニングのためのデータ共有の増加の趨勢は、これらのモデルが特にデータポイズニング攻撃に脆弱になるようになっています。本論文では、新しいデータポイズニング手法「サイレントブランディングアタック」を紹介します。この手法は、テキストから画像への拡散モデルを操作し、特定のブランドロゴや象徴を含む画像を生成するためのテキストトリガーがない場合も含む。我々は、特定の視覚パターンが訓練データに繰り返されると、モデルはそれらを自然に再現するように学習し、プライムについていない場合も含むことを見出しました。これを活用し、自動化されたデータポイズニングアルゴリズムを開発し、ロゴを無視できるように元画像にロゴを隠さず自然に混じり、検出されないようにします。このポイズンデータセットを用いて訓練されたモデルは、ロゴを含む画像を生成することができ、画像の質の低下やテキストの対位の崩れを避けることができます。我々は、大規模な高品質画像データセットとスタイルパーソナリゼーションデータセットの2つの実用的な設定で、特定のテキストトリガーがない場合も高い成功率を収めることを実験的に証明しました。人間評価とロゴ検出の定量的メトリックを含むことで、我々の方法がロゴを隠れて埋め込むことができることが証明されました。",
      "upvotes": 25,
      "discussionId": "67d37759e07f664c7325f3c5",
      "projectPage": "https://silent-branding.github.io/",
      "ai_keywords": [
        "text-to-image diffusion models",
        "high-quality contents",
        "text prompts",
        "data poisoning attacks",
        "Silent Branding Attack",
        "visual patterns",
        "data poisoning algorithm",
        "logos",
        "style personalization datasets",
        "logo detection"
      ]
    },
    "publishedAt": "2025-03-12T13:21:57.000Z",
    "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on\n  Text-to-Image Diffusion Models",
    "summary": "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality contents from text prompts. However, their reliance on publicly\navailable data and the growing trend of data sharing for fine-tuning make these\nmodels particularly vulnerable to data poisoning attacks. In this work, we\nintroduce the Silent Branding Attack, a novel data poisoning method that\nmanipulates text-to-image diffusion models to generate images containing\nspecific brand logos or symbols without any text triggers. We find that when\ncertain visual patterns are repeatedly in the training data, the model learns\nto reproduce them naturally in its outputs, even without prompt mentions.\nLeveraging this, we develop an automated data poisoning algorithm that\nunobtrusively injects logos into original images, ensuring they blend naturally\nand remain undetected. Models trained on this poisoned dataset generate images\ncontaining logos without degrading image quality or text alignment. We\nexperimentally validate our silent branding attack across two realistic\nsettings on large-scale high-quality image datasets and style personalization\ndatasets, achieving high success rates even without a specific text trigger.\nHuman evaluation and quantitative metrics including logo detection show that\nour method can stealthily embed logos.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbf972d8d676a2299cdb44",
      "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
      "fullname": "Sangwon",
      "name": "agwmon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10639",
      "authors": [
        {
          "_id": "67d3a632db36a4d5d95dbcff",
          "user": {
            "_id": "65b8724123d948d884b379b1",
            "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
            "isPro": false,
            "fullname": "Rongyao Fang",
            "user": "LucasFang",
            "type": "user"
          },
          "name": "Rongyao Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:14.110Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd00",
          "user": {
            "_id": "64a2b496e2e19de17db7de65",
            "avatarUrl": "/avatars/241448ca487833d6cc5d57bb1fdb6ee5.svg",
            "isPro": false,
            "fullname": "Duan Chengqi",
            "user": "gogoduan",
            "type": "user"
          },
          "name": "Chengqi Duan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:01.612Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd01",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd02",
          "user": {
            "_id": "65fc7c824d36be78e66ba92d",
            "avatarUrl": "/avatars/d4a55c820cae533f91724e062427516a.svg",
            "isPro": false,
            "fullname": "Linjiang Huang",
            "user": "LjHuang",
            "type": "user"
          },
          "name": "Linjiang Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:08.707Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd03",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd04",
          "user": {
            "_id": "65273fea0ef49cfb783fa5c1",
            "avatarUrl": "/avatars/0c9e204bc2151c8cc533311900d05a36.svg",
            "isPro": false,
            "fullname": "shilinyan",
            "user": "shilinyan",
            "type": "user"
          },
          "name": "Shilin Yan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:18.212Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd05",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd06",
          "user": {
            "_id": "666d4a0fe70e5838d95aebee",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6dkjoFA_sOjCkjvcvozZ5.jpeg",
            "isPro": false,
            "fullname": "zengxingyu",
            "user": "zengxingyu",
            "type": "user"
          },
          "name": "Xingyu Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:07:42.264Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd07",
          "name": "Rui Zhao",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd08",
          "user": {
            "_id": "64686f7172d9180d4ac8b4e4",
            "avatarUrl": "/avatars/db67dd6c4b2b41054ddcce5a18ade6f8.svg",
            "isPro": false,
            "fullname": "Jifeng Dai",
            "user": "daijifeng",
            "type": "user"
          },
          "name": "Jifeng Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:14:59.460Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd09",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:14:50.924Z",
          "hidden": false
        },
        {
          "_id": "67d3a632db36a4d5d95dbd0a",
          "user": {
            "_id": "65c04e9c27a5fdca81abcbd9",
            "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
            "isPro": false,
            "fullname": "Hongsheng LI",
            "user": "hsli-cuhk",
            "type": "user"
          },
          "name": "Hongsheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T09:14:43.402Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:59.000Z",
      "submittedOnDailyAt": "2025-03-14T02:16:04.349Z",
      "title": "GoT: ビジュアル生成と編集に向けた多模態大規模言語モデルの理由論能力の解放",
      "submittedOnDailyBy": {
        "_id": "65b8724123d948d884b379b1",
        "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
        "isPro": false,
        "fullname": "Rongyao Fang",
        "user": "LucasFang",
        "type": "user"
      },
      "summary": "現在の画像生成と編集手法は、テキストプロンプトを直接入力として処理し、視覚的な構成と明記的な操作に関する理由を考慮しない。私たちは、出力画像の前に明記的な言語理由論理プロセスを通じて生成と編集を可能にする新しいパラダイム「Generation Chain-of-Thought (GoT)」を提案します。このアプローチは、語義的関係と空間配置を分析するための理由論理ガイドフレームワークに変換し、価値を高める。GoTの定式化を定義し、語義的-空間関係を捉える詳細な理由チェーンを含む9M以上のサンプルを含む大規模なGoTデータセットを構築します。GoTの優れた側面を活用するために、Qwen2.5-VLを用いた理由チェーン生成と、私たちの新しい語義的-空間ガイドモジュールによって強化された端末からの拡散モデルを統合した一連のフレームワークを実装します。実験は、生成と編集タスクでベースラインより大幅な向上を示し、理由論理プロセスの明記的な変更を可能にし、画像の調整を正確に行うことができる相互作用的な可視化生成を可能にします。GoTは、理由論理をドライバーとした可視化生成と編集の新しい方向を開拓し、人間の意図によりより良い合わせの画像を生成することを可能にします。将来の研究のために、私たちはデータセット、コード、プレトレーンドモデルを公開します。",
      "upvotes": 21,
      "discussionId": "67d3a636db36a4d5d95dbdeb",
      "githubRepo": "https://github.com/rongyaofang/GoT",
      "ai_keywords": [
        "Generation Chain-of-Thought (GoT)",
        "text-to-image generation",
        "editing tasks",
        "reasoning chain generation",
        "end-to-end diffusion model",
        "Semantic-Spatial Guidance Module",
        "semantic relationships",
        "spatial arrangements",
        "large-scale GoT datasets",
        "detailed reasoning chains",
        "semantic-spatial relationships",
        "interactive visual generation",
        "reasoning steps",
        "human intent"
      ]
    },
    "publishedAt": "2025-03-13T13:59:59.000Z",
    "title": "GoT: Unleashing Reasoning Capability of Multimodal Large Language Model\n  for Visual Generation and Editing",
    "summary": "Current image generation and editing methods primarily process textual\nprompts as direct inputs without reasoning about visual composition and\nexplicit operations. We present Generation Chain-of-Thought (GoT), a novel\nparadigm that enables generation and editing through an explicit language\nreasoning process before outputting images. This approach transforms\nconventional text-to-image generation and editing into a reasoning-guided\nframework that analyzes semantic relationships and spatial arrangements. We\ndefine the formulation of GoT and construct large-scale GoT datasets containing\nover 9M samples with detailed reasoning chains capturing semantic-spatial\nrelationships. To leverage the advantages of GoT, we implement a unified\nframework that integrates Qwen2.5-VL for reasoning chain generation with an\nend-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance\nModule. Experiments show our GoT framework achieves excellent performance on\nboth generation and editing tasks, with significant improvements over\nbaselines. Additionally, our approach enables interactive visual generation,\nallowing users to explicitly modify reasoning steps for precise image\nadjustments. GoT pioneers a new direction for reasoning-driven visual\ngeneration and editing, producing images that better align with human intent.\nTo facilitate future research, we make our datasets, code, and pretrained\nmodels publicly available at https://github.com/rongyaofang/GoT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10639.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b8724123d948d884b379b1",
      "avatarUrl": "/avatars/ce189d1d8d688c17912f9b869035b2d0.svg",
      "fullname": "Rongyao Fang",
      "name": "LucasFang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10633",
      "authors": [
        {
          "_id": "67d3ba5e4d3a41ed9f8651eb",
          "user": {
            "_id": "630dd4218df86f1e5beb2ed7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
            "isPro": false,
            "fullname": "Eliahu Horwitz",
            "user": "Eliahu",
            "type": "user"
          },
          "name": "Eliahu Horwitz",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:04.270Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ec",
          "user": {
            "_id": "674ec6d1ce68874ee4f2d53b",
            "avatarUrl": "/avatars/4c15c9bdcf51d4bf5e6fceb86195e480.svg",
            "isPro": false,
            "fullname": "Nitzan Kurer",
            "user": "nitzankur",
            "type": "user"
          },
          "name": "Nitzan Kurer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T10:09:18.093Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ed",
          "user": {
            "_id": "6465fd33dac127ac80f0b334",
            "avatarUrl": "/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg",
            "isPro": false,
            "fullname": "Jonathan Kahana",
            "user": "jonkahana",
            "type": "user"
          },
          "name": "Jonathan Kahana",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T10:09:25.338Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ee",
          "user": {
            "_id": "669ffff5944b597ce2a1aa5b",
            "avatarUrl": "/avatars/559ca0ad82b1a52208510f09492fafa6.svg",
            "isPro": false,
            "fullname": "Liel Amar",
            "user": "LielAmar",
            "type": "user"
          },
          "name": "Liel Amar",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:01.634Z",
          "hidden": false
        },
        {
          "_id": "67d3ba5e4d3a41ed9f8651ef",
          "user": {
            "_id": "646cfc3b4220471ca0c56b20",
            "avatarUrl": "/avatars/19d6ab141ec2cd25c1c3b45fd8f69910.svg",
            "isPro": false,
            "fullname": "Yedid Hoshen",
            "user": "yedid",
            "type": "user"
          },
          "name": "Yedid Hoshen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-14T10:09:39.859Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/630dd4218df86f1e5beb2ed7/HClm12KfVuYMozbJaIp9_.png"
      ],
      "publishedAt": "2025-03-13T17:59:53.000Z",
      "submittedOnDailyAt": "2025-03-14T03:51:41.703Z",
      "title": "ハウギングフェイスのモデルアトラスをチャートとナビゲートする",
      "submittedOnDailyBy": {
        "_id": "630dd4218df86f1e5beb2ed7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
        "isPro": false,
        "fullname": "Eliahu Horwitz",
        "user": "Eliahu",
        "type": "user"
      },
      "summary": "現在、公開に利用可能なニューラルネットワークが数百万存在しているため、モデルのディレクトリを検索し分析することは重要になります。これらのモデルを様々なものから選ぶには、アトラスが必要ですが、ほとんどのモデルは記述が不足しており、このアトラスを作成するのは難しいです。モデルディレクトリの潜在的な可能性を探るために、私たちはHugging Faceの記述された部分を表す初步的なアトラスを作成します。これは、モデルの構造と進化を驚きの視覚化することができます。私たちは、このアトラスの記述された部分を使用して、モデルの属性（例：精度）を予測することや、コンピュータビジョンモデルのトレンドを分析することを示します。しかし、現在のアトラスは未完成のままで、私たちは未記述の領域をチャートする方法を提案します。特に、私たちは実世界的なモデルのトレーニングプラクティスに基づいた高信頼性の構造的な先驅を特定し、これらの先驅を活用して、アトラスの以前に記述されていなかった部分を正確にマッピングすることができるようにします。私たちは、データセット、コード、およびインタラクティブなアトラスを公開します。",
      "upvotes": 20,
      "discussionId": "67d3ba634d3a41ed9f86533a",
      "projectPage": "https://horwitz.ai/model-atlas",
      "githubRepo": "https://github.com/eliahuhorwitz/Model-Atlas",
      "ai_keywords": [
        "neural networks",
        "model repositories",
        "atlas",
        "model landscape",
        "model evolution",
        "predicting model attributes",
        "trends in computer vision models",
        "high-confidence structural priors",
        "dominant real-world model training practices",
        "interactive atlas"
      ]
    },
    "publishedAt": "2025-03-13T13:59:53.000Z",
    "title": "Charting and Navigating Hugging Face's Model Atlas",
    "summary": "As there are now millions of publicly available neural networks, searching\nand analyzing large model repositories becomes increasingly important.\nNavigating so many models requires an atlas, but as most models are poorly\ndocumented charting such an atlas is challenging. To explore the hidden\npotential of model repositories, we chart a preliminary atlas representing the\ndocumented fraction of Hugging Face. It provides stunning visualizations of the\nmodel landscape and evolution. We demonstrate several applications of this\natlas including predicting model attributes (e.g., accuracy), and analyzing\ntrends in computer vision models. However, as the current atlas remains\nincomplete, we propose a method for charting undocumented regions.\nSpecifically, we identify high-confidence structural priors based on dominant\nreal-world model training practices. Leveraging these priors, our approach\nenables accurate mapping of previously undocumented areas of the atlas. We\npublicly release our datasets, code, and interactive atlas.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/630dd4218df86f1e5beb2ed7/HClm12KfVuYMozbJaIp9_.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10633.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630dd4218df86f1e5beb2ed7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg",
      "fullname": "Eliahu Horwitz",
      "name": "Eliahu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09662",
      "authors": [
        {
          "_id": "67d3daf40034469b0d6cc872",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc873",
          "name": "Zikai Zhou",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc874",
          "name": "Dian Xie",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc875",
          "name": "Yuetong Fang",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc876",
          "name": "Tian Ye",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc877",
          "name": "Lichen Bai",
          "hidden": false
        },
        {
          "_id": "67d3daf40034469b0d6cc878",
          "name": "Zeke Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T15:15:25.000Z",
      "submittedOnDailyAt": "2025-03-14T06:07:49.038Z",
      "title": "CoRe^2: コレクト、リフレクト、リファインして、より良いおみくじを早く生成する",
      "submittedOnDailyBy": {
        "_id": "66015e8aa4d296af07de538e",
        "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
        "isPro": false,
        "fullname": "Ye",
        "user": "Owen777",
        "type": "user"
      },
      "summary": "テキストから画像（T2I）の生成モデルのサンプリングを両方とも高速かつ良いようにすることは、有望な研究方向として代表的である。過去の研究は、合成画像の視覚質量を向上させるためにサンプリング効率を負担にしたり、サンプリングを大幅に加速しながらベースモデルの生成能力を向上させなかったりしました。また、近くすべての推論方法は、ディフューションモデル（DMs）と視覚自動復元モデル（ARMs）の両方で安定した性能を確保することができませんでした。本論文では、新しいプラグインとパラマイト推論パラダイム、CoRe^2を紹介します。CoRe^2は、Collect、Reflect、Refineの3つのプロセスから構成されています。CoRe^2は最初にクラスファイラー無制限ガイダンス（CFG）のトラジェクトを収集し、その収集されたデータを用いて、容易に学習できる内容を反映しながら推論時の関数評価数を半分に減らした弱いモデルを訓練します。次に、CoRe^2は弱いガイダンスから強いガイダンスによる条件付き出力を精進し、基礎モデルが捉えやすい高周波と写実な内容の生成能力を向上させます。私たちの知識の限り、CoRe^2は、SDXL、SD3.5、FLUXなどのディフューションモデルとLlamaGenなどの視覚自動復元モデルの広い範囲で、両方とも効率と効果性を示していることは初めてです。HPD v2、Pick-of-Pic、Drawbench、GenEval、T2I-Compbenchにおいて、显著な性能向上が見られました。また、CoRe^2は最先端のZ-Samplingと無間雑な統合が可能で、PickScoreとAESでそれよりも0.3と0.16の改善を示し、SD3.5では5.64秒の時間削減を収めました。コードは、https://github.com/xie-lab-ml/CoRe/tree/mainに公開されています。",
      "upvotes": 20,
      "discussionId": "67d3dafb0034469b0d6ccac0",
      "ai_keywords": [
        "diffusion models (DMs)",
        "visual autoregressive models (ARMs)",
        "classifier-free guidance (CFG)",
        "HPD v2",
        "Pick-of-Pic",
        "Drawbench",
        "GenEval",
        "T2I-Compbench",
        "PickScore",
        "AES",
        "Z-Sampling",
        "SDXL",
        "SD3.5",
        "FLUX",
        "LlamaGen"
      ]
    },
    "publishedAt": "2025-03-12T11:15:25.000Z",
    "title": "CoRe^2: Collect, Reflect and Refine to Generate Better and Faster",
    "summary": "Making text-to-image (T2I) generative model sample both fast and well\nrepresents a promising research direction. Previous studies have typically\nfocused on either enhancing the visual quality of synthesized images at the\nexpense of sampling efficiency or dramatically accelerating sampling without\nimproving the base model's generative capacity. Moreover, nearly all inference\nmethods have not been able to ensure stable performance simultaneously on both\ndiffusion models (DMs) and visual autoregressive models (ARMs). In this paper,\nwe introduce a novel plug-and-play inference paradigm, CoRe^2, which comprises\nthree subprocesses: Collect, Reflect, and Refine. CoRe^2 first collects\nclassifier-free guidance (CFG) trajectories, and then use collected data to\ntrain a weak model that reflects the easy-to-learn contents while reducing\nnumber of function evaluations during inference by half. Subsequently, CoRe^2\nemploys weak-to-strong guidance to refine the conditional output, thereby\nimproving the model's capacity to generate high-frequency and realistic\ncontent, which is difficult for the base model to capture. To the best of our\nknowledge, CoRe^2 is the first to demonstrate both efficiency and effectiveness\nacross a wide range of DMs, including SDXL, SD3.5, and FLUX, as well as ARMs\nlike LlamaGen. It has exhibited significant performance improvements on HPD v2,\nPick-of-Pic, Drawbench, GenEval, and T2I-Compbench. Furthermore, CoRe^2 can be\nseamlessly integrated with the state-of-the-art Z-Sampling, outperforming it by\n0.3 and 0.16 on PickScore and AES, while achieving 5.64s time saving using\nSD3.5.Code is released at https://github.com/xie-lab-ml/CoRe/tree/main.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09662.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "66015e8aa4d296af07de538e",
      "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
      "fullname": "Ye",
      "name": "Owen777",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10622",
      "authors": [
        {
          "_id": "67d3b0a87443e648e8aa1ea6",
          "user": {
            "_id": "6552126dd8a8835b66653767",
            "avatarUrl": "/avatars/0b1dad9ebaeada8f5e7ebe453123960b.svg",
            "isPro": false,
            "fullname": "Jiachen Zhu",
            "user": "JiachenZhu",
            "type": "user"
          },
          "name": "Jiachen Zhu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T05:13:31.648Z",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1ea7",
          "name": "Xinlei Chen",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1ea8",
          "name": "Kaiming He",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1ea9",
          "name": "Yann LeCun",
          "hidden": false
        },
        {
          "_id": "67d3b0a87443e648e8aa1eaa",
          "name": "Zhuang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:06.000Z",
      "submittedOnDailyAt": "2025-03-14T02:59:49.783Z",
      "title": "Transformers without Normalization",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "正規化層は現代のニューラルネットワークではどこにもないが、長年にわたり重要視されてきました。本論文では、正規化を含まないTransformerが、非常に簡単な手法を用いて同様またはより良い性能を実現できることを示します。Dynamic Tanh（DyT）という要素ごとの操作DyT(x) = tanh(alpha x)をTransformerの正規化層のドロップイン置換として紹介します。DyTはTransformerの正規化層がS形の入出力マッピングを生成することを観察した上で、tanh関数のような性質をもって設計されました。DyTを採用することで、正規化を含まないTransformerは、ほとんどパラメータ調整を行わずに正規化されたツールコンペの性能を追い越すことができます。DyTを挟むTransformerの効果を認識、生成、監督学習、自動学習、コンピュータビジョン、言語モデルなど多様な設定で検証しました。これらの発見は、現代のニューラルネットワークで正規化層が不可欠であるという伝統的な理解を質疑し、深いネットワークでの正規化層の役割に新しい視点を提供します。",
      "upvotes": 16,
      "discussionId": "67d3b0a97443e648e8aa1f22",
      "ai_keywords": [
        "Dynamic Tanh (DyT)",
        "Transformers",
        "normalization layers",
        "layer normalization",
        "hyperparameter tuning",
        "supervised learning",
        "self-supervised learning",
        "computer vision",
        "language models"
      ]
    },
    "publishedAt": "2025-03-13T13:59:06.000Z",
    "title": "Transformers without Normalization",
    "summary": "Normalization layers are ubiquitous in modern neural networks and have long\nbeen considered essential. This work demonstrates that Transformers without\nnormalization can achieve the same or better performance using a remarkably\nsimple technique. We introduce Dynamic Tanh (DyT), an element-wise operation\nDyT(x) = tanh(alpha x), as a drop-in replacement for normalization\nlayers in Transformers. DyT is inspired by the observation that layer\nnormalization in Transformers often produces tanh-like, S-shaped input-output\nmappings. By incorporating DyT, Transformers without normalization can match or\nexceed the performance of their normalized counterparts, mostly without\nhyperparameter tuning. We validate the effectiveness of Transformers with DyT\nacross diverse settings, ranging from recognition to generation, supervised to\nself-supervised learning, and computer vision to language models. These\nfindings challenge the conventional understanding that normalization layers are\nindispensable in modern neural networks, and offer new insights into their role\nin deep networks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10622.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10596",
      "authors": [
        {
          "_id": "67d3a8950ada3dfbf617fc23",
          "name": "Rui Hu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc24",
          "name": "Lianghui Zhu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc25",
          "name": "Yuxuan Zhang",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc26",
          "name": "Tianheng Cheng",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc27",
          "name": "Lei Liu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc28",
          "name": "Heng Liu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc29",
          "name": "Longjin Ran",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc2a",
          "name": "Xiaoxin Chen",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc2b",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67d3a8950ada3dfbf617fc2c",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:43:10.000Z",
      "submittedOnDailyAt": "2025-03-14T02:31:30.611Z",
      "title": "GroundingSuite: 複雑な多粒度ピクセルゲーティングの評価",
      "submittedOnDailyBy": {
        "_id": "646b3db131968a60a01e4cf5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
        "isPro": false,
        "fullname": "Tianheng Cheng",
        "user": "wondervictor",
        "type": "user"
      },
      "summary": "Pixel groundingは、Referring Expression Segmentation (RES)などのタスクを含む、視覚と言語のモデル間の隙を埋める巨大な可能性を持って、相当な関注を集めています。しかし、現在の進展は、既存データセットにおける対象物カテゴリの限界、文脈の多様性の不足、高品質の注釈の不足などの制限によって制約されています。これらの制限を軽減するために、GroundingSuiteを紹介します。GroundingSuiteは以下の3つの構成要素を含みます：1) Vision-Language Model (VLM)アグエントを利用するデータ注釈のフレームワーク、2) 956万件の多様なReferring Expressionおよびその対応する分割を含む大規模な訓練データセット、3) 3,800枚の画像から構築された細かくカレントデータベースの評価ベンチマーク。GroundingSuiteの訓練データセットは、その上で訓練されたモデルが最先端の結果を実現することを可能にします。特に、gRefCOCOではcIoU 68.9、RefCOCOmではgIoU 55.3を達成します。また、GroundingSuiteの注釈フレームワークは、現在の先進的なデータ注釈方法と比べて、GLaMMより4.5倍速く効率的です。",
      "upvotes": 15,
      "discussionId": "67d3a8960ada3dfbf617fc8d",
      "ai_keywords": [
        "Referring Expression Segmentation (RES)",
        "Vision-Language Model (VLM)",
        "GroundingSuite",
        "cIoU",
        "gIoU",
        "gRefCOCO",
        "RefCOCOm",
        "GLaMM"
      ]
    },
    "publishedAt": "2025-03-13T13:43:10.000Z",
    "title": "GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding",
    "summary": "Pixel grounding, encompassing tasks such as Referring Expression Segmentation\n(RES), has garnered considerable attention due to its immense potential for\nbridging the gap between vision and language modalities. However, advancements\nin this domain are currently constrained by limitations inherent in existing\ndatasets, including limited object categories, insufficient textual diversity,\nand a scarcity of high-quality annotations. To mitigate these limitations, we\nintroduce GroundingSuite, which comprises: (1) an automated data annotation\nframework leveraging multiple Vision-Language Model (VLM) agents; (2) a\nlarge-scale training dataset encompassing 9.56 million diverse referring\nexpressions and their corresponding segmentations; and (3) a meticulously\ncurated evaluation benchmark consisting of 3,800 images. The GroundingSuite\ntraining dataset facilitates substantial performance improvements, enabling\nmodels trained on it to achieve state-of-the-art results. Specifically, a cIoU\nof 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the\nGroundingSuite annotation framework demonstrates superior efficiency compared\nto the current leading data annotation method, i.e., 4.5 times faster than\nthe GLaMM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10596.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646b3db131968a60a01e4cf5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
      "fullname": "Tianheng Cheng",
      "name": "wondervictor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 27
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10351",
      "authors": [
        {
          "_id": "67d39b35acb72b994659d4fd",
          "name": "Sinuo Liu",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d4fe",
          "name": "Chenyang Lyu",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d4ff",
          "name": "Minghao Wu",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d500",
          "name": "Longyue Wang",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d501",
          "name": "Weihua Luo",
          "hidden": false
        },
        {
          "_id": "67d39b35acb72b994659d502",
          "name": "Kaifu Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6527d8b077bceabaab382a75/3_8muRazw1wwHmG9IxRGk.png"
      ],
      "publishedAt": "2025-03-13T13:27:53.000Z",
      "submittedOnDailyAt": "2025-03-14T01:29:07.562Z",
      "title": "新しいモダンマシン翻訳のトレンドについて、大規模な理由論モデルを使用した方法を紹介します。",
      "submittedOnDailyBy": {
        "_id": "6527d8b077bceabaab382a75",
        "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
        "isPro": false,
        "fullname": "Chenyang Lyu",
        "user": "ChenyangLyu",
        "type": "user"
      },
      "summary": "最近の大規模推論モデル（LRMs）の進展、特にChain-of-Thought推論（CoT）を利用したものは、機械翻訳（MT）に新たな可能性を開拓しています。この立場論文は、LRMsが翻訳をコンテキスト的、文化的、言語的理解と推論のための動的な推論任務として再構成し、伝統的なニューラルMTやLLMsベースのMTパラダイムを大幅に変革したことを主張しています。私たちは3つの基盤的な変化を特定しています：1）コンテキスト的な一貫性、LRMsは文間や複雑なコンテキスト、またはコンテキストの欠落を明示的に推論することで不明確性を解決し、論理構造を保存しています；2）文化的な意図性、モデルが話者の意図、聴衆の期待、ソシャル言語規範を推論して出力を適応することができるようになりました；3）自覚、LRMsは推論時に潜在的な翻訳エラーを修正することができ、特に非常にノイズポラーな場合にもより良い強固性を示します。私たちは、スタイリズド翻訳、ドキュメントレベル翻訳、モデル多様化翻訳などの様々な翻訳シナリオを調査し、これらの実験例を示してLRMsの翻訳の上位性を示しています。また、LRMsの翻訳に関する興味深い現象や自動ピバート翻訳、翻訳の過度地域化、推論の効率性などの重要な課題を特定しています。結論において、私たちはLRMsが翻訳システムをテキスト変換器ではなく多言語的な認知アガントとして翻訳を再定義していることを考えています。このパラダイムの変化は、LRMsを利用して翻訳の問題をより広いコンテキストで考えることを促し、それによってもう一つの可能性を開拓できることを思い出します。",
      "upvotes": 14,
      "discussionId": "67d39b40acb72b994659d916",
      "ai_keywords": [
        "Chain-of-Thought reasoning (CoT)",
        "Large Reasoning Models (LRMs)",
        "Neural MT",
        "Contextual coherence",
        "Cultural intentionality",
        "Self-reflection",
        "Stylized translation",
        "Document-level translation",
        "Multimodal translation",
        "Auto-pivot translation",
        "Over-localisation",
        "Inference efficiency",
        "Multilingual cognitive agents"
      ]
    },
    "publishedAt": "2025-03-13T09:27:53.000Z",
    "title": "New Trends for Modern Machine Translation with Large Reasoning Models",
    "summary": "Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6527d8b077bceabaab382a75/3_8muRazw1wwHmG9IxRGk.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10351.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6527d8b077bceabaab382a75",
      "avatarUrl": "/avatars/69caacf9153dbf6a3796693a968b363f.svg",
      "fullname": "Chenyang Lyu",
      "name": "ChenyangLyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04723",
      "authors": [
        {
          "_id": "67d39576de5ce3cc428b1909",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190a",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190b",
          "user": {
            "_id": "637f228152229c63921119c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
            "isPro": false,
            "fullname": "Zhiqiang Hu",
            "user": "Zhiqiang007",
            "type": "user"
          },
          "name": "Zhiqing Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:31.682Z",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190c",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190d",
          "name": "Ming Shan Hee",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190e",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "67d39576de5ce3cc428b190f",
          "name": "Roy Ka-Wei Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T18:59:37.000Z",
      "submittedOnDailyAt": "2025-03-14T01:04:48.148Z",
      "title": "Shifting Long-Context LLMs Research from Input to Output",
      "submittedOnDailyBy": {
        "_id": "63369da91ba5d5ece24118a4",
        "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
        "isPro": false,
        "fullname": "wuyuhao",
        "user": "mozhu",
        "type": "user"
      },
      "summary": "最近の長文脈大語言モデル（LLM）の進展は、長文脈の読解に集中し、長文脈の理解について大きな進歩を遂げました。しかし、長文脈の出力生成という同等重要な面では、相対的に少しだけ注目を受けました。本論文は、NLP研究のパラダイムの変更を主張し、長文脈の出力生成の挑戦を解決することに向けています。新作の書き下ろし、長期的な計画、複雑な理由を要求するタスクは、モデルが極めて幅広い文脈を理解し、その中でコネクティブで文脈豊富で両立した長文脈の文章を生成することが必要です。これらの要求は、現在のLLMの能力における重要な欠点を明らかにします。この調査不足の領域の重要性を強調し、高品質の長文脈の出力を生成するための基盤的なLLMの開発に向けて集中的な努力を求めています。これらのモデルは、実世界的な応用において巨大なポテンシャルを持ちます。",
      "upvotes": 12,
      "discussionId": "67d39577de5ce3cc428b194f",
      "ai_keywords": [
        "long-context Large Language Models (LLMs)",
        "long-context comprehension",
        "long-output generation",
        "novel writing",
        "long-term planning",
        "complex reasoning",
        "coherent",
        "contextually rich",
        "logically consistent",
        "extended text",
        "high-quality",
        "long-form outputs"
      ]
    },
    "publishedAt": "2025-03-06T13:59:37.000Z",
    "title": "Shifting Long-Context LLMs Research from Input to Output",
    "summary": "Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04723.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63369da91ba5d5ece24118a4",
      "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
      "fullname": "wuyuhao",
      "name": "mozhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10582",
      "authors": [
        {
          "_id": "67d387ff45b17e31c16d05d1",
          "name": "Yiming Jia",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d2",
          "name": "Jiachen Li",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d3",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d4",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d5",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d6",
          "name": "Kai Zou",
          "hidden": false
        },
        {
          "_id": "67d387ff45b17e31c16d05d7",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-14T01:36:13.720Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/VBzj4fQkEBEzfx26BsANS.png"
      ],
      "publishedAt": "2025-03-13T17:32:48.000Z",
      "submittedOnDailyAt": "2025-03-14T00:47:38.699Z",
      "title": "VisualWebInstruct: ウェブ検索による多模構指令データの拡大",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "ビジョン・ラングワイズモデルは、多くの観察フォーカスタスクにおいて顕著な進歩を達していますが、理由フォーカスタスクにおいての進歩は、高品質と多様性のないトレーニングデータの不足によって限られているようです。本研究では、理由フォーカスの多モーダルデータセットの不足問題を解決することを目指しています。VisualWebInstructという新しいアプローチを提案します。これは、検索エンジンを利用して、数学、物理学、財務、化学などの複数の分野を横断する多様的で高品質なデータセットを作成することを目的としています。30,000枚の細かく選択されたシード画像から始め、Google Image Searchを使用して、類似画像を含むウェブサイトを特定します。700,000以上の異なるURLソースからのHTMLを集め、処理します。内容抽出、フィルタリングと合成のパイプラインを通じて、約900,000の質問答えペアを構築します。これにより、ビジョン・ワーブ・インストラクト上で微調節されたモデルは、以下のような性能向上を示します：1) Llava-OV-midからの訓練では、ベンチマーク上で10-20%の絶対点の収益を示し、2) MAmmoTH-VLからの訓練では、5%の絶対収益を示します。最良のモデルであるMAmmoTH-VL2は、10BパラメータクラスでMMMU-Pro-std（40.7%）、MathVerse（42.6%）、DynaMath（55.7%）で最先端の性能を示しています。これらの驚人的な結果は、VLMの複雑な多モーダルタスクの理由能力を向上させるデータセットの効果を明らかにしています。",
      "upvotes": 9,
      "discussionId": "67d3880d45b17e31c16d09d1",
      "projectPage": "https://tiger-ai-lab.github.io/VisualWebInstruct/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/VisualWebInstruct",
      "ai_keywords": [
        "Vision-Language Models",
        "VisualWebInstruct",
        "search engine",
        "question-answer pairs",
        "visual QA pairs",
        "text QA pairs",
        "fine-tuned",
        "Llava-OV-mid",
        "MAmmoTH-VL",
        "MAmmoTH-VL2",
        "MMMU-Pro-std",
        "MathVerse",
        "DynaMath"
      ]
    },
    "publishedAt": "2025-03-13T13:32:48.000Z",
    "title": "VisualWebInstruct: Scaling up Multimodal Instruction Data through Web\n  Search",
    "summary": "Vision-Language Models have made significant progress on many\nperception-focused tasks, however, their progress on reasoning-focused tasks\nseem to be limited due to the lack of high-quality and diverse training data.\nIn this work, we aim to address the scarcity issue of reasoning-focused\nmultimodal datasets. We propose VisualWebInstruct - a novel approach that\nleverages search engine to create a diverse, and high-quality dataset spanning\nmultiple disciplines like math, physics, finance, chemistry, etc. Starting with\nmeticulously selected 30,000 seed images, we employ Google Image search to\nidentify websites containing similar images. We collect and process the HTMLs\nfrom over 700K unique URL sources. Through a pipeline of content extraction,\nfiltering and synthesis, we build a dataset of approximately 900K\nquestion-answer pairs, with 40% being visual QA pairs and the rest as text QA\npairs. Models fine-tuned on VisualWebInstruct demonstrate significant\nperformance gains: (1) training from Llava-OV-mid shows 10-20% absolute point\ngains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain.\nOur best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B\nparameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath\n(55.7%). These remarkable results highlight the effectiveness of our dataset in\nenhancing VLMs' reasoning capabilities for complex multimodal tasks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/VBzj4fQkEBEzfx26BsANS.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10582.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10618",
      "authors": [
        {
          "_id": "67d3d2dec4a225b653154b3a",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3b",
          "name": "Rui Qian",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3c",
          "name": "Wenze Hu",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3d",
          "name": "Tsu-Jui Fu",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3e",
          "name": "Lezhi Li",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b3f",
          "name": "Bowen Zhang",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b40",
          "name": "Alex Schwing",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b41",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67d3d2dec4a225b653154b42",
          "name": "Yinfei Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:57:25.000Z",
      "submittedOnDailyAt": "2025-03-14T05:26:46.588Z",
      "title": "DiT-Air: ディフュージョンモデルアーキテクチャの効率性を再評価して、テキストから画像生成のデザインに関する研究",
      "submittedOnDailyBy": {
        "_id": "656c2fa772c19de72367bd69",
        "avatarUrl": "/avatars/540bb3d8a2afe2ef927b80d895cae28b.svg",
        "isPro": false,
        "fullname": "Alex Yang",
        "user": "yyf86",
        "type": "user"
      },
      "summary": "この研究では、Diffusion Transformers（DiTs）を用いたテキストから画像生成に関する実験的な研究を行い、構造的な選択、テキスト条件化戦略、トレーニングプロトコルに焦点を当てています。DiTsベースの様々な構造を評価し、直接の結合されたテキストとノイズの入力を処理する標準的なDiTsベースの構造と比較しています。驚くべきことに、標準的なDiTsの性能は、特化されたモデルと比較しても比較的に高いことがわかり、特にパラメータの効率性が高いことがわかります。層ごとのパラメータ共有戦略を活用し、MMDiT構造に比べて66%のモデルサイズ削減を実現し、最小限の性能の影響を受けるようにしました。テキストエンコーダとVariational Auto-Encoders（VAEs）などの重要な成分についての詳細な分析を基に、DiT-AirとDiT-Air-Liteを紹介しました。DiT-Airは、サブジェクトとレwardの微調節を用いて、GenEvalとT2I CompBenchで最先端の性能を達成し、DiT-Air-Liteは小さなサイズでも高度な競争力を示し、現在のモデルを超えます。",
      "upvotes": 8,
      "discussionId": "67d3d302c4a225b6531556d6",
      "ai_keywords": [
        "Diffusion Transformers (DiTs)",
        "text-to-image generation",
        "architectural choices",
        "text-conditioning strategies",
        "training protocols",
        "PixArt-style",
        "MMDiT variants",
        "concatenated text and noise inputs",
        "parameter-efficiency",
        "layer-wise parameter sharing strategy",
        "Variational Auto-Encoders (VAEs)",
        "DiT-Air",
        "DiT-Air-Lite",
        "supervised and reward fine-tuning",
        "GenEval",
        "T2I CompBench",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-03-13T13:57:25.000Z",
    "title": "DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture\n  Design in Text to Image Generation",
    "summary": "In this work, we empirically study Diffusion Transformers (DiTs) for\ntext-to-image generation, focusing on architectural choices, text-conditioning\nstrategies, and training protocols. We evaluate a range of DiT-based\narchitectures--including PixArt-style and MMDiT variants--and compare them with\na standard DiT variant which directly processes concatenated text and noise\ninputs. Surprisingly, our findings reveal that the performance of standard DiT\nis comparable with those specialized models, while demonstrating superior\nparameter-efficiency, especially when scaled up. Leveraging the layer-wise\nparameter sharing strategy, we achieve a further reduction of 66% in model size\ncompared to an MMDiT architecture, with minimal performance impact. Building on\nan in-depth analysis of critical components such as text encoders and\nVariational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With\nsupervised and reward fine-tuning, DiT-Air achieves state-of-the-art\nperformance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly\ncompetitive, surpassing most existing models despite its compact size.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10618.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656c2fa772c19de72367bd69",
      "avatarUrl": "/avatars/540bb3d8a2afe2ef927b80d895cae28b.svg",
      "fullname": "Alex Yang",
      "name": "yyf86",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10589",
      "authors": [
        {
          "_id": "67d3adc5c14480a46038cecf",
          "name": "Yuwei Guo",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced0",
          "name": "Ceyuan Yang",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced1",
          "name": "Ziyan Yang",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced2",
          "name": "Zhibei Ma",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced3",
          "name": "Zhijie Lin",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced4",
          "name": "Zhenheng Yang",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced5",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "67d3adc5c14480a46038ced6",
          "name": "Lu Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:40:07.000Z",
      "submittedOnDailyAt": "2025-03-14T02:47:38.511Z",
      "title": "長文脈調整による映画生成",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "最近の映像生成の進展は、スケーラブルなディフュージョントランスフォーマーを用いて、1分間の写実的な単一スラッシュ映像を生成できるようになった。しかし、実世界的なナレーティブ映像は、ショット間での視覚的および動的な一貫性を求める多スラッシュシーンが必要である。本論文では、長期コンテキストチューニング（LCT）を導入し、予め学習された単一スラッシュ映像のディフュージョンモデルのコンテキストウィンドウを拡大し、データから直接的にシーンレベルの一貫性を学習するティニングパラダイムを提案します。私たちの方法は、単一スラッシュごとの全注意機能を、シーン内の全スラッシュを拡大し、3D位置埋めと非同期ノイズスタラテジを組み合わせ、追加のパラメータを含めないように、共通的なおよび自動帰り的なスラッシュ生成を可能にします。LCT後のバイデリクションアテンションを持つモデルは、コンテキストカウスアテンションでのファイナルチューニングを行うことで、効率的なKVキャッシュを用いた自動帰り的な生成を可能にします。実験は、LCT後の単一スラッシュモデルが、一貫的な多スラッシュシーンを生成し、構成的な生成と相互作用ショット拡大などの新しい能力を示すことを示し、実用的な可視内容の作成に向けての道を開きます。詳細は、https://guoyww.github.io/projects/long-context-video/ を参照してください。",
      "upvotes": 6,
      "discussionId": "67d3adc7c14480a46038cf5e",
      "ai_keywords": [
        "scalable diffusion transformers",
        "context window",
        "scene-level consistency",
        "Long Context Tuning (LCT)",
        "full attention mechanisms",
        "interleaved 3D position embedding",
        "asynchronous noise strategy",
        "joint and auto-regressive shot generation",
        "bidirectional attention",
        "context-causal attention",
        "efficient KV-cache",
        "compositional generation",
        "interactive shot extension"
      ]
    },
    "publishedAt": "2025-03-13T13:40:07.000Z",
    "title": "Long Context Tuning for Video Generation",
    "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10589.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10437",
      "authors": [
        {
          "_id": "67d3adf57360ea908cf5f0bc",
          "name": "Wanhua Li",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0bd",
          "user": {
            "_id": "66f0d2036a483077eed42bfb",
            "avatarUrl": "/avatars/f7f3f726842c26b8e52c9bdd48774b8e.svg",
            "isPro": false,
            "fullname": "Renping Zhou",
            "user": "rpzhou",
            "type": "user"
          },
          "name": "Renping Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:08.159Z",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0be",
          "name": "Jiawei Zhou",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0bf",
          "name": "Yingwei Song",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c0",
          "name": "Johannes Herter",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c1",
          "name": "Minghan Qin",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c2",
          "name": "Gao Huang",
          "hidden": false
        },
        {
          "_id": "67d3adf57360ea908cf5f0c3",
          "name": "Hanspeter Pfister",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/H5DUFDgeiid82tHVuW9cx.mp4"
      ],
      "publishedAt": "2025-03-13T14:58:22.000Z",
      "submittedOnDailyAt": "2025-03-14T02:49:34.475Z",
      "title": "4D LangSplat: 4D言語ガウススプラットを多様化ラージング大語言モデルによって実現",
      "submittedOnDailyBy": {
        "_id": "658bb7e47459b6e471b9d2e6",
        "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
        "isPro": false,
        "fullname": "Wanhua Li",
        "user": "EthanTaylor",
        "type": "user"
      },
      "summary": "4D言語領域の学習は、動的なスケーンで時間適応的、開放範囲の言語クエリを実現するために不可欠です。LangSplatは、CLIP特徴を3Dガウス表現に基づいて、3D静的スケーンでの精度と効率を達成しましたが、CLIPは静的画像テキストタスク向けのため、映画の時系列動作を捉えることができません。実世界的な環境は、物の意味が時間によって変化することによって、動的です。4D言語領域の準確な構築には、現在の視覚モデルが難しいピクセル対応の物のビデオ特徴を得る必要があります。これらの課題を解決するために、4D LangSplatを提案します。4D LangSplatは、4D言語領域を学習して、動的なスケーンで時間適応的または時間無関係的な開放ボキャブラリークエリを効率的に処理することを目指しています。4D LangSplatは、視覚特徴からの言語領域の学習を回避し、物のビデオカプチャーから生成されたテキストをモデルとしてMultimodal Large Language Models (MLLMs)から直接学習します。特に、我々は、ビデオプロンプティングメソッドを提案し、これは、ビジュアルおよびテキストプロンプトを用いてMLLMsをディープな、時系列的な、高品質のカプチャーを生成させるものです。これらのカプチャーは、Large Language Modelを用いてフレーム対応的な、物の特徴をサポートし、共有の埋め込み空間を通じて開放ボキャブラリーのテキストクエリを実現します。4Dスケーンの物は、状態間で平滑な遷移を示します。そこで、我々は、時系列的な変化を有効にモデル化するために状態変形ネットワークを提案します。多くのベンチマークでの結果は、4D LangSplatが時間適応的か時間無関係的な開放ボキャブラリークエリに対して準確かつ効率的な結果を実現しました。",
      "upvotes": 6,
      "discussionId": "67d3adf87360ea908cf5f182",
      "ai_keywords": [
        "4D language fields",
        "time-sensitive queries",
        "open-ended language queries",
        "LangSplat",
        "CLIP features",
        "3D Gaussian representations",
        "dynamic 4D fields",
        "temporal dynamics",
        "pixel-aligned",
        "object-wise video features",
        "4D LangSplat",
        "time-agnostic queries",
        "Multimodal Large Language Models (MLLMs)",
        "multimodal object-wise video prompting",
        "visual prompts",
        "text prompts",
        "detailed captions",
        "temporally consistent captions",
        "sentence embeddings",
        "shared embedding spaces",
        "status deformable network",
        "continuous changes"
      ]
    },
    "publishedAt": "2025-03-13T10:58:22.000Z",
    "title": "4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large\n  Language Models",
    "summary": "Learning 4D language fields to enable time-sensitive, open-ended language\nqueries in dynamic scenes is essential for many real-world applications. While\nLangSplat successfully grounds CLIP features into 3D Gaussian representations,\nachieving precision and efficiency in 3D static scenes, it lacks the ability to\nhandle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot\ncapture temporal dynamics in videos. Real-world environments are inherently\ndynamic, with object semantics evolving over time. Building a precise 4D\nlanguage field necessitates obtaining pixel-aligned, object-wise video\nfeatures, which current vision models struggle to achieve. To address these\nchallenges, we propose 4D LangSplat, which learns 4D language fields to handle\ntime-agnostic or time-sensitive open-vocabulary queries in dynamic scenes\nefficiently. 4D LangSplat bypasses learning the language field from vision\nfeatures and instead learns directly from text generated from object-wise video\ncaptions via Multimodal Large Language Models (MLLMs). Specifically, we propose\na multimodal object-wise video prompting method, consisting of visual and text\nprompts that guide MLLMs to generate detailed, temporally consistent,\nhigh-quality captions for objects throughout a video. These captions are\nencoded using a Large Language Model into high-quality sentence embeddings,\nwhich then serve as pixel-aligned, object-specific feature supervision,\nfacilitating open-vocabulary text queries through shared embedding spaces.\nRecognizing that objects in 4D scenes exhibit smooth transitions across states,\nwe further propose a status deformable network to model these continuous\nchanges over time effectively. Our results across multiple benchmarks\ndemonstrate that 4D LangSplat attains precise and efficient results for both\ntime-sensitive and time-agnostic open-vocabulary queries.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/H5DUFDgeiid82tHVuW9cx.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10437.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "658bb7e47459b6e471b9d2e6",
      "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
      "fullname": "Wanhua Li",
      "name": "EthanTaylor",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10357",
      "authors": [
        {
          "_id": "67d3da3ce0b767b3d0fae2a4",
          "user": {
            "_id": "63bbfd74141c7d395c471768",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
            "isPro": false,
            "fullname": "Viktor Moskvoretskii",
            "user": "VityaVitalich",
            "type": "user"
          },
          "name": "Viktor Moskvoretskii",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T07:38:54.624Z",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a5",
          "name": "Alina Lobanova",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a6",
          "name": "Ekaterina Neminova",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a7",
          "name": "Chris Biemann",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a8",
          "name": "Alexander Panchenko",
          "hidden": false
        },
        {
          "_id": "67d3da3ce0b767b3d0fae2a9",
          "name": "Irina Nikishina",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63bbfd74141c7d395c471768/7wZapqnRg87iwXLqxiKQL.jpeg"
      ],
      "publishedAt": "2025-03-13T13:37:54.000Z",
      "submittedOnDailyAt": "2025-03-14T05:58:28.512Z",
      "title": "私は`cat.n.01`のように見えますか？ 分類学画像生成ベンチマーク",
      "submittedOnDailyBy": {
        "_id": "63bbfd74141c7d395c471768",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
        "isPro": false,
        "fullname": "Viktor Moskvoretskii",
        "user": "VityaVitalich",
        "type": "user"
      },
      "summary": "この論文は、ゼロショットセットでテキストから画像モデルを使用してタクソニマイズ概念の画像を生成する可能性を調査します。タクソニマイズの拡張においてテキストベースの方法は既に確立されていますが、可視的な次元の可能性はまだ調査されていません。これに対して、私たちはタクソニマイズ画像生成の評価基準を提案します。この評価基準は、モデルがタクソニマイズ概念を理解し、関連性のある高品質の画像を生成する能力を評価します。評価基準には、一般知識とランダムにサンプリングされたWordNet概念、LLMが生成した予測も含まれています。12モデルは9種類の新しいタクソニマイズ関連のテキストから画像への評価基準と人間のフィードバックを用いて評価されます。また、GPT-4のフィードバックを用いた対比的な評価を初めて導入し、画像生成を行います。実験結果によると、モデルの順位は標準的なT2Iタスクと显著に異なります。Playground-v2とFLUXは様々なメトリックとサブセットで一貫して上位に位置し、検索ベースのアプローチは不良です。これらの発見は、構造化されたデータリソースの自動化カレーティングの可能性を明らかにします。",
      "upvotes": 6,
      "discussionId": "67d3da42e0b767b3d0fae455",
      "projectPage": "https://huggingface.co/collections/VityaVitalich/generated-image-wordnet-67d2c868ff1414ec2f8e0d3d",
      "ai_keywords": [
        "text-to-image models",
        "zero-shot setup",
        "taxonomy concepts",
        "text-based methods",
        "taxonomy enrichment",
        "comprehensive benchmark",
        "WordNet",
        "LLM generated predictions",
        "taxonomy-related text-to-image metrics",
        "pairwise evaluation",
        "GPT-4 feedback",
        "image generation",
        "ranking of models",
        "retrieval-based approach",
        "automating the curation of structured data resources"
      ]
    },
    "publishedAt": "2025-03-13T09:37:54.000Z",
    "title": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark",
    "summary": "This paper explores the feasibility of using text-to-image models in a\nzero-shot setup to generate images for taxonomy concepts. While text-based\nmethods for taxonomy enrichment are well-established, the potential of the\nvisual dimension remains unexplored. To address this, we propose a\ncomprehensive benchmark for Taxonomy Image Generation that assesses models'\nabilities to understand taxonomy concepts and generate relevant, high-quality\nimages. The benchmark includes common-sense and randomly sampled WordNet\nconcepts, alongside the LLM generated predictions. The 12 models are evaluated\nusing 9 novel taxonomy-related text-to-image metrics and human feedback.\nMoreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for\nimage generation. Experimental results show that the ranking of models differs\nsignificantly from standard T2I tasks. Playground-v2 and FLUX consistently\noutperform across metrics and subsets and the retrieval-based approach performs\npoorly. These findings highlight the potential for automating the curation of\nstructured data resources.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63bbfd74141c7d395c471768/7wZapqnRg87iwXLqxiKQL.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10357.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bbfd74141c7d395c471768",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
      "fullname": "Viktor Moskvoretskii",
      "name": "VityaVitalich",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09642",
      "authors": [
        {
          "_id": "67d3ab8d032b54ab876cb7ec",
          "name": "Xiangyu Peng",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ed",
          "name": "Zangwei Zheng",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ee",
          "name": "Chenhui Shen",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ef",
          "name": "Tom Young",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f0",
          "name": "Xinying Guo",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f1",
          "name": "Binluo Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f2",
          "name": "Hang Xu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f3",
          "name": "Hongxin Liu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f4",
          "name": "Mingyan Jiang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f5",
          "name": "Wenjun Li",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f6",
          "name": "Yuhui Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f7",
          "name": "Anbang Ye",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f8",
          "name": "Gang Ren",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7f9",
          "name": "Qianran Ma",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fa",
          "name": "Wanying Liang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fb",
          "name": "Xiang Lian",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fc",
          "name": "Xiwen Wu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fd",
          "name": "Yuting Zhong",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7fe",
          "name": "Zhuangyan Li",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb7ff",
          "name": "Chaoyu Gong",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb800",
          "name": "Guojun Lei",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb801",
          "name": "Leijun Cheng",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb802",
          "name": "Limin Zhang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb803",
          "name": "Minghao Li",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb804",
          "name": "Ruijie Zhang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb805",
          "name": "Silan Hu",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb806",
          "name": "Shijie Huang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb807",
          "name": "Xiaokang Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb808",
          "name": "Yuanheng Zhao",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb809",
          "name": "Yuqi Wang",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb80a",
          "name": "Ziang Wei",
          "hidden": false
        },
        {
          "_id": "67d3ab8d032b54ab876cb80b",
          "name": "Yang You",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T05:00:07.000Z",
      "submittedOnDailyAt": "2025-03-14T02:39:06.938Z",
      "title": "Open-Sora 2.0: 20万美元で商用レベルのビデオ生成モデルの訓練",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "ビデオ生成モデルは、過去の1年間で驚異的な進歩を遂げました。AIビデオの質は進歩していますが、その代わりにモデルサイズの大きさが増え、データ量が増え、訓練計算量の要求が増えています。このレポートでは、ビデオ生成モデルのコマーシャルレベルのOpen-Sora 2.0を紹介します。このモデルは、そのみ$200kで訓練されています。このモデルを使用して、トップパフォーマンスのビデオ生成モデルの訓練費用が高度に制御できることを示します。この効率向上の原因の全ての技術を詳しく説明します。それは、データカレーティオン、モデルアーキテクチャ、訓練戦略、システム最適化に含まれます。人間評価結果とVBenchスコアによると、Open-Sora 2.0は、オープンソースのHunyuanVideoとクローズドソースのRunway Gen-3 Alphaを含む世界先駆ビデオ生成モデルと比較的です。Open-Sora 2.0を完全にオープンソースにして、先進的なビデオ生成技術のアクセスを民主化し、内容作成の広いイノベーションと創造性を促進することを目指しています。すべてのリソースは、https://github.com/hpcaitech/Open-Soraで公開しています。",
      "upvotes": 6,
      "discussionId": "67d3ab93032b54ab876cb9b0",
      "ai_keywords": [
        "video generation models",
        "data curation",
        "model architecture",
        "training strategy",
        "system optimization",
        "human evaluation",
        "VBench scores",
        "HunyuanVideo",
        "Runway Gen-3 Alpha",
        "open-source"
      ]
    },
    "publishedAt": "2025-03-12T01:00:07.000Z",
    "title": "Open-Sora 2.0: Training a Commercial-Level Video Generation Model in\n  $200k",
    "summary": "Video generation models have achieved remarkable progress in the past year.\nThe quality of AI video continues to improve, but at the cost of larger model\nsize, increased data quantity, and greater demand for training compute. In this\nreport, we present Open-Sora 2.0, a commercial-level video generation model\ntrained for only $200k. With this model, we demonstrate that the cost of\ntraining a top-performing video generation model is highly controllable. We\ndetail all techniques that contribute to this efficiency breakthrough,\nincluding data curation, model architecture, training strategy, and system\noptimization. According to human evaluation results and VBench scores,\nOpen-Sora 2.0 is comparable to global leading video generation models including\nthe open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By\nmaking Open-Sora 2.0 fully open-source, we aim to democratize access to\nadvanced video generation technology, fostering broader innovation and\ncreativity in content creation. All resources are publicly available at:\nhttps://github.com/hpcaitech/Open-Sora.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09642.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10630",
      "authors": [
        {
          "_id": "67d39f4828221b583a33be2c",
          "name": "Hang Yin",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be2d",
          "name": "Xiuwei Xu",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be2e",
          "name": "Lingqing Zhao",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be2f",
          "name": "Ziwei Wang",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be30",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "67d39f4828221b583a33be31",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:48.000Z",
      "submittedOnDailyAt": "2025-03-14T01:45:48.071Z",
      "title": "UniGoal: 向けて普遍的なZero-shot Goal-oriented Navigationへ",
      "submittedOnDailyBy": {
        "_id": "648ac65fd044b25978015634",
        "avatarUrl": "/avatars/2278a66fdc953220e9f8fc0ccce3ff00.svg",
        "isPro": false,
        "fullname": "Xiuwei Xu",
        "user": "xuxw98",
        "type": "user"
      },
      "summary": "この論文では、普遍的な零次元ゴール向けナビゲーションの一般的なフレームワークを提案します。現在の零次元方法は、特定のタスクに対して大規模な言語モデル（LLM）を基に推論フレームワークを構築し、全体的なパイプラインが大きく異なり、異なるゴール間での一般化ができません。普遍的な零次元ナビゲーションの目的に向けて、異なるゴールを統一するために、一貫したグラフ表現を提案し、オブジェクトカテゴリ、インスタンス画像、テキスト説明を含む。また、アウトプットの観測をオンラインで維持するシーングラフに変換し、この一貫したシーンとゴールの表現を用いて、テキストのみに比べて多数の構造情報を保存し、LLMをグラフ基の理由に活用できるようにします。特に、シーングラフとゴールグラフのグラフマッチングを時間の各インスタントで行い、異なるマッチング状態に応じて長期ゴールの探索を生成するための異なる戦略を提案します。アウトプットは、ゼロマッチングの場合にはゴールの部分グラフを反復的に探索し、部分マッチングの場合は座標プロジェクションとアンカーペアアライメントを用いてゴールの位置を推定します。最後に、シーングラフ修正とゴール確認を適用して完全なマッチングを実現します。また、順序の適切な切り替えを可能にするためにブラックリスト機能を導入します。数多くのベンチマークでの拡張的な実験により、我々のUniGoalは3つの研究されたナビゲーションタスクに対して、一つのモデルで最先端の零次元性能を達成し、タスク専門的な零次元方法や観覧的な普遍的な方法を超えることができます。",
      "upvotes": 5,
      "discussionId": "67d39f4928221b583a33be7f",
      "ai_keywords": [
        "universal zero-shot goal-oriented navigation",
        "zero-shot methods",
        "large language models (LLM)",
        "uniform graph representation",
        "object category",
        "instance image",
        "text description",
        "scene graph",
        "explicit graph-based reasoning",
        "graph matching",
        "long-term goal of exploration",
        "subgraph search",
        "coordinate projection",
        "anchor pair alignment",
        "scene graph correction",
        "goal verification",
        "blacklist mechanism",
        "UniGoal"
      ]
    },
    "publishedAt": "2025-03-13T13:59:48.000Z",
    "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
    "summary": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10630.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648ac65fd044b25978015634",
      "avatarUrl": "/avatars/2278a66fdc953220e9f8fc0ccce3ff00.svg",
      "fullname": "Xiuwei Xu",
      "name": "xuxw98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10460",
      "authors": [
        {
          "_id": "67d3a87df6ea55297c3cd071",
          "name": "Liang Wen",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd072",
          "name": "Yunke Cai",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd073",
          "name": "Fenrui Xiao",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd074",
          "name": "Xin He",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd075",
          "name": "Qi An",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd076",
          "name": "Zhenyu Duan",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd077",
          "name": "Yimin Du",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd078",
          "name": "Junchen Liu",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd079",
          "name": "Lifu Tang",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07a",
          "name": "Xiaowei Lv",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07b",
          "name": "Haosheng Zou",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07c",
          "name": "Yongchao Deng",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07d",
          "name": "Shousheng Jia",
          "hidden": false
        },
        {
          "_id": "67d3a87df6ea55297c3cd07e",
          "name": "Xiangzheng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T15:29:22.000Z",
      "submittedOnDailyAt": "2025-03-14T02:25:27.538Z",
      "title": "Light-R1: カリキュラムSFT、DPOとRLをスクラッチから長期COTよりも進む",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "この論文では、Light-R1 シリーズに関する研究成果を紹介します。モデル、データ、コードがすべて公開されています。\n\nまず、長期COTモデルのスタートからの学習に焦点を当てます。特に、最初に長期COT能力がないモデルから始めることを主張します。2ステージのSFTと半パラメーターポリシーDPOを構成したカレキュリムトレーニングレシピを使用して、Qwen2.5-32B-InstructからLight-R1-32Bモデルを学習し、DeepSeek-R1-Distill-Qwen-32Bと比較して数学性能が上位に位置しました。数学データのみで学習されたLight-R1-32Bは他の領域でも強い一般化性能を示しました。\n\n次のステップでは、2番目のSFTステージに向けて構築された3kデータセットの重要な効果を主張し、このデータセットを使用してDeepSeek-R1-Distilledモデルを微調節して新しいSOTAモデルを得ました。7Bと14Bモデルにおいては、32BモデルであるLight-R1-32B-DSはQwQ-32BとDeepSeek-R1と同等の性能を示しました。\n\nまた、長期COTモデルに強化学習、特にGRPOを適用して推理性能を進化させることを試みました。最終的なLight-R1-14B-DSモデルはRLを使用して学習され、数学領域で14BパラメーターモデルのSOTA性能を達成しました。AIME24と25のスコアはそれぞれ74.0と60.2で、Light-R1-14B-DSは32BモデルやDeepSeek-R1-Distill-Llama-70Bを超えました。また、RL学習によって期待される行動を示し、レスポンスの長さと報酬スコアが同時に増加しました。\n\nLight-R1 シリーズの研究は、長期COTモデルのスタートからの学習を証明し、SFTデータの芸術性を示し、RLを用いたSOTAモデルの公開を実現しました。",
      "upvotes": 5,
      "discussionId": "67d3a87ef6ea55297c3cd0d0",
      "ai_keywords": [
        "COT models",
        "curriculum training",
        "two-stage SFT",
        "semi-on-policy DPO",
        "long COT capabilities",
        "SOTA",
        "generalization",
        "3k dataset",
        "fine-tuning",
        "GRPO",
        "reasoning performance",
        "AIME24 & 25 scores",
        "response length",
        "reward score"
      ]
    },
    "publishedAt": "2025-03-13T11:29:22.000Z",
    "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond",
    "summary": "This paper presents our work on the Light-R1 series, with models, data, and\ncode all released.\n  We first focus on training long COT models from scratch, specifically\nstarting from models initially lacking long COT capabilities. Using a\ncurriculum training recipe consisting of two-stage SFT and semi-on-policy DPO,\nwe train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in\nsuperior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite\nbeing trained exclusively on math data, Light-R1-32B shows strong\ngeneralization across other domains. In the subsequent phase of this work, we\nhighlight the significant benefit of the 3k dataset constructed for the second\nSFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled\nmodels using this dataset, we obtain new SOTA models in 7B and 14B, while the\n32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.\n  Furthermore, we extend our work by applying reinforcement learning,\nspecifically GRPO, on long-COT models to further improve reasoning performance.\nWe successfully train our final Light-R1-14B-DS with RL, achieving SOTA\nperformance among 14B parameter models in math. With AIME24 & 25 scores of 74.0\nand 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and\nDeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected\nbehavior, showing simultaneous increase in response length and reward score.\n  The Light-R1 series of work validates training long-COT models from scratch,\nshowcases the art in SFT data and releases SOTA models from RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10460.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09905",
      "authors": [
        {
          "_id": "67d393660d51cf27930a5e5d",
          "user": {
            "_id": "67d3930e4d3a41ed9f7ac71e",
            "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
            "isPro": false,
            "fullname": "Allison Andreyev",
            "user": "allisonandreyev",
            "type": "user"
          },
          "name": "Allison Andreyev",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T02:25:50.445Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T23:50:35.000Z",
      "submittedOnDailyAt": "2025-03-14T00:57:22.699Z",
      "title": "OpenAIのWhisperモデルのデジタル化：比較的アナリジス",
      "submittedOnDailyBy": {
        "_id": "67d3930e4d3a41ed9f7ac71e",
        "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
        "isPro": false,
        "fullname": "Allison Andreyev",
        "user": "allisonandreyev",
        "type": "user"
      },
      "summary": "自動語音識別モデル（ASR）は、キャプション、語言翻訳、ライブテキスト化などの應用において重要な役割を果たしています。本論文では、Whisperと2つのモデルの変体を研究しています：ライブスピークストリーミング向けのモデルと、オフラインテキスト化向けのモデルです。特に、これらのモデルは、誤読や幻想内容の生成により、テキスト化の信頼性が低下することが見られています。また、大きなモデルは、ラジエンスコンストラインドデバイス上での扱いによって、ラジエンスの増加と扱いの課題があります。本論文では、3つのWhisperモデルの類似点と異なり点を分析し、定性的な評価を行います。次に、モデルのデータ化処理對策がラジエンスにどのように影響を与えるかを定量的に評価し、エッジデバイスの扱いの可能性を評価します。LibriSpeechデータセットを使用して、3つのデータ化方法（INT4、INT5、INT8）を用いて、Whispercppの単語誤り率（WER）とラジエンスの分析を行います。結果として、データ化処理はラジエンスを19％削減し、モデルサイズを45％削減し、同時にテキスト化精度を保っていることがわかります。これらの結果は、各種のWhisperモデルの最適使用ケースとエッジデバイスの扱いの可能性についての見通しを提供します。すべてのコード、データセット、実装詳細は、公開されたGitHubリポジトリにあります：https://github.com/allisonandreyev/WhisperQuantization.git",
      "upvotes": 5,
      "discussionId": "67d393670d51cf27930a5e98",
      "githubRepo": "https://github.com/allisonandreyev/WhisperQuantization",
      "ai_keywords": [
        "Whisper",
        "live speech streaming",
        "offline transcription",
        "hallucinated content",
        "latency",
        "deployment",
        "quantization",
        "LibriSpeech dataset",
        "word error rate (WER)",
        "whispercpp",
        "INT4",
        "INT5",
        "INT8",
        "speech recognition (ASR)",
        "transcription accuracy",
        "edge deployment"
      ]
    },
    "publishedAt": "2025-03-12T19:50:35.000Z",
    "title": "Quantization for OpenAI's Whisper Models: A Comparative Analysis",
    "summary": "Automated speech recognition (ASR) models have gained prominence for\napplications such as captioning, speech translation, and live transcription.\nThis paper studies Whisper and two model variants: one optimized for live\nspeech streaming and another for offline transcription. Notably, these models\nhave been found to generate hallucinated content, reducing transcription\nreliability. Furthermore, larger model variants exhibit increased latency and\npose challenges for deployment on resource-constrained devices. This study\nanalyzes the similarities and differences between three Whisper models,\nqualitatively examining their distinct capabilities. Next, this study\nquantifies the impact of model quantization on latency and evaluates its\nviability for edge deployment. Using the open source LibriSpeech dataset, this\npaper evaluates the word error rate (WER) along with latency analysis of\nwhispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that\nquantization reduces latency by 19\\% and model size by 45\\%, while preserving\ntranscription accuracy. These findings provide insights into the optimal use\ncases of different Whisper models and edge device deployment possibilities. All\ncode, datasets, and implementation details are available in a public GitHub\nrepository: https://github.com/allisonandreyev/WhisperQuantization.git",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09905.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67d3930e4d3a41ed9f7ac71e",
      "avatarUrl": "/avatars/4829b1d58376f7987a8891d41108c9c9.svg",
      "fullname": "Allison Andreyev",
      "name": "allisonandreyev",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09641",
      "authors": [
        {
          "_id": "67d3b008e18f86384bd33fa1",
          "name": "Junsong Chen",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa2",
          "name": "Shuchen Xue",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa3",
          "name": "Yuyang Zhao",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa4",
          "name": "Jincheng Yu",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa5",
          "user": {
            "_id": "5f7fbd813e94f16a85448745",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
            "isPro": false,
            "fullname": "Sayak Paul",
            "user": "sayakpaul",
            "type": "user"
          },
          "name": "Sayak Paul",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:06.327Z",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa6",
          "name": "Junyu Chen",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa7",
          "name": "Han Cai",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa8",
          "name": "Enze Xie",
          "hidden": false
        },
        {
          "_id": "67d3b008e18f86384bd33fa9",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T04:53:07.000Z",
      "submittedOnDailyAt": "2025-03-14T02:57:11.544Z",
      "title": "SANA-Sprint: 1ステップディフュージョンと連続時間の一致性の経験的転送",
      "submittedOnDailyBy": {
        "_id": "5f7fbd813e94f16a85448745",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
        "isPro": false,
        "fullname": "Sayak Paul",
        "user": "sayakpaul",
        "type": "user"
      },
      "summary": "この論文では、SANA-Sprintという効率的なディフュージョンモデルを紹介し、超高速なテキストから画像（T2I）の生成を実現します。SANA-Sprintは、事前学習された基盤モデルにビルドされ、ハイブリッドのディスティルデーションを加味し、計算時間を大幅に削減します（20ステップから1-4ステップまで）。私たちは、3つのキーのイノベーションを紹介します：（1）無学習アプローチを提案し、継続時間の一致性のディスティルデーション（sCM）にフローマッチングモデルを変換し、スタートからの高額な学習を除去し、高い学習効率を実現します。ハイブリッドのディスティルデーション戦略は、sCMと潜在的な相手抗争ディスティルデーション（LADD）を組み合わせます：sCMは教師モデルとのアライメントを確保し、LADDは1ステップの生成の信頼性を向上させます。（2）SANA-Sprintは、1-4ステップで高品質の生成を実現し、ステップ別の学習を除去し、効率を向上させます。この一連のイノベーションは、速度と品質のトレードオフに新たなパロードファントリーを設定し、1ステップで7.59 FIDと0.74 GenEvalの最先端の性能を実現します（FLUX-schnell（7.94 FID / 0.71 GenEval）を上回り、H100で10倍速く（0.1s vs 1.1s））。また、H100で1024 x 1024画像のT2Iで0.1s、ControlNetで0.25sのラテンシーを実現し、RTX 4090ではT2Iで0.31sを収め、AIポータブルな消費者アプリケーション（AIPC）の機能性と潜力を示します。コードと事前学習されたモデルは、オープンソースに公開されます。",
      "upvotes": 5,
      "discussionId": "67d3b00be18f86384bd3408f",
      "ai_keywords": [
        "diffusion model",
        "text-to-image (T2I)",
        "pre-trained foundation model",
        "hybrid distillation",
        "flow-matching model",
        "continuous-time consistency distillation (sCM)",
        "latent adversarial distillation (LADD)",
        "unified step-adaptive model",
        "ControlNet",
        "real-time interactive image generation",
        "FID",
        "GenEval",
        "FLUX-schnell",
        "H100",
        "RTX 4090",
        "AI-powered consumer applications (AIPC)"
      ]
    },
    "publishedAt": "2025-03-12T00:53:07.000Z",
    "title": "SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency\n  Distillation",
    "summary": "This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast\ntext-to-image (T2I) generation. SANA-Sprint is built on a pre-trained\nfoundation model and augmented with hybrid distillation, dramatically reducing\ninference steps from 20 to 1-4. We introduce three key innovations: (1) We\npropose a training-free approach that transforms a pre-trained flow-matching\nmodel for continuous-time consistency distillation (sCM), eliminating costly\ntraining from scratch and achieving high training efficiency. Our hybrid\ndistillation strategy combines sCM with latent adversarial distillation (LADD):\nsCM ensures alignment with the teacher model, while LADD enhances single-step\ngeneration fidelity. (2) SANA-Sprint is a unified step-adaptive model that\nachieves high-quality generation in 1-4 steps, eliminating step-specific\ntraining and improving efficiency. (3) We integrate ControlNet with SANA-Sprint\nfor real-time interactive image generation, enabling instant visual feedback\nfor user interaction. SANA-Sprint establishes a new Pareto frontier in\nspeed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID\nand 0.74 GenEval in only 1 step - outperforming FLUX-schnell (7.94 FID / 0.71\nGenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s\n(T2I) and 0.25s (ControlNet) latency for 1024 x 1024 images on H100, and 0.31s\n(T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for\nAI-powered consumer applications (AIPC). Code and pre-trained models will be\nopen-sourced.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09641.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f7fbd813e94f16a85448745",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
      "fullname": "Sayak Paul",
      "name": "sayakpaul",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 586
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10637",
      "authors": [
        {
          "_id": "67d3a2a6977358f62157977d",
          "user": {
            "_id": "636daf1b56c0762cfda074b5",
            "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
            "isPro": false,
            "fullname": "Rohit Gandikota",
            "user": "RohitGandikota",
            "type": "user"
          },
          "name": "Rohit Gandikota",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:16.089Z",
          "hidden": false
        },
        {
          "_id": "67d3a2a6977358f62157977e",
          "name": "David Bau",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:59:56.000Z",
      "submittedOnDailyAt": "2025-03-14T02:03:33.686Z",
      "title": "ディフュージョンモデルでの多様性と制御の収納",
      "submittedOnDailyBy": {
        "_id": "636daf1b56c0762cfda074b5",
        "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
        "isPro": false,
        "fullname": "Rohit Gandikota",
        "user": "RohitGandikota",
        "type": "user"
      },
      "summary": "蒸馏ディフフェーションモデルは、基盤モデルと比較してサンプルの多様性が減少する重大な制限がある。本研究では、この多様性の減少に関しても、蒸馏モデルは基盤モデルの基本的な概念表現を残していることを明らかにした。これにより、コントロールディスティルダイジェスト（Control Distillation）を示唆し、基盤モデルで訓練されたコントロール機構（例えばConcept SlidersとLoRAs）が無視なく蒸馏モデルへと移行でき、逆にも同様の効果を発揮できることを示唆した。この表現構造の保存により、蒸馏過程の多様性崩壊機構について調査を開始した。ディフフェーションターゲット（DT）ビジュアライゼーションという分析とデバッグツールを導入し、モデルが最終的な出力を予測するための中間ステップでの動作を明確にした。DT-Visualizationを通じて、生成フィードバック、不連続性を識別し、初期ディフフェーションステップが出力の多様性を不均衡に決定することを示唆し、後期ステップは主に詳細を精確化することを示唆した。これらの見解に基づき、多様性ディスティルダイジェストというハイブリッド推論アプローチを導入し、基盤モデルを最初の重要なステップだけに使用し、効率的な蒸馏モデルに移行することを戦略的に行うことを示唆した。この単純な変更は、基盤モデルからの多様性機能を復元し、驚くべきように超えることを示唆し、同時に蒸馏推論の計算効率を維持することを示唆し、追加的な訓練やモデル変更を必要としないように示唆した。本研究のコードおよびデータは、https://distillation.baulab.info から利用可能です。",
      "upvotes": 4,
      "discussionId": "67d3a2ab977358f6215798fc",
      "projectPage": "https://distillation.baulab.info",
      "githubRepo": "https://github.com/rohitgandikota/distillation",
      "ai_keywords": [
        "distilled diffusion models",
        "sample diversity",
        "base models",
        "Concept Sliders",
        "LoRAs",
        "control distillation",
        "representational structure",
        "diversity collapse",
        "Diffusion Target (DT) Visualization",
        "intermediate steps",
        "generation artifacts",
        "inconsistencies",
        "diffusion timesteps",
        "diversity distillation",
        "hybrid inference approach"
      ]
    },
    "publishedAt": "2025-03-13T13:59:56.000Z",
    "title": "Distilling Diversity and Control in Diffusion Models",
    "summary": "Distilled diffusion models suffer from a critical limitation: reduced sample\ndiversity compared to their base counterparts. In this work, we uncover that\ndespite this diversity loss, distilled models retain the fundamental concept\nrepresentations of base models. We demonstrate control distillation - where\ncontrol mechanisms like Concept Sliders and LoRAs trained on base models can be\nseamlessly transferred to distilled models and vice-versa, effectively\ndistilling control without any retraining. This preservation of\nrepresentational structure prompted our investigation into the mechanisms of\ndiversity collapse during distillation. To understand how distillation affects\ndiversity, we introduce Diffusion Target (DT) Visualization, an analysis and\ndebugging tool that reveals how models predict final outputs at intermediate\nsteps. Through DT-Visualization, we identify generation artifacts,\ninconsistencies, and demonstrate that initial diffusion timesteps\ndisproportionately determine output diversity, while later steps primarily\nrefine details. Based on these insights, we introduce diversity distillation -\na hybrid inference approach that strategically employs the base model for only\nthe first critical timestep before transitioning to the efficient distilled\nmodel. Our experiments demonstrate that this simple modification not only\nrestores the diversity capabilities from base to distilled models but\nsurprisingly exceeds it, while maintaining nearly the computational efficiency\nof distilled inference, all without requiring additional training or model\nmodifications. Our code and data are available at\nhttps://distillation.baulab.info",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10637.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "636daf1b56c0762cfda074b5",
      "avatarUrl": "/avatars/f44be5eb110acfa2efbd09de6b416239.svg",
      "fullname": "Rohit Gandikota",
      "name": "RohitGandikota",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10615",
      "authors": [
        {
          "_id": "67d3aa3f2f42ed5552e8ea0c",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea0d",
          "name": "Xiaoxuan He",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea0e",
          "name": "Hongkun Pan",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea0f",
          "name": "Xiyan Jiang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea10",
          "name": "Yan Deng",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea11",
          "name": "Xingtao Yang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea12",
          "name": "Haoyu Lu",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea13",
          "name": "Dacheng Yin",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea14",
          "name": "Fengyun Rao",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea15",
          "name": "Minfeng Zhu",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea16",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67d3aa3f2f42ed5552e8ea17",
          "name": "Wei Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:56:05.000Z",
      "submittedOnDailyAt": "2025-03-14T02:32:20.499Z",
      "title": "R1-Onevision: 拡張化多モーダル推理を進めるためのクロスモーダル形式化",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "大語言モデルは複雑な文脈タスクで出色の理由論能力を示しています。しかし、ビジュアルと文字情報の統合が必要な多モーダル理由論は大きな課題として残されています。現在のビジュアル・ラベルモデルはビジュアル内容の有効な分析と理由論に難しく、複雑な理由論タスクでは最適な性能を示さないことが多いです。また、詳細なベンチマークの欠如は、多モーダル理由論能力の正確な評価に妨げています。本論文では、R1-Onevisionという多モーダル理由論モデルを紹介します。このモデルは、ビジュアル認識と深い理由論の間のギャップを結ぶために設計されています。これを実現するために、画像を正式的な文脈表現に変換するための交差モーダル理由論パイプラインを提案します。このパイプラインを活用して、R1-Onevisionデータセットを構築し、多様な領域での詳細なステップごとの多モーダル理由論アノテーションを提供します。また、R1-Onevisionモデルは、規格化の微調込みと強化学習をベースに、高度な理由論能力と強固な一般化能力を養います。多モーダル理由論性能の総合的な評価を行うために、中学校から大学までのヒューマン教育ステージに合わせたR1-Onevision-Benchベンチマークを紹介します。実験結果から、R1-Onevisionは最先端の性能を達成し、GPT-4oやQwen2.5-VLや他の複雑な多モーダル理由論ベンチマークでの性能を超えることがわかりました。",
      "upvotes": 4,
      "discussionId": "67d3aa422f42ed5552e8eaee",
      "ai_keywords": [
        "multimodal reasoning",
        "cross-modal reasoning",
        "formal textural representations",
        "R1-Onevision dataset",
        "supervised fine-tuning",
        "reinforcement learning",
        "R1-Onevision-Bench",
        "GPT-4o",
        "Qwen2.5-VL"
      ]
    },
    "publishedAt": "2025-03-13T13:56:05.000Z",
    "title": "R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization",
    "summary": "Large Language Models have demonstrated remarkable reasoning capability in\ncomplex textual tasks. However, multimodal reasoning, which requires\nintegrating visual and textual information, remains a significant challenge.\nExisting visual-language models often struggle to effectively analyze and\nreason visual content, resulting in suboptimal performance on complex reasoning\ntasks. Moreover, the absence of comprehensive benchmarks hinders the accurate\nassessment of multimodal reasoning capabilities. In this paper, we introduce\nR1-Onevision, a multimodal reasoning model designed to bridge the gap between\nvisual perception and deep reasoning. To achieve this, we propose a cross-modal\nreasoning pipeline that transforms images into formal textural representations,\nenabling precise language-based reasoning. Leveraging this pipeline, we\nconstruct the R1-Onevision dataset which provides detailed, step-by-step\nmultimodal reasoning annotations across diverse domains. We further develop the\nR1-Onevision model through supervised fine-tuning and reinforcement learning to\ncultivate advanced reasoning and robust generalization abilities. To\ncomprehensively evaluate multimodal reasoning performance across different\ngrades, we introduce R1-Onevision-Bench, a benchmark aligned with human\neducational stages, covering exams from junior high school to university and\nbeyond. Experimental results show that R1-Onevision achieves state-of-the-art\nperformance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple\nchallenging multimodal reasoning benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10615.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6364
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10568",
      "authors": [
        {
          "_id": "67d3a952687a7a8a4963a030",
          "user": {
            "_id": "64e86fbd0c2413c3571ef7a6",
            "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
            "isPro": false,
            "fullname": "Haopeng Li",
            "user": "hp-l33",
            "type": "user"
          },
          "name": "Haopeng Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:10.236Z",
          "hidden": false
        },
        {
          "_id": "67d3a952687a7a8a4963a031",
          "name": "Jinyue Yang",
          "hidden": false
        },
        {
          "_id": "67d3a952687a7a8a4963a032",
          "name": "Guoqi Li",
          "hidden": false
        },
        {
          "_id": "67d3a952687a7a8a4963a033",
          "name": "Huan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T17:19:51.000Z",
      "submittedOnDailyAt": "2025-03-14T02:32:07.479Z",
      "title": "ランダム化並列解码を用いた自回帰画像生成",
      "submittedOnDailyBy": {
        "_id": "64e86fbd0c2413c3571ef7a6",
        "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
        "isPro": false,
        "fullname": "Haopeng Li",
        "user": "hp-l33",
        "type": "user"
      },
      "summary": "ARPGを紹介します。ARPGは新しい可視的な自動関数回帰モデルで、現在のラスター順序アプローチの固有の制限を解決し、推論の効率とゼロショットの一般化に負担をかけないように、ランダム化された並行生成を可能にします。私たちの主な見解は、効果的なランダム順序モデリングには、次の予測タグの位置を決定するための明示的なガイドラインが必要であることです。このため、私たちは、位置のガイドラインと内容表現を分離し、別々にクエリとキー-バリューペアでエンコードする新しいガイドドコーディングフレームワークを提案します。このガイドラインを直接的に原因的なアテンション機構に組み込むことで、我々のアプローチは、完全なランダム順序の訓練と生成を可能にし、双方向的なアテンションの必要性を排除します。このように、ARPGは画像のインパイント、オオパイント、レゾリューション拡大などのゼロショットタスクに対して自然に一般化します。また、ランダムサンプリングステップを64ステップで、ImageNet-1K 256ベンチマークでは、代表的な最近の自動関数回帰モデルと同じサイズで75%以上のメモリ消費を削減しながら20倍以上のトランシット増加を実現します。",
      "upvotes": 4,
      "discussionId": "67d3a957687a7a8a4963a179",
      "projectPage": "https://hp-l33.github.io/projects/arpg",
      "githubRepo": "https://github.com/hp-l33/ARPG",
      "ai_keywords": [
        "autoregressive model",
        "randomized parallel generation",
        "raster-order approaches",
        "inference efficiency",
        "zero-shot generalization",
        "sequential, predefined token generation order",
        "guided decoding framework",
        "positional guidance",
        "content representation",
        "queries",
        "key-value pairs",
        "causal attention mechanism",
        "fully random-order training",
        "bidirectional attention",
        "image inpainting",
        "outpainting",
        "resolution expansion",
        "parallel inference",
        "ImageNet-1K 256 benchmark",
        "FID",
        "sampling steps",
        "throughput",
        "memory consumption"
      ]
    },
    "publishedAt": "2025-03-13T13:19:51.000Z",
    "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
    "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10568.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e86fbd0c2413c3571ef7a6",
      "avatarUrl": "/avatars/960e17ba6a0d03fbd4700cd198adf5af.svg",
      "fullname": "Haopeng Li",
      "name": "hp-l33",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10391",
      "authors": [
        {
          "_id": "67d39679ea264394acf948ad",
          "name": "Yufan Deng",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948ae",
          "name": "Xun Guo",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948af",
          "name": "Yizhi Wang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b0",
          "name": "Jacob Zhiyuan Fang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b1",
          "name": "Angtian Wang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b2",
          "user": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "isPro": false,
            "fullname": "YSH",
            "user": "BestWishYsh",
            "type": "user"
          },
          "name": "Shenghai Yuan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:29.782Z",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b3",
          "name": "Yiding Yang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b4",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b5",
          "name": "Haibin Huang",
          "hidden": false
        },
        {
          "_id": "67d39679ea264394acf948b6",
          "name": "Chongyang Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T14:07:58.000Z",
      "submittedOnDailyAt": "2025-03-14T01:07:59.040Z",
      "title": "CINEMA: コヒーレント・マルチ・サブジェクト・ビデオ生成によるMLLMベースのガイド",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "ビデオ生成は、深層生成モデル、特にディフュージョンモデルの登場により驚異的な進歩を見せています。既存の方法は、テキストプラントや単一の画像から高品質のビデオを生成することで優れていますが、個人化された多個の主題のビデオ生成は大きな課題であり、まだ大きく探索されていません。この課題は、区別された複数の主題を含むビデオを合成し、時間的および空間的な一貫性を確保することによって複雑です。現在のアプローチは主に主題画像をテキストプラントのキーワードに対応させることを依存していますが、これは不明確性を引き起こし、主題の関係を有効にモデル化することができないようになっています。本論文では、Multimodal Large Language Model (MLLM)を拡張することにより、一貫的な多個の主題のビデオ生成の新しいフレームワークを提案します。我々のアプローチは、主題画像とテキストエンティティの間の明確な対応が必要となることを排除し、不明確性を軽減し、注釈の努力を減らします。MLLMを主題関係を解釈することにより、我々の方法はスケーラビリティを促進し、大きく多様なデータセットを使用して訓練することができます。さらに、我々のフレームワークは、変動可能な主題の数に基づいて条件付けされることができ、個人化内容の創作によりより大きな柔軟性を提供します。拡張的な評価を通じて、我々のアプローチが主題の一貫性と全体のビデオの一貫性を大幅に向上させ、物語伝え、相互作用マディア、個人化ビデオ生成の先進的なアプリケーションに向けて道を開けています。",
      "upvotes": 4,
      "discussionId": "67d3967aea264394acf94915",
      "ai_keywords": [
        "diffusion models",
        "multimodal large language model (MLLM)"
      ]
    },
    "publishedAt": "2025-03-13T10:07:58.000Z",
    "title": "CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance",
    "summary": "Video generation has witnessed remarkable progress with the advent of deep\ngenerative models, particularly diffusion models. While existing methods excel\nin generating high-quality videos from text prompts or single images,\npersonalized multi-subject video generation remains a largely unexplored\nchallenge. This task involves synthesizing videos that incorporate multiple\ndistinct subjects, each defined by separate reference images, while ensuring\ntemporal and spatial consistency. Current approaches primarily rely on mapping\nsubject images to keywords in text prompts, which introduces ambiguity and\nlimits their ability to model subject relationships effectively. In this paper,\nwe propose CINEMA, a novel framework for coherent multi-subject video\ngeneration by leveraging Multimodal Large Language Model (MLLM). Our approach\neliminates the need for explicit correspondences between subject images and\ntext entities, mitigating ambiguity and reducing annotation effort. By\nleveraging MLLM to interpret subject relationships, our method facilitates\nscalability, enabling the use of large and diverse datasets for training.\nFurthermore, our framework can be conditioned on varying numbers of subjects,\noffering greater flexibility in personalized content creation. Through\nextensive evaluations, we demonstrate that our approach significantly improves\nsubject consistency, and overall video coherence, paving the way for advanced\napplications in storytelling, interactive media, and personalized video\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 34
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10636",
      "authors": [
        {
          "_id": "67d39ae1faaad4ed2df1cc61",
          "user": {
            "_id": "63041b541dd5d3c62486c294",
            "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
            "isPro": false,
            "fullname": "Ho Kei Cheng",
            "user": "hkchengrex",
            "type": "user"
          },
          "name": "Ho Kei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:56:19.361Z",
          "hidden": false
        },
        {
          "_id": "67d39ae1faaad4ed2df1cc62",
          "name": "Alexander Schwing",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63041b541dd5d3c62486c294/ZHc69zwDiWkyl0cNnp5zn.png"
      ],
      "publishedAt": "2025-03-13T17:59:56.000Z",
      "submittedOnDailyAt": "2025-03-14T01:27:30.007Z",
      "title": "条件の呪い：条件付きフローベース生成の最適輸送を分析し改善する",
      "submittedOnDailyBy": {
        "_id": "63041b541dd5d3c62486c294",
        "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
        "isPro": false,
        "fullname": "Ho Kei Cheng",
        "user": "hkchengrex",
        "type": "user"
      },
      "summary": "Minibatch 最適輸送コピングは無条件フローのパスを直します。これは、テスト時に数値微分方程式を解く際に、少ない積分ステップと複雑さの低い数値解法を使用できることにより、計算量が少なくなります。しかし、条件付きの場合、minibatch 最適輸送は不足します。これは、デフォルトの最適輸送マッピングが条件を無視し、トレーニング時に条件付きに歪みが生じるからです。対照的に、テスト時には歪みのある先頭分布にアクセスできなく、全ての無偏の先頭分布からサンプリングします。このトレーニングとテストの間の隙間は、機能のパフォーマンスが低くなります。この隙間を埋めるために、我々は条件付き最適輸送 C^2OT を提案します。C^2OT は、最適輸送割り当てを計算する際に、コスト行列に条件付き重み付けを追加します。実験は、8gaussians-to-moons、CIFAR-10、ImageNet-32x32、ImageNet-256x256 で、離散や連続条件にも対応します。我々の方法は、異なる関数評価バッジ内で現在のベースラインと比較して、全体的により良い性能を示します。コードは、https://hkchengrex.github.io/C2OT で利用できます。",
      "upvotes": 3,
      "discussionId": "67d39ae4faaad4ed2df1cd42",
      "projectPage": "https://hkchengrex.com/C2OT/",
      "githubRepo": "https://github.com/hkchengrex/C2OT",
      "ai_keywords": [
        "minibatch optimal transport",
        "flow matching",
        "integration steps",
        "numerical solvers",
        "ordinary differential equation",
        "optimal transport mapping",
        "conditional optimal transport",
        "C^2OT",
        "cost matrix",
        "optimal transport assignment",
        "discrete conditions",
        "continuous conditions",
        "8gaussians-to-moons",
        "CIFAR-10",
        "ImageNet-32x32",
        "ImageNet-256x256",
        "function evaluation budgets"
      ]
    },
    "publishedAt": "2025-03-13T13:59:56.000Z",
    "title": "The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation",
    "summary": "Minibatch optimal transport coupling straightens paths in unconditional flow\nmatching. This leads to computationally less demanding inference as fewer\nintegration steps and less complex numerical solvers can be employed when\nnumerically solving an ordinary differential equation at test time. However, in\nthe conditional setting, minibatch optimal transport falls short. This is\nbecause the default optimal transport mapping disregards conditions, resulting\nin a conditionally skewed prior distribution during training. In contrast, at\ntest time, we have no access to the skewed prior, and instead sample from the\nfull, unbiased prior distribution. This gap between training and testing leads\nto a subpar performance. To bridge this gap, we propose conditional optimal\ntransport C^2OT that adds a conditional weighting term in the cost matrix when\ncomputing the optimal transport assignment. Experiments demonstrate that this\nsimple fix works with both discrete and continuous conditions in\n8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method\nperforms better overall compared to the existing baselines across different\nfunction evaluation budgets. Code is available at\nhttps://hkchengrex.github.io/C2OT",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63041b541dd5d3c62486c294/ZHc69zwDiWkyl0cNnp5zn.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10636.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63041b541dd5d3c62486c294",
      "avatarUrl": "/avatars/a5286d562f7b9082730f760e66c3bf29.svg",
      "fullname": "Ho Kei Cheng",
      "name": "hkchengrex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 25
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10072",
      "authors": [
        {
          "_id": "67d390de29a092bdbb0a2aeb",
          "user": {
            "_id": "6331c3f618711776b468e9ec",
            "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
            "isPro": false,
            "fullname": "Mia Mohammad Imran",
            "user": "imranraad",
            "type": "user"
          },
          "name": "Mia Mohammad Imran",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-14T02:14:33.331Z",
          "hidden": false
        },
        {
          "_id": "67d390de29a092bdbb0a2aec",
          "name": "Jaydeb Sarker",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-13T05:39:29.000Z",
      "submittedOnDailyAt": "2025-03-14T00:44:11.345Z",
      "title": "\"無言ではない\": Bug レポート議論における毒性の調査",
      "submittedOnDailyBy": {
        "_id": "6331c3f618711776b468e9ec",
        "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
        "isPro": false,
        "fullname": "Mia Mohammad Imran",
        "user": "imranraad",
        "type": "user"
      },
      "summary": "バグ報告の討論での毒性は、開放ソースソフトウェア開発の協力構造に重大な課題をもたらします。バグ報告は、欠陥の識別と解決に重要ですが、その本質的な問題フォーカスな性質と感情的に充満したコンテキストにより、毒性的な相互作用に容易になります。本研究は、203バグツリー（81つの毒性のあるものを含む）の質的な分析を通じてGitHubのバグ報告の毒性に取り掛けます。私たちの発見は、バグの厳重さと優先順位の誤解、ツールに対する解決したことのない不満、そして専門的なコミュニケーションの欠陥からの毒性の発生が頻繁であることを示しています。これらの毒性的な相互作用は、生産的な討論をやり捨て、実用的な結果の可能性を減らします。私たちの初期発見は、毒性の軽減によるバグ解決の改善に対する具体的な実行可能なリコメンドを提供します。",
      "upvotes": 2,
      "discussionId": "67d390df29a092bdbb0a2b2d",
      "projectPage": "https://zenodo.org/records/15015619"
    },
    "publishedAt": "2025-03-13T01:39:29.000Z",
    "title": "\"Silent Is Not Actually Silent\": An Investigation of Toxicity on Bug\n  Report Discussion",
    "summary": "Toxicity in bug report discussions poses significant challenges to the\ncollaborative dynamics of open-source software development. Bug reports are\ncrucial for identifying and resolving defects, yet their inherently\nproblem-focused nature and emotionally charged context make them susceptible to\ntoxic interactions. This study explores toxicity in GitHub bug reports through\na qualitative analysis of 203 bug threads, including 81 toxic ones. Our\nfindings reveal that toxicity frequently arises from misaligned perceptions of\nbug severity and priority, unresolved frustrations with tools, and lapses in\nprofessional communication. These toxic interactions not only derail productive\ndiscussions but also reduce the likelihood of actionable outcomes, such as\nlinking issues with pull requests. Our preliminary findings offer actionable\nrecommendations to improve bug resolution by mitigating toxicity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10072.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6331c3f618711776b468e9ec",
      "avatarUrl": "/avatars/af2c4bba031e474bf4fd2ea19e415aaf.svg",
      "fullname": "Mia Mohammad Imran",
      "name": "imranraad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.10291",
      "authors": [
        {
          "_id": "67d3cbea16d1ecea57ed096c",
          "name": "Weiyun Wang",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed096d",
          "name": "Zhangwei Gao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed096e",
          "name": "Lianjie Chen",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed096f",
          "name": "Zhe Chen",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0970",
          "name": "Jinguo Zhu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0971",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0972",
          "name": "Yangzhou Liu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0973",
          "name": "Yue Cao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0974",
          "name": "Shenglong Ye",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0975",
          "name": "Xizhou Zhu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0976",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0977",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0978",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed0979",
          "name": "Jifeng Dai",
          "hidden": false
        },
        {
          "_id": "67d3cbea16d1ecea57ed097a",
          "name": "Wenhai Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VPQRFEXv78LkPf2h5dgXm.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/w91GHKp_sv2u4U-8VFLOn.png",
        "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/rG_h65J1RHcO0Lkbo5rg4.png"
      ],
      "publishedAt": "2025-03-13T12:03:37.000Z",
      "submittedOnDailyAt": "2025-03-14T06:40:16.854Z",
      "title": "VisualPRM: 有效なモデル化プロセス報酬モデルの多様的な理由",
      "submittedOnDailyBy": {
        "_id": "619507e7b74b6c591f794340",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
        "isPro": false,
        "fullname": "Weiyun Wang",
        "user": "Weiyun1025",
        "type": "user"
      },
      "summary": "VisualPRMは、8Bパラメータを持つ進歩的な多モデルプロセス報酬モデル（PRM）です。これは、現在の多モデル大語言モデル（MLLMs）の理由能力を、様々なモデルスケールとファミリーの間で、Best-of-N（BoN）評価戦略を用いて向上させます。特に、我々のモデルは、3種類のMLLMと4種類のモデルスケールの理由性能を向上させます。InternVL2.5-78Bという高度な能力を持つモデルに対しても、7つの多モデル理由ベンチマークで5.9点の向上を達成しました。実験結果から、我々のモデルはOutcome Reward ModelsとSelf-Consistencyと比較して、BoN評価で上位の性能を示しています。また、多モデルPRMの訓練を促進するために、自動化されたデータパイプラインを用いてVisualPRM400Kという多モデルプロセスサブジェクトデータセットを構築しました。また、多モデルPRMの評価において、我々は、人間の注釈付きステップごとの正確性ラベルを持つVisualProcessBenchというベンチマークを提案し、PRMが多モデル理由タスクでのエラーステップを検出する能力を評価することを目的とします。我々の研究は、将来の研究により多モデル大語言モデルの開発に貢献し、さらなる進歩を促進したいと考えています。我々のモデル、データ、ベンチマークは、https://internvl.github.io/blog/2025-03-13-VisualPRM/で公開されています。",
      "upvotes": 1,
      "discussionId": "67d3cbed16d1ecea57ed0a75",
      "projectPage": "https://internvl.github.io/blog/2025-03-13-VisualPRM/",
      "githubRepo": "https://github.com/OpenGVLab/InternVL",
      "ai_keywords": [
        "Process Reward Model (PRM)",
        "Multimodal Large Language Models (MLLMs)",
        "Best-of-N (BoN)",
        "multimodal reasoning benchmarks",
        "Automated data pipeline",
        "VisualPRM400K",
        "VisualProcessBench",
        "human-annotated step-wise correctness labels",
        "multimodal PRMs",
        "erroneous steps in multimodal reasoning tasks"
      ]
    },
    "publishedAt": "2025-03-13T08:03:37.000Z",
    "title": "VisualPRM: An Effective Process Reward Model for Multimodal Reasoning",
    "summary": "We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM)\nwith 8B parameters, which improves the reasoning abilities of existing\nMultimodal Large Language Models (MLLMs) across different model scales and\nfamilies with Best-of-N (BoN) evaluation strategies. Specifically, our model\nimproves the reasoning performance of three types of MLLMs and four different\nmodel scales. Even when applied to the highly capable InternVL2.5-78B, it\nachieves a 5.9-point improvement across seven multimodal reasoning benchmarks.\nExperimental results show that our model exhibits superior performance compared\nto Outcome Reward Models and Self-Consistency during BoN evaluation. To\nfacilitate the training of multimodal PRMs, we construct a multimodal process\nsupervision dataset VisualPRM400K using an automated data pipeline. For the\nevaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with\nhuman-annotated step-wise correctness labels, to measure the abilities of PRMs\nto detect erroneous steps in multimodal reasoning tasks. We hope that our work\ncan inspire more future research and contribute to the development of MLLMs.\nOur model, data, and benchmark are released in\nhttps://internvl.github.io/blog/2025-03-13-VisualPRM/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/VPQRFEXv78LkPf2h5dgXm.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/w91GHKp_sv2u4U-8VFLOn.png",
      "https://cdn-uploads.huggingface.co/production/uploads/619507e7b74b6c591f794340/rG_h65J1RHcO0Lkbo5rg4.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10291.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619507e7b74b6c591f794340",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
      "fullname": "Weiyun Wang",
      "name": "Weiyun1025",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09368",
      "authors": [
        {
          "_id": "67d2ca4be4696fda20bac029",
          "user": {
            "_id": "656c8721e8bf55919a9732c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
            "isPro": false,
            "fullname": "Nikolai",
            "user": "Nikolai10",
            "type": "user"
          },
          "name": "Nikolai Körber",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-14T08:57:39.634Z",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02a",
          "name": "Eduard Kromer",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02b",
          "name": "Andreas Siebert",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02c",
          "name": "Sascha Hauke",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02d",
          "name": "Daniel Mueller-Gritschneder",
          "hidden": false
        },
        {
          "_id": "67d2ca4be4696fda20bac02e",
          "name": "Björn Schuller",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T13:14:51.000Z",
      "submittedOnDailyAt": "2025-03-14T07:51:16.414Z",
      "title": "PerCoV2: 隠れヒエラーキードマスク画像モデリングによる改善された超低ビットレート視覚的画像圧縮",
      "submittedOnDailyBy": {
        "_id": "656c8721e8bf55919a9732c5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
        "isPro": false,
        "fullname": "Nikolai",
        "user": "Nikolai10",
        "type": "user"
      },
      "summary": "PerCoV2は、バンディットとストアージャンスが制限されたアプリケーションに向けて、新しいオープンな超低ビットレートの視覚的な画像圧縮システムです。Careilらの先行研究に基づいて、PerCoV2はStable Diffusion 3エコシステムに拡張し、離散の超潜像分布を明示的にモデル化してエントロピーコーディングの効率を向上させます。これにより、最近の自動帰りゲーム方法（VARとMaskGIT）を詳細に比較し、大規模なMSCOCO-30kベンチマーク上で検証します。PerCoV2は、先行研究に比べて、（i）ビットレートが更に低くなるものの画像の忠実度が高まり、視覚的な品質が競争的である、（ii）進めるビットレート削減のためのハイブリッド生成モードを掲げ、また（iii）公開のコンポーネントのみによって構築されています。コードと訓練モデルは、https://github.com/Nikolai10/PerCoV2で公開します。",
      "upvotes": 1,
      "discussionId": "67d2ca50e4696fda20bac1a8",
      "githubRepo": "https://github.com/Nikolai10/PerCoV2",
      "ai_keywords": [
        "PerCoV2",
        "ultralow bit-rate",
        "perceptual image compression",
        "bandwidth-constrained applications",
        "Stable Diffusion 3",
        "entropy coding",
        "discrete hyper-latent image distribution",
        "autoregressive methods",
        "VAR",
        "MaskGIT",
        "MSCOCO-30k",
        "image fidelity",
        "perceptual quality",
        "hybrid generation mode",
        "public components"
      ]
    },
    "publishedAt": "2025-03-12T09:14:51.000Z",
    "title": "PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with\n  Implicit Hierarchical Masked Image Modeling",
    "summary": "We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image\ncompression system designed for bandwidth- and storage-constrained\napplications. Building upon prior work by Careil et al., PerCoV2 extends the\noriginal formulation to the Stable Diffusion 3 ecosystem and enhances entropy\ncoding efficiency by explicitly modeling the discrete hyper-latent image\ndistribution. To this end, we conduct a comprehensive comparison of recent\nautoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our\napproach on the large-scale MSCOCO-30k benchmark. Compared to previous work,\nPerCoV2 (i) achieves higher image fidelity at even lower bit-rates while\nmaintaining competitive perceptual quality, (ii) features a hybrid generation\nmode for further bit-rate savings, and (iii) is built solely on public\ncomponents. Code and trained models will be released at\nhttps://github.com/Nikolai10/PerCoV2.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09368.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656c8721e8bf55919a9732c5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bvIaJLEvlWal1PHCL4tDo.jpeg",
      "fullname": "Nikolai",
      "name": "Nikolai10",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]