[
  {
    "paper": {
      "id": "2503.00865",
      "authors": [
        {
          "_id": "67c666245e2443d7d5e9b76a",
          "user": {
            "_id": "64802face9ff472e30dc1ceb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64802face9ff472e30dc1ceb/bcwTlgpaUrU7m2RMB5zCc.png",
            "isPro": false,
            "fullname": "Yiran Zhao",
            "user": "Yiran0924",
            "type": "user"
          },
          "name": "Yiran Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-04T08:51:21.231Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76b",
          "user": {
            "_id": "61657b0b20606e5e73f611cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61657b0b20606e5e73f611cc/6ZPne2GYlWkxrx35ND1P8.png",
            "isPro": false,
            "fullname": "CHAOQUN LIU",
            "user": "lukecq",
            "type": "user"
          },
          "name": "Chaoqun Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:27:33.956Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76c",
          "name": "Yue Deng",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76d",
          "user": {
            "_id": "671609f7664f44a151f1f0e8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fEQLuH1kdW5Pd9Y_J64hN.png",
            "isPro": false,
            "fullname": "jiahao ying",
            "user": "jhying",
            "type": "user"
          },
          "name": "Jiahao Ying",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:39.926Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76e",
          "user": {
            "_id": "6539c87ba318a98bf0d15dd8",
            "avatarUrl": "/avatars/beb9ba6eeacb61addc5897836bd59f55.svg",
            "isPro": false,
            "fullname": "Mahani Aljunied",
            "user": "maljunied",
            "type": "user"
          },
          "name": "Mahani Aljunied",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:33.285Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b76f",
          "name": "Zhaodonghui Li",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b770",
          "user": {
            "_id": "6454685a548f22be598414c4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg",
            "isPro": false,
            "fullname": "Lidong Bing",
            "user": "LidongBing",
            "type": "user"
          },
          "name": "Lidong Bing",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:19.611Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b771",
          "user": {
            "_id": "604f67ef0fe8ff3ec13d71ef",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
            "isPro": false,
            "fullname": "Hou Pong (Ken) Chan",
            "user": "kenchan0226",
            "type": "user"
          },
          "name": "Hou Pong Chan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:35:50.272Z",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b772",
          "name": "Yu Rong",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b773",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "67c666245e2443d7d5e9b774",
          "user": {
            "_id": "60dff6ae19a362a8c27862aa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60dff6ae19a362a8c27862aa/LIYzLB3cdPh-B3XIBgBCC.jpeg",
            "isPro": false,
            "fullname": "Wenxuan Zhang",
            "user": "isakzhang",
            "type": "user"
          },
          "name": "Wenxuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:27:36.769Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-02T11:53:55.000Z",
      "title": "バベル：開放モニグラル大語言モデル 90%以上の世界の話者をサービスする",
      "summary": "大語言モデル（LLMs）は、自然言語処理（NLP）に革命をもたらしましたが、開放ソースの多言語LLMsは稀少で、既存のモデルが言語カバー面積に制限されています。これらのモデルは通常資源豊富な言語を優先し、広く使用されているが資源不足の言語は遺棄されます。この差異を解決するために、我々はBabelという開放ソースの多言語LLMsを紹介します。これは、語数が多い前25位の言語をカバーし、世界人口の90%を超える人々の言語をサポートし、他の開放ソースの多言語LLMsが遺棄している許多の言語を含みます。これらのモデルは、通常の継続的な予測学習アプローチと異なり、パラメータ数を層拡張テクニックによって拡張し、その性能の上限を高めます。我々は、Babel-9BとBabel-83Bの2つのバージョンを紹介します。前者は効率的な推論と微調節を目的として、後者は開放ソースの多言語LLMsの新たな標準を設定します。多言語タスクにおいての極めて広範な評価で、比較的サイズの開放ソースのLLMsと比較して上位の性能を示します。また、開放ソースのスーパーバイバーディングフィニティングデータセットを使用して、Babelは卓越した性能を達成し、Babel-9B-Chatは10BサイズのLLMsの中で最も優秀であり、Babel-83B-Chatは多言語タスクにおいて新たな標準を設定し、商業モデルのレベルに達します。",
      "upvotes": 32,
      "discussionId": "67c666255e2443d7d5e9b7b3",
      "projectPage": "https://babel-llm.github.io/babel-llm/",
      "githubRepo": "https://github.com/babel-llm/babel-llm"
    },
    "publishedAt": "2025-03-05T21:49:03.700Z",
    "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00865.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64802face9ff472e30dc1ceb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64802face9ff472e30dc1ceb/bcwTlgpaUrU7m2RMB5zCc.png",
      "fullname": "Yiran Zhao",
      "name": "Yiran0924",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.00329",
      "authors": [
        {
          "_id": "67c755f898a2e37274c62c96",
          "name": "Benjamin Schneider",
          "hidden": false
        },
        {
          "_id": "67c755f898a2e37274c62c97",
          "name": "Florian Kerschbaum",
          "hidden": false
        },
        {
          "_id": "67c755f898a2e37274c62c98",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:50:07.881Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-01T03:29:02.000Z",
      "title": "ABC: マルチモーダルエンベディングのより良い制御を実現するためのVLMsの使用",
      "summary": "ビジュアルエンベッディングモデルは、ビジュアル検索や分類などのゼロショットタスクに優れています。しかし、これらのモデルは、構想不明やユーザーの指示が必要なタスクに使用できません。これらのタスクは、ビジュアルと自然言語の入力を組み合わせたエンベッディングを出力するための多モーダルエンベッディングモデルが必要となります。現在のCLIPベースのアプローチは、画像とテキストを独立にエンブディングし、結果を融合しています。私たちは、これがモーダル間の弱い相互作用とユーザーの表現制御の悪い性能を帯びていることを見出しました。私たちは、ビジョンラベルモデルバックボードを使用して画像特徴と自然言語の指示を深く統合するための開放ソース多モーダルエンベッディングモデルABCを紹介します。ABCは、MSCOCO画像からのテキスト検索で最も良い性能を達成し、マジックマルチモーダルエンベッディングベンチマークで分類やVQAタスクで最も優れています。強力なビジョンラベル表現を持つABCは、自然言語を使って軽微なやつらやその可能性がある構想不明なビジュアル検索問題を解決できます。この能力を評価するために、私たちは、正しい検索を求めるためにテキストインストラクションと画像内容を交互にするベンチマークCtrlBenchを設計しました。ABCは、高品質の表現と柔軟な自然言語制御を提供して、多モーダルエンベッディングの状態を進めます。私たちのモデルとデータセットは、プロジェクトページで提供されています。",
      "upvotes": 10,
      "discussionId": "67c7560298a2e37274c6311d",
      "projectPage": "https://tiger-ai-lab.github.io/ABC/",
      "githubRepo": "https://github.com/TIGER-AI-Lab/ABC"
    },
    "publishedAt": "2025-03-05T21:33:37.945Z",
    "title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/b2vg-4UWwvcEboAZgK-Sv.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00329.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03278",
      "authors": [
        {
          "_id": "67c94e5f8c4ef8be73583f4b",
          "name": "Jun Li",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4c",
          "user": {
            "_id": "631b9ff5824f2502e3557c7e",
            "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
            "isPro": false,
            "fullname": "liu",
            "user": "che111",
            "type": "user"
          },
          "name": "Che Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:03:06.882Z",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4d",
          "name": "Wenjia Bai",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4e",
          "name": "Rossella Arcucci",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f4f",
          "name": "Cosmin I. Bercea",
          "hidden": false
        },
        {
          "_id": "67c94e5f8c4ef8be73583f50",
          "name": "Julia A. Schnabel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T09:02:33.000Z",
      "title": "知識説明を用いた視覚言語モデルの異常性基礎化の向上",
      "summary": "Visual Language Models (VLMs) は、視覚基礎化タスクにおいて驚異的な能力を示しています。しかし、医療領域での効果性、特に医療画像内での異常検出と局在化においては、まだ調査が浅い状態です。主な課題は、医療用語の複雑さと抽象性で、病理学的な異常ティームと対応する視覚的な特徴との直接的な連想が難しいことです。本稿では、分解された医療知識を活用して、VLM の医療異常検出と局在化の性能を向上させる新しいアプローチを提案します。直接的にモデルに特定の異常を認識させるよりも、医療概念を基本的な属性と共通的な視覚的パターンに分解することを焦点とします。この戦略は、記述と視覚的な特徴とのより強い一致を促成し、医療画像内での異常の認識と局在化を両方改善します。我々の方法は、0.23B Florence-2 base model において評価され、7B LLaVA ベースの医療用 VLM と比較的な性能を達成します。特に、このモデルは、そのデータの1.5%だけを学習していることにも関わらず、異常検出の性能が向上します。実験結果は、既知の異常と以前に見ぬ異常においても、我々のアプローチの効果性を示し、強い一般化能力を示しています。",
      "upvotes": 9,
      "discussionId": "67c94e608c4ef8be73583f7b",
      "projectPage": "https://lijunrio.github.io/AG-KD/"
    },
    "publishedAt": "2025-03-06T02:29:15.964Z",
    "title": "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631b9ff5824f2502e3557c7e",
      "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
      "fullname": "liu",
      "name": "che111",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.03751",
      "authors": [
        {
          "_id": "67c912b1b5903dd437cc2370",
          "user": {
            "_id": "658529d61c461dfe88afe8e8",
            "avatarUrl": "/avatars/a22c1b07d28c2662833c462c6537d835.svg",
            "isPro": false,
            "fullname": "Xuanchi Ren",
            "user": "xrenaa",
            "type": "user"
          },
          "name": "Xuanchi Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T09:55:04.321Z",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2371",
          "name": "Tianchang Shen",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2372",
          "name": "Jiahui Huang",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2373",
          "name": "Huan Ling",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2374",
          "name": "Yifan Lu",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2375",
          "name": "Merlin Nimier-David",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2376",
          "name": "Thomas Müller",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2377",
          "name": "Alexander Keller",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2378",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "67c912b1b5903dd437cc2379",
          "name": "Jun Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T18:59:50.000Z",
      "title": "GEN3C: 3D情報による世界一致の動画生成と準確なカメラ制御",
      "summary": "GEN3Cは、構造的なカメラ制御と時間的な3D一致性を持つ生成的な映像モデルです。先行の映像モデルは写実的な映像を生成しますが、3D情報の利用が少なく、オブジェクトの存在の出入りなどの不確実性がありました。カメラの制御は、カメラパラメータが神経ネットワークの入力であり、そのようなカメラによって映像がどのように変化するかを推論する必要があるため、正確な制御が難しいです。GEN3Cは、3Dキャッチバッファーをガイドにしています：シード画像または前回生成されたフレームのピクセルごとの深さを予測したポイントクラスターです。次のフレームを生成する際には、ユーザーが提供された新しいカメラトライアクタイと3Dキャッチバッファーの2Dレンダリングに基づいています。重要なことに、GEN3Cは前回生成したものを記憶する必要もなく、カメラの姿勢から画像の構造を推論する必要もなく、すべての生成力を前に見たことのない領域へと集中できます。モデルは、その前に見たことのない領域への生成力を全力で発揮し、シーンの状態を次のフレームへ進めることができます。我々の結果は、先行技術よりも更に正確なカメラ制御を示し、稀視ビューの新しいビュー合成にも最先端の結果を示しています。特に、駆転転場面やモノチャンバー映像などの難しい設定でも優れた性能を示しています。結果は映像で最も良く見られます。ホームページをご覧ください！ https://research.nvidia.com/labs/toronto-ai/GEN3C/",
      "upvotes": 9,
      "discussionId": "67c912b9b5903dd437cc2505"
    },
    "publishedAt": "2025-03-05T22:13:22.552Z",
    "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03751.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6288
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02951",
      "authors": [
        {
          "_id": "67c907ea7568a12737ad4535",
          "user": {
            "_id": "653df1323479e9ebbe3eb6cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
            "isPro": true,
            "fullname": "Zhangchen Xu",
            "user": "flydust",
            "type": "user"
          },
          "name": "Zhangchen Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:26:50.636Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4536",
          "user": {
            "_id": "637c88b6d55081513c5690d8",
            "avatarUrl": "/avatars/6766e23ebf46b46d6c8b48351c571907.svg",
            "isPro": false,
            "fullname": "Yang Liu",
            "user": "nlpyang",
            "type": "user"
          },
          "name": "Yang Liu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-06T02:26:54.940Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4537",
          "user": {
            "_id": "605e8dfd5abeb13e714c4c18",
            "avatarUrl": "/avatars/bc27a0ed17b2bd4311e89d3028fa327b.svg",
            "isPro": false,
            "fullname": "yueqin yin",
            "user": "yyqoni",
            "type": "user"
          },
          "name": "Yueqin Yin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:26:48.614Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4538",
          "user": {
            "_id": "653b2524b77b5e255f2d29d2",
            "avatarUrl": "/avatars/f69aea8de84c435295e7638bad5bd82e.svg",
            "isPro": false,
            "fullname": "Mingyuan Zhou",
            "user": "mingyuanzhou",
            "type": "user"
          },
          "name": "Mingyuan Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:03:56.474Z",
          "hidden": false
        },
        {
          "_id": "67c907ea7568a12737ad4539",
          "name": "Radha Poovendran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T19:17:36.000Z",
      "title": "KodCode: 多様性、難しい、バリデーション可能な合成データセットです。",
      "summary": "KodCodeは、ラージェットラングアウェイモデルの学習における高品質で確認可能な訓練データの取得の長期の課題を解決するための合成データセットです。現在のコーディングフォーカスリソースは、幅広いカバー（例：簡単なコーディングタスクから高度なアルゴリズム問題まで）または確認可能な正確性（例：ユニットテスト）の両方を確保することができません。対照的に、KodCodeは、自動認証プロセスでシステマティックに認証された問題-解決策-テストタリータルを構成しています。我々のパイプラインは、広範囲のコーディング問題を合成し、さらに難しい問題に対して追加的な試行を配布して解決策とテストケースを生成します。最後に、訓練後のデータ合成は、理由モデル（DeepSeek R1）からテストベースの拒否サンプリングプロセスを通じて質変わりのフォーマットでの質問を書き換え、回答を生成します。このパイプラインは、大規模的で強固な多様性のあるコーディングデータセットを生成します。KodCodeは、規範的訓練で使用可能で、ペアのユニットテストもRL調整にも大きなポテンシャルを提供します。コーディングベンチマーク（HumanEval（+）、MBPP（+）、BigCodeBench、LiveCodeBench）における訓練実験は、KodCodeで調整されたモデルが最先端の性能を達成し、Qwen2.5-Coder-32B-InstructやDeepSeek-R1-Distill-Llama-70Bよりも上回ることを示しました。",
      "upvotes": 7,
      "discussionId": "67c907ee7568a12737ad4633",
      "projectPage": "https://kodcode-ai.github.io/",
      "githubRepo": "https://github.com/KodCode-AI/kodcode"
    },
    "publishedAt": "2025-03-05T21:31:01.626Z",
    "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "flydust",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01836",
      "authors": [
        {
          "_id": "67c94c32dd505e6a4db201a2",
          "user": {
            "_id": "65d9a453f20e4fc19480afba",
            "avatarUrl": "/avatars/27bfa034a13a5e7cb5fc3b647515a201.svg",
            "isPro": false,
            "fullname": "yisen li",
            "user": "yisenL",
            "type": "user"
          },
          "name": "Yisen Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:04:09.687Z",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a3",
          "name": "Lingfeng Yang",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a4",
          "name": "Wenxuan Shen",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a5",
          "name": "Pan Zhou",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a6",
          "name": "Yao Wan",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a7",
          "name": "Weiwei Lin",
          "hidden": false
        },
        {
          "_id": "67c94c32dd505e6a4db201a8",
          "user": {
            "_id": "643be8879f5d314db2d9ed23",
            "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
            "isPro": false,
            "fullname": "Chen Dongping",
            "user": "shuaishuaicdp",
            "type": "user"
          },
          "name": "Dongping Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-06T10:09:00.496Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T18:56:44.000Z",
      "title": "CrowdSelect: 合成インストラクションデータ選択による多LLM知識",
      "summary": "大語言モデルの指導則遵守能力を選択されたサブセットを使用して小さなモデルに導出する手法がモデル訓練の主流となっています。現在の合成的な指導データ選択戦略は主に単次元の信号に基づいています（例えば、報酬スコア、モデルの難易度），それらは多様な分野での指導則遵守の複雑性を捉えずにいます。そこで、我々は、複雑な指導応答ペアの特性を捉えるために、より多様な信号を検討し、Multi-LLMの知識を活用した3つの基盤メトリックを提案します。これらの基盤メトリックに基づいて、我々はクラスタリングベースのアプローチを採用したCrowdSelectを提案します。我々の検証結果によると、4つの基盤モデルを通じてMT-benchとArena-Hardでの性能が一致していることが示されます。CrowdSelectは、すべてのメトリックを効率的に統合し、Llama-3.2-3b-instructではArena-Hardで4.81%、MT-benchで11.1%の向上を示します。我々の研究結果は、将来の研究において有効なエリアを提供しようとします。コードは、https://github.com/listentm/crowdselectに公開されています。",
      "upvotes": 6,
      "discussionId": "67c94c33dd505e6a4db201f6",
      "githubRepo": "https://github.com/listentm/crowdselect"
    },
    "publishedAt": "2025-03-06T02:20:38.735Z",
    "title": "CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01836.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643be8879f5d314db2d9ed23",
      "avatarUrl": "/avatars/64e9bb2c4e10fbe03e2b81afedf40865.svg",
      "fullname": "Chen Dongping",
      "name": "shuaishuaicdp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.18860",
      "authors": [
        {
          "_id": "67c95acf88c3b4201c10b9e9",
          "name": "Md Mehrab Tanjim",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ea",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9eb",
          "name": "Mike Rimer",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ec",
          "name": "Xiang Chen",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ed",
          "name": "Sungchul Kim",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ee",
          "name": "Vaishnavi Muppala",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9ef",
          "name": "Tong Yu",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f0",
          "name": "Zhengmian Hu",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f1",
          "name": "Ritwik Sinha",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f2",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f3",
          "name": "Iftikhar Ahamath Burhanuddin",
          "hidden": false
        },
        {
          "_id": "67c95acf88c3b4201c10b9f4",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:24:24.968Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T06:05:29.000Z",
      "title": "探索される対話タスクの違いに応じたリツリーティングアプローチ",
      "summary": "会話サソシアンは、過去の相互作用の一部を利用して、ユーザーの質問または要求に意味のある（正確な）回答を提供するための質問改変アルゴリズムを必要とすることが多いです。しかし、正確な改変アプローチは、会話サソシアンがサポートするタスクや他の制約によって異なります。本論文では、2つの基本的に異なる生成タスクの上で、2つの異なるアプローチ、「改変」と「融合」について系統的に調査します。これらのタスクは、テキストからテキストの生成タスクと、テキストを入力として、ユーザーの質問に答える可視化またはデータテーブルを生成する多モデル生成タスクです。私たちの結果は、特定の改変または融合アプローチは、軽い使用ケースと生成タスクによって非常に依存します。特に、会話サソシアンの場合、クエリ改変アプローチが最適であり、データ分析サソシアンの場合、クエリ融合アプローチが最適です。特に、データ分析サソシアンの使用ケースについて、短いものと長いコンバーションの2つのデータセットを調査し、クエリ融合は常により良い性能を示し、一方で、会話テキストベースの質問回答においては、クエリ改変アプローチが最適です。",
      "upvotes": 2,
      "discussionId": "67c95acf88c3b4201c10ba22"
    },
    "publishedAt": "2025-03-06T03:20:40.127Z",
    "title": "Exploring Rewriting Approaches for Different Conversational Tasks",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18860.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.03044",
      "authors": [
        {
          "_id": "67c94e6ad325e95d82f23433",
          "user": {
            "_id": "5e7749883d77a72421292d07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
            "isPro": false,
            "fullname": "Gabriele Sarti",
            "user": "gsarti",
            "type": "user"
          },
          "name": "Gabriele Sarti",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:24:30.601Z",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23434",
          "name": "Vilém Zouhar",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23435",
          "name": "Grzegorz Chrupała",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23436",
          "name": "Ana Guerberof-Arenas",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23437",
          "name": "Malvina Nissim",
          "hidden": false
        },
        {
          "_id": "67c94e6ad325e95d82f23438",
          "name": "Arianna Bisazza",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T22:50:17.000Z",
      "title": "QE4PE: 言語レベルの品質評価ツールの人間の後編集用",
      "summary": "単語レベルの品質評価（QE）は機械翻訳の間違いを検出し、人間の後編集を指導し、促進する。単語レベルのQEシステムの精度は広く評価されていますが、その利用可能性と、人間の後編集の速さ、品質、編集選択に及ぼす影響はまだ調査不足です。我々のQE4PE研究は、2つの翻訳方向の42名の専門的な後編集者が参加した実用的な設定で、機械翻訳（MT）の後編集中の単語レベルのQEの影響を調査しています。我々は、最先端のニューラルMTモデルの出力における潜在的な誤りを特定するために、4つの誤りスパンのハイライトモディオンを比較しています。後編集の努力と生産性は行動ログで評価され、品質の向上は単語レベルと段落レベルの人間のアノテーションにより評価されています。我々は、フィールド、言語と編集者の速さが高ライトの効果性を決定する重要な要因であることを見出し、人間のハンドライトと自動化されたQEハイライトの間での少しの違いが専門的なワークフロー内での精度と利用可能性の間の隙間を示していることを見出しました。",
      "upvotes": 2,
      "discussionId": "67c94e6fd325e95d82f23524"
    },
    "publishedAt": "2025-03-06T02:30:17.431Z",
    "title": "QE4PE: Word-level Quality Estimation for Human Post-Editing",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.03044.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e7749883d77a72421292d07",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
      "fullname": "Gabriele Sarti",
      "name": "gsarti",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 213
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.20317",
      "authors": [
        {
          "_id": "67c95b2f1fcfdc62ba3a620b",
          "name": "Yongjia Lei",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620c",
          "name": "Haoyu Han",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620d",
          "name": "Ryan A. Rossi",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620e",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Franck Dernoncourt",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:24:22.970Z",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a620f",
          "name": "Nedim Lipka",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a6210",
          "user": {
            "_id": "637c6d95a8716d642050b50f",
            "avatarUrl": "/avatars/0955a10113807348f24db968c7bd7c7a.svg",
            "isPro": false,
            "fullname": "Mahantesh Halappanavar",
            "user": "mhalappa",
            "type": "user"
          },
          "name": "Mahantesh M Halappanavar",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-06T08:22:09.090Z",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a6211",
          "name": "Jiliang Tang",
          "hidden": false
        },
        {
          "_id": "67c95b2f1fcfdc62ba3a6212",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-27T17:42:52.000Z",
      "title": "グラフ上の構造と文脈の併用検索\n知識ベース",
      "summary": "グラフ知識ベース（TG-KB）が、文脈と構造的な知識を提供して質問を答えるために重要になりました。しかし、現在の検索方法は、これらの2つの知識の相互的な強化を考慮しないように、それらを離れて検索しています。また、一部のハイブリッド方法は、隣接の集約後に構造的な検索を完全に回避しています。この空間を埋めるために、ここでは、構造的と文脈的な検索をプランニング・理由・組織化のフレームワークを通じてミックスで検索するMixture of Structural-and-Textual Retrieval（MoR）を提案します。プランニングステージで、MoRは質問を答えるための論理を明確にするために文脈的なプランニンググラフを生成します。その後、理由ステージでは、MoRは構造的な探索と文脈的なマッチングを組み合わせて、TG-KBから候補を得ます。そして、組織化ステージでは、MoRは構造的なトラジェクトに基づいて取得した候補をさらに再スコアします。広範囲の実験は、MoRが構造的と文脈的な検索を調和することの優れた性能を示し、質問の論理による検索性能の不均一性や構造的なトラジェクトの統合による候補の再スコアの利益などのヒントを含みます。コードは、https://github.com/Yoega/MoR に公開されています。",
      "upvotes": 1,
      "discussionId": "67c95b311fcfdc62ba3a62a5"
    },
    "publishedAt": "2025-03-06T03:22:14.664Z",
    "title": "Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.20317.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01763",
      "authors": [
        {
          "_id": "67c92e9c746bbcdbdfa8ebd4",
          "name": "Zhengliang Shi",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd5",
          "name": "Yuhan Wang",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd6",
          "name": "Lingyong Yan",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd7",
          "name": "Pengjie Ren",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd8",
          "name": "Shuaiqiang Wang",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebd9",
          "name": "Dawei Yin",
          "hidden": false
        },
        {
          "_id": "67c92e9c746bbcdbdfa8ebda",
          "name": "Zhaochun Ren",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T17:37:16.000Z",
      "title": "レビュアルモデルはツールに詳しくない：大語言モデルのツールレビュアルをベンチマークする",
      "summary": "ツール学習は、多様なツールを持つ大規模言語モデル（LLMs）に加え、実用的なタスクを解決するアガントとして活用することを目的としています。ツールを使用するLLMsのコンテキスト長の制限を考慮し、情報検索（IR）モデルを用いて大規模なツールセットから有用なツールを選択することが重要な最初のステップです。しかし、IRモデルがツール検索タスクにおける性能は詳しく調査されていません。ツール使用ベンチマークでは、これを簡単化するために、各タスクに関連する小さなツールセットを手動で事前標準化していますが、実世界的なシナリオから遠く離れています。この論文では、7.6k種類の多様な検索タスクと43kのツールコーパスを構成した異なるツール検索ベンチマーク「ToolRet」を提案します。ToolRet上では6種類のモデルをベンチマークします。驚くべきに、伝統的なIRベンチマークで強い性能を示すモデルもToolRetでは劣しい性能を示します。この低い検索品質は、ツール使用LLMsのタスク通過率を低下させます。さらに、200k以上のインスタンスを含む大規模な訓練データセットを提供し、IRモデルのツール検索能力を大幅に最適化します。",
      "upvotes": 1,
      "discussionId": "67c92e9e746bbcdbdfa8ec57"
    },
    "publishedAt": "2025-03-06T00:12:07.867Z",
    "title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01763.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01729",
      "authors": [
        {
          "_id": "67c92e8b5650d7efeba5b48c",
          "name": "Santiago Bou Betran",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48d",
          "name": "Alberta Longhini",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48e",
          "name": "Miguel Vasco",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b48f",
          "name": "Yuchong Zhang",
          "hidden": false
        },
        {
          "_id": "67c92e8b5650d7efeba5b490",
          "name": "Danica Kragic",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T16:49:15.000Z",
      "title": "FLAME: ロボット操作のフェデレイドラーニングベンチマーク",
      "summary": "最近のロボット操作の進展は、多様な環境で採集された大規模なデータセットによって駆動されています。これらのデータセットでロボット操作ポリシーの訓練は、これまでは中心化されたモチョードで行われており、スケーラブルさ、適応性、データプライバシーに関する懸念がありました。一方で、フェデレイドラーニングは分散的で、プライバシーを保護するように訓練することができるため、ロボット操作においても応用が残されています。私たちは、ロボット操作のフェデレイドラーニングに適した最初のベンチマークとしてFLAME（Manipulation Environmentsでのフェデレイドラーニング）を紹介します。FLAMEは以下の2つの構成要素を含みます：（i）多様な操作タスクのエキスパートデモンストレーションの160,000点以上の大規模なデータセットのセット；（ii）ロボットポリシー学習のフェデレイドラーニング設定での訓練と評価フレームワーク。FLAMEでは標準のフェデレイドラーニングアルゴリズムを評価し、分散されたポリシー学習の可能性を示し、重要な課題を明らかにします。このベンチマークは、スケーラブルで適応性のあるロボット学習の基盤を築きます。",
      "upvotes": 1,
      "discussionId": "67c92e8d5650d7efeba5b519"
    },
    "publishedAt": "2025-03-06T00:11:48.501Z",
    "title": "FLAME: A Federated Learning Benchmark for Robotic Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01729.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01449",
      "authors": [
        {
          "_id": "67c92e738d5fe8c860571103",
          "name": "Ting Zhang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571104",
          "name": "Chengran Yang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571105",
          "name": "Yindu Su",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571106",
          "name": "Martin Weyssow",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571107",
          "name": "Hung Nguyen",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571108",
          "name": "Tan Bui",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c860571109",
          "name": "Hong Jin Kang",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110a",
          "name": "Yikun Li",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110b",
          "name": "Eng Lieh Ouh",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110c",
          "name": "Lwin Khin Shar",
          "hidden": false
        },
        {
          "_id": "67c92e738d5fe8c86057110d",
          "name": "David Lo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T11:56:00.000Z",
      "title": "バージョンチェックファイルをダウンロードする",
      "summary": "最近の生成AIの進歩は、ソフトウェアエンジニアリングにおいて大規模な言語モデル（LLMs）の広泛な採用を促し、長年の諸多の課題を解決しました。しかし、ソフトウェアセキュリティの重要な一環としてのソフトウェア脆弱性検出（SVD）におけるLLMsの能力を詳細に調査する詳細な研究は、現在までにあまり見られていません。現在の研究は主にC/C++データセットを用いてLLMsを評価しています。通常、プロジェクトエンジニアリング、インストラクショントーニング、シーケンスクラスファイナルファイニングのうちの1つや2つの戦略を調査しています。このような研究の結果、LLMsが多様なプログラミング言語で脆弱性を検出する効果性についての知識欠如が大きく残っています。この知識欠如を解決するために、LLMsのSVDタスクの性能を評価する詳細な実験研究を行います。プロジェクトを準備していますPythonで8,260ファンクション、Javaで7,505ファンクション、JavaScriptで28,983ファンクションの脆弱なファンクションを含む詳細なデータセットを準備しています。5つのプロジェクトソースLLMsをプロジェクトエンジニアリング、インストラクショントーニング、シーケンスクラスファイナルファイニングなどの複数のアプローチを用いて評価します。これらのLLMsは5つのファイナルファイニングされた小規模な言語モデルと2つのプロジェクトソース静的アプリケーションセキュリティテストツールと比較されます。また、LLMsのSVDタスクの性能を向上させるために、データの視点とモデルの視点の2つのアプローチを調査します。データの視点では、データをダウンサンプリングした平衡データセットを用いてモデルを再学習します。モデルの視点では、複数のLLMsからの予測を結合するエンサンス学習の方法を調査します。詳細な実験により、SVDはLLMsの難しいタスクとして残っていることが示されました。この研究は、LLMsがSVDにおける役割を詳細に理解し、生成AIを活用してソフトウェアセキュリティの実践を向上させるための実用的なガイドラインを提供します。",
      "upvotes": 1,
      "discussionId": "67c92e748d5fe8c860571142"
    },
    "publishedAt": "2025-03-06T00:11:25.013Z",
    "title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01449.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01378",
      "authors": [
        {
          "_id": "67c92e537ae0115c7a7b9fa3",
          "name": "Artem Lykov",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa4",
          "name": "Valerii Serpiva",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa5",
          "name": "Muhammad Haris Khan",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa6",
          "name": "Oleg Sautenkov",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa7",
          "name": "Artyom Myshlyaev",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa8",
          "name": "Grik Tadevosyan",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9fa9",
          "name": "Yasheerah Yaqoot",
          "hidden": false
        },
        {
          "_id": "67c92e537ae0115c7a7b9faa",
          "name": "Dzmitry Tsetserukou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:21:36.000Z",
      "title": "コガクドーン：UAVでの実時間での認知タスク解決と理由論のVLAモデルと評価ベンチマーク",
      "summary": "この論文では、複雑な無人飛行機（UAV）タスクに適した新しい Vision-Language-Action (VLA) モデル「CognitiveDrone」を紹介します。このモデルは、8,000以上の訓練データセットを基に、人間認識、記号理解、理由論理の3つのカテゴリにおいて、1人目の視覚入力と文字列指示に基づいて時系列的に4次元の行動命令を生成します。複雑なシナリオでの性能向上を図るために、追加の Vision-Language Model (VLM) 理由論理モジュールを組み込み、高頻度制御の前にタスク指示を簡単化するための「CognitiveDrone-R1」を提案します。私たちの開放ソースベンチマーク「CognitiveDroneBench」を用いた実験評価により、レース向けのモデル（RaceVLA）は全体の成功率が31.3%でしたが、基本的なCognitiveDroneモデルは59.6%、CognitiveDrone-R1は77.2%の成功率を達成しました。これらの結果は、重要な認知タスクにおける30%程度の向上を示し、無人飛行機制御システムに進歩的な理由論理能力を組み込む効果を強調します。私たちの貢献は、最先端のVLAモデルの開発と、無人飛行機操作の認知タスクの評価に向けた最初の専用ベンチマークの導入です。完全なリポジトリは cognitivedrone.github.io にアクセスできます。",
      "upvotes": 1,
      "discussionId": "67c92e547ae0115c7a7b9fe6"
    },
    "publishedAt": "2025-03-06T00:10:56.364Z",
    "title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01378.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.00502",
      "authors": [
        {
          "_id": "67c8427047c2aa135346dced",
          "user": {
            "_id": "66d3290364c1e9b73208af82",
            "avatarUrl": "/avatars/e0ea5f2b366927c7b146f248028a2e59.svg",
            "isPro": false,
            "fullname": "Shiyu Fang",
            "user": "FanGShiYuu",
            "type": "user"
          },
          "name": "Shiyu Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-05T15:46:48.691Z",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcee",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcef",
          "name": "Chengkai Xu",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf0",
          "name": "Chen Lv",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf1",
          "name": "Peng Hang",
          "hidden": false
        },
        {
          "_id": "67c8427047c2aa135346dcf2",
          "name": "Jian Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-01T14:15:52.000Z",
      "title": "Interact, Instruct to Improve: LLMドリバイドンプラライアルアクター・レジナーフレームワークの自動運転車の相互作用を向上させる",
      "summary": "自動運転車（AVs）は商用化ステージに入り、その相互作用と意図表現の限られた能力は、人間運転車（HVs）との相互作用に課題を残しています。大規模言語モデル（LLMs）の最近の進歩は、両方向の人間マシンコミュニケーションを可能にしましたが、遅い推論速度と実時間決定の必要との衝突は、実用的な採用に課題をつけています。これらの問題を解決するために、本論文では、複数のシナリオでの明確な両方向的なAV-HV相互作用を可能にするための並列アクター・レジョナーフレームワークを介しています。まず、LLM駆動のレジョナーと異なるシミュレーションされたHVsとの相互作用を促進することで、アクターと呼ばれる相互作用メモリデータベースを設立します。次に、メモリパーティションモジュールと2層メモリ検索モジュールを導入することで、アクターの異なるHVsを処理する能力が大幅に向上します。アブレーション研究と他の決定論方法との比較により、提案されたアクター・レジョナーフレームワークは安全性と効率性を大幅に向上させることを示しています。最後に、レジョナーの理由による外部人間マシンインターフェース（eHMI）情報とアクターから採用可能な行動解決策の組み合わせで、提案されたアクター・レジョナーの効果が複数のシナリオでの野外相互作用で確認されています。コードは、https://github.com/FanGShiYuu/Actor-Reasoner に公開されています。",
      "upvotes": 1,
      "discussionId": "67c8427247c2aa135346dd84",
      "projectPage": "https://fangshiyuu.github.io/Actor-Reasoner/",
      "githubRepo": "https://github.com/FanGShiYuu/Actor-Reasoner"
    },
    "publishedAt": "2025-03-05T21:37:18.981Z",
    "title": "Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.00502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d3290364c1e9b73208af82",
      "avatarUrl": "/avatars/e0ea5f2b366927c7b146f248028a2e59.svg",
      "fullname": "Shiyu Fang",
      "name": "FanGShiYuu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.01372",
      "authors": [
        {
          "_id": "67c6bd6e8f3e7fd471affd06",
          "name": "Joel Niklaus",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd07",
          "name": "Jakob Merane",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd08",
          "name": "Luka Nenadic",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd09",
          "name": "Sina Ahmadi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0a",
          "name": "Yingqiang Gao",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0b",
          "name": "Cyrill A. H. Chevalley",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0c",
          "name": "Claude Humbel",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0d",
          "name": "Christophe Gösken",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0e",
          "name": "Lorenzo Tanzi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd0f",
          "name": "Thomas Lüthi",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd10",
          "name": "Stefan Palombo",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd11",
          "name": "Spencer Poff",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd12",
          "name": "Boling Yang",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd13",
          "name": "Nan Wu",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd14",
          "name": "Matthew Guillod",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd15",
          "name": "Robin Mamié",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd16",
          "name": "Daniel Brunner",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd17",
          "name": "Julio Pereyra",
          "hidden": false
        },
        {
          "_id": "67c6bd6e8f3e7fd471affd18",
          "name": "Niko Grupen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T10:10:30.000Z",
      "title": "SwiLTra-Bench: シュイフル法規翻訳ベンチマーク",
      "summary": "スイスでは、法律翻訳は国の4つの正規言語と多言語の法律文書の要求により特別に重要です。しかし、このプロセスは、法律専門家とスキルフルな翻訳者の両方とも必要な専門家に依存し、ボトルネックを生み出し、公正の効果的なアクセスに影響を及ぼします。この挑戦を解決するために、私たちは、すべてのスイス言語（含む英語）の法律、ヘッドノート、プレスリリースの180K以上の対応翻訳ペアの詳細な多言語ベンチマーク、SwiLTra-Benchを紹介します。このベンチマークは、LLMベースの翻訳システムを評価するために設計されています。ロジスティック的な評価により、先進モデルはすべてのドキュメントタイプで上位の翻訳性能を収め、専門的な翻訳システムは特に法律で優れているが、ヘッドノートでは劣ります。厳格なテストと人間の専門家の評価により、私たちは、開放システムの微調節は翻訳の品質を大幅に向上させることを示し、それらはベストのゼロショットプロンプトされた先進モデル（例えばClaude-3.5-Sonnet）に比べてもらって落ち込みます。また、SwiLTra-Judgeという専門的なLLM評価システムを紹介し、それは人間の専門家の評価に最も一致します。",
      "upvotes": 0,
      "discussionId": "67c6bd708f3e7fd471affd5d"
    },
    "publishedAt": "2025-03-06T00:10:21.173Z",
    "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01372.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640d3eaa3623f6a56dde856d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg",
      "fullname": "vansin",
      "name": "vansin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  }
]