[
  {
    "paper": {
      "id": "2502.12152",
      "authors": [
        {
          "_id": "67b41ed52867282b4eb37ce4",
          "name": "Xialin He",
          "hidden": false
        },
        {
          "_id": "67b41ed52867282b4eb37ce5",
          "user": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "isPro": false,
            "fullname": "Runpei Dong",
            "user": "RunpeiDong",
            "type": "user"
          },
          "name": "Runpei Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:13.178Z",
          "hidden": false
        },
        {
          "_id": "67b41ed52867282b4eb37ce6",
          "name": "Zixuan Chen",
          "hidden": false
        },
        {
          "_id": "67b41ed52867282b4eb37ce7",
          "name": "Saurabh Gupta",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T18:59:06.000Z",
      "title": "Learning Getting-Up Policies for Real-World Humanoid Robots",
      "summary": "自動的フォールリカバリーは、人形ロボットが信頼性のある部署において重要な前提条件です。人形がフォールした後の多様な構造により、ロボットが操作する難しい地形により、手作業でコントローラーを設計するのは難しいです。本論文では、人形ロボットが多様な構造と多様な地形から立つことを可能にするコントローラーを生成する学習フレームワークを開発します。前の成功した人形ロボットの移動学習の応用と違い、立つタスクには複雑な接触パターンが含まれ、これには衝突の幾何学的モデリングと稀疏な報酬の説明が必要です。これらの挑戦に対して、2段階アプローチをカレクリウムに従って解決します。最初のステップは、平滑性やスピード/トルクの制限を最小限に抑えた状態で、良い立つタライトを発見することを焦点とします。次のステップでは、発見した動作を実際に利用可能な（つまり平滑で徐々な）動作に改良し、初期構造と地形の変化に対して強固であるものにします。私たちは、これらの革新的な技術が、実世界でのG1人形ロボットが2つの主要な状態から立つことを可能にしました：a）顔上に横たわっている状態とb）顔下に横たわっている状態、両方は平坦で可塑性のある、滑りやすい表面と斜め（例：滑りやすい草と雪野）でテストされました。私たちの知識の限り、これは人間サイズの人形ロボットが実世界で学習した立つポリシーの最初の成功な実験です。プロジェクトページ：https://humanoid-getup.github.io/",
      "upvotes": 23,
      "discussionId": "67b41edb2867282b4eb37ddf"
    },
    "publishedAt": "2025-02-18T00:49:53.124Z",
    "title": "Learning Getting-Up Policies for Real-World Humanoid Robots",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/x35BuXOhc6ubukxLfiVzt.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12152.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6201fc5d91d53938a6432fbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
      "fullname": "Runpei Dong",
      "name": "RunpeiDong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11190",
      "authors": [
        {
          "_id": "67b420dfb2528c023491f455",
          "name": "Haoming Xu",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f456",
          "name": "Ningyuan Zhao",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f457",
          "name": "Liming Yang",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f458",
          "name": "Sendong Zhao",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f459",
          "name": "Shumin Deng",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f45a",
          "name": "Mengru Wang",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f45b",
          "name": "Bryan Hooi",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f45c",
          "name": "Nay Oo",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f45d",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67b420dfb2528c023491f45e",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:11.243Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T16:31:00.000Z",
      "title": "ReLearn: 大語言モデルによる忘却を学習によって実現する",
      "summary": "現在の大規模言語モデルの忘却方法は、逆復元最適化を通じて目標トークンの確率を減少しています。しかし、このパラダイムは後続トークンの予測を破壊し、モデルの性能と語学の一貫性を低下させます。また、現在の評価指標は、コンテキスト忘却を過剰に重視し、応答の流れと関連性を不十分に評価しています。これらの課題に対処するために、私たちはReLearnを提案します。ReLearnは、効果的な忘却を実現するためのデータ拡張と微調校パイプラインです。また、詳細な評価フレームワークを採用しています。このフレームワークは、知識の保存を評価するための知識忘却率（KFR）と知識保存率（KRR）を、生成品質を評価するための語学スコア（LS）を導入しています。私たちの実験は、ReLearnが目標忘却を実現しながら高品質な出力を保持することを成功していることを示しています。機構的な分析を通じて、逆復元最適化がコンテキストの一貫性を破壊することを、ReLearnがこの基本的な能力を保持することを進一度に示しています。コードは、https://github.com/zjunlp/unlearn に公開されています。",
      "upvotes": 11,
      "discussionId": "67b420e2b2528c023491f506"
    },
    "publishedAt": "2025-02-18T00:58:24.094Z",
    "title": "ReLearn: Unlearning via Learning for Large Language Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/A4YB7t6hDVty6QrvLN0a7.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11190.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12115",
      "authors": [
        {
          "_id": "67b41a72a38d04cc6148d80e",
          "name": "Samuel Miserendino",
          "hidden": false
        },
        {
          "_id": "67b41a72a38d04cc6148d80f",
          "name": "Michele Wang",
          "hidden": false
        },
        {
          "_id": "67b41a72a38d04cc6148d810",
          "name": "Tejal Patwardhan",
          "hidden": false
        },
        {
          "_id": "67b41a72a38d04cc6148d811",
          "name": "Johannes Heidecke",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T18:41:16.000Z",
      "title": "SWE-Lancer: フロンティアLLMsは、本格的なフリーランスソフトウェアエンジニアリングで100万ドルを稼ぐことはできるでしょうか？",
      "summary": "We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at $1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from 50 bug fixes to $32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.",
      "upvotes": 9,
      "discussionId": "67b41a74a38d04cc6148d84b"
    },
    "publishedAt": "2025-02-18T00:28:31.293Z",
    "title": "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12115.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6128
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12148",
      "authors": [
        {
          "_id": "67b40c8cdb88dfd19ab917f3",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67b40c8cdb88dfd19ab917f4",
          "user": {
            "_id": "653e5d31ffd60206c8b64bb5",
            "avatarUrl": "/avatars/5076795722ec1f9e031654f301d30e8f.svg",
            "isPro": false,
            "fullname": "Xinchen Zhang",
            "user": "comin",
            "type": "user"
          },
          "name": "Xinchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:31.841Z",
          "hidden": false
        },
        {
          "_id": "67b40c8cdb88dfd19ab917f5",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "67b40c8cdb88dfd19ab917f6",
          "name": "Chenming Shang",
          "hidden": false
        },
        {
          "_id": "67b40c8cdb88dfd19ab917f7",
          "name": "Minghao Xu",
          "hidden": false
        },
        {
          "_id": "67b40c8cdb88dfd19ab917f8",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67b40c8cdb88dfd19ab917f9",
          "name": "Bin Cui",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T18:57:51.000Z",
      "title": "ヘリマスフロー：多モデル理解と生成の隙を無間に閉じる",
      "summary": "順語法のモデルの偉大的な成功は、多モーダル大語言モデル（MLLMs）において顕著な進歩を達成しました。Show-o、Transfusion、Emu3などの強力なモデルが画期的な進展を達成し、統一的な画像理解と生成において注目を集めました。まずは、MLLMsの理解能力が通常である生成能力よりも強く、これらの間に大きな間違いがあることを発見しました。この見解に基づき、HermesFlowという簡単で一般的なフレームワークを提案しました。このフレームワークは、理解と生成の間の隙間を無難に連結することを目的としています。特に、同様なデータを入力として、理解と生成の両方の同様な好みデータをカレートし、Pair-DPOと自分自身のプレイを通じて、同様な好みデータを用いて多モーダルの理解と生成を適切に調整します。拡張された実験は、先行方法よりも我々のアプローチの顕著な優位性を示し、特に多モーダルの理解と生成の間の間違いを狭めることに特に適しています。これらの発見は、HermesFlowが次世代の多モーダルファンダメンタルモデルの一般的な調整フレームワークとしての可能性を明らかにしています。コード：https://github.com/Gen-Verse/HermesFlow",
      "upvotes": 9,
      "discussionId": "67b40c8edb88dfd19ab9183f"
    },
    "publishedAt": "2025-02-17T23:29:29.396Z",
    "title": "HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12148.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653e5d31ffd60206c8b64bb5",
      "avatarUrl": "/avatars/5076795722ec1f9e031654f301d30e8f.svg",
      "fullname": "Xinchen Zhang",
      "name": "comin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11167",
      "authors": [
        {
          "_id": "67b4221bbc387d2eda6f8637",
          "user": {
            "_id": "650267e7e751d03da933a24a",
            "avatarUrl": "/avatars/f047a047d1de304cd97027463541bdf3.svg",
            "isPro": false,
            "fullname": "Bohan22",
            "user": "Bohan22",
            "type": "user"
          },
          "name": "Bohan Lyu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:06.388Z",
          "hidden": false
        },
        {
          "_id": "67b4221bbc387d2eda6f8638",
          "name": "Siqiao Huang",
          "hidden": false
        },
        {
          "_id": "67b4221bbc387d2eda6f8639",
          "user": {
            "_id": "67286718746a95c09d04cb1d",
            "avatarUrl": "/avatars/317efa8459cca08c2ff56c3ab116e15c.svg",
            "isPro": false,
            "fullname": "Zichen Liang",
            "user": "zcliang22",
            "type": "user"
          },
          "name": "Zichen Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:08.469Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T15:38:19.000Z",
      "title": "SURGE: 大規模言語モデルの一般的なコード実行機能の可能性",
      "summary": "大型言語モデル（LLMs）は、コード相関タスクにおいて卓越した能力を示しています。例えば、コード理解とコード生成など。しかし、同等重要でありますが、調査が不足している問題は、LLMsが一般的なコード実行者として役立つことができるかどうかです。この能力を体系的に調査するために、SURGEという詳細なベンチマークを導入します。SURGEは、8つのキーの面で構成されています：多言語プログラミングタスク、コンペティションレベルのプログラミング問題、リポジトリレベルのコード分析、高コストの科学計算、時間複雑さ強いアルゴリズム、バグコード分析、特定のコンパイラまたは実行環境に依存するプログラム、そして形式的な数学証明検証。SURGEで複数の開放ソースおよび所有権あるLLMsを評価し、モデルサイズと学習データサイズの影響を分析するスケーリングスタディを行います。また、モデル予測エラーを分類し、改善の可能性のある領域を探ることも行います。我々の発見は、LLMsは特定の場合ではコード実行結果を予測できることを示しますが、一般的なコード実行者としての機能性に欠点を見出します。この研究は、LLMsを代わりのコード実行者として使用する可能性についての実証的な見通しを提供します。コードとデータセットは、https://github.com/Imbernoulli/SURGE でリリースされています。",
      "upvotes": 7,
      "discussionId": "67b4221ebc387d2eda6f8717"
    },
    "publishedAt": "2025-02-18T01:01:24.331Z",
    "title": "SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11167.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "650267e7e751d03da933a24a",
      "avatarUrl": "/avatars/f047a047d1de304cd97027463541bdf3.svg",
      "fullname": "Bohan22",
      "name": "Bohan22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12146",
      "authors": [
        {
          "_id": "67b40ce4d3c5f50aa9b71df5",
          "name": "Ye Tian",
          "hidden": false
        },
        {
          "_id": "67b40ce4d3c5f50aa9b71df6",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67b40ce4d3c5f50aa9b71df7",
          "user": {
            "_id": "653e5d31ffd60206c8b64bb5",
            "avatarUrl": "/avatars/5076795722ec1f9e031654f301d30e8f.svg",
            "isPro": false,
            "fullname": "Xinchen Zhang",
            "user": "comin",
            "type": "user"
          },
          "name": "Xinchen Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:29.697Z",
          "hidden": false
        },
        {
          "_id": "67b40ce4d3c5f50aa9b71df8",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "67b40ce4d3c5f50aa9b71df9",
          "name": "Mengdi Wang",
          "hidden": false
        },
        {
          "_id": "67b40ce4d3c5f50aa9b71dfa",
          "name": "Bin Cui",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T18:57:26.000Z",
      "title": "Diffusion-Sharpening: ディフュージョンモデルの微調節におけるデノイズトラジェクトのチャーペンING",
      "summary": "ディフュージョン-シャープニングを提案します。これは、サンプリングプロセスのトラジェクトを最適化して、下流のアライメントを向上させる微調整アプローチです。現在のRLベースの微調整方法は、単一のトレーニングステップを焦点にし、トラジェクトレベルのアライメントを飛ばしていますが、最近のサンプリングトラジェクト最適化方法は、過大的な推論NFEコストを課します。ディフュージョン-シャープニングは、トレーニング中に最適なトラジェクトを選択するためのパス積分フレームワークを使用し、報酬フィードバックを活用し、推論コストを乗り越えます。我々の方法は、追加のNFEが必要なくても最善の推論エフィシェンスを示し、効率的なトレーニング効率と早い収束を見せます。拡大した実験は、テキストアライメント、構成能力、ヒューマンの好みなどの多様なメトリックで、RLベースの微調整方法（例：ディフュージョン-DPO）とサンプリングトラジェクト最適化方法（例：推論スケーリング）を超える結果を示します。コード：https://github.com/Gen-Verse/Diffusion-Sharpening",
      "upvotes": 6,
      "discussionId": "67b40ce8d3c5f50aa9b71f9a"
    },
    "publishedAt": "2025-02-17T23:30:53.097Z",
    "title": "Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12146.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653e5d31ffd60206c8b64bb5",
      "avatarUrl": "/avatars/5076795722ec1f9e031654f301d30e8f.svg",
      "fullname": "Xinchen Zhang",
      "name": "comin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.10458",
      "authors": [
        {
          "_id": "67b3ea0f4dd7ea0538ce589d",
          "user": {
            "_id": "6354bda206d707b33249c4c2",
            "avatarUrl": "/avatars/bbd9f76274ac52214df92084d50bc7b5.svg",
            "isPro": false,
            "fullname": "Zhenxing Mi",
            "user": "Mifucius",
            "type": "user"
          },
          "name": "Zhenxing Mi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:52.837Z",
          "hidden": false
        },
        {
          "_id": "67b3ea0f4dd7ea0538ce589e",
          "name": "Kuan-Chieh Wang",
          "hidden": false
        },
        {
          "_id": "67b3ea0f4dd7ea0538ce589f",
          "name": "Guocheng Qian",
          "hidden": false
        },
        {
          "_id": "67b3ea0f4dd7ea0538ce58a0",
          "name": "Hanrong Ye",
          "hidden": false
        },
        {
          "_id": "67b3ea0f4dd7ea0538ce58a1",
          "name": "Runtao Liu",
          "hidden": false
        },
        {
          "_id": "67b3ea0f4dd7ea0538ce58a2",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "67b3ea0f4dd7ea0538ce58a3",
          "name": "Kfir Aberman",
          "hidden": false
        },
        {
          "_id": "67b3ea0f4dd7ea0538ce58a4",
          "name": "Dan Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T05:30:08.000Z",
      "title": "「思い出して、それを拡散させる：拡散モデルでの多モーダル場合ごとの理由論」",
      "summary": "この論文では、ThinkDiffという新しいアラインメントパラダイムを紹介します。これは、視覚言語モデル（VLMs）の強みを統合して、テキストから画像のディフュージョンモデルに多タイプのコンテキスト理解と推論能力を提供します。現在の多タイプディフュージョンの微調編集方法は、ピクセルレベルの再構成に焦点を当てているが、コンテキスト推論には少なく、推論データセットの複雑さと有限性に制限されています。ThinkDiffは、視覚言語訓練を仮タスクとして、VLMsとエンコーダー・デコーダーの大規模言語モデル（LLM）のデコーダーをアラインさせることで、これらの挑戦を解決しています。この仮タスクは、LLMのデコーダーが、ディフュージョンデコーダーが使用する対応するLLMエンコーダーでのプロンプト埋め込みを使用している場合に同じ入力特徴空間を共有していることを観察しています。このため、VLMsとディフュージョンデコーダーのアラインメントを簡単に行うことができます。複雑な訓練やデータセットが必要なく、ThinkDiffはディフュージョンモデルに理解、推論、および組み立ての能力を効果的に発揮します。実験は、CoBSATの挑戦的なベンチマークでの精度を19.2%から46.3%に大幅に向上させ、4つのA100GPUで5時間の訓練でも実現できました。また、ThinkDiffは、複数の画像とテキストをロジック的に一貫した画像に組み立てるための出色な性能を示しています。プロジェクトページ：https://mizhenxing.github.io/ThinkDiff。",
      "upvotes": 5,
      "discussionId": "67b3ea124dd7ea0538ce592d"
    },
    "publishedAt": "2025-02-18T04:33:41.120Z",
    "title": "I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10458.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6354bda206d707b33249c4c2",
      "avatarUrl": "/avatars/bbd9f76274ac52214df92084d50bc7b5.svg",
      "fullname": "Zhenxing Mi",
      "name": "Mifucius",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11196",
      "authors": [
        {
          "_id": "67b42223c2fe54b8d43efed6",
          "name": "Yixin Ou",
          "hidden": false
        },
        {
          "_id": "67b42223c2fe54b8d43efed7",
          "name": "Yunzhi Yao",
          "hidden": false
        },
        {
          "_id": "67b42223c2fe54b8d43efed8",
          "user": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "isPro": false,
            "fullname": "Ningyu Zhang",
            "user": "Ningyu",
            "type": "user"
          },
          "name": "Ningyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:04.227Z",
          "hidden": false
        },
        {
          "_id": "67b42223c2fe54b8d43efed9",
          "name": "Hui Jin",
          "hidden": false
        },
        {
          "_id": "67b42223c2fe54b8d43efeda",
          "name": "Jiacheng Sun",
          "hidden": false
        },
        {
          "_id": "67b42223c2fe54b8d43efedb",
          "name": "Shumin Deng",
          "hidden": false
        },
        {
          "_id": "67b42223c2fe54b8d43efedc",
          "name": "Zhenguo Li",
          "hidden": false
        },
        {
          "_id": "67b42223c2fe54b8d43efedd",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T16:55:43.000Z",
      "title": "LLMは新しい知識をどのように取得しますか？継続的な予め学習の知識サイクルの視点から",
      "summary": "エクステンダブルな知識密集型タスクにおいて特別な能力を持つために、大規模言語モデル（LLMs）は、新しい知識を内部化する方法を理解する重要な欠陥を見出しています。特に、その学習された知識をニューラルコンピューティングに構造化的に埋め込む方法については、理解が難しいことがあります。我々は、知識回路の進化を通じてこの問題を解決し、知識の保存と処理を促進する計算的なサブグラフを特定しています。連続的な予ち練習の全過程での回路進化の系統的な分析から、以下のような重要な発見が得られました：1）新しい知識の取得は、既存の知識との関連性に影響されています；2）知識回路の進化は形成から最適化への違ったステップを見出します；3）知識回路の進化は深かって浅いパターンに従います。これらの見解は、LLMsでの新しい知識の取得機構の理論的な理解を進め、連続的な予ち練習の戦略を改良してモデルの性能を向上させるための潜在的な意味を提供します。コードとデータは、https://github.com/zjunlp/DynamicKnowledgeCircuits から利用できます。",
      "upvotes": 5,
      "discussionId": "67b42225c2fe54b8d43eff9b"
    },
    "publishedAt": "2025-02-18T01:02:25.236Z",
    "title": "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/_LGnwvwslWc3YDIirfOKS.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11196.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11438",
      "authors": [
        {
          "_id": "67b406993d0f54ab381594f5",
          "name": "Jimin Lee",
          "hidden": false
        },
        {
          "_id": "67b406993d0f54ab381594f6",
          "name": "Ingeol Baek",
          "hidden": false
        },
        {
          "_id": "67b406993d0f54ab381594f7",
          "name": "Byeongjeong Kim",
          "hidden": false
        },
        {
          "_id": "67b406993d0f54ab381594f8",
          "name": "Hwanhee Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T04:52:24.000Z",
      "title": "SAFE-SQL: 文本からSQLへの自動化フィングレイドサンプリングによるテキストフォーマット内でのフィードバック学習",
      "summary": "Text-to-SQLは、自然言語の質問を実行可能なSQLクエリに変換することを目的としています。前のアプローチでは、骨格マスク選択などの手法が、類似した学習例を検索して大規模な言語モデル（LLM）を指導し、強力な性能を示しましたが、実世界的なシナリオでは、このような例がない場合には困難を見せます。この制限を克服するために、私たちは、Text-to-SQLのSQL生成を改善するための新しいフレームワーク「Self-Augmentation in-context learning with Fine-grained Example selection for Text-to-SQL (SAFE-SQL)」を提案します。SAFE-SQLは、自動生成された例を生成してフィルタリングし、高品質なin-context learning例を構築します。SAFE-SQLは、自動生成された例を使用して、先ほどの零ショット、少数ショットのText-to-SQLフレームワークを超え、実行精度を高めます。特に、我々のアプローチは、単なる方法が失敗する非常に難しいおそらく見つからないシナリオでも、追加の性能向上を提供します。",
      "upvotes": 5,
      "discussionId": "67b4069a3d0f54ab38159520"
    },
    "publishedAt": "2025-02-17T23:06:03.562Z",
    "title": "SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11438.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f6f245e94ed998c46316df",
      "avatarUrl": "/avatars/9c0ec8682d4a85b96d2180602b1bbe6c.svg",
      "fullname": "ingeolbaek",
      "name": "ingeol",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.09061",
      "authors": [
        {
          "_id": "67b401de3995f28d45c212d6",
          "name": "Debangshu Banerjee",
          "hidden": false
        },
        {
          "_id": "67b401de3995f28d45c212d7",
          "name": "Tarun Suresh",
          "hidden": false
        },
        {
          "_id": "67b401de3995f28d45c212d8",
          "name": "Shubham Ugare",
          "hidden": false
        },
        {
          "_id": "67b401de3995f28d45c212d9",
          "name": "Sasa Misailovic",
          "hidden": false
        },
        {
          "_id": "67b401de3995f28d45c212da",
          "name": "Gagandeep Singh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-13T08:23:42.000Z",
      "title": "CRANE: 制約付きLLM生成に基づく推理",
      "summary": "コード生成、符号的数学論理、その他のタスクにおいて、LLMは合成的語法と意味的に正しい出力を生成する必要があります。LLMの生成を正式的語法に従うことを強制することは、実験的に見られたように、理由論理能力を低下させることが多いです。本研究では、まず、LLMの出力を非常に制限された語法に従うことが理由論理能力を低下させる理由を理論的に説明します。次に、出力語法に謹密に設計された追加のルールを追加して、理由論理能力を保持しながら合成的語法と意味的な正確性を確保することが可能であることを示します。これらの理論的なエイリアスに基づき、理由論理を強化した制約付き解码アルゴリズム、CRANEを提案します。CRANEは制約付き生成の正確性と無制約付き生成の柔軟性をバランスし、複数の開放ソースLLMとベンチマークでの実験により、GSM-symbolicとFOLIOの難しい符号的論理ベンチマークで基準に対して10%点程度の精度向上を示します。",
      "upvotes": 4,
      "discussionId": "67b401e03995f28d45c21354"
    },
    "publishedAt": "2025-02-17T22:43:51.555Z",
    "title": "CRANE: Reasoning with constrained LLM generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09061.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65e7bb35e5e78134ab049942",
      "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
      "fullname": "Tarun Suresh",
      "name": "tarsur909",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.11275",
      "authors": [
        {
          "_id": "67b3fa2862838a378b21860d",
          "name": "Letian Peng",
          "hidden": false
        },
        {
          "_id": "67b3fa2862838a378b21860e",
          "name": "Zilong Wang",
          "hidden": false
        },
        {
          "_id": "67b3fa2862838a378b21860f",
          "name": "Feng Yao",
          "hidden": false
        },
        {
          "_id": "67b3fa2862838a378b218610",
          "name": "Jingbo Shang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T21:32:20.000Z",
      "title": "クッカーオ：大規模のネットワーク構築による、IE無料ライナーが誕生したLLMの巣",
      "summary": "巨大な高品質データ、つまり予習用の裸のテキストと予習後のアノテーション、は慎重に準備されていて、高度な大規模言語モデル（LLMs）の開発を促進しています。対比的に、情報抽出（IE）において、予習用データのようにBIOタグ付けされたシーケンスはスケーリングアップが難しいです。私たちは、IEモデルがLLMデータのリソースにフリーライダーとして作用することを示します。具髪、次のテキスト予測を抽出のために再構成します。特に、私たちが提案した次のテキスト抽出（NTE）パラダイムは、LLMの予習データと予習後のデータから102.6Mの抽出データを変換して、多様性のあるIEモデルCuckooを学習します。少ショット設定では、Cuckooは従来のやっとした複雑なインストラクション従いIEに対して、既存の予習されたIEモデルよりもより良い性能で効果的に適応します。フリーライダーであるCuckooは、LLMデータの準備の進歩に自然に進化し、LLMの訓練パイプラインの改善を受け益づき、手動の追加の努力を必要としません。",
      "upvotes": 4,
      "discussionId": "67b3fa2962838a378b21867b"
    },
    "publishedAt": "2025-02-17T22:10:49.900Z",
    "title": "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11275.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64323dd503d81fa4d26deaf9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64323dd503d81fa4d26deaf9/x3ES8VXEZJljxDWvFWaAf.png",
      "fullname": "Letian Peng",
      "name": "KomeijiForce",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.11901",
      "authors": [
        {
          "_id": "67b3f8cc1bfe04e82830b752",
          "name": "Dylan Zhang",
          "hidden": false
        },
        {
          "_id": "67b3f8cc1bfe04e82830b753",
          "name": "Justin Wang",
          "hidden": false
        },
        {
          "_id": "67b3f8cc1bfe04e82830b754",
          "name": "Tianran Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T15:24:11.000Z",
      "title": "データの不足にも対応した64%優位の証明取向付きプログラマーの開発",
      "summary": "現在のLMは、証明取向づけされたプログラミングにおいてデータ不足が問題となり、これは2つの主な点にまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでまでま",
      "upvotes": 3,
      "discussionId": "67b3f8cd1bfe04e82830b77f"
    },
    "publishedAt": "2025-02-17T22:05:54.047Z",
    "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11901.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642b8add48f67b6f21d4eb20",
      "avatarUrl": "/avatars/f15025b39248daa19a18e6ccb2eaaa0c.svg",
      "fullname": "Dylan",
      "name": "shizhuo2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12054",
      "authors": [
        {
          "_id": "67b44a6888813676da9f8239",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f823a",
          "name": "Yuxuan Dong",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f823b",
          "name": "Yanrui Wu",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f823c",
          "name": "Jiaxing Huang",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f823d",
          "user": {
            "_id": "6602548a68d519ed324b47c5",
            "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
            "isPro": false,
            "fullname": "ChengyouJia",
            "user": "ChengyouJia",
            "type": "user"
          },
          "name": "Chengyou Jia",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:30:47.313Z",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f823e",
          "name": "Basura Fernando",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f823f",
          "name": "Mike Zheng Shou",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f8240",
          "name": "Lingling Zhang",
          "hidden": false
        },
        {
          "_id": "67b44a6888813676da9f8241",
          "name": "Jun Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T17:24:14.000Z",
      "title": "物理理由：基于物理的推理的全面基准",
      "summary": "大語言モデルは、数学や論理推理において特別な能力を示していますが、現在の評価は物理的な論理を飛ばしています。物理的な論理は、物理学的定理や制約を必要とする複雑なタスクです。我々は、知識ベース（25%）と論理ベース（75%）の問題を含む1,200問のベンチマーク「PhysReason」を提案します。論理ベースの問題は、容易、中難、難しい3つの難易度レベルに分けられています。特に、問題は平均で8.1ステップの解法が必要で、難しい問題は15.6ステップを必要とし、物理的な論理の複雑性を反映しています。我々は、物理的な解答の自動評価フレームワークを提案します。これは、効率的な回答レベルと構成レベルの詳細な評価を含むものです。Deepseek-R1、Gemini-2.0-Flash-Thinking、o3-mini-highなどの優れたモデルは、回答レベルの評価で60%未満を達成し、知識問題（75.11%）から難しい問題（31.95%）までの性能が低下しています。ステップレベルの評価を通じて、我々は4つのキーボックスを特定しました：物理学的定理の適用、物理的過程の理解、計算、物理的条件の分析。これらの発見は、大語言モデルの物理的な論理能力を評価するための新しいや綜合的なベンチマークとしての「PhysReason」の位置を確立しています。我々のコードおよびデータは、https://dxzxy12138.github.io/PhysReasonに公開します。",
      "upvotes": 2,
      "discussionId": "67b44a6988813676da9f82d0"
    },
    "publishedAt": "2025-02-18T03:53:47.570Z",
    "title": "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12054.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602548a68d519ed324b47c5",
      "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
      "fullname": "ChengyouJia",
      "name": "ChengyouJia",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11330",
      "authors": [
        {
          "_id": "67b42c5632929e97a92dee90",
          "name": "Minbyul Jeong",
          "hidden": false
        },
        {
          "_id": "67b42c5632929e97a92dee91",
          "name": "Jungho Cho",
          "hidden": false
        },
        {
          "_id": "67b42c5632929e97a92dee92",
          "name": "Minsoo Khang",
          "hidden": false
        },
        {
          "_id": "67b42c5632929e97a92dee93",
          "name": "Dawoon Jung",
          "hidden": false
        },
        {
          "_id": "67b42c5632929e97a92dee94",
          "name": "Teakgyu Hong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T01:05:31.000Z",
      "title": "システムメッセージ生成におけるユーザーの好みを用いたオープンソースモデルの利用",
      "summary": "システムメッセージは、大規模言語モデル（LLM）とのインタラクションにおいて重要な役割を果たします。これらのメッセージは、会話を開始するためのプロンプトとして使用され、ユーザーが特定の役割を割り当て、挙動を決め、背景情報を挟むことや、出力フォーマットやコミュニケーションスタイルを指定することができます。この多様性に加えて、公開されているデータは、システムメッセージを含むものが少なく、業界では厳格なライセンス制約によって制限されています。公開されているデータにシステムメッセージを手動でラベル付けすることは、ユーザーの指示に合わせたものを作成するためには、大幅なリソースが必要となります。このような課題に対して、我々の研究では、システムメッセージを生成するパイプライン「SysGen」を導入しました。このパイプラインは、システムメッセージを含まないデータセットから、サブジェクトフィーチャインニングデータセットから生成されたシステムメッセージを使用して、より対応した助手のレスポンスを得ることができます。SysGenデータによる訓練は、Multifacetベンチマークで示されたモデルのレスポンスとシステムメッセージ、ユーザー指示との一致性に大幅な向上を示し、他の未見されたベンチマーク（例：Open LLM Leaderboard 2）に対しては最小限の影響を持つものです。我々の質的な分析は、異なるコンテキストでのより良い適応性を確保するために、多様なシステムメッセージの重要性を強調しています。",
      "upvotes": 2,
      "discussionId": "67b42c5732929e97a92deed7"
    },
    "publishedAt": "2025-02-18T01:45:36.359Z",
    "title": "System Message Generation for User Preferences using Open-Source Models",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11330.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64587be872b60ae7a3817858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png",
      "fullname": "Minbyul Jeong",
      "name": "Minbyul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.09083",
      "authors": [
        {
          "_id": "67b30726d4665a0448e6436d",
          "user": {
            "_id": "6698cffdb2ebada9f4a7e7d7",
            "avatarUrl": "/avatars/e66d946c14595d3b008185f2be8d2f57.svg",
            "isPro": false,
            "fullname": "Greta Warren",
            "user": "gretawarren",
            "type": "user"
          },
          "name": "Greta Warren",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:32:34.585Z",
          "hidden": false
        },
        {
          "_id": "67b30726d4665a0448e6436e",
          "name": "Irina Shklovski",
          "hidden": false
        },
        {
          "_id": "67b30726d4665a0448e6436f",
          "name": "Isabelle Augenstein",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-13T08:56:25.000Z",
      "title": "「働きを見せてください：説明可能な自動化事実検証の課題」",
      "summary": "大語言モデルと生成AIがオンラインメディアに広く普及しており、これらの技術の普及に伴い、誤信頼の量や複雑性が増加し、効果的な自動的な事実検証の必要が高まっています。事実検証の複雑な性質により、自動的な事実検証システムは、事実検証者による出力の検討を促すために、どのような説明を提供するかが重要です。しかし、これらの説明は事実検証者の決定機能と理由論の過程にどのように合わせられるかが明確ではありません。事実検証専門家との半構造的なインタビューを通じて、私たちは以下の点でこの間違いを埋めることができます： (i) 事実検証者が証拠を評価し、決定をし、そのプロセスを説明する方法を説明する； (ii) 事実検証者が実際に自動的なツールをどのように使用するかを調査する； (iii) 自動的な事実検証ツールの事実検証者の説明要求を特定する。この調査結果は、未満された説明の必要性を示し、重複可能な事実検証の説明の重要な基準を特定し、モデルの理由のパスを跡変え、特定の証拠を参照し、不確かさと情報の欠陥を特徴的にすることを示しています。",
      "upvotes": 1,
      "discussionId": "67b30727d4665a0448e6438d"
    },
    "publishedAt": "2025-02-18T04:37:21.573Z",
    "title": "Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6698cffdb2ebada9f4a7e7d7/55xAEeg9Xsk87DXHTH9gM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09083.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6698cffdb2ebada9f4a7e7d7",
      "avatarUrl": "/avatars/e66d946c14595d3b008185f2be8d2f57.svg",
      "fullname": "Greta Warren",
      "name": "gretawarren",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12135",
      "authors": [
        {
          "_id": "67b4028237db78705fb256e1",
          "user": {
            "_id": "64fb31a34c8924c4fe7498bc",
            "avatarUrl": "/avatars/6c8e4a66e1b8b3c786a4000210089392.svg",
            "isPro": false,
            "fullname": "Chaoyue Song",
            "user": "chaoyue7",
            "type": "user"
          },
          "name": "Chaoyue Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:40.771Z",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e2",
          "name": "Jianfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e3",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e4",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e5",
          "name": "Yiwen Chen",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e6",
          "name": "Zhongcong Xu",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e7",
          "name": "Jun Hao Liew",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e8",
          "name": "Xiaoyang Guo",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256e9",
          "name": "Fayao Liu",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256ea",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67b4028237db78705fb256eb",
          "name": "Guosheng Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T18:53:27.000Z",
      "title": "MagicArticulate: 3Dモデルのアーチュラクション準備をする",
      "summary": "3Dコンテンツ作成の爆発的な成長に伴い、静的な3Dモデルを動的なアーチュレーション準備したバージョンに自動的に変換することを求めることが増加しています。伝統的なアプローチは、手動注釈によりより時間と労力が必要となり、また、大規模なベンチマークの欠落が学習基づきの解決策の開発において課題になっています。本論文では、MagicArticulateという効果的なフレームワークを提案し、静的な3Dモデルを動的なアーチュレーション準備したアセットに自動的に変換することを実現します。私たちの主な貢献は三つです。まず、Articulation-XLという大規模なベンチマークを提案し、それは33,000点以上の高品質なアーチュレーション注釈を含む3Dモデルから精選されたものです。次に、新しいスケルタル生成方法を提案し、このタスクを順序モデリング問題として構成し、自動協調変換ドライバーを利用してスケルタル内の骨または関節の数の変動や、3Dモデル間の固有の依存関係を自然に処理することを目的とします。最後に、関節と頂点の体積計算ジオディスタンス先頭を含む機能的なディフュージョンプロセスを使用してスキンニング重みを予測します。拡散的な実験により、MagicArticulateは異なるオブジェクトカテゴリーで現在の方法を大幅に超え、高品質なアーチュレーションを実現し、実写的なアニメーションを可能にします。プロジェクトページ：https://chaoyuesong.github.io/MagicArticulate.",
      "upvotes": 1,
      "discussionId": "67b4028437db78705fb25726"
    },
    "publishedAt": "2025-02-18T04:34:15.786Z",
    "title": "MagicArticulate: Make Your 3D Models Articulation-Ready",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12135.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64fb31a34c8924c4fe7498bc",
      "avatarUrl": "/avatars/6c8e4a66e1b8b3c786a4000210089392.svg",
      "fullname": "Chaoyue Song",
      "name": "chaoyue7",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11831",
      "authors": [
        {
          "_id": "67b450cf315f7b69956df3d6",
          "name": "Quentin Garrido",
          "hidden": false
        },
        {
          "_id": "67b450cf315f7b69956df3d7",
          "name": "Nicolas Ballas",
          "hidden": false
        },
        {
          "_id": "67b450cf315f7b69956df3d8",
          "name": "Mahmoud Assran",
          "hidden": false
        },
        {
          "_id": "67b450cf315f7b69956df3d9",
          "name": "Adrien Bardes",
          "hidden": false
        },
        {
          "_id": "67b450cf315f7b69956df3da",
          "name": "Laurent Najman",
          "hidden": false
        },
        {
          "_id": "67b450cf315f7b69956df3db",
          "name": "Michael Rabbat",
          "hidden": false
        },
        {
          "_id": "67b450cf315f7b69956df3dc",
          "name": "Emmanuel Dupoux",
          "hidden": false
        },
        {
          "_id": "67b450cf315f7b69956df3dd",
          "name": "Yann LeCun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T14:27:14.000Z",
      "title": "直覚的物理学理解は、自然ビデオ上の自己規範的予備学習から生み出されます。",
      "summary": "私たちは、自然な映画で隠された領域を予測するための一般的な深層ニューラルネットワークモデルで直感的物理的認識の発生を調査しています。期待破綻フレームワークを活用し、学習された表現空間で結果を予測するモデルが、物体の永久性と形状の一貫性などの直感的物理的性質を理解することを見出しました。それに対して、ピクセル空間での映像予測とテキストを通じて理由を見つける多様的な大規模な言語モデルは、運命の近い性能を達成します。これらのアーキテクチャを比較することで、感覚入力の欠損部分を予測する同時に抽象的表現空間を学習すること（予測コーディングのようなもの）で直感的物理的認識を得ることが可能であることが明らかになり、1週間の独自の映画で訓練されたモデルも運命よりも高い性能を達成することが見出されます。これは、核心知識（世界を理解するための種の遺伝的なシステム）が直感的物理的認識の理解に必要とするものであるというアイデアを質疑しています。",
      "upvotes": 1,
      "discussionId": "67b450d0315f7b69956df3f9"
    },
    "publishedAt": "2025-02-18T04:20:25.916Z",
    "title": "Intuitive physics understanding emerges from self-supervised pretraining on natural videos",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11831.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f1158120c833276f61f1a84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
      "fullname": "Niels Rogge",
      "name": "nielsr",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 763
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.11085",
      "authors": [
        {
          "_id": "67b44f44620ae0bad17d6699",
          "name": "Yasir Ghunaim",
          "hidden": false
        },
        {
          "_id": "67b44f44620ae0bad17d669a",
          "user": {
            "_id": "642b51385bf2355d02a23d15",
            "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
            "isPro": true,
            "fullname": "Hasan Abed Al Kader Hammoud",
            "user": "hammh0a",
            "type": "user"
          },
          "name": "Hasan Abed Al Kader Hammoud",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:30:43.057Z",
          "hidden": false
        },
        {
          "_id": "67b44f44620ae0bad17d669b",
          "name": "Bernard Ghanem",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T11:46:23.000Z",
      "title": "データ効率化のための原子的性質予測のための予め学習",
      "summary": "この論文は、原子物質の性質予測における最近のパラダイムを挑戦しています。このパラダイムは、進歩とデータセットのサイズや計算リソースの増大が関係していることを示しています。私たちは、適切に選択された、仕事に関連するデータセットでの予備学習が、大規模な予備学習を追い越すことができることを示し、その計算コストを1/24に抑えることができることを示します。私たちは、コンピュータビジョンのFr\\'echet Inception Distanceによるものをモデルとした新しい指標、化学的類似性指数（CSI）を分子グラフに応用し、上流の予備学習データセットと下流の仕事の間の一致性を定量化します。CSIの距離が最小となる最も関連性のあるデータセットを選択することで、小さいデータセットでの予備学習モデルが、大規模な混合データセット（例えばJMP）での予備学習モデルを経験的に上回ることを示します。逆に、関連性のないデータを無駄に追加することは、モデルの性能を低下させることがあることを示します。我々の発見は、原子物質の性質予測の予備学習では、質の方が量よりも優れていることを示しています。",
      "upvotes": 1,
      "discussionId": "67b44f45620ae0bad17d66b0"
    },
    "publishedAt": "2025-02-18T04:16:28.219Z",
    "title": "Towards Data-Efficient Pretraining for Atomic Property Prediction",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/bLvTbh56AkUmcmRst8mT3.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11085.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "642b51385bf2355d02a23d15",
      "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
      "fullname": "Hasan Abed Al Kader Hammoud",
      "name": "hammh0a",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11775",
      "authors": [
        {
          "_id": "67b4147f7721b4fe4d2bd466",
          "name": "Guangzhi Sun",
          "hidden": false
        },
        {
          "_id": "67b4147f7721b4fe4d2bd467",
          "name": "Yudong Yang",
          "hidden": false
        },
        {
          "_id": "67b4147f7721b4fe4d2bd468",
          "name": "Jimin Zhuang",
          "hidden": false
        },
        {
          "_id": "67b4147f7721b4fe4d2bd469",
          "name": "Changli Tang",
          "hidden": false
        },
        {
          "_id": "67b4147f7721b4fe4d2bd46a",
          "name": "Yixuan Li",
          "hidden": false
        },
        {
          "_id": "67b4147f7721b4fe4d2bd46b",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67b4147f7721b4fe4d2bd46c",
          "name": "Zejun MA",
          "hidden": false
        },
        {
          "_id": "67b4147f7721b4fe4d2bd46d",
          "name": "Chao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T13:07:40.000Z",
      "title": "video-SALMONN-o1: 理由強化ビデオアウディオ大語言モデル",
      "summary": "最近の理由計算の最適化の進歩は、大規模言語モデル（LLMs）の能力を大幅に向上させましたが、現在の努力は数学問題の解決と可視的グラフィック入力の焦点を当てていて、一般的なビデオ理解の広範なアプリケーションに注目していません。本論文では、一般的なビデオ理解タスクに向けて理由計算を強化した最初の開放ソースの語言モデルを提案します。これをvideo-SALMONN-o1としています。理由計算の能力を向上させるために、複雑な音声ビデオクエスチョンを含むステップごとの解決方法を特徴とした理由計算の強化データセットを開発しました。また、対比的ステップ選択を活用した過程直接好み最適化（pDPO）を提案し、多タイプ入力に適したステップレベル報酬モデリングを効率的に実現しました。また、RivaBenchという理由計算の強化ビデオ理解ベンチマークを導入しました。これはスタンドアップコメディ、学術講演、合成ビデオ検出などのビデオ理解のスケーナーで4,000点以上の高品質、専門家が編集した質問回答ペアを特徴としています。video-SALMONN-o1は、異なるビデオ理由計算ベンチマークでLLaVA-OneVisionの基準に対して3-8%の精度向上を実現しました。また、pDPOはRivaBenchのサブジェクト調整モデルに対して6-8%の向上を実現しました。理由計算の向上により、video-SALMONN-o1は合成ビデオ検出のゼロショット能力を実現します。",
      "upvotes": 1,
      "discussionId": "67b414827721b4fe4d2bd534"
    },
    "publishedAt": "2025-02-18T00:06:55.671Z",
    "title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11775.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6128
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.11098",
      "authors": [
        {
          "_id": "67b411e45e634139c0d86a1e",
          "name": "Zhao Wang",
          "hidden": false
        },
        {
          "_id": "67b411e45e634139c0d86a1f",
          "name": "Sota Moriyama",
          "hidden": false
        },
        {
          "_id": "67b411e45e634139c0d86a20",
          "name": "Wei-Yao Wang",
          "hidden": false
        },
        {
          "_id": "67b411e45e634139c0d86a21",
          "name": "Briti Gangopadhyay",
          "hidden": false
        },
        {
          "_id": "67b411e45e634139c0d86a22",
          "name": "Shingo Takamatsu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T12:26:58.000Z",
      "title": "構造的に話し、階層的に行動する：LLMのコラボレーションフレームワークの多エージェントシステム",
      "summary": "最近、LLMベースの多効果者システム（LLM-MA）における進展は期待できるものの、複雑なタスクでの効果者間のコミュニケーションと改善に関する重大な課題が残っています。本論文では、コンテキスト豊富な交換のための構造化されたコミュニケーションプロトコルと、不正な出力、フェイス、バイアスなどの問題を解決するための階層的な改善システムを導入する新しいフレームワーク「Talk Structurally, Act Hierarchically (TalkHier)」を提案します。TalkHierは、現在のLLMおよび単一効果者ベースラインワーク（例：ReAct、GPT4o）に対する推論スケーリングモデル（OpenAI-o1）、オープンソース多効果者モデル（例：AgentVerse）、そして多様なタスクでの効果的さ、適応性、コラボレーション性を超えるような最先端技術（SoTA）を超えています。これらの結果は、LLM-MAシステムの新しい標準を設定することができることを示し、より効果的、適応的、コラボレーション的な多効果者フレームワークの開発に道が開かれています。コードは、https://github.com/sony/talkhier から利用可能です。",
      "upvotes": 1,
      "discussionId": "67b411e55e634139c0d86a4c"
    },
    "publishedAt": "2025-02-17T23:51:50.821Z",
    "title": "Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11098.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6128
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.10454",
      "authors": [
        {
          "_id": "67b40e56bffd44cc85976ecd",
          "name": "Yinghui Li",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ece",
          "name": "Jiayi Kuang",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ecf",
          "name": "Haojing Huang",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed0",
          "name": "Zhikun Xu",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed1",
          "name": "Xinnian Liang",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed2",
          "name": "Yi Yu",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed3",
          "name": "Wenlian Lu",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed4",
          "name": "Yangning Li",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed5",
          "name": "Xiaoyu Tan",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed6",
          "name": "Chao Qu",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed7",
          "name": "Ying Shen",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed8",
          "name": "Hai-Tao Zheng",
          "hidden": false
        },
        {
          "_id": "67b40e56bffd44cc85976ed9",
          "name": "Philip S. Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-12T02:01:10.000Z",
      "title": "一例を示すと、多くの概念が知られる！数学のLLMでの反例をドライブする概念的な理由論",
      "summary": "数学の大語言モデル（LLMs）を用いた証明生成は、LLMs研究の基本的なトピックである。現在のLLMsが証明を行う能力は、訓練中に関連する証明プロセスを経験したかどうかに大きく依存していると述べます。この依存関係は、数学定理や関連概念の深い理解に限界をもたらしています。人間の数学教育で通常使用される「反例での証明」の教育方法をポイントに、我々の研究はLLMsの数学的論理と証明の能力を反例を用いて向上させることを目的としています。具体的には、我々は手動で高品質な大学レベルの数学ベンチマーク、CounterMATHを作成し、LLMsが数学的論理を行うために反例を提供して数学的論理を証明することを求め、その理解度を評価します。また、我々はデータ工学フレームワークを開発し、モデルの進化に向けた訓練データを自動的に取得することを目指しています。拡張的な実験と詳細な分析は、CounterMATHが難しいことを示し、OpenAI o1などのLLMsが反例を用いた証明能力が十分ではないことを明らかにします。また、モデルの訓練における検討は、LLMsの反例を用いた概念的論理能力の強化がその数学的記憶力全体の向上に重要であることを示しています。我々は、我々の研究は数学的LLMsのコミュニティに新しい視点を提供していると信じています。",
      "upvotes": 1,
      "discussionId": "67b40e57bffd44cc85976f0e"
    },
    "publishedAt": "2025-02-17T23:37:16.770Z",
    "title": "One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6128
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.11574",
      "authors": [
        {
          "_id": "67b435c29e5685b308a8edac",
          "user": {
            "_id": "65bcbc01d6d0ffbceb8b2e6e",
            "avatarUrl": "/avatars/73edb2d6b7b11208439ac88b365079e8.svg",
            "isPro": false,
            "fullname": "Johan Boye",
            "user": "jboye",
            "type": "user"
          },
          "name": "Johan Boye",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-18T07:24:50.956Z",
          "hidden": false
        },
        {
          "_id": "67b435c29e5685b308a8edad",
          "user": {
            "_id": "6033e34a9aa44495c80dd043",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614079701740-6033e34a9aa44495c80dd043.jpeg",
            "isPro": false,
            "fullname": "Birger Moell",
            "user": "birgermoell",
            "type": "user"
          },
          "name": "Birger Moell",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:30:49.328Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T09:07:32.000Z",
      "title": "大型言語モデルと数学的理由の失敗",
      "summary": "この論文は、50件の新規構築した中学校レベルの問題を使用して、大規模言語モデル（LLMs）の数学的推理能力を調査しています。先行研究では答えの正確性だけを焦点としていたが、私たちは最終的な答えと解答ステップを厳密に分析し、推理の失敗を識別します。最新のモデル8つ（Mixtral、Llama、Gemini、GPT-4o、OpenAIのo1バージョンやその他）を評価し、新しいモデル（例えばo3-mini、deepseek-r1）がより高い精度を達成することを発見しましたが、すべてのモデルは空間的推理、戦略的計画、算術においてエラーを示し、時には間違った論理を通じて正しい答えを生成します。常見な失敗モードは、無駄な仮定、数値パターンの過度依存、物理的直感を数学的ステップに翻訳する困難などです。手動での分析は、多段階の推論や実世界的知識を必要とする問題においてモデルが難しいことを示し、広い数学的知識を持っているものの、これらの問題に対応することが難しいことを示します。私たちの結果は、理由の評価に焦点を当てることの重要性を強調し、LLMsの問題解決能力を過剰に高估することを警告しています。本研究は、LLMsの一般化能力の持ち残りの欠点を明らかにし、構造的な理由の改善と制約の処理における特定の向上の必要性を強調しています。",
      "upvotes": 0,
      "discussionId": "67b435c29e5685b308a8edf1"
    },
    "publishedAt": "2025-02-18T02:26:18.856Z",
    "title": "Large Language Models and Mathematical Reasoning Failures",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11574.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6033e34a9aa44495c80dd043",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614079701740-6033e34a9aa44495c80dd043.jpeg",
      "fullname": "Birger Moell",
      "name": "birgermoell",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11578",
      "authors": [
        {
          "_id": "67b435475bff5f34c1ebee1b",
          "user": {
            "_id": "6033e34a9aa44495c80dd043",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614079701740-6033e34a9aa44495c80dd043.jpeg",
            "isPro": false,
            "fullname": "Birger Moell",
            "user": "birgermoell",
            "type": "user"
          },
          "name": "Birger Moell",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:30:52.639Z",
          "hidden": false
        },
        {
          "_id": "67b435475bff5f34c1ebee1c",
          "user": {
            "_id": "65bcbc01d6d0ffbceb8b2e6e",
            "avatarUrl": "/avatars/73edb2d6b7b11208439ac88b365079e8.svg",
            "isPro": false,
            "fullname": "Johan Boye",
            "user": "jboye",
            "type": "user"
          },
          "name": "Johan Boye",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-18T07:22:48.554Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T09:09:58.000Z",
      "title": "言語複雑さの測定を、LLMの性能評価のノイズサインゼロショットプロキシとして使用する",
      "summary": "大語言モデル（LLMs）は、自然語言生成において重要な進展を遂げていますが、精度の高い計算や構造的解析を必要とするタスクでは、多くの場合は問題を抱えます。本論文では、LIX読解度メトリックと平均依存距離（ADD）の計算を通じて、最先端のLLMsの言語複雑さ測定タスクにおける性能を調査します。スウェーデンの高校レベルおよび大学レベルの論文を使用して、モデルのLIXスコアの計算および依存解析の能力を評価し、既に確立された基準と比較します。我々の発見は、すべてのモデルがこれらのタスクにどのような能力を示すかを示し、ChatGPT-o1-miniが最も一貫して高い精度を達成し、LIXの計算と依存解析の両方で最も高い精度を達成します。また、モデルのLIXの計算精度と、Massive Multitask Language Understanding（MMLU）ベンチマークの全体的な性能との間に、強い有意な相関（-0.875, p 0.026, N=6）が見出されます。これらの結果は、言語複雑さ測定能力がLLMsの一般的な能力を評価するためのノイズも含むゼロショットの代理としての役割を果たすことを示し、広範囲のベンチマークデータセットを必要とする場合には実用的なモデル評価方法を提供します。",
      "upvotes": 0,
      "discussionId": "67b435485bff5f34c1ebee52"
    },
    "publishedAt": "2025-02-18T02:23:29.869Z",
    "title": "Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11578.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6033e34a9aa44495c80dd043",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614079701740-6033e34a9aa44495c80dd043.jpeg",
      "fullname": "Birger Moell",
      "name": "birgermoell",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": true
  }
]