[
  {
    "paper": {
      "id": "2504.02605",
      "authors": [
        {
          "_id": "67ef4d92c1e251f239495a13",
          "name": "Daoguang Zan",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a14",
          "name": "Zhirong Huang",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a15",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a16",
          "name": "Hanwu Chen",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a17",
          "name": "Linhao Zhang",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a18",
          "name": "Shulin Xin",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a19",
          "name": "Lu Chen",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a1a",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a1b",
          "name": "Xiaojian Zhong",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a1c",
          "name": "Aoyan Li",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a1d",
          "name": "Siyao Liu",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a1e",
          "name": "Yongsheng Xiao",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a1f",
          "name": "Liangqiang Chen",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a20",
          "name": "Yuyu Zhang",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a21",
          "name": "Jing Su",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a22",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a23",
          "name": "Rui Long",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a24",
          "name": "Kai Shen",
          "hidden": false
        },
        {
          "_id": "67ef4d92c1e251f239495a25",
          "name": "Liang Xiang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/61527edf8b55dbdae72874fa/PS_Q49kWYAB6DdJy5YY9k.png",
        "https://cdn-uploads.huggingface.co/production/uploads/61527edf8b55dbdae72874fa/GhnnOocFnA-YN2oyeNjPB.png"
      ],
      "publishedAt": "2025-04-03T14:06:17.000Z",
      "submittedOnDailyAt": "2025-04-07T02:30:50.286Z",
      "title": "Multi-SWE-bench: 問題解決のための多言語ベンチマーク",
      "submittedOnDailyBy": {
        "_id": "61527edf8b55dbdae72874fa",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61527edf8b55dbdae72874fa/ZGWSBf_KSrDof6WyMoDMU.jpeg",
        "isPro": false,
        "fullname": "Daoguang Zan",
        "user": "Daoguang",
        "type": "user"
      },
      "summary": "問題解決の任務は、コードベースを改変して、与えられた問題を解決するペーチを生成することです。しかし、現在のベンチマークの例であるSWE-benchは、Pythonを中心に焦点を当てているため、多様なソフトウェアエコシステムの大規模な言語モデル（LLMs）の評価には十分ではありません。これに対して、私たちはJava、TypeScript、JavaScript、Go、Rust、C、C++をカバーした多言語問題解決ベンチマーク、Multi-SWE-benchを紹介します。このベンチマークは、68名の専門家のエノテーターが2,456名のキャンディドから選んで、1,632件の高品質のインスタンスを作成し、正確かつ信頼性のある評価を提供することを確保しています。Multi-SWE-benchに基づき、私たちはAgentless、SWE-agent、OpenHandsの3つの代表的な方法を用いて、最新のモデルを評価し、詳細な分析とキーの実証的なインサイトを提供します。また、私たちは、問題解決タスクのための大規模な強化学習（RL）の訓練データセットを構築するためのMulti-SWE-RLオープンソースコミュニティを開始します。最初の貢献として、私たちは7つのプログラミング言語を拡がる4,723件のよい構造のインスタンスをリリースし、この領域のRL研究に強い基盤を築いています。より重要なことに、私たちは、データの生産パイプラインと詳細なチュートリアルをオープンソースに公開し、オープンソースコミュニティが継続的に貢献し、データセットを拡張するよう促しています。私たちは、Multi-SWE-benchと、その拡大するMulti-SWE-RLコミュニティを、RLの全能性に向けた進歩を促進するキャサイドとして、人工知能の曙を近づけることを想像しています。",
      "upvotes": 28,
      "discussionId": "67ef4d93c1e251f239495a9b",
      "projectPage": "https://multi-swe-bench.github.io",
      "githubRepo": "https://github.com/multi-swe-bench/multi-swe-bench",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Multi-SWE-bench",
        "Agentless",
        "SWE-agent",
        "OpenHands",
        "Multi-SWE-RL",
        "reinforcement learning (RL)",
        "AGI"
      ]
    },
    "publishedAt": "2025-04-03T10:06:17.000Z",
    "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
    "summary": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/61527edf8b55dbdae72874fa/PS_Q49kWYAB6DdJy5YY9k.png",
      "https://cdn-uploads.huggingface.co/production/uploads/61527edf8b55dbdae72874fa/GhnnOocFnA-YN2oyeNjPB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02605.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61527edf8b55dbdae72874fa",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61527edf8b55dbdae72874fa/ZGWSBf_KSrDof6WyMoDMU.jpeg",
      "fullname": "Daoguang Zan",
      "name": "Daoguang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03553",
      "authors": [
        {
          "_id": "67f345c983edbd64f15deeb3",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deeb4",
          "name": "Zhisong Qiu",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deeb5",
          "name": "Baochang Ren",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deeb6",
          "name": "Xiaobin Wang",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deeb7",
          "name": "Xiangyuan Ru",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deeb8",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deeb9",
          "name": "Xiang Chen",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deeba",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deebb",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deebc",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67f345c983edbd64f15deebd",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T16:03:38.000Z",
      "submittedOnDailyAt": "2025-04-07T02:45:21.106Z",
      "title": "Agentic Knowledgeable Self-awareness\n\nエージェント的知識的な自覚性",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、多くのアジェント計画タスクで相当の性能を達成しています。しかし、伝統的なアジェント計画アプローチは、「水田灌漑」のマシンで、金のタロック、外部のフィードバック、ドライブナンスキャンパス知識を無選択的にアジェントモデルに注入しています。この実践は、決断時のシティギャリング自我認識の基本的な人間の認知原理を飛ばしています。それは、決断時に状況の要求を動的に評価し、戦略的にリソースを使用する能力を評価しています。私たちは、この空間を填ぐためにアジェント知識的な自我認識を提案します。これは、LLMベースのアジェントが自動的に知識の利用を調節するための新しいパラダイムです。具体的には、私たちは、KnowSelfというデータ中心的なアプローチを提案します。これは、人類のように知識的な自我認識を持つアジェントを実現することを目的としています。実際には、私たちは、アジェントの自己探索タロック上に特殊トークンをマークするためのヒューリスティックな状況判断基準を設計します。2段階の学習プロセスを通じて、アジェントモデルは特定の特殊トークンを生成して異なる状況を切り替え、最小限のコストで最適な計画効果を達成することができます。私たちの実験は、KnowSelfは外部の知識を最小限に使用して、異なるタスクとモデルで様々な強いベースラインを超えることを示しています。コードは、https://github.com/zjunlp/KnowSelfにアクセスできます。",
      "upvotes": 14,
      "discussionId": "67f345cd83edbd64f15def73",
      "githubRepo": "https://github.com/zjunlp/KnowSelf",
      "ai_keywords": [
        "agentic planning",
        "flood irrigation methodology",
        "gold trajectories",
        "external feedback",
        "domain knowledge",
        "self-awareness",
        "decision-making",
        "agentic knowledgeable self-awareness",
        "KnowSelf",
        "data-centric approach",
        "situation judgement criterion",
        "special tokens",
        "two-stage training process",
        "trajectory-based training"
      ]
    },
    "publishedAt": "2025-04-04T12:03:38.000Z",
    "title": "Agentic Knowledgeable Self-awareness",
    "summary": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03553.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03561",
      "authors": [
        {
          "_id": "67f351f068751b2bb84cc751",
          "name": "Runnan Fang",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc752",
          "name": "Xiaobin Wang",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc753",
          "name": "Yuan Liang",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc754",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc755",
          "name": "Jialong Wu",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc756",
          "name": "Zekun Xi",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc757",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc758",
          "name": "Yong Jiang",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc759",
          "name": "Pengjun Xie",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc75a",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "67f351f068751b2bb84cc75b",
          "name": "Huajun Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T16:10:57.000Z",
      "submittedOnDailyAt": "2025-04-07T02:48:19.567Z",
      "title": "SynWorld: アウトプットシナリオ合成によるアウトプットアクション知識の精進",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "効果的なエージェントと環境の間の相互作用では、エージェントは戦略的に行動を計画し、実行することで能力を拡大します。しかし、LLMベースのエージェントは新しい環境での扱いや行動空間の非標準的な構造を扱う場合には、大きな課題を見出します。エージェントが自動的に環境を探索し、ワークフローを最適化し、行動の理解を向上させることを目的として、SynWorldフレームワークを提案します。このフレームワークでは、行動空間内での多段階行動の組み合わせを合成し、モンテカルロ木検索（MCTS）探索を行い、現在の環境での行動知識を効果的に精確化することができます。実験結果から明らかになっているように、SynWorldは新しい環境での行動知識の学習に効果的で一般的なアプローチです。コードは、https://github.com/zjunlp/SynWorld から利用できます。",
      "upvotes": 9,
      "discussionId": "67f351f168751b2bb84cc789",
      "ai_keywords": [
        "LLM-based agents",
        "multi-step action invocation",
        "Monte Carlo Tree Search (MCTS)"
      ]
    },
    "publishedAt": "2025-04-04T12:10:57.000Z",
    "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement",
    "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03561.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.02949",
      "authors": [
        {
          "_id": "67f350a5e11bd4b05575a831",
          "name": "Xianwei Zhuang",
          "hidden": false
        },
        {
          "_id": "67f350a5e11bd4b05575a832",
          "name": "Yuxin Xie",
          "hidden": false
        },
        {
          "_id": "67f350a5e11bd4b05575a833",
          "name": "Yufan Deng",
          "hidden": false
        },
        {
          "_id": "67f350a5e11bd4b05575a834",
          "name": "Dongchao Yang",
          "hidden": false
        },
        {
          "_id": "67f350a5e11bd4b05575a835",
          "name": "Liming Liang",
          "hidden": false
        },
        {
          "_id": "67f350a5e11bd4b05575a836",
          "name": "Jinghan Ru",
          "hidden": false
        },
        {
          "_id": "67f350a5e11bd4b05575a837",
          "name": "Yuguo Yin",
          "hidden": false
        },
        {
          "_id": "67f350a5e11bd4b05575a838",
          "name": "Yuexian Zou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T18:06:28.000Z",
      "submittedOnDailyAt": "2025-04-07T02:42:39.671Z",
      "title": "VARGPT-v1.1: 視覚自動復元大統一モデルをイテレーティブインストラクションチューニングと強化学習によって改善する",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "この研究では、先ほどのフレームワークVARGPTに基づいた進歩的な統一ビジュアル自動復元モデルVARGPT-v1.1を提出します。このモデルは、ビジュアル理解のための次のトークン予測と画像合成のための次のスケール生成のダブルパラダイムを維持しています。特に、VARGPT-v1.1は以下の5つの機能を統合しています：（1）新しい訓練戦略、イテレーション的なビジュアルインストラクションチューニングとDirect Preference Optimization（DPO）による強化学習の組み合わせ、（2）8.3Mのビジュアル・ジェネレーションインストラクションペアを含む拡張された訓練コーパス、（3）Qwen2を使用したアップグレードされた言語モデルバックボード、（4）画像生成の解像度の向上、（5）アーキテクチャの変更なしでの画像編集機能の発見。これらの進歩は、VARGPT-v1.1が多タイプ理解と文から画像のインストラクション従いタスクで最先端の性能を達成することを可能にし、理解と生成の両方でメトリックにおける显著な向上を示します。特に、ビジュアルインストラクションチューニングにより、モデルはその前身と同じアーキテクチャの一致を維持しながら画像編集機能を獲得することができ、統一的なビジュアル理解、生成、編集の可能性を明らかにします。我々の発見は、効果的な統一的なビジュアル自動復元モデルが、大規模な言語モデル（LLMs）からの柔軟な訓練戦略を適用でき、可期待なスケーラビリティを示すことを示しています。コードベースとモデル重みは、https://github.com/VARGPT-family/VARGPT-v1.1 で公開しています。",
      "upvotes": 8,
      "discussionId": "67f350a9e11bd4b05575a921",
      "githubRepo": "https://github.com/VARGPT-family/VARGPT-v1.1",
      "ai_keywords": [
        "unified visual autoregressive model",
        "next-token prediction",
        "next-scale generation",
        "iterative visual instruction tuning",
        "reinforcement learning",
        "Direct Preference Optimization (DPO)",
        "visual-generative instruction pairs",
        "Qwen2",
        "image generation resolution",
        "emergent image editing capabilities",
        "multimodal understanding",
        "text-to-image instruction-following tasks",
        "comprehension and generation metrics",
        "large language models (LLMs)"
      ]
    },
    "publishedAt": "2025-04-03T14:06:28.000Z",
    "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via\n  Iterative Instruction Tuning and Reinforcement Learning",
    "summary": "In this work, we present VARGPT-v1.1, an advanced unified visual\nautoregressive model that builds upon our previous framework VARGPT. The model\npreserves the dual paradigm of next-token prediction for visual understanding\nand next-scale generation for image synthesis. Specifically, VARGPT-v1.1\nintegrates: (1) a novel training strategy combining iterative visual\ninstruction tuning with reinforcement learning through Direct Preference\nOptimization (DPO), (2) an expanded training corpus containing 8.3M\nvisual-generative instruction pairs, (3) an upgraded language model backbone\nusing Qwen2, (4) enhanced image generation resolution, and (5) emergent image\nediting capabilities without architectural modifications. These advancements\nenable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal\nunderstanding and text-to-image instruction-following tasks, demonstrating\nsignificant improvements in both comprehension and generation metrics. Notably,\nthrough visual instruction tuning, the model acquires image editing\nfunctionality while maintaining architectural consistency with its predecessor,\nrevealing the potential for unified visual understanding, generation, and\nediting. Our findings suggest that well-designed unified visual autoregressive\nmodels can effectively adopt flexible training strategies from large language\nmodels (LLMs), exhibiting promising scalability. The codebase and model weights\nare publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 43
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03641",
      "authors": [
        {
          "_id": "67f34f5dfb6d8a613926ac2b",
          "name": "Wulin Xie",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac2c",
          "name": "Yi-Fan Zhang",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac2d",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac2e",
          "name": "Yang Shi",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac2f",
          "name": "Bingyan Nie",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac30",
          "name": "Hongkai Chen",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac31",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac32",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "67f34f5dfb6d8a613926ac33",
          "name": "Tieniu Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T17:59:55.000Z",
      "submittedOnDailyAt": "2025-04-07T02:38:07.467Z",
      "title": "MME-Unify: 統一モノモダル理解と生成モデルの全面的ベンチマーク",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "現在のMLLMベンチマークは、U-MLLM（統一モデル）の評価において以下の2つの重大な課題を抱えています：1）傳統タスクに関する標準化ベンチマークの欠如により、比較の不均等性が生じ、2）混合モデライズ生成に関するベンチマークの欠如により、多モデライズ理由能力の評価ができない。ここで、U-MLLMのシステマティック評価フレームワークを紹介します。ベンチマークには以下の3つの部分を含みます：1）標準化された傳統タスク評価、2）統一タスク評価、3）全面的なモデルベンチマーク。標準化された傳統タスク評価では、12データセットからサンプリングし、10タスクの30サブタスクを採用し、研究間の一貫したお互いの比較を保証します。統一タスク評価では、画像編集、画像生成との常識QA、および幾何的理由能力を測定する5つの新しいタスクを導入します。また、全面的なモデルベンチマークでは、12つの先進的なU-MLLM（例：Janus-Pro、EMU3、VILA-U、Gemini2-flash）と、専門的な理解モデル（例：Claude-3.5-Sonnet）と生成モデル（例：DALL-E-3）を評価します。これらの発見は、現在のU-MLLMにおける大きな性能間隔を明らかにし、混合モデライズタスクを効果的に扱うためのより強固なモデルの必要性を強調します。コードと評価データは、https://mme-unify.github.io/ からダウンロードできます。",
      "upvotes": 7,
      "discussionId": "67f34f64fb6d8a613926ada9",
      "projectPage": "https://mme-unify.github.io/",
      "githubRepo": "https://github.com/MME-Benchmarks/MME-Unify"
    },
    "publishedAt": "2025-04-04T13:59:55.000Z",
    "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal\n  Understanding and Generation Models",
    "summary": "Existing MLLM benchmarks face significant challenges in evaluating Unified\nMLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional\ntasks, leading to inconsistent comparisons; 2) absence of benchmarks for\nmixed-modality generation, which fails to assess multimodal reasoning\ncapabilities. We present a comprehensive evaluation framework designed to\nsystematically assess U-MLLMs. Our benchmark includes: Standardized Traditional\nTask Evaluation. We sample from 12 datasets, covering 10 tasks with 30\nsubtasks, ensuring consistent and fair comparisons across studies.\" 2. Unified\nTask Assessment. We introduce five novel tasks testing multimodal reasoning,\nincluding image editing, commonsense QA with image generation, and geometric\nreasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs,\nsuch as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized\nunderstanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3).\nOur findings reveal substantial performance gaps in existing U-MLLMs,\nhighlighting the need for more robust models capable of handling mixed-modality\ntasks effectively. The code and evaluation data can be found in\nhttps://mme-unify.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03641.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03601",
      "authors": [
        {
          "_id": "67f36505e11bd4b05579afbf",
          "name": "Akshara Prabhakar",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc0",
          "name": "Zuxin Liu",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc1",
          "name": "Weiran Yao",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc2",
          "name": "Jianguo Zhang",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc3",
          "name": "Ming Zhu",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc4",
          "name": "Shiyu Wang",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc5",
          "name": "Zhiwei Liu",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc6",
          "name": "Tulika Awalgaonkar",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc7",
          "name": "Haolin Chen",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc8",
          "name": "Thai Hoang",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afc9",
          "name": "Juan Carlos Niebles",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afca",
          "name": "Shelby Heinecke",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afcb",
          "name": "Huan Wang",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afcc",
          "name": "Silvio Savarese",
          "hidden": false
        },
        {
          "_id": "67f36505e11bd4b05579afcd",
          "name": "Caiming Xiong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T17:13:57.000Z",
      "submittedOnDailyAt": "2025-04-07T04:10:25.529Z",
      "title": "APIGen-MT: アガンツィックパイプラインでシミュレーションされたアガント・ヒマンインタラクションをよって多ターンデータ生成",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "多転換インタラクションに対応する有効なAIアガントの訓練には、実際の人間とアガントの動態を捉える高品質なデータが必要ですが、これらのデータは少なく、手動で収集することが難しく、費用が高いです。我々は、証明可能で多様な多転換アガントデータを生成するための2段階フレームワークAPIGen-MTを紹介します。最初の段階で、我々のアガントパイプラインは、LLMレビュー者のコミッティーと反復的なフィードバックループを活用して、詳細なタスクブルーメインを生成します。これらのブルーメインは、証明可能で多様な多転換アガントデータを生成するために、記録された人間とアガントの相互作用を記録して完成させます。我々は、1Bから70Bパラメータの範囲内で構成されるxLAM-2-fc-rシリーズのモデルファミリーを訓練します。我々のモデルは、GPT-4oとClaude 3.5の先鋒モデルよりも、tau-benchとBFCLベンチマークで上位に位置し、小さなモデルは大きなモデルを上回り、特に多転換設定では、複数の試行でも一致性が高いことを示します。詳細な実験は、我々の証明可能なブルーメインから詳細化するアプローチが高品質な訓練データを提供し、信頼性の高い、効率的な、能力のあるアガントの開発につながることを示します。我々は、合成データと訓練されたxLAM-2-fc-rモデルを開放ソースし、AIアガントの研究の進歩に貢献します。モデルは、HuggingFaceで以下のURLから利用できます。\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nプロジェクトウェブサイトは、https://apigen-mt.github.io",
      "upvotes": 5,
      "discussionId": "67f36507e11bd4b05579b020",
      "ai_keywords": [
        "agentic pipeline",
        "task blueprints",
        "ground-truth actions",
        "LLM reviewers",
        "iterative feedback loops",
        "simulated human-agent interplay",
        "xLAM-2-fc-r series",
        "$\\tau$-bench",
        "BFCL benchmarks",
        "multi-turn settings",
        "verified blueprint-to-details approach"
      ]
    },
    "publishedAt": "2025-04-04T13:13:57.000Z",
    "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay",
    "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on tau-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03601.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6594
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.03011",
      "authors": [
        {
          "_id": "67f35d8bdb1a843e1ceff38f",
          "name": "Junying Wang",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff390",
          "name": "Jingyuan Liu",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff391",
          "name": "Xin Sun",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff392",
          "name": "Krishna Kumar Singh",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff393",
          "name": "Zhixin Shu",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff394",
          "name": "He Zhang",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff395",
          "name": "Jimei Yang",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff396",
          "name": "Nanxuan Zhao",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff397",
          "name": "Tuanfeng Y. Wang",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff398",
          "name": "Simon S. Chen",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff399",
          "name": "Ulrich Neumann",
          "hidden": false
        },
        {
          "_id": "67f35d8bdb1a843e1ceff39a",
          "name": "Jae Shin Yoon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T20:10:50.000Z",
      "submittedOnDailyAt": "2025-04-07T03:39:50.539Z",
      "title": "「全面のリライト：一般化可能で、一覧的に一致するモノライトヒマンリライトティングとハーモニゼーション」",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "この論文では、人間の任意の体の部分を含むシーンの画像またはビデオからの照明の制御と調和を行う最初の全方位アプローチ「Comprehensive Relighting」を介紹します。このような一般化可能なモデルの構築は、データセットの不足により非常に難しいため、現在の画像基準の照明再構築モデルは特定のシナリオ（例：顔または静的な人）に限定されています。この挑戦を解決するために、私たちは事前学習済みのディフュージョンモデルを一般的な画像プロイヤーとして再利用し、人間の照明再構築と背景の調和を粗到微フレームワークで共にモデル化します。また、照明の時間的な一貫性を進めるために、私たちは時間的な照明モデルを導入し、多くの実世界的なビデオからの照明の循環一致性を学習します。推論時には、時間的な照明モジュールは、空間時間的な特徴ブレンディングアルゴリズムを通じてディフュージョンモデルと組み合わせられ、額外のトレーニングを不要にします。また、入力画像からの高頻度の詳細を保存するために、新しいガイドされたリファイナル化を追加処理として適用します。実験では、Comprehensive Relightingは強い一般化可能さと照明の時間的な一貫性を示し、現在の画像基準の人間の照明再構築と調和方法を上回っています。",
      "upvotes": 4,
      "discussionId": "67f35d91db1a843e1ceff47c",
      "ai_keywords": [
        "diffusion models",
        "image prior",
        "coarse-to-fine framework",
        "temporal lighting model",
        "lighting cycle consistency",
        "spatio-temporal feature blending",
        "guided refinement"
      ]
    },
    "publishedAt": "2025-04-03T16:10:50.000Z",
    "title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human\n  Relighting and Harmonization",
    "summary": "This paper introduces Comprehensive Relighting, the first all-in-one approach\nthat can both control and harmonize the lighting from an image or video of\nhumans with arbitrary body parts from any scene. Building such a generalizable\nmodel is extremely challenging due to the lack of dataset, restricting existing\nimage-based relighting models to a specific scenario (e.g., face or static\nhuman). To address this challenge, we repurpose a pre-trained diffusion model\nas a general image prior and jointly model the human relighting and background\nharmonization in the coarse-to-fine framework. To further enhance the temporal\ncoherence of the relighting, we introduce an unsupervised temporal lighting\nmodel that learns the lighting cycle consistency from many real-world videos\nwithout any ground truth. In inference time, our temporal lighting module is\ncombined with the diffusion models through the spatio-temporal feature blending\nalgorithms without extra training; and we apply a new guided refinement as a\npost-processing to preserve the high-frequency details from the input image. In\nthe experiments, Comprehensive Relighting shows a strong generalizability and\nlighting temporal coherence, outperforming existing image-based human\nrelighting and harmonization methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03011.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6594
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.02402",
      "authors": [
        {
          "_id": "67f0a09a2c873f5ba90cd14a",
          "user": {
            "_id": "67ee782979018bf61e2522a4",
            "avatarUrl": "/avatars/819318421eb40b9ba7e4054770823a8a.svg",
            "isPro": false,
            "fullname": "HaoYin",
            "user": "yyzqy",
            "type": "user"
          },
          "name": "Hao Yin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-06T08:11:07.967Z",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd14b",
          "name": "Shi Guo",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd14c",
          "name": "Xu Jia",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd14d",
          "name": "Xudong XU",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd14e",
          "name": "Lu Zhang",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd14f",
          "name": "Si Liu",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd150",
          "name": "Dong Wang",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd151",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "67f0a09a2c873f5ba90cd152",
          "name": "Tianfan Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-03T08:51:17.000Z",
      "submittedOnDailyAt": "2025-04-07T05:44:25.380Z",
      "title": "イベントベースの非接触音響の効果的な空間時間モデリングからの音響の復元",
      "submittedOnDailyBy": {
        "_id": "67ee782979018bf61e2522a4",
        "avatarUrl": "/avatars/819318421eb40b9ba7e4054770823a8a.svg",
        "isPro": false,
        "fullname": "HaoYin",
        "user": "yyzqy",
        "type": "user"
      },
      "summary": "音波が物体に当たった際に起こる振動は、高周波と微妙な可視的な変化を生み出すことができ、これを利用して音波を復元することができます。早期の研究では、サンプリングレート、バンドワイフ、フィールドオブジェ、光学パスの簡単性に関する補減を取ることがありました。イベントカメラのハードウェアの最近の進歩は、高周波信号の捕捉能力の上位にあるため、音波の可視的な復元においてもよい潜力を示しています。しかし、現在のイベントベースの振動復元手法は、音波復元には最適ではありません。本稿では、イベントストリームからの空間時間情報を完全に利用する非接触音波復元の新しいパイプラインを提案します。まず、新しいシミュレーションパイプラインを用いて大きな訓練セットを生成します。次に、イベントの稀薄性を活用して空間情報を捉え、Mambaを用いて長期間の時間情報をモデル化するネットワークを設計します。最後に、空間情報の統合を行うスペクトラルアグラグレーションブロックを訓練し、信号質を進一步に向上させます。音波波によるイベント信号を捉えるためには、ラザーマトリックスを用いた写像システムを設計し、テスト用に複数のデータシーケンスを収集します。合成データと実世界データに対する実験結果は、本方法の効果を示しています。",
      "upvotes": 4,
      "discussionId": "67f0a09e2c873f5ba90cd26c",
      "projectPage": "https://yyzq1.github.io/EvMic/",
      "githubRepo": "https://github.com/yyzq1/EvMic",
      "ai_keywords": [
        "event camera",
        "high-frequency signals",
        "event stream",
        "spatial-temporal information",
        "novel simulation pipeline",
        "Mamba",
        "spatial aggregation block",
        "laser matrix"
      ]
    },
    "publishedAt": "2025-04-03T04:51:17.000Z",
    "title": "EvMic: Event-based Non-contact sound recovery from effective\n  spatial-temporal modeling",
    "summary": "When sound waves hit an object, they induce vibrations that produce\nhigh-frequency and subtle visual changes, which can be used for recovering the\nsound. Early studies always encounter trade-offs related to sampling rate,\nbandwidth, field of view, and the simplicity of the optical path. Recent\nadvances in event camera hardware show good potential for its application in\nvisual sound recovery, because of its superior ability in capturing\nhigh-frequency signals. However, existing event-based vibration recovery\nmethods are still sub-optimal for sound recovery. In this work, we propose a\nnovel pipeline for non-contact sound recovery, fully utilizing spatial-temporal\ninformation from the event stream. We first generate a large training set using\na novel simulation pipeline. Then we designed a network that leverages the\nsparsity of events to capture spatial information and uses Mamba to model\nlong-term temporal information. Lastly, we train a spatial aggregation block to\naggregate information from different locations to further improve signal\nquality. To capture event signals caused by sound waves, we also designed an\nimaging system using a laser matrix to enhance the gradient and collected\nmultiple data sequences for testing. Experimental results on synthetic and\nreal-world data demonstrate the effectiveness of our method.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.02402.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ee782979018bf61e2522a4",
      "avatarUrl": "/avatars/819318421eb40b9ba7e4054770823a8a.svg",
      "fullname": "HaoYin",
      "name": "yyzqy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24067",
      "authors": [
        {
          "_id": "67ecb89d0210bff02fd3591e",
          "user": {
            "_id": "66b4e3d850b87d84498bbc89",
            "avatarUrl": "/avatars/526c0cabf3a2c019a13bb82fcc7b43e9.svg",
            "isPro": false,
            "fullname": "YixingLi",
            "user": "Yixinglee",
            "type": "user"
          },
          "name": "Yixing Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-02T04:10:06.783Z",
          "hidden": false
        },
        {
          "_id": "67ecb89d0210bff02fd3591f",
          "name": "Ruobing Xie",
          "hidden": false
        },
        {
          "_id": "67ecb89d0210bff02fd35920",
          "user": {
            "_id": "62c4057732fa66fedacca0db",
            "avatarUrl": "/avatars/813e8c9fcd14dde1fdd415408f61bec2.svg",
            "isPro": false,
            "fullname": "AndyYang",
            "user": "andyyang",
            "type": "user"
          },
          "name": "Zhen Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-02T08:21:39.891Z",
          "hidden": false
        },
        {
          "_id": "67ecb89d0210bff02fd35921",
          "name": "Xingwu Sun",
          "hidden": false
        },
        {
          "_id": "67ecb89d0210bff02fd35922",
          "name": "Shuaipeng Li",
          "hidden": false
        },
        {
          "_id": "67ecb89d0210bff02fd35923",
          "name": "Weidong Han",
          "hidden": false
        },
        {
          "_id": "67ecb89d0210bff02fd35924",
          "name": "Zhanhui Kang",
          "hidden": false
        },
        {
          "_id": "67ecb89d0210bff02fd35925",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "67ecb89d0210bff02fd35926",
          "name": "Chengzhong Xu",
          "hidden": false
        },
        {
          "_id": "67ecb89d0210bff02fd35927",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "67ecb89d0210bff02fd35928",
          "name": "Jie Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T13:26:24.000Z",
      "submittedOnDailyAt": "2025-04-07T07:35:17.947Z",
      "title": "TransMamba: 変換器とMambaの適切な選択を行うように軽便に変換する",
      "submittedOnDailyBy": {
        "_id": "62c4057732fa66fedacca0db",
        "avatarUrl": "/avatars/813e8c9fcd14dde1fdd415408f61bec2.svg",
        "isPro": false,
        "fullname": "AndyYang",
        "user": "andyyang",
        "type": "user"
      },
      "summary": "Transformersは現代の大規模な言語モデルの基礎となっていますが、長文順序の処理において二次的計算複雑性が存在し、これが効率を制限しています。最近、状態空間モデル（SSM）のマンバー（Mamba）において線形複雑性を持つ最新の進展は、効率的な効果を示していますが、不穩定的なコンテキスト認識と多タスク拡張性に欠点があります。本論文では、TransformerとMambaを共有パラメータ行列（例：QKVとCBx）で統合する新しいフレームワークTransMambaを提案し、長さや層で動的に注意機構とSSM機構を切り替えることができることを示します。また、TransformerとMambaをブリッジするためにMemory converterを設計し、変換が行われるTransPointで無間の情報流を確保します。また、TransPointのスケジューリングも詳細に調査し、進展を進めることを試みました。拡張された実験を通じて、TransMambaは基準と比較して優れた学習効率と性能を達成し、TransformerとMambaパラダイムの深い一致性を証明し、次世代のシーケンスモデリングのスケーラブルな解決策を提供します。",
      "upvotes": 4,
      "discussionId": "67ecb89e0210bff02fd3596b",
      "ai_keywords": [
        "Transformers",
        "state space model (SSM)",
        "attention",
        "QKV",
        "CBx",
        "TransMamba",
        "parameter matrices",
        "Memory converter",
        "TransPoints",
        "TransPoint scheduling",
        "next-generation sequence modeling"
      ]
    },
    "publishedAt": "2025-03-31T09:26:24.000Z",
    "title": "TransMamba: Flexibly Switching between Transformer and Mamba",
    "summary": "Transformers are the cornerstone of modern large language models, but their\nquadratic computational complexity limits efficiency in long-sequence\nprocessing. Recent advancements in Mamba, a state space model (SSM) with linear\ncomplexity, offer promising efficiency gains but suffer from unstable\ncontextual learning and multitask generalization. This paper proposes\nTransMamba, a novel framework that unifies Transformer and Mamba through shared\nparameter matrices (e.g., QKV and CBx), and thus could dynamically switch\nbetween attention and SSM mechanisms at different token lengths and layers. We\ndesign the Memory converter to bridge Transformer and Mamba by converting\nattention outputs into SSM-compatible states, ensuring seamless information\nflow at TransPoints where the transformation happens. The TransPoint scheduling\nis also thoroughly explored for further improvements. We conducted extensive\nexperiments demonstrating that TransMamba achieves superior training efficiency\nand performance compared to baselines, and validated the deeper consistency\nbetween Transformer and Mamba paradigms, offering a scalable solution for\nnext-generation sequence modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24067.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c4057732fa66fedacca0db",
      "avatarUrl": "/avatars/813e8c9fcd14dde1fdd415408f61bec2.svg",
      "fullname": "AndyYang",
      "name": "andyyang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.03536",
      "authors": [
        {
          "_id": "67f364118188d683931bec4a",
          "name": "Boyuan Wang",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec4b",
          "name": "Runqi Ouyang",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec4c",
          "name": "Xiaofeng Wang",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec4d",
          "user": {
            "_id": "656e9b562cd7a3e348011d26",
            "avatarUrl": "/avatars/bcca51bdc27c664f8f132420e6ed99fa.svg",
            "isPro": false,
            "fullname": "Zheng Zhu",
            "user": "ZhengZhu",
            "type": "user"
          },
          "name": "Zheng Zhu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-07T05:35:15.515Z",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec4e",
          "name": "Guosheng Zhao",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec4f",
          "name": "Chaojun Ni",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec50",
          "name": "Guan Huang",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec51",
          "name": "Lihong Liu",
          "hidden": false
        },
        {
          "_id": "67f364118188d683931bec52",
          "name": "Xingang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-04T15:35:14.000Z",
      "submittedOnDailyAt": "2025-04-07T04:06:19.815Z",
      "title": "HumanDreamer-X: 写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写真写",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "単一画像での人間再構築は、デジタル人間モデリングアプリケーションにとって重要であるが、極端に難しい課題であります。現在のアプローチは、生成モデルを利用して、3D再構築とアニメーションのための多角度画像を合成することに依存しています。しかし、単一の人間画像から直接多角度画像を生成することは、幾何学的な不連続性を伴い、再構築モデルにおける肢の断片化やフォーカス不足などの問題を生じます。これらの制限を克服するために、我々はHumanDreamer-Xという新しいフレームワークを提案します。このフレームワークでは、多角度の人間生成と再構築を一つのパイプラインに統合し、再構築された3Dモデルの幾何学的一致性と視覚的なフィデリティを大幅に向上させます。このフレームワークでは、3Dガウススプレッティングは明示的な3D表現として、初期のギエメトリーとアプリアンスの優先順位を提供します。この基盤により、HumanFixerは3DGS渲染画像を復元することを学習し、写実的な結果を確保します。また、多角度の人間生成に伴う注意機能の固有の課題に深みを及ぼし、幾何学的な詳細や同一性の一致を効果的に向上させるために、注意機能の調節戦略を提案します。実験結果は、我々のアプローチが生成と再構築のPSNR品質メトリックをそれぞれ16.45%と12.65%程度に向上させ、PSNRが25.62dBまで達成し、オンラインデータにも対応し、多様な人間再構築ベースモデルにも適用可能であることを示しています。",
      "upvotes": 3,
      "discussionId": "67f364138188d683931beca2",
      "ai_keywords": [
        "3D Gaussian Splatting",
        "HumanDreamer-X",
        "HumanFixer",
        "attention modulation",
        "PSNR"
      ]
    },
    "publishedAt": "2025-04-04T11:35:14.000Z",
    "title": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction\n  via Gaussian Restoration",
    "summary": "Single-image human reconstruction is vital for digital human modeling\napplications but remains an extremely challenging task. Current approaches rely\non generative models to synthesize multi-view images for subsequent 3D\nreconstruction and animation. However, directly generating multiple views from\na single human image suffers from geometric inconsistencies, resulting in\nissues like fragmented or blurred limbs in the reconstructed models. To tackle\nthese limitations, we introduce HumanDreamer-X, a novel framework that\nintegrates multi-view human generation and reconstruction into a unified\npipeline, which significantly enhances the geometric consistency and visual\nfidelity of the reconstructed 3D models. In this framework, 3D Gaussian\nSplatting serves as an explicit 3D representation to provide initial geometry\nand appearance priority. Building upon this foundation, HumanFixer is\ntrained to restore 3DGS renderings, which guarantee photorealistic results.\nFurthermore, we delve into the inherent challenges associated with attention\nmechanisms in multi-view human generation, and propose an attention modulation\nstrategy that effectively enhances geometric details identity consistency\nacross multi-view. Experimental results demonstrate that our approach markedly\nimproves generation and reconstruction PSNR quality metrics by 16.45% and\n12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing\ngeneralization capabilities on in-the-wild data and applicability to various\nhuman reconstruction backbone models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.03536.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6594
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.24310",
      "authors": [
        {
          "_id": "67ecb513c8ae971f9ad15bd1",
          "user": {
            "_id": "6478fc1512ae749b62ebbbd5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478fc1512ae749b62ebbbd5/zw-wYy1vEWnG9-t6c9W_x.jpeg",
            "isPro": false,
            "fullname": "Alok Abhishek",
            "user": "alokabhishek",
            "type": "user"
          },
          "name": "Alok Abhishek",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-02T03:59:10.671Z",
          "hidden": false
        },
        {
          "_id": "67ecb513c8ae971f9ad15bd2",
          "name": "Lisa Erickson",
          "hidden": false
        },
        {
          "_id": "67ecb513c8ae971f9ad15bd3",
          "user": {
            "_id": "657372396da136b50f5489a0",
            "avatarUrl": "/avatars/57a693058e9a05a2c32f02bab1d8e819.svg",
            "isPro": false,
            "fullname": "Tushar Bandopadhyay",
            "user": "tbandopa",
            "type": "user"
          },
          "name": "Tushar Bandopadhyay",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-02T08:13:38.947Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T16:56:52.000Z",
      "submittedOnDailyAt": "2025-04-07T03:47:34.752Z",
      "title": "BEATS: 大語言モデルの偏り評価および評価テストシステム",
      "submittedOnDailyBy": {
        "_id": "6478fc1512ae749b62ebbbd5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478fc1512ae749b62ebbbd5/zw-wYy1vEWnG9-t6c9W_x.jpeg",
        "isPro": false,
        "fullname": "Alok Abhishek",
        "user": "alokabhishek",
        "type": "user"
      },
      "summary": "この研究では、大規模言語モデル（LLMs）における偏見、倫理、公平性、および事実性の評価に向けて新しいフレームワーク「BEATS」を介紹します。BEATSフレームワークの基礎により、LLMsに対する偏見ベンチマークを提案し、29種類の異なるメトリックでの性能を測定します。これらのメトリックは、人口学的、認知的、および社会的な偏見、および倫理的な理由、グループの公平性、および事実性に関連する不正情報リスクを含みます。これらのメトリックは、LLM生成された回答が社会の偏見を継承し、システム的不平等を拡大することを定量的に評価することができます。このベンチマークに高いスコアを達成するためには、LLMの回答には非常に公平な行動を示す必要があります。これは責任付きAI評価の厳格な標準となります。実験データに基づく実験結果から、37.65%の出力が偏見を含んでいることが明らかになり、これらのモデルを重要な決策システムに使用するとしては、相当のリスクがあることがわかります。BEATSフレームワークとベンチマークは、LLMsのベンチマーク、偏見を原因とする要因の診断、および対策策略の開発においてスケーラブルで統計的に厳格な方法を提供します。BEATSフレームワークをもとに、我々の目標は、より社会的に責任付きで倫理的に一致するAIモデルの開発に役立てることです。",
      "upvotes": 2,
      "discussionId": "67ecb513c8ae971f9ad15c02",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Bias benchmark",
        "Bias",
        "Ethics",
        "Fairness",
        "Factuality",
        "Demographic biases",
        "Cognitive biases",
        "Social biases",
        "Ethical reasoning",
        "Group fairness",
        "Factuality related misinformation risk",
        "Equitable behavior",
        "Responsible AI evaluation",
        "Critical decision making systems",
        "Scalable methodology",
        "Statistically rigorous methodology",
        "Diagnose factors driving biases",
        "Mitigation strategies",
        "Socially responsible AI models",
        "Ethically aligned AI models"
      ]
    },
    "publishedAt": "2025-03-31T12:56:52.000Z",
    "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language\n  Models",
    "summary": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24310.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6478fc1512ae749b62ebbbd5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478fc1512ae749b62ebbbd5/zw-wYy1vEWnG9-t6c9W_x.jpeg",
      "fullname": "Alok Abhishek",
      "name": "alokabhishek",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  }
]