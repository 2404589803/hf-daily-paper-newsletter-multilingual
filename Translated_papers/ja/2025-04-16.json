[
  {
    "paper": {
      "id": "2504.08672",
      "authors": [
        {
          "_id": "67fcb7294a92187863e805ee",
          "user": {
            "_id": "64e6cf78ecce34cb442dc889",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
            "isPro": false,
            "fullname": "Fangzhi Xu",
            "user": "xufangzhi",
            "type": "user"
          },
          "name": "Fangzhi Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-14T09:46:16.537Z",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805ef",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f0",
          "name": "Chang Ma",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f1",
          "name": "Haiteng Zhao",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f2",
          "user": {
            "_id": "6064a0eeb1703ddba0d458b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
            "isPro": false,
            "fullname": "Qiushi",
            "user": "QiushiSun",
            "type": "user"
          },
          "name": "Qiushi Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:24:08.107Z",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f3",
          "name": "Kanzhi Cheng",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f4",
          "name": "Junxian He",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f5",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "67fcb7294a92187863e805f6",
          "name": "Zhiyong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T16:26:23.000Z",
      "submittedOnDailyAt": "2025-04-16T05:46:28.754Z",
      "title": "天才：一般化であり、純粋な無マニュアルの自己学習フレームワーク\n  進歩的な理由に適した\n\n（この翻訳は、原文の専門性と正確性を保ったものとして提供されます。）",
      "submittedOnDailyBy": {
        "_id": "64e6cf78ecce34cb442dc889",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
        "isPro": false,
        "fullname": "Fangzhi Xu",
        "user": "xufangzhi",
        "type": "user"
      },
      "summary": "LLMの理由論の向上は広く興味を呼び起こしています。しかし、現在のトレーニング後の技術は、結果のサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブサブジェクティブ",
      "upvotes": 35,
      "discussionId": "67fcb72a4a92187863e8061b",
      "projectPage": "https://github.com/xufangzhi/Genius",
      "githubRepo": "https://github.com/xufangzhi/Genius",
      "ai_keywords": [
        "self-training framework",
        "Genius",
        "stepwise foresight re-sampling strategy",
        "advantage-calibrated optimization (ACO) loss function"
      ]
    },
    "publishedAt": "2025-04-11T12:26:23.000Z",
    "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
    "summary": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6cf78ecce34cb442dc889",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
      "fullname": "Fangzhi Xu",
      "name": "xufangzhi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10481",
      "authors": [
        {
          "_id": "67fdc1b41d1bc292f7b9358e",
          "user": {
            "_id": "64e18e9ec20c27fcc8df384e",
            "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
            "isPro": false,
            "fullname": "Ding Chen",
            "user": "Hush-cd",
            "type": "user"
          },
          "name": "Ding Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T07:54:22.449Z",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b9358f",
          "user": {
            "_id": "6455ff584095c967f9a847bb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6455ff584095c967f9a847bb/A5wjtWsudC73fLVmgASBr.jpeg",
            "isPro": false,
            "fullname": "Qingchen Yu",
            "user": "Duguce",
            "type": "user"
          },
          "name": "Qingchen Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:45.275Z",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93590",
          "name": "Pengyuan Wang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93591",
          "name": "Wentao Zhang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93592",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93593",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93594",
          "name": "Xinchi Li",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93595",
          "name": "Minchuan Yang",
          "hidden": false
        },
        {
          "_id": "67fdc1b41d1bc292f7b93596",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:59:36.000Z",
      "submittedOnDailyAt": "2025-04-16T00:53:50.942Z",
      "title": "xVerify: 理由モデル評価の効率的な答え検証ツール",
      "submittedOnDailyBy": {
        "_id": "64e18e9ec20c27fcc8df384e",
        "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
        "isPro": false,
        "fullname": "Ding Chen",
        "user": "Hush-cd",
        "type": "user"
      },
      "summary": "o1モデルのリリースに伴い、ショートタイムステップを採用した推理モデルが徐々に登場しました。これらのモデルが生成する回答は、複雑な推理、中間ステップ、そして自覚を含むため、既存の評価方法が多くの場合には不十分です。それらは、LLMの出力が実際に参照答えと等価であるかどうかを判断できないことや、長い複雑な回答から最終的な答えを特定して抽出できないことがあります。この問題に対処するために、xVerifyという推理モデルの評価に向けての効率的な答えの検証機能を提案します。xVerifyは等価判断に強い能力を示し、理由論モデルが生成した答えが参照答えと等価であるかどうかを、多種多様の目的的な質問に対して有効に判断できるように設計されています。xVerifyの訓練と評価には、多種多様なデータセットから生成された質問・答えペアを集め、理由論モデルと特に理由論モデルの評価に向けて設計された評価セットを利用して作成したVARデータセットを使用します。多回の注釈プロセスを使用してラベルの正確性を確保します。VARデータセットに基づいて、異なるサイズの複数のxVerifyモデルを訓練します。テストセットと一般化セットの評価実験では、すべてのxVerifyモデルが全体的なF1スコアと正確性が95％を超えました。特に、最小サイズのバージョンであるxVerify-0.5B-Iは、GPT-4oを除くすべての評価方法を超え、xVerify-3B-Ibは全体的な性能でGPT-4oを超えました。これらの結果はxVerifyの効果と一般化能力を証明しています。",
      "upvotes": 28,
      "discussionId": "67fdc1b51d1bc292f7b935e8",
      "githubRepo": "https://github.com/IAAR-Shanghai/xVerify",
      "ai_keywords": [
        "reasoning models",
        "o1 model",
        "slow thinking strategies",
        "complex reasoning",
        "intermediate steps",
        "self-reflection",
        "evaluation methods",
        "LLM output",
        "reference answer",
        "final answer",
        "xVerify",
        "equivalence judgment",
        "VAR dataset",
        "multi-round annotation process",
        "F1 scores",
        "xVerify-0.5B-I",
        "xVerify-3B-Ib",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-04-14T13:59:36.000Z",
    "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
    "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10481.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e18e9ec20c27fcc8df384e",
      "avatarUrl": "/avatars/64ef866b9fa385efcefb34ea76b76802.svg",
      "fullname": "Ding Chen",
      "name": "Hush-cd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10465",
      "authors": [
        {
          "_id": "67ff26c3414c03ebc1d42529",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252a",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:53.598Z",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252b",
          "name": "Zilong Huang",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252c",
          "name": "Yanwei Li",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252d",
          "name": "Weixian Lei",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252e",
          "name": "Xueqing Deng",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d4252f",
          "name": "Shihao Chen",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d42530",
          "name": "Shunping Ji",
          "hidden": false
        },
        {
          "_id": "67ff26c3414c03ebc1d42531",
          "name": "Jiashi Feng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:52:22.000Z",
      "submittedOnDailyAt": "2025-04-16T02:11:29.898Z",
      "title": "Pixel-SAIL: ピクセルベースの理解を実現するSingle Transformer",
      "submittedOnDailyBy": {
        "_id": "63958b4414513eaf9029ebf1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
        "isPro": false,
        "fullname": "Xiangtai Li",
        "user": "LXT",
        "type": "user"
      },
      "summary": "多モダル大語言モデル（MLLMs）は、細かいピクセルレベルの理解タスクに関して驚異的な性能を達成します。しかし、すべての研究は、ビジョンエンコーダ（CLIP）、分割エクスプラインなどの追加機能を重く依存しています、これによりシステム複雑性が高く、モデルのスケーリングが制限されています。本研究では、追加機能を引き入れないように高度に簡素化されたMLLMを探索することを目標としています。本研究は、Single trAnsformer as a unified vIsion-Language Model（SAIL）設計における最近の研究に基づいています、これらの研究では、transformersでビジョントークンとテキストトークンを共に学習しています。Pixel-SAIL、ピクセルレベルのMLLMタスクに対する単一transformerを紹介します。特に、基本的なベースラインにおいて3つの技術的な改善を実施します。まず、可視トークン特徴を精確化するための学習可能なアップサンプリングモジュールを設計します。次に、単一transformerが可視プロンプト入力を理解し、可視プロンプトエンベディングと可視トークンの早期融合から利益を得ることを可能にするための新しい可視プロンプト注入戦略を提案します。最後に、可視エクスプラインの精確な特徴抽出能力を効率的に向上させるための可視エクスプラインの経験的な発達戦略を導入します。また、手動チェックを用いて収集した詳細なピクセル理解ベンチマーク（PerBench）を紹介します。これには、詳細な物件説明、可視プロンプトベースの質問回答、可視文言参照分割の3つのタスクが含まれています。4つの参照分割ベンチマーク、1つの可視プロンプトベンチマークと我々のPerBenchにおいて拡張的な実験を行い、我々のPixel-SAILがより簡単なパイプラインで比較的またはより良い結果を収めることを示します。コードとモデルは、https://github.com/magic-research/Sa2VA で公開します。",
      "upvotes": 21,
      "discussionId": "67ff26c6414c03ebc1d425de",
      "ai_keywords": [
        "multimodal large language models (MLLMs)",
        "pixel-level understanding",
        "vision encoder (CLIP)",
        "segmentation experts",
        "single transformer as a unified vision-language model (SAIL)",
        "pixel-wise MLLM tasks",
        "learnable upsampling module",
        "visual prompt injection",
        "visual prompt embeddings",
        "vision expert distillation",
        "pixel understanding benchmark (PerBench)",
        "detailed object description",
        "visual prompt-based question answering",
        "visual-text referring segmentation",
        "referring segmentation benchmarks",
        "visual prompt benchmark"
      ]
    },
    "publishedAt": "2025-04-14T13:52:22.000Z",
    "title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding",
    "summary": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for\nfine-grained pixel-level understanding tasks. However, all the works rely\nheavily on extra components, such as vision encoder (CLIP), segmentation\nexperts, leading to high system complexity and limiting model scaling. In this\nwork, our goal is to explore a highly simplified MLLM without introducing extra\ncomponents. Our work is motivated by the recent works on Single trAnsformer as\na unified vIsion-Language Model (SAIL) design, where these works jointly learn\nvision tokens and text tokens in transformers. We present Pixel-SAIL, a single\ntransformer for pixel-wise MLLM tasks. In particular, we present three\ntechnical improvements on the plain baseline. First, we design a learnable\nupsampling module to refine visual token features. Secondly, we propose a novel\nvisual prompt injection strategy to enable the single transformer to understand\nvisual prompt inputs and benefit from the early fusion of visual prompt\nembeddings and vision tokens. Thirdly, we introduce a vision expert\ndistillation strategy to efficiently enhance the single transformer's\nfine-grained feature extraction capability. In addition, we have collected a\ncomprehensive pixel understanding benchmark (PerBench), using a manual check.\nIt includes three tasks: detailed object description, visual prompt-based\nquestion answering, and visual-text referring segmentation. Extensive\nexperiments on four referring segmentation benchmarks, one visual prompt\nbenchmark, and our PerBench show that our Pixel-SAIL achieves comparable or\neven better results with a much simpler pipeline. Code and model will be\nreleased at https://github.com/magic-research/Sa2VA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10465.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63958b4414513eaf9029ebf1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
      "fullname": "Xiangtai Li",
      "name": "LXT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10337",
      "authors": [
        {
          "_id": "67fddae99a03686367721718",
          "user": {
            "_id": "6471a24381ded91f253ceb1c",
            "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
            "isPro": false,
            "fullname": "Wesley Shi",
            "user": "WesleyShi",
            "type": "user"
          },
          "name": "Wenlei Shi",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-15T06:43:40.277Z",
          "hidden": false
        },
        {
          "_id": "67fddae99a03686367721719",
          "name": "Xing Jin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T15:46:33.000Z",
      "submittedOnDailyAt": "2025-04-16T00:52:23.733Z",
      "title": "ヘイムダール：生成認識の検証時のスケーリングテスト",
      "submittedOnDailyBy": {
        "_id": "6471a24381ded91f253ceb1c",
        "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
        "isPro": false,
        "fullname": "Wesley Shi",
        "user": "WesleyShi",
        "type": "user"
      },
      "summary": "AIシステムは、知識を作成しても維持することができる限り、その知識を確認できる限りです。長期コンショードオブラインティング（Chain-of-Thought reasoning）における最近の研究は、LLM（大規模言語モデル）が競争的な問題を解くにおいて大きなポテンシャルを示していますが、その確認能力は弱く、十分に調査されていません。本論文では、長期コンショードオブラインティングの確認を行うLLM、ヘイムダル（Heimdall）を提案します。ヘイムダルは、解決策の正確性を正確に判断することができます。強化学習をプライムとして、競争的な数学問題での確認精度を62.5%から94.5%に向上させました。再現サンプリングを用いてスケーリングすると、精度は更に97.5%に達しました。人間評価により、ヘイムダルは印象的な一般化能力を示し、訓練中に含まれていない難しい数学証明の多くの問題を正しく検出しました。また、ペソプティムバリューティ（Pessimistic Verification）を提案し、ヘイムダルの機能を問題解決のスケーリングに拡張しました。ペソプティムバリューティは、解決モデルからの解決策をヘイムダルに判断し、最も確信度の低い解決策を選択します。DeepSeek-R1-Distill-Qwen-32Bを解決モデルとして、ペソプティムバリューティは、AIME2025の解決精度を54.2%から70.0%に、16倍の計算バケットで、もっと計算バケットを設定すると83.3%に向上させました。ゲミニ2.5 Proの強力な解決モデルを使用すると、スコアは93.0%に達しました。最後に、自動的な知識発見システムのプロトタイプを構築しました。このシステムでは、一つ目のソースが問題を提案し、二つ目のソースが解決策を提供し、三つ目のソースが解決策を確認します。NuminaMathのデータ合成ワークを最初の2コンポーネントとして使用し、ヘイムダルはデータセット内の問題のレコードを有効に検出し、近半のデータがフラウトであることを明らかにしました。これは、NuminaMathの最近の消減研究と興味深い一致を示します。",
      "upvotes": 21,
      "discussionId": "67fddaea9a03686367721776",
      "ai_keywords": [
        "Chain-of-Thought reasoning",
        "LLMs (Large Language Models)",
        "Heimdall",
        "long CoT verification",
        "pure reinforcement learning",
        "synthetic math problems",
        "human evaluation",
        "generalization capabilities",
        "Pessimistic Verification",
        "DeepSeek-R1-Distill-Qwen-32B",
        "AIME2025",
        "Gemini 2.5 Pro",
        "solution accuracy",
        "automatic knowledge discovery system",
        "ternary system",
        "NuminaMath",
        "data synthesis",
        "data records",
        "flawed data"
      ]
    },
    "publishedAt": "2025-04-14T11:46:33.000Z",
    "title": "Heimdall: test-time scaling on the generative verification",
    "summary": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10337.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6471a24381ded91f253ceb1c",
      "avatarUrl": "/avatars/31d447068fe1fd4200ab5d08ab31eed4.svg",
      "fullname": "Wesley Shi",
      "name": "WesleyShi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11346",
      "authors": [
        {
          "_id": "67ff18961dc5d56fdd6ca724",
          "name": "Yu Gao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca725",
          "name": "Lixue Gong",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca726",
          "name": "Qiushan Guo",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca727",
          "name": "Xiaoxia Hou",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca728",
          "name": "Zhichao Lai",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca729",
          "name": "Fanshi Li",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72a",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72b",
          "name": "Xiaochen Lian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72c",
          "name": "Chao Liao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72d",
          "name": "Liyang Liu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72e",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca72f",
          "name": "Yichun Shi",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca730",
          "name": "Shiqi Sun",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca731",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca732",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca733",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca734",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca735",
          "name": "Xuanda Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca736",
          "name": "Xun Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca737",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca738",
          "name": "Guofeng Wu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca739",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73a",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73b",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73c",
          "name": "Zhonghua Zhai",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73d",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73e",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca73f",
          "name": "Yuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca740",
          "name": "Shijia Zhao",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca741",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "67ff18961dc5d56fdd6ca742",
          "name": "Weilin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:19:07.000Z",
      "submittedOnDailyAt": "2025-04-16T01:10:39.295Z",
      "title": "Seedream 3.0 技術報告",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "種実夢 3.0 は、高性能の中国語・英語のバイリンガル画像生成ベースモデルです。種実夢 2.0 における既存の課題を解決するために、プロジェクトではデータ構築からモデルの部署までのプロセス全体で様々な技術的改善を実施しました。特に、複雑なプロンプトの対応、細かいタイポグラフィーの生成、画質の不適切さ、画像の解像度の制限などの問題点を解決しました。種実夢 3.0 の進歩は、データ構築からモデルの部署までのプロセス全体での改善に基づきます。データ層では、ディフェクトに関心のあるトレーニングパラダイムと双軸コラボレーションデータサンプリングフレームワークを用いてデータセットを2倍に増やしました。また、予め学習させる際には、混合レンジュトレーニング、クロスモーダリティーRoPE、表現の対応損失、レンジに関心のあるタイムステップサンプリングなどの効果的な手法を採用しました。後ディープ学習ステップでは、SFTで多様な美術的なコメントを利用し、スケーリングされたVLMベースの報酬モデルを使用して、人間の好みに合わせた出力を実現しました。また、種実夢 3.0 は新しい加速パラダイムを開拓しました。一貫したノイズ期待値と重要性に関心のあるタイムステップサンプリングを用いて、画像の品質を維持する同時に4から8倍のスピードアップを実現しました。種実夢 3.0 は、種実夢 2.0 より显著な改善を示します：特に複雑な中国字のテキストレンディングの能力を向上させ、これはプロジェクト的なタイポグラフィー生成に重要です。また、本機の高解像度出力（最高2K）を提供し、高品質の画像を生成できるようにしました。",
      "upvotes": 20,
      "discussionId": "67ff189c1dc5d56fdd6ca8e0",
      "projectPage": "https://team.doubao.com/zh/tech/seedream3_0",
      "ai_keywords": [
        "mixed-resolution training",
        "cross-modality RoPE",
        "representation alignment loss",
        "resolution-aware timestep sampling",
        "SFT (Supervised Fine-Tuning)",
        "VLM (Vision Language Model)",
        "consistent noise expectation",
        "importance-aware timestep sampling"
      ]
    },
    "publishedAt": "2025-04-15T12:19:07.000Z",
    "title": "Seedream 3.0 Technical Report",
    "summary": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11346.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 47
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11442",
      "authors": [
        {
          "_id": "67ff1387e1bfbb6bdd79ab72",
          "user": {
            "_id": "628b671f8fb67b90658613f7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653303027789-noauth.jpeg",
            "isPro": false,
            "fullname": "Leon Guertler",
            "user": "LeonGuertler",
            "type": "user"
          },
          "name": "Leon Guertler",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:02.481Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab73",
          "user": {
            "_id": "653879fbf5f5016df355d010",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653879fbf5f5016df355d010/hU3mrTOw3DQ8auUGoQceW.jpeg",
            "isPro": false,
            "fullname": "Bobby Cheng",
            "user": "bobbycxy",
            "type": "user"
          },
          "name": "Bobby Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:06.026Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab74",
          "user": {
            "_id": "636681feaa6a4af6073ba73e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636681feaa6a4af6073ba73e/u_0moYNu6as9Sszp-ej95.png",
            "isPro": true,
            "fullname": "Simon Yu",
            "user": "simonycl",
            "type": "user"
          },
          "name": "Simon Yu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:08.368Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab75",
          "user": {
            "_id": "635e3a76106f984574c36409",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
            "isPro": false,
            "fullname": "Bo Liu",
            "user": "Benjamin-eecs",
            "type": "user"
          },
          "name": "Bo Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:46:16.265Z",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab76",
          "name": "Leshem Choshen",
          "hidden": false
        },
        {
          "_id": "67ff1387e1bfbb6bdd79ab77",
          "name": "Cheston Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:55:20.000Z",
      "submittedOnDailyAt": "2025-04-16T01:06:25.215Z",
      "title": "TextArena",
      "submittedOnDailyBy": {
        "_id": "635e3a76106f984574c36409",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
        "isPro": false,
        "fullname": "Bo Liu",
        "user": "Benjamin-eecs",
        "type": "user"
      },
      "summary": "テキストアリアンは、大規模言語モデル（LLMs）のアガント行動の訓練と評価のための開放ソースの競技的なテキストベースゲームの集合です。57以上の独自環境（シングルプレイヤー、ツアープレイヤー、マルチプレイヤーの設定を含む）を収め、オンラインプレイシステムを通じて人間や他の提出モデルとの対戦でモデルの能力を簡単に評価することができます。実時間のTrueSkillスコアを提供します。傳統的なベンチマークは、ネジジョー、マインドシンクとデジューションのような動的なソシャルスキルを評価することが少ないことから、このギャップを解決することができます。研究、コミュニティ、拡張性を前提として設計され、テキストアリアンは新しいゲームの追加、フレームワークの変更、モデルのテスト、モデルとのプレイ、モデルの訓練を優先しています。環境、ゲーム、ランキングボード、例を含む詳細な記述は、https://github.com/LeonGuertler/TextArena と https://www.textarena.ai/ から利用できます。",
      "upvotes": 18,
      "discussionId": "67ff1388e1bfbb6bdd79abbe",
      "projectPage": "https://textarena.ai/",
      "githubRepo": "https://github.com/LeonGuertler/TextArena",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "TrueSkill scores",
        "negotiation",
        "theory of mind",
        "deception",
        "dynamic social skills"
      ]
    },
    "publishedAt": "2025-04-15T13:55:20.000Z",
    "title": "TextArena",
    "summary": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11442.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "635e3a76106f984574c36409",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png",
      "fullname": "Bo Liu",
      "name": "Benjamin-eecs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10766",
      "authors": [
        {
          "_id": "67ff114a3026f8abc4bf7e43",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e44",
          "name": "Yanhong Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e45",
          "name": "Ziyue Li",
          "hidden": false
        },
        {
          "_id": "67ff114a3026f8abc4bf7e46",
          "user": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "isPro": false,
            "fullname": "Tianyi Zhou",
            "user": "zhoutianyi",
            "type": "user"
          },
          "name": "Tianyi Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:46:19.086Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T23:53:47.000Z",
      "submittedOnDailyAt": "2025-04-16T00:40:29.697Z",
      "title": "インストラクションデータと理由データがトレーニング後にどのように影響を及ぼしているか：レイヤ毎の勾配から見たデータの質",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）の後学習が指導を従うことから複雑な理由論の課題へと進む中、異なるデータが微調整の動態にどのように影響を与えるかを理解することは主に探索されていません。本論文では、LLMの後学習による低/高品質の指導や理由論データによるレイヤ毎の勾配のスペクトル分析を提案します。我々の分析は、データ評価により構成される準備的な指標（例：IFD、InsTag、Difficulty、Reward）が勾配の固有値分解（SVD）から計算されるスペクトル特性によって解釈され、統一されることを示します。特に、高品質のデータは通常、核ノルムが低く、効果的なレンキングが高くなることが見られます。特に、効果的なレンキングは、核ノルムによる微妙な品質の違いを捉えることでより良い強固性と解像度を示します。例えば、理由論データは指導データよりも大幅に高い効果的なレンキングを示し、複雑な課題ではより豊富な勾配構造を持つことを示します。実験も、同じフamilyのモデルはサイズによらず似た勾配パターンを共有し、モデルのフamilyが異なる場合は显著に違うことを示します。指導データと理由論データのデータ品質の影響を一貫した視点を提供することで、この研究はデータ品質と学習の安定性の間の相互作用を明らかにし、後学習のデータ探索戦略の開発に新しい見解を提供します。",
      "upvotes": 16,
      "discussionId": "67ff11503026f8abc4bf7fed",
      "githubRepo": "https://github.com/MingLiiii/Gradient_Unified",
      "ai_keywords": [
        "spectral analysis",
        "layer-wise gradients",
        "low/high-quality instruction",
        "reasoning data",
        "IFD",
        "InsTag",
        "Difficulty",
        "Reward",
        "singular value decomposition (SVD)",
        "nuclear norms",
        "effective ranks",
        "gradient structures",
        "training stability"
      ]
    },
    "publishedAt": "2025-04-14T19:53:47.000Z",
    "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients",
    "summary": "As the post-training of large language models (LLMs) advances from\ninstruction-following to complex reasoning tasks, understanding how different\ndata affect finetuning dynamics remains largely unexplored. In this paper, we\npresent a spectral analysis of layer-wise gradients induced by low/high-quality\ninstruction and reasoning data for LLM post-training. Our analysis reveals that\nwidely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and\nReward, can be explained and unified by spectral properties computed from\ngradients' singular value decomposition (SVD). Specifically, higher-quality\ndata are usually associated with lower nuclear norms and higher effective\nranks. Notably, effective rank exhibits better robustness and resolution than\nnuclear norm in capturing subtle quality differences. For example, reasoning\ndata achieves substantially higher effective ranks than instruction data,\nimplying richer gradient structures on more complex tasks. Our experiments also\nhighlight that models within the same family share similar gradient patterns\nregardless of their sizes, whereas different model families diverge\nsignificantly. Providing a unified view on the effects of data quality across\ninstruction and reasoning data, this work illuminates the interplay between\ndata quality and training stability, shedding novel insights into developing\nbetter data exploration strategies for post-training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10766.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10462",
      "authors": [
        {
          "_id": "67ff2aa6a0346c2e622afdb2",
          "name": "Weixian Lei",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb3",
          "name": "Jiacong Wang",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb4",
          "name": "Haochen Wang",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb5",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:51.715Z",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb6",
          "name": "Jun Hao Liew",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb7",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "67ff2aa6a0346c2e622afdb8",
          "name": "Zilong Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T17:50:20.000Z",
      "submittedOnDailyAt": "2025-04-16T02:27:49.065Z",
      "title": "スカラビリティの簡単性：ビジョン・ラングラジュ学習の実験的解析を一つのTransformerで行う",
      "submittedOnDailyBy": {
        "_id": "63958b4414513eaf9029ebf1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
        "isPro": false,
        "fullname": "Xiangtai Li",
        "user": "LXT",
        "type": "user"
      },
      "summary": "この論文では、SAILという単一のトランジフォーマーを構成する統一モノモダル大規模言語モデル（MLLM）を介して、ピクセルエンコーディングと言語解釈を一つのアーキテクチャ内で統合しています。現在のモジュール化モデルラインモデルと違い、SAILは事前学習されたビジョントランジフォーマー（ViT）の独立なビジョンエンコーダーの必要性を除去し、よりミニマリストなアーキテクチャデザインを提供しています。新しいアーキテクチャコンポーネントを導入する代わりに、SAILは、ビジュアルと文字のモデライの特徴に合わせるように、ミックスアテンション機構とモノモダル位置エンコーディングを適用しています。SAILのスケーラビリティ、クロスモデル情報フローパターン、ビジュアル表現能力などの性質をモジュール化MLLMと比較して、システマティックに比較しています。トレーニングデータとモデルサイズの両方をスケーリングすることで、SAILはモジュール化MLLMと同等の性能を達成します。特に、事前学習されたViTコンポーネントの除去は、SAILのスケーラビリティを向上させ、クロスモデル情報フローパターンがそれほど異なるものになります。また、SAILは強力なビジュアル表現能力を示し、セマンティックセグメンテーションなどの視覚タスクでViT-22Bと同等の結果を収めます。コードとモデルは、https://github.com/bytedance/SAIL から利用可能です。",
      "upvotes": 12,
      "discussionId": "67ff2aa7a0346c2e622afe08",
      "ai_keywords": [
        "single transformer",
        "unified multimodal large language model (MLLM)",
        "raw pixel encoding",
        "language decoding",
        "vision transformer (ViT)",
        "mix-attention mechanisms",
        "multimodal positional encodings",
        "scalability",
        "cross-modal information flow patterns",
        "visual representation capabilities",
        "semantic segmentation",
        "ViT-22B"
      ]
    },
    "publishedAt": "2025-04-14T13:50:20.000Z",
    "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer",
    "summary": "This paper introduces SAIL, a single transformer unified multimodal large\nlanguage model (MLLM) that integrates raw pixel encoding and language decoding\nwithin a singular architecture. Unlike existing modular MLLMs, which rely on a\npre-trained vision transformer (ViT), SAIL eliminates the need for a separate\nvision encoder, presenting a more minimalist architecture design. Instead of\nintroducing novel architectural components, SAIL adapts mix-attention\nmechanisms and multimodal positional encodings to better align with the\ndistinct characteristics of visual and textual modalities. We systematically\ncompare SAIL's properties-including scalability, cross-modal information flow\npatterns, and visual representation capabilities-with those of modular MLLMs.\nBy scaling both training data and model size, SAIL achieves performance\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\nenhances SAIL's scalability and results in significantly different cross-modal\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\nrepresentation capabilities, achieving results on par with ViT-22B in vision\ntasks such as semantic segmentation. Code and models are available at\nhttps://github.com/bytedance/SAIL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10462.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63958b4414513eaf9029ebf1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
      "fullname": "Xiangtai Li",
      "name": "LXT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10559",
      "authors": [
        {
          "_id": "67ff1df03b42083b37219456",
          "name": "Keyu Duan",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219457",
          "name": "Zichen Liu",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219458",
          "name": "Xin Mao",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b37219459",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945a",
          "name": "Changyu Chen",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945b",
          "name": "Qiguang Chen",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945c",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        },
        {
          "_id": "67ff1df03b42083b3721945d",
          "user": {
            "_id": "6214e4ee1e35c843d42d1f88",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
            "isPro": true,
            "fullname": "Longxu Dou",
            "user": "dreamerdeo",
            "type": "user"
          },
          "name": "Longxu Dou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:43:00.444Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T14:53:56.000Z",
      "submittedOnDailyAt": "2025-04-16T01:34:03.069Z",
      "title": "有効なプロセス報酬モデル訓練を達成するための活性化学習法",
      "submittedOnDailyBy": {
        "_id": "6214e4ee1e35c843d42d1f88",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
        "isPro": true,
        "fullname": "Longxu Dou",
        "user": "dreamerdeo",
        "type": "user"
      },
      "summary": "Process Reward Models (PRMs)は、大規模言語モデル（LLMs）にステップレベルのサブジェクションを提供しますが、トレーニングデータの注釈のスケーリングは両方人間とLLMsにとって難しいです。この制限に対処するために、私たちは活性学習アプローチ、ActPRMを提案します。ActPRMは、最も不確かなサンプルを主動的に選択し、ラベリングコストを大幅に減少させます。トレーニング中、PRMを使用して、順伝播後の不確かさを評価し、そのみにハイリスクのデータを残します。その後、このデータをラベル化するために、能力のあるだけにコストが高い推理モデルを使用します。その後、ラベルに対する損失を計算し、PRMの重みを更新します。ActPRMとベージャーの微調節を比較し、ポールベースの活性学習設定で、ActPRMは50%の注釈を削減し、比較的またはより良い性能を実現します。注釈の効率を超えて、私たちはActPRMで1M+の数学論理データロードをフィルタリングし、60%のデータを残します。その後、この選択されたデータセットによるトレーニングは、ProcessBench（75.0%）とPRMBench（65.5%）で新たな最先端（SOTA）のPRMを実現します。",
      "upvotes": 8,
      "discussionId": "67ff1df23b42083b372194a8",
      "githubRepo": "https://github.com/sail-sg/ActivePRM",
      "ai_keywords": [
        "active learning",
        "ActPRM",
        "uncertainty estimation",
        "labeling costs",
        "vanilla fine-tuning",
        "pool-based active learning",
        "annotation efficiency",
        "math reasoning trajectories",
        "state-of-the-art (SOTA)",
        "ProcessBench",
        "PRMBench"
      ]
    },
    "publishedAt": "2025-04-14T10:53:56.000Z",
    "title": "Efficient Process Reward Model Training via Active Learning",
    "summary": "Process Reward Models (PRMs) provide step-level supervision to large language\nmodels (LLMs), but scaling up training data annotation remains challenging for\nboth humans and LLMs. To address this limitation, we propose an active learning\napproach, ActPRM, which proactively selects the most uncertain samples for\ntraining, substantially reducing labeling costs. During training, we use the\nPRM to estimate uncertainty after the forward pass, retaining only highly\nuncertain data. A capable yet costly reasoning model then labels this data.\nThen we compute the loss with respect to the labels and update the PRM's\nweights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active\nlearning setting, demonstrating that ActPRM reduces 50% annotation, but\nachieving the comparable or even better performance. Beyond annotation\nefficiency, we further advance the actively trained PRM by filtering over 1M+\nmath reasoning trajectories with ActPRM, retaining 60% of the data. A\nsubsequent training on this selected dataset yields a new state-of-the-art\n(SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same\nsized models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10559.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6214e4ee1e35c843d42d1f88",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg",
      "fullname": "Longxu Dou",
      "name": "dreamerdeo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 19
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11427",
      "authors": [
        {
          "_id": "67ff1cc4372d6790b1b7da90",
          "name": "Yanrui Bin",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da91",
          "name": "Wenbo Hu",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da92",
          "name": "Haoyuan Wang",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da93",
          "name": "Xinya Chen",
          "hidden": false
        },
        {
          "_id": "67ff1cc4372d6790b1b7da94",
          "name": "Bing Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/dKoipt1ASgjiyDt33avVt.mp4"
      ],
      "publishedAt": "2025-04-15T17:39:07.000Z",
      "submittedOnDailyAt": "2025-04-16T01:28:48.790Z",
      "title": "NormalCrafter: 映画からの時系列一致するノルマルベクトルの学習\n  拡散先頭プロイジング",
      "submittedOnDailyBy": {
        "_id": "657a7458afbb0117ba15c59f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
        "isPro": false,
        "fullname": "Wenbo Hu",
        "user": "wbhu-tc",
        "type": "user"
      },
      "summary": "表面ノルマ推定は、複数のコンピュータビジョンアプリケーションの基盤として役立ちます。静的画像シナリオに対してのたくさんの努力を記録しているのに対して、映像ベースのノルマ推定で時間的な一致性を確保するのは難しい挑戦です。既存の方法にそのみ時間的な成分を追加するだけではなく、私たちは、映像ディフュージョンモデルの固有の時間的な先驭を活用するためのNormalCrafterを紹介します。シーケンス全体で高品質のノルマ推定を確保するために、私たちは、ディフュージョン特徴量を語意的カットオオと一致させ、モデルがシーンの固有の語意的性質に集中させるよう勧告するSemantic Feature Regularization（SFR）を提案します。また、私たちは、潜在空間とピクセル空間の学習を両方活用する2段階の学習プロトコルを導入し、空間的な精度を維持しながら長期的な時間的なコンテキストを維持することを目指します。拡張的な評価は、我々の方法の効果を示し、複数の映像から複雑な詳細を持つ時間的に一致したノルマシーケンスの生成で上位の性能を示します。",
      "upvotes": 5,
      "discussionId": "67ff1cc5372d6790b1b7daee",
      "projectPage": "https://normalcrafter.github.io/",
      "githubRepo": "https://github.com/Binyr/NormalCrafter",
      "ai_keywords": [
        "video diffusion models",
        "Semantic Feature Regularization (SFR)",
        "latent space",
        "pixel space",
        "temporal coherence",
        "spatial accuracy",
        "long temporal context",
        "temporally consistent",
        "intricate details"
      ]
    },
    "publishedAt": "2025-04-15T13:39:07.000Z",
    "title": "NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors",
    "summary": "Surface normal estimation serves as a cornerstone for a spectrum of computer\nvision applications. While numerous efforts have been devoted to static image\nscenarios, ensuring temporal coherence in video-based normal estimation remains\na formidable challenge. Instead of merely augmenting existing methods with\ntemporal components, we present NormalCrafter to leverage the inherent temporal\npriors of video diffusion models. To secure high-fidelity normal estimation\nacross sequences, we propose Semantic Feature Regularization (SFR), which\naligns diffusion features with semantic cues, encouraging the model to\nconcentrate on the intrinsic semantics of the scene. Moreover, we introduce a\ntwo-stage training protocol that leverages both latent and pixel space learning\nto preserve spatial accuracy while maintaining long temporal context. Extensive\nevaluations demonstrate the efficacy of our method, showcasing a superior\nperformance in generating temporally consistent normal sequences with intricate\ndetails from diverse videos.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/dKoipt1ASgjiyDt33avVt.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a7458afbb0117ba15c59f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
      "fullname": "Wenbo Hu",
      "name": "wbhu-tc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11343",
      "authors": [
        {
          "_id": "67ff2d4a86e7ad2b4bea1349",
          "name": "Wei Xiong",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134a",
          "name": "Jiarui Yao",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134b",
          "name": "Yuhui Xu",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134c",
          "name": "Bo Pang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134d",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134e",
          "name": "Doyen Sahoo",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea134f",
          "name": "Junnan Li",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1350",
          "name": "Nan Jiang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1351",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1352",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "67ff2d4a86e7ad2b4bea1353",
          "name": "Hanze Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:15:02.000Z",
      "submittedOnDailyAt": "2025-04-16T02:39:25.160Z",
      "title": "ミニマリスティックアプローチのLLM推論：実験拒否サンプリングから再強化",
      "submittedOnDailyBy": {
        "_id": "643e59806db6ba8c5ee123f3",
        "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
        "isPro": false,
        "fullname": "Wei Xiong",
        "user": "weqweasdas",
        "type": "user"
      },
      "summary": "強化学習（RL）は、複雑な理由論タスクに対して大規模言語モデル（LLMs）を調整するための主流的なアプローチになりました。最近の方法群の中で、GRPOはDeepSeek-R1などのモデルの訓練において実験的な成功を示し、その効果の元については理解が浅かったりします。本稿では、GRPOを再評価し、ポリシーグラデイアンそのような強化学習アルゴリズムの観点からその核心的な構成要素を分析します。驚くべきに、簡単な拒否サンプリングベースラインフォームであるRAFTが、正の報酬を受けたサンプルだけで訓練されていることにより、GRPOやPPOと比較的な性能を示します。ノーマライゼーションの効果ではなく、GRPOの主な優れた点は、完全に間違った回答を持つプロンプトを捨てることにあります。この見解に基づいて、完全に間違ったおよび完全に正しいサンプルをフィルタリングする政策グラデイアンの最小限の拡張版であるReinforce-Rejを提案します。Reinforce-Rejは、KLの効率と安定性を向上させ、複雑なRLアルゴリズムに比べて軽量で効果的な代替として役立ちます。RAFTを強固で解釈可能なベースラインとして採用し、将来の進歩には、負のサンプルを無関心に使うよりも、もっと原則的な設計を重視することを主張します。我々の発見は、報酬ベースのLLMの後処理における将来の研究についてのガイドラインを提供します。",
      "upvotes": 4,
      "discussionId": "67ff2d4b86e7ad2b4bea1381",
      "ai_keywords": [
        "GRPO",
        "DeepSeek-R1",
        "reinforcement learning (RL)",
        "fine-tuning",
        "large language models (LLMs)",
        "complex reasoning tasks",
        "RAFT",
        "positively rewarded samples",
        "policy gradient",
        "KL efficiency"
      ]
    },
    "publishedAt": "2025-04-15T12:15:02.000Z",
    "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce",
    "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11343.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643e59806db6ba8c5ee123f3",
      "avatarUrl": "/avatars/4052f2a250107f43b3634c3ee3cc30a1.svg",
      "fullname": "Wei Xiong",
      "name": "weqweasdas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.10188",
      "authors": [
        {
          "_id": "67fe602166c0e8f3c2df22a9",
          "user": {
            "_id": "649d59cec6b4fdd84ebe0d47",
            "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
            "isPro": false,
            "fullname": "Deyuan Liu",
            "user": "SempraETY",
            "type": "user"
          },
          "name": "Deyuan Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-15T16:46:56.270Z",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22aa",
          "name": "Peng Sun",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22ab",
          "name": "Xufeng Li",
          "hidden": false
        },
        {
          "_id": "67fe602166c0e8f3c2df22ac",
          "name": "Tao Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T12:43:17.000Z",
      "submittedOnDailyAt": "2025-04-16T03:03:14.152Z",
      "title": "エフィシェントな生成モデルトレーニングによるインサイド表現ワームアップ",
      "submittedOnDailyBy": {
        "_id": "649d59cec6b4fdd84ebe0d47",
        "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
        "isPro": false,
        "fullname": "Deyuan Liu",
        "user": "SempraETY",
        "type": "user"
      },
      "summary": "Diffusionモデルは高次元データの生成に優れていますが、自動認識方法に比べてトレーニング効率と表現質の向上に欠陥を見出します。私たちは鍵のブロックブロックを特定しました：トレーニング中に高品質で意味豊富な表現の利用不足が、特に収束を遅らせています。システマティックな分析により、生成が行われる前に、主に早期層で意味と構造のパターンの学習が行われる重要な表現処理領域を明らかにしました。これに対処し、Embedded Representation Warmup (ERW)を提案します。ERWは、最初のステップでERWモジュールが暖め上げとして、高品質で事前学習された表現で初期化します。この暖め上げは、表現の学習をスタートから学習することでボリュームを最小限に抑え、収束を加速し、性能を向上させます。理論的な分析により、ERWの効果は、特に表現処理領域での特定のニューラルネットワーク層の精密な統合によって見出されます。また、ERWは、表現質を向上させることで、収束の加速を実際的に証明します：実験的に、現在の最先端の方法REPAに比べて、トレーニング速度を40倍加速します。コードは、https://github.com/LINs-lab/ERWに提供されています。",
      "upvotes": 4,
      "discussionId": "67fe602266c0e8f3c2df2334",
      "projectPage": "https://lins-lab.github.io/ERW/",
      "githubRepo": "https://github.com/LINs-lab/ERW",
      "ai_keywords": [
        "diffusion models",
        "high-dimensional data",
        "self-supervised methods",
        "high-quality representations",
        "semantic representations",
        "structural pattern learning",
        "Embedded Representation Warmup (ERW)",
        "warmup",
        "early layers",
        "representation processing region",
        "pretrained representations",
        "convergence",
        "training convergence",
        "representation quality",
        "REPA"
      ]
    },
    "publishedAt": "2025-04-14T08:43:17.000Z",
    "title": "Efficient Generative Model Training via Embedded Representation Warmup",
    "summary": "Diffusion models excel at generating high-dimensional data but fall short in\ntraining efficiency and representation quality compared to self-supervised\nmethods. We identify a key bottleneck: the underutilization of high-quality,\nsemantically rich representations during training notably slows down\nconvergence. Our systematic analysis reveals a critical representation\nprocessing region -- primarily in the early layers -- where semantic and\nstructural pattern learning takes place before generation can occur. To address\nthis, we propose Embedded Representation Warmup (ERW), a plug-and-play\nframework where in the first stage we get the ERW module serves as a warmup\nthat initializes the early layers of the diffusion model with high-quality,\npretrained representations. This warmup minimizes the burden of learning\nrepresentations from scratch, thereby accelerating convergence and boosting\nperformance. Our theoretical analysis demonstrates that ERW's efficacy depends\non its precise integration into specific neural network layers -- termed the\nrepresentation processing region -- where the model primarily processes and\ntransforms feature representations for later generation. We further establish\nthat ERW not only accelerates training convergence but also enhances\nrepresentation quality: empirically, our method achieves a 40times\nacceleration in training speed compared to REPA, the current state-of-the-art\nmethods. Code is available at https://github.com/LINs-lab/ERW.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10188.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649d59cec6b4fdd84ebe0d47",
      "avatarUrl": "/avatars/a070e15659c0686fdfc69e559f3d6493.svg",
      "fullname": "Deyuan Liu",
      "name": "SempraETY",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11447",
      "authors": [
        {
          "_id": "67ff1026f8afab940cc23f88",
          "name": "An Zhaol",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f89",
          "name": "Shengyuan Zhang",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8a",
          "name": "Ling Yang",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8b",
          "name": "Zejian Li",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8c",
          "name": "Jiale Wu",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8d",
          "name": "Haoran Xu",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8e",
          "name": "AnYang Wei",
          "hidden": false
        },
        {
          "_id": "67ff1026f8afab940cc23f8f",
          "name": "Perry Pengyun GU Lingyun Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:57:13.000Z",
      "submittedOnDailyAt": "2025-04-16T00:35:02.754Z",
      "title": "Diffusion Distillation を用いた直接偏好最適化による効率的な3D LiDAR スケーンの完成",
      "submittedOnDailyBy": {
        "_id": "63943c882b9483beb473ec25",
        "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
        "isPro": false,
        "fullname": "Shengyuan Zhang",
        "user": "SYZhang0805",
        "type": "user"
      },
      "summary": "3D LiDARシーン完了におけるディフュージョンモデルの応用は、ディフュージョンの遅いサンプリング速度によって制限されています。スコアの節約加速了サンプリング速度が加速されるが、性能が低下します。直接ポリシー最適化（DPO）を用いたトレーニング後では、好みデータを使用して性能を向上させます。本論文では、好みのアライメントを持つ新しいディフュージョンの節約フレームワーク「Distillation-DPO」を提案します。まず、学生モデルは異なる初期ノイズで組み合わされた組み合わせの完了シーンを生成します。次に、LiDARシーン評価指標を好みとして、勝ち負けのサンプルペアを構築します。この構築は合理であり、多数のLiDARシーン指標は情報がありますが、直接最適化できないので、そのような構築は合理です。さらに、Distillation-DPOは、教師モデルと学生モデルのスコア関数の差を利用して学生モデルを最適化します。この手順は収束まで繰り返します。広範囲の実験は、状態の最先端のLiDARシーン完了ディフュージョンモデルと比較して、Distillation-DPOはより高品質なシーン完了を実現し、5倍以上の完了速度を加速します。私たちの方法は、好み学習を節約に採用することを試みることを知的にしています。私たちのコードは、https://github.com/happyw1nd/DistillationDPO で公開しています。",
      "upvotes": 3,
      "discussionId": "67ff1027f8afab940cc23fd4",
      "ai_keywords": [
        "diffusion models",
        "LiDAR scene completion",
        "score distillation",
        "direct policy optimization (DPO)",
        "preference alignment",
        "student model",
        "paired completion scenes",
        "LiDAR scene evaluation metrics",
        "winning and losing sample pairs",
        "score functions",
        "preference learning"
      ]
    },
    "publishedAt": "2025-04-15T13:57:13.000Z",
    "title": "Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion",
    "summary": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11447.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63943c882b9483beb473ec25",
      "avatarUrl": "/avatars/abd2aae43e68c34770159c15a01c8297.svg",
      "fullname": "Shengyuan Zhang",
      "name": "SYZhang0805",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11326",
      "authors": [
        {
          "_id": "67ff25b765b52d1b69c1f6c1",
          "user": {
            "_id": "67ff29ecbf6889a333c69c7a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
            "isPro": false,
            "fullname": "Henghui Ding",
            "user": "HenghuiDing",
            "type": "user"
          },
          "name": "Henghui Ding",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T03:55:17.825Z",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c2",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c3",
          "name": "Nikhila Ravi",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c4",
          "name": "Shuting He",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c5",
          "name": "Yunchao Wei",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c6",
          "name": "Song Bai",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c7",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c8",
          "name": "Kehuan Song",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6c9",
          "name": "Xinglin Xie",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6ca",
          "name": "Kexin Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cb",
          "name": "Licheng Jiao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cc",
          "name": "Lingling Li",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cd",
          "name": "Shuyuan Yang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6ce",
          "name": "Xuqiang Cao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6cf",
          "name": "Linnan Zhao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d0",
          "name": "Jiaxuan Zhao",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d1",
          "name": "Fang Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d2",
          "name": "Mengjiao Wang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d3",
          "name": "Junpei Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d4",
          "name": "Xu Liu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d5",
          "name": "Yuting Yang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d6",
          "name": "Mengru Ma",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d7",
          "name": "Hao Fang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d8",
          "name": "Runmin Cong",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6d9",
          "name": "Xiankai Lu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6da",
          "name": "Zhiyang Che",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6db",
          "name": "Wei Zhan",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6dc",
          "name": "Tianming Liang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6dd",
          "name": "Haichao Jiang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6de",
          "name": "Wei-Shi Zheng",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6df",
          "name": "Jian-Fang Hu",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e0",
          "name": "Haobo Yuan",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e1",
          "user": {
            "_id": "63958b4414513eaf9029ebf1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
            "isPro": false,
            "fullname": "Xiangtai Li",
            "user": "LXT",
            "type": "user"
          },
          "name": "Xiangtai Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:58.429Z",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e2",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e3",
          "name": "Lu Qi",
          "hidden": false
        },
        {
          "_id": "67ff25b765b52d1b69c1f6e4",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T16:02:47.000Z",
      "submittedOnDailyAt": "2025-04-16T02:26:49.281Z",
      "title": "PVUW 2025 チャレンジレポート：自然にある複雑なビデオのピクセルレベル理解の進歩",
      "submittedOnDailyBy": {
        "_id": "67ff29ecbf6889a333c69c7a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
        "isPro": false,
        "fullname": "Henghui Ding",
        "user": "HenghuiDing",
        "type": "user"
      },
      "summary": "このレポートは、2025年のCVPRに伴い開催された4次目のPixel-level Video Understanding in the Wild (PVUW) チャレンジについて、詳細な概要を提供します。これは、チャレンジの結果、参加手法、将来の研究方向についての要約です。チャレンジは2つのトラックを挙げています。一方は、複雑な場面のビデオオブジェクト分割に焦点を当てるMOSEと、もう一方は、動きをガイドにした言語ベースのビデオ分割に向けています。両トラックは、実世界的なスキャンライトをより正確に反映するために新しい、より難しいデータセットを紹介しています。詳細な評価と分析を通じて、チャレンジは、複雑なビデオ分割の現在の最先端技術と新興のトレンドについて有價値なエイリアスを提供しています。より多くの情報は、ワークショップウェブサイトにて見つかります：https://pvuw.github.io/。",
      "upvotes": 3,
      "discussionId": "67ff25b865b52d1b69c1f736"
    },
    "publishedAt": "2025-04-15T12:02:47.000Z",
    "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild",
    "summary": "This report provides a comprehensive overview of the 4th Pixel-level Video\nUnderstanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025.\nIt summarizes the challenge outcomes, participating methodologies, and future\nresearch directions. The challenge features two tracks: MOSE, which focuses on\ncomplex scene video object segmentation, and MeViS, which targets\nmotion-guided, language-based video segmentation. Both tracks introduce new,\nmore challenging datasets designed to better reflect real-world scenarios.\nThrough detailed evaluation and analysis, the challenge offers valuable\ninsights into the current state-of-the-art and emerging trends in complex video\nsegmentation. More information can be found on the workshop website:\nhttps://pvuw.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11326.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ff29ecbf6889a333c69c7a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
      "fullname": "Henghui Ding",
      "name": "HenghuiDing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.06949",
      "authors": [
        {
          "_id": "67ff12ea58ed263257af79b5",
          "name": "Zhixuan Lin",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b6",
          "name": "Johan Obando-Ceron",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b7",
          "user": {
            "_id": "66906c4e37eadb9c577984d3",
            "avatarUrl": "/avatars/b81765472942fdf94c0ee885ca62df2d.svg",
            "isPro": false,
            "fullname": "Owen He",
            "user": "littleowen",
            "type": "user"
          },
          "name": "Xu Owen He",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T02:16:11.020Z",
          "hidden": false
        },
        {
          "_id": "67ff12ea58ed263257af79b8",
          "name": "Aaron Courville",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-09T14:57:55.000Z",
      "submittedOnDailyAt": "2025-04-16T01:28:49.703Z",
      "title": "Adaptive Computation Pruning for the Forgetting Transformer",
      "submittedOnDailyBy": {
        "_id": "6694cc1009326cb83f2d11bb",
        "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
        "isPro": false,
        "fullname": "Zhixuan Lin",
        "user": "zhixuan-lin",
        "type": "user"
      },
      "summary": "最近に提案されたForgetting Transformer (FoX)は、softmax attentionに忘却ゲートを挿入し、標準的なRoPEベースTransformerと比較して、一貫してより良いまたは同等の性能を示している。特に、FoXの多くの注意ヘッドは忘却するのが速いことがあり、それぞれの時間ステップでの出力は主に局所的なコンテキストに依存している。この観察に基づき、私たちはFoXに対してAdaptive Computation Pruning (ACP)を提案します。ACPは、忘却ゲートによって強烈に減衰した入力出力の依存関係に関する計算を動的に削除する方法で、動的に設定される削減閾値を用いて、削減された注意重みが微觀的に見えないようにしています。ACPはFoXを用いた言語モデルの事前学習に適用され、異なるモデルサイズとコンテキスト長でsoftmax attentionのFLOP数を約70%減少させ、トレーニングタスクのトランシープスに約10%から35%の改善を収得します。また、長いコンテキスト長がより大きな計算コスト削減を収得することが見られます。これらのスピードアップは、性能の低下による影響を受けないように実現されています。また、私たちは、削減パターンの検討や、異なる注意ヘッドのFLOP削減の分布を分析するなど、方法に深いエンジンを提供するために複数の分析を行いました。コードは、https://github.com/zhixuan-lin/arctic-fox から利用できます。",
      "upvotes": 3,
      "discussionId": "67ff12eb58ed263257af79fc",
      "ai_keywords": [
        "Forgetting Transformer (FoX)",
        "forget gate",
        "softmax attention",
        "RoPE-based Transformer",
        "Adaptive Computation Pruning (ACP)",
        "input-output dependencies",
        "pruning threshold",
        "FLOPs",
        "training throughput",
        "pruning patterns"
      ]
    },
    "publishedAt": "2025-04-09T10:57:55.000Z",
    "title": "Adaptive Computation Pruning for the Forgetting Transformer",
    "summary": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate\ninto softmax attention and has shown consistently better or on-par performance\ncompared to the standard RoPE-based Transformer. Notably, many attention heads\nin FoX tend to forget quickly, causing their output at each timestep to rely\nprimarily on the local context. Based on this observation, we propose Adaptive\nComputation Pruning (ACP) for FoX, a method that dynamically prunes\ncomputations involving input-output dependencies that are strongly decayed by\nthe forget gate. This is achieved using a dynamically set pruning threshold\nthat ensures that the pruned attention weights remain negligible. We apply ACP\nto language model pretraining with FoX and show it consistently reduces the\nnumber of FLOPs in softmax attention by around 70% across different model sizes\nand context lengths, resulting in a roughly 10% to 35% improvement in training\nthroughput. Furthermore, longer context lengths yield greater computational\nsavings. All these speed improvements are achieved without any performance\ndegradation. We also perform several analyses to provide deeper insights into\nour method, such as examining the pruning patterns and analyzing the\ndistribution of FLOP savings across different attention heads. Our code is\navailable at https://github.com/zhixuan-lin/arctic-fox.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.06949.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6694cc1009326cb83f2d11bb",
      "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
      "fullname": "Zhixuan Lin",
      "name": "zhixuan-lin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.11456",
      "authors": [
        {
          "_id": "67ff79b3d68757d92e9c168e",
          "name": "Zhiwei He",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c168f",
          "name": "Tian Liang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1690",
          "name": "Jiahao Xu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1691",
          "name": "Qiuzhi Liu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1692",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1693",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1694",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1695",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1696",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1697",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1698",
          "name": "Zhuosheng Zhang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c1699",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c169a",
          "name": "Zhaopeng Tu",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c169b",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "67ff79b3d68757d92e9c169c",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-15T17:59:51.000Z",
      "submittedOnDailyAt": "2025-04-16T08:09:13.393Z",
      "title": "DeepMath-103K: 大規模、難しい、汚染されていない、バリデーション可能な数学データセットで、理由論を進める",
      "submittedOnDailyBy": {
        "_id": "60107b385ac3e86b3ea4fc34",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
        "isPro": true,
        "fullname": "Daniel van Strien",
        "user": "davanstrien",
        "type": "user"
      },
      "summary": "複雑な数学的論理能力は人工知能の重要なベンチマークです。強化学習（RL）をLLMに適用したものは望ましい結果を示していますが、進展は、大規模なトレーニングデータの不足、このデータが十分に難しく、RLに適した可証明性のある答えの形式を持つこと、評価ベンチマークによる汚染を除去することができないことによって、大きく妨げられています。これらの制限を解決するために、DeepMath-103Kを紹介します。DeepMath-103Kは、約103Kの数学問題を含む新しい大規模なデータセットです。これは、RLを介して高度な論理モデルをトレーニングするために特に設計されています。DeepMath-103Kは、源データの分析、複数のベンチマークに対する厳格な汚染除去、高難度（主にレベル5-9）のフィルタリングによって、細かく調整されています。各問題には、可証明性のある最終的な答えが含まれており、ルールベースのRLを可能にします。また、3つの異なるR1生成された解決策が含まれており、複数のトレーニングパラダイム（例えば、観察学習フィニングや煙突）に適しています。数学的な主題の幅広い範囲を拡げ、DeepMath-103Kは一般化可能な論理の開発を促進します。DeepMath-103Kでトレーニンされたモデルは、難しい数学ベンチマークで显著な向上を示し、その効果を証明しました。DeepMath-103Kは公開的にリリースされ、より強力なAI論理システムの構築につながるコミュニティの進歩を促進することを目的としています：https://github.com/zwhe99/DeepMath.",
      "upvotes": 2,
      "discussionId": "67ff79b4d68757d92e9c16e1",
      "githubRepo": "https://github.com/zwhe99/DeepMath",
      "ai_keywords": [
        "reinforcement learning",
        "large-scale dataset",
        "mathematical problems",
        "training data",
        "verifiable answer formats",
        "decontamination",
        "benchmark",
        "curriculum learning",
        "rule-based RL",
        "supervised fine-tuning",
        "distillation",
        "generalizable reasoning",
        "AI reasoning systems"
      ]
    },
    "publishedAt": "2025-04-15T13:59:51.000Z",
    "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning",
    "summary": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11456.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60107b385ac3e86b3ea4fc34",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
      "fullname": "Daniel van Strien",
      "name": "davanstrien",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 586
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.11001",
      "authors": [
        {
          "_id": "67ff4bdb1dc5d56fdd7a1bc4",
          "user": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
            "isPro": false,
            "fullname": "Alan Dao",
            "user": "alandao",
            "type": "user"
          },
          "name": "Alan Dao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-16T06:19:08.903Z",
          "hidden": false
        },
        {
          "_id": "67ff4bdb1dc5d56fdd7a1bc5",
          "name": "Thinh Le",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/s2uxHvFhyqjBXMY5wkDLy.mp4"
      ],
      "publishedAt": "2025-04-15T09:18:21.000Z",
      "submittedOnDailyAt": "2025-04-16T04:49:41.555Z",
      "title": "ReZero: モノマシームを試すことでLLMの検索能力を向上させる",
      "submittedOnDailyBy": {
        "_id": "62d7b2339b629105a5d6888a",
        "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
        "isPro": false,
        "fullname": "Alan Dao",
        "user": "alandao",
        "type": "user"
      },
      "summary": "レタイブレードアウガーデーション（RAG）は、知識密集タスクでの大規模言語モデル（LLM）の性能を向上させるが、初期の検索クエリの品質に大きく依存しています。現在の方法では、通常、強化学習（RL）を使用して、クエリの構成や結果の理由を焦点に置いていますが、失敗した検索後の再検索を明示的に促すことはありません。私たちは、ReZero（リトライゼロ）という新しいRLフレームワークを紹介します。ReZeroは、初期の失敗した検索後の再検索の行為を直接報酬し、LLMが代わりのクエリを探索することを奨励します。ReZeroは、基準値25%より46.88%の精度を達成し、初期クエリが十分でない複雑な情報探求シナリオでLLMの強固性を向上させます。",
      "upvotes": 2,
      "discussionId": "67ff4bdc1dc5d56fdd7a1c36",
      "githubRepo": "https://github.com/menloresearch/ReZero",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "Large Language Model (LLM)",
        "knowledge-intensive tasks",
        "Reinforcement Learning (RL)",
        "query formulation",
        "reasoning over results",
        "ReZero (Retry-Zero)",
        "persistence",
        "search query",
        "unsuccessful attempt",
        "alternative queries",
        "robustness",
        "information-seeking scenarios"
      ]
    },
    "publishedAt": "2025-04-15T05:18:21.000Z",
    "title": "ReZero: Enhancing LLM search ability by trying one-more-time",
    "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/s2uxHvFhyqjBXMY5wkDLy.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.11001.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d7b2339b629105a5d6888a",
      "avatarUrl": "/avatars/c3f164fde6b8f9a671890e08ce8a3e75.svg",
      "fullname": "Alan Dao",
      "name": "alandao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.10049",
      "authors": [
        {
          "_id": "67ff5b335cf0fe153845d1c9",
          "user": {
            "_id": "60d35154d7b174177faabd55",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
            "isPro": false,
            "fullname": "Théo Gigant",
            "user": "gigant",
            "type": "user"
          },
          "name": "Théo Gigant",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:49.462Z",
          "hidden": false
        },
        {
          "_id": "67ff5b335cf0fe153845d1ca",
          "name": "Camille Guinaudeau",
          "hidden": false
        },
        {
          "_id": "67ff5b335cf0fe153845d1cb",
          "name": "Frédéric Dufaux",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-14T09:55:01.000Z",
      "submittedOnDailyAt": "2025-04-16T05:55:17.253Z",
      "title": "モダルと構造の影響についての多モデル表現の要約",
      "submittedOnDailyBy": {
        "_id": "60d35154d7b174177faabd55",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
        "isPro": false,
        "fullname": "Théo Gigant",
        "user": "gigant",
        "type": "user"
      },
      "summary": "ビジョン・ラングジャードモデル（VLMs）は、複数のフォーマットで可視的および文字的な情報を処理できます：テキスト、画像、交差したテキストと画像、または長約1時間のビデオ。本研究では、VLMsを使用した多モーダルプレゼンテーションの自動要約について、細かい定量的および質的な分析を行います。これらの実験から、VLMsを使用して、テキストが豊富な多モーダルドキュメントから要約を生成するためのコスト適切な戦略を提案します。また、ビデオストリームから抽出されたスライドを元のビデオと比較して利用することが有利であることを示し、交差したスライドとテキストからの構造化された表現が最も良い性能を提供します。最後に、多モーダルプレゼンテーションのクロスモーダルインタラクションの性質についての反省とその改善のためのアドバイスを提供します。",
      "upvotes": 2,
      "discussionId": "67ff5b355cf0fe153845d215",
      "ai_keywords": [
        "Vision-Language Models (VLMs)",
        "automatic summarization",
        "multimodal presentations",
        "text-heavy multimodal documents",
        "input-length budgets",
        "slides",
        "video stream",
        "raw video",
        "structured representation",
        "interleaved slides",
        "transcript",
        "cross-modal interactions"
      ]
    },
    "publishedAt": "2025-04-14T05:55:01.000Z",
    "title": "Summarization of Multimodal Presentations with Vision-Language Models:\n  Study of the Effect of Modalities and Structure",
    "summary": "Vision-Language Models (VLMs) can process visual and textual information in\nmultiple formats: texts, images, interleaved texts and images, or even\nhour-long videos. In this work, we conduct fine-grained quantitative and\nqualitative analyses of automatic summarization of multimodal presentations\nusing VLMs with various representations as input. From these experiments, we\nsuggest cost-effective strategies for generating summaries from text-heavy\nmultimodal documents under different input-length budgets using VLMs. We show\nthat slides extracted from the video stream can be beneficially used as input\nagainst the raw video, and that a structured representation from interleaved\nslides and transcript provides the best performance. Finally, we reflect and\ncomment on the nature of cross-modal interactions in multimodal presentations\nand share suggestions to improve the capabilities of VLMs to understand\ndocuments of this nature.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.10049.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d35154d7b174177faabd55",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg",
      "fullname": "Théo Gigant",
      "name": "gigant",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.08846",
      "authors": [
        {
          "_id": "67ff25f33026f8abc4c4a10d",
          "name": "Mostafa Faghih Shojaei",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a10e",
          "name": "Rahul Gulati",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a10f",
          "name": "Benjamin A. Jasperson",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a110",
          "name": "Shangshang Wang",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a111",
          "user": {
            "_id": "6729342804227d5ea3b283c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
            "isPro": false,
            "fullname": "Simone Cimolato",
            "user": "simocimolato",
            "type": "user"
          },
          "name": "Simone Cimolato",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T08:42:55.794Z",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a112",
          "user": {
            "_id": "672ad19141a93b8e140e8689",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VRMw7_PIE5QoD_eWP0hOp.png",
            "isPro": false,
            "fullname": "Dangli Cao",
            "user": "Dinzhenzhenzhu",
            "type": "user"
          },
          "name": "Dangli Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-16T09:23:14.535Z",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a113",
          "name": "Willie Neiswanger",
          "hidden": false
        },
        {
          "_id": "67ff25f33026f8abc4c4a114",
          "user": {
            "_id": "67859d73e670c62966ba5767",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mx3fE-YPsjsxpYsBh_DDn.png",
            "isPro": false,
            "fullname": "Krishna Garikipati",
            "user": "garikipati",
            "type": "user"
          },
          "name": "Krishna Garikipati",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-16T05:32:49.978Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-11T01:26:34.000Z",
      "submittedOnDailyAt": "2025-04-16T07:21:37.722Z",
      "title": "AI大学：科学クラスランド向けのインストラクションアラインメントプラットフォーム",
      "submittedOnDailyBy": {
        "_id": "6729342804227d5ea3b283c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
        "isPro": false,
        "fullname": "Simone Cimolato",
        "user": "simocimolato",
        "type": "user"
      },
      "summary": "AI大学（AI-U）は、AIを駆使したコースコンテンツの配送に柔軟なフレームワークを提供し、教師の教え方に適応します。その核心では、AI-Uは、検索アウゲージドジェネレーション（RAG）を用いて大規模な言語モデル（LLM）を微調節し、講義ビデオ、ノート、教科書から教師に合わせたレスポンスを生成します。グラデートレベルの有限要素法（FEM）コースをケーススタディとして、システマチックに訓練データを構築、オープンソースLLMをLow-Rank Adaptation（LoRA）で微調節し、RAGによる合成によりレスポンスを最適化するスケーラブルなパイプラインを提出します。コサイン類似度、LLMベースの評価、専門家のレビューを組み合わせた評価により、コースマテリアルとの強い適合性が示されます。また、特定のコースマテリアルのセクションと時間スタンプ付きの開放アクセスビデオ講義と連携してトレースベースを強化するプロトタイプのWebアプリケーション「https://my-ai-university.com」が開発されています。専門家モデルは、86%のテストケースで参照とのコサイン類似度が高く、LLMジュージはほぼ5回のうち4回にわたって基礎Llama 3.2モデルを超えた性能を示しました。AI-Uは、AIディスクリミネーション教育のスケーラブルなアプローチを提供し、高等教育でより広く採用されることを促進します。ここでは、FEMのコースでフレームワークを紹介しますが、この設定は科学研究内容におけるLLMの微調節の広いコンテキストの特定の例です。",
      "upvotes": 2,
      "discussionId": "67ff25f43026f8abc4c4a16c",
      "projectPage": "https://my-ai-university.com",
      "githubRepo": "https://github.com/my-ai-university/finite-element-method",
      "ai_keywords": [
        "large language model (LLM)",
        "retrieval-augmented generation (RAG)",
        "finite-element-method (FEM)",
        "Low-Rank Adaptation (LoRA)",
        "RAG-based synthesis",
        "cosine similarity",
        "LLM-based assessment",
        "traceability",
        "base Llama 3.2 model"
      ]
    },
    "publishedAt": "2025-04-10T21:26:34.000Z",
    "title": "AI-University: An LLM-based platform for instructional alignment to\n  scientific classrooms",
    "summary": "We introduce AI University (AI-U), a flexible framework for AI-driven course\ncontent delivery that adapts to instructors' teaching styles. At its core, AI-U\nfine-tunes a large language model (LLM) with retrieval-augmented generation\n(RAG) to generate instructor-aligned responses from lecture videos, notes, and\ntextbooks. Using a graduate-level finite-element-method (FEM) course as a case\nstudy, we present a scalable pipeline to systematically construct training\ndata, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and\noptimize its responses through RAG-based synthesis. Our evaluation - combining\ncosine similarity, LLM-based assessment, and expert review - demonstrates\nstrong alignment with course materials. We also have developed a prototype web\napplication, available at https://my-ai-university.com, that enhances\ntraceability by linking AI-generated responses to specific sections of the\nrelevant course material and time-stamped instances of the open-access video\nlectures. Our expert model is found to have greater cosine similarity with a\nreference on 86% of test cases. An LLM judge also found our expert model to\noutperform the base Llama 3.2 model approximately four times out of five. AI-U\noffers a scalable approach to AI-assisted education, paving the way for broader\nadoption in higher education. Here, our framework has been presented in the\nsetting of a class on FEM - a subject that is central to training PhD and\nMaster students in engineering science. However, this setting is a particular\ninstance of a broader context: fine-tuning LLMs to research content in science.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.08846.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6729342804227d5ea3b283c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_GPZiRH3LA39QleNtRqYH.png",
      "fullname": "Simone Cimolato",
      "name": "simocimolato",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.09454",
      "authors": [
        {
          "_id": "67ff6f8c661b74d0050d2774",
          "name": "Weinan Jia",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2775",
          "name": "Mengqi Huang",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2776",
          "name": "Nan Chen",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2777",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "67ff6f8c661b74d0050d2778",
          "name": "Zhendong Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-13T06:33:28.000Z",
      "submittedOnDailyAt": "2025-04-16T07:21:45.180Z",
      "title": "D^2iT: 動的ディフュージョンチャンネルライブラリーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォローワーフォロ",
      "submittedOnDailyBy": {
        "_id": "630636bcd37ce67e0e4d1d42",
        "avatarUrl": "/avatars/c3ba695d339eaaf9e810bfa0d9a7689a.svg",
        "isPro": false,
        "fullname": "Mengqi Huang",
        "user": "CoreloneH",
        "type": "user"
      },
      "summary": "ディフュージョンモデルは、高品質な画像を生成する能力により広く認められています。ディフュージョントランスフォーマー（DiT）アーキテクチャの優秀な性能とスケーラビリティにもとづいていますが、ディフュージョンプロセス中には、異なる画像領域に対して固定的な圧縮を適用し、これらの領域に存在する自然な変動する情報密度を無視しています。しかし、大きな圧縮は局所的な写真写真度を制限し、小さな圧縮は計算複雑性を増加させ、グローバルな一貫性を破壊し、最終的に生成画像の品質を影響させます。これらの制限を解決するために、我々は、異なる領域の重要性を認識し、異なる領域を動的に圧縮することで画像生成の効果性と効率性を向上させるための新しい2段階フレームワークを提案します。まずは、動的なVAE（DVAE）は、特定の情報密度に合わせた異なるダウンサンプリングレートで異なる画像領域をハイライドエンコーダーでエンコードし、ディフュージョンプロセスによりより正確な自然な潜在コードを提供します。次に、動的なディフュージョントランスフォーマー（D^2iT）は、動的なグリーントランスフォーマーと動的なコンテンツトランスフォーマーの新しい組み合わせをよって、コアエルス（平滑な領域で少ない潜在コード）とフィニットエルス（詳細な領域で多くの潜在コード）を含む多グリーンノイズを予測して画像を生成します。ノイズの粗略な予測と詳細な領域の補正との組み合わせは、グローバルな一貫性と局所的な写真写真度の統合を実現します。多様な生成タスクにおいて詳細な実験を行い、我々のアプローチの効果性を認識しました。コードは、https://github.com/jiawn-creator/Dynamic-DiT でリリースされます。",
      "upvotes": 1,
      "discussionId": "67ff6f8f661b74d0050d28a9",
      "ai_keywords": [
        "Diffusion models",
        "Diffusion Transformer (DiT)",
        "compression",
        "image regions",
        "information densities",
        "local realism",
        "computational complexity",
        "global consistency",
        "generated images",
        "Dynamic VAE (DVAE)",
        "hierarchical encoder",
        "downsampling rates",
        "latent codes",
        "Dynamic Diffusion Transformer (D$^2$iT)",
        "multi-grained noise",
        "coarse-grained",
        "fine-grained",
        "Dynamic Grain Transformer",
        "Dynamic Content Transformer",
        "rough prediction",
        "detailed regions correction"
      ]
    },
    "publishedAt": "2025-04-13T02:33:28.000Z",
    "title": "D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation",
    "summary": "Diffusion models are widely recognized for their ability to generate\nhigh-fidelity images. Despite the excellent performance and scalability of the\nDiffusion Transformer (DiT) architecture, it applies fixed compression across\ndifferent image regions during the diffusion process, disregarding the\nnaturally varying information densities present in these regions. However,\nlarge compression leads to limited local realism, while small compression\nincreases computational complexity and compromises global consistency,\nultimately impacting the quality of generated images. To address these\nlimitations, we propose dynamically compressing different image regions by\nrecognizing the importance of different regions, and introduce a novel\ntwo-stage framework designed to enhance the effectiveness and efficiency of\nimage generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical\nencoder to encode different image regions at different downsampling rates,\ntailored to their specific information densities, thereby providing more\naccurate and natural latent codes for the diffusion process. (2) Dynamic\nDiffusion Transformer (D^2iT) at second stage generates images by predicting\nmulti-grained noise, consisting of coarse-grained (less latent code in smooth\nregions) and fine-grained (more latent codes in detailed regions), through an\nnovel combination of the Dynamic Grain Transformer and the Dynamic Content\nTransformer. The strategy of combining rough prediction of noise with detailed\nregions correction achieves a unification of global consistency and local\nrealism. Comprehensive experiments on various generation tasks validate the\neffectiveness of our approach. Code will be released at\nhttps://github.com/jiawn-creator/Dynamic-DiT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.09454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630636bcd37ce67e0e4d1d42",
      "avatarUrl": "/avatars/c3ba695d339eaaf9e810bfa0d9a7689a.svg",
      "fullname": "Mengqi Huang",
      "name": "CoreloneH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]