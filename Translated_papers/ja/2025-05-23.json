[
  {
    "paper": {
      "id": "2505.16938",
      "authors": [
        {
          "_id": "682fe3a565bac3ec3556fc6c",
          "name": "NovelSeek Team",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc6d",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc6e",
          "name": "Shiyang Feng",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc6f",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc70",
          "name": "Jiakang Yuan",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc71",
          "name": "Zhiyin Yu",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc72",
          "name": "Xiaohan He",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc73",
          "name": "Songtao Huang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc74",
          "name": "Shaowei Hou",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc75",
          "name": "Zheng Nie",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc76",
          "name": "Zhilong Wang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc77",
          "name": "Jinyao Liu",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc78",
          "name": "Runmin Ma",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc79",
          "name": "Tianshuo Peng",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7a",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7b",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7c",
          "name": "Shufei Zhang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7d",
          "name": "Xiaosong Wang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7e",
          "name": "Yilan Zhang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc7f",
          "name": "Meng Li",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc80",
          "name": "Zhongying Tu",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc81",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc82",
          "name": "Wangli Ouyang",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc83",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "682fe3a565bac3ec3556fc84",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:27:43.000Z",
      "submittedOnDailyAt": "2025-05-23T01:25:34.477Z",
      "title": "NovelSeek: 科学者になるアガント -- 仮説から証明までの閉路システムの構築",
      "submittedOnDailyBy": {
        "_id": "643dfd235aafbdca3a5792c0",
        "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg",
        "isPro": false,
        "fullname": "Bo Zhang",
        "user": "BoZhang",
        "type": "user"
      },
      "summary": "人工知能（AI）は、科学研究のパラダイムの変革を加速させ、研究の効率化を強化しながら、イノベーションを駆動しています。ニューラルシークを紹介します。ニューラルシークは、様々な科学研究領域で自動転移する科学研究（ASR）を行うための統一された閉じたループマルチアグエントフレームワークです。これにより、研究者はこれらの領域で前所未有の速さと精度で複雑な問題を解決することができます。ニューラルシークは、3つの主な優れ点を特徴としています：1) スケーラブル性：ニューラルシークは12つの科学研究タスクにおいて多様性を示し、ベースラインコードの性能向上に貢献するイノベーションを生成することができます。2) インタラクティビティ：ニューラルシークは、自動化された端末からのヒューマンエキスパートのフィードバックとマルチアグエントの相互作用を提供し、ディスカイバルエキスパートの知識の無間統合を可能にします。3) 効率：ニューラルシークは、人間の努力に比べて大幅に時間コストを削減しながら、様々な科学領域で驚異的な性能収益を収めています。例えば、反応産出予測では、12時間で27.6%から35.4%に達し、エンハンサー活性予測では、4時間で0.52から0.79に達し、2Dセマンティックセグメンテーションでは、30時間で78.8%から81.0%に達しました。",
      "upvotes": 70,
      "discussionId": "682fe3a865bac3ec3556fd21"
    },
    "publishedAt": "2025-05-22T13:27:43.000Z",
    "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification",
    "summary": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16938.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "643dfd235aafbdca3a5792c0",
      "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg",
      "fullname": "Bo Zhang",
      "name": "BoZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16410",
      "authors": [
        {
          "_id": "682fd6045e83dc325675312b",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312c",
          "name": "Yifei Chen",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312d",
          "name": "Xiaoxi Li",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312e",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc325675312f",
          "name": "Hongjin Qian",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753130",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753131",
          "name": "Hangyu Mao",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753132",
          "name": "Guorui Zhou",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753133",
          "name": "Zhicheng Dou",
          "hidden": false
        },
        {
          "_id": "682fd6045e83dc3256753134",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T09:00:19.000Z",
      "submittedOnDailyAt": "2025-05-23T00:31:41.669Z",
      "title": "Tool-Star: 強化学習によるLLMブラインドの多機能チェッカーを支援する",
      "submittedOnDailyBy": {
        "_id": "61cd4b833dd34ba1985e0753",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
        "isPro": false,
        "fullname": "KABI",
        "user": "dongguanting",
        "type": "user"
      },
      "summary": "最近、大語言モデル（LLMs）は、大規模な強化学習（RL）を通じて驚異的な理由論能力を示している。しかし、RLアルゴリズムを活用してLLMsによる効果的な多ツールの協議や理由論を強化することは開放的な挑戦である。本論文では、Tool-StarというRLに基づくフレームワークを紹介し、LLMsがステップごとの理由論の際に自動的に複数の外部ツールを呼び出すことを可能にすることを目的としています。Tool-Starは6種類のツールを統合し、データの合成と訓練のシステマティックな設計を採用しています。ツールの使用データの不足を解決するために、一般的なツール統合された理由論データの合成パイプラインを提案し、ツール統合されたプロンプトとヒントベースのサンプリングを組み合わせて自動的にスケーラブルにツールの使用プロセスを生成します。次に、品質の正規化と難易度に関するクラス分類プロセスを通じて低品質のサンプルを除去し、データセットを難しいほどから簡単なほどに並べます。また、2段階の訓練フレームワークを提案し、多ツールの協議や理由論を強化するために、(1)冷やかなスタートの微調節と(2)ヒューリスティックな報酬設計を採用した多ツールの自己批判ロジックロバストアルゴリズムを提案します。10以上の難しい理由論ベンチマークに対する実験分析は、Tool-Starの効果と効率を明らかにしています。コードは、https://github.com/dongguanting/Tool-Star に公開されています。",
      "upvotes": 37,
      "discussionId": "682fd6055e83dc3256753187",
      "projectPage": "https://github.com/dongguanting/Tool-Star/",
      "githubRepo": "https://github.com/dongguanting/Tool-Star/",
      "ai_summary": "Tool-Star, an RL-based framework, enables LLMs to autonomously use multiple tools for stepwise reasoning, leveraging data synthesis and hierarchical reward design.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "large-scale reinforcement learning",
        "RL",
        "multi-tool collaborative reasoning",
        "tool-use data",
        "tool-integrated reasoning",
        "tool-invocation feedback",
        "multi-tool self-critic",
        "hierarchical reward design"
      ]
    },
    "publishedAt": "2025-05-22T05:00:19.000Z",
    "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement\n  Learning",
    "summary": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16410.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61cd4b833dd34ba1985e0753",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
      "fullname": "KABI",
      "name": "dongguanting",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14810",
      "authors": [
        {
          "_id": "682ea2b450671dc82688b8ad",
          "user": {
            "_id": "640ad17a1ee054d66a74783e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640ad17a1ee054d66a74783e/u0PjIkyC-9HkGzEyUQ7JN.jpeg",
            "isPro": false,
            "fullname": "Tingchen Fu",
            "user": "TingchenFu",
            "type": "user"
          },
          "name": "Tingchen Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:15:44.217Z",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8ae",
          "name": "Jiawei Gu",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8af",
          "user": {
            "_id": "63f3502a520c14618925825a",
            "avatarUrl": "/avatars/e986a2a6625e7be6890616a417f908d2.svg",
            "isPro": false,
            "fullname": "Yafu Li",
            "user": "yaful",
            "type": "user"
          },
          "name": "Yafu Li",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-22T04:06:13.396Z",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8b0",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "682ea2b450671dc82688b8b1",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T18:18:01.000Z",
      "submittedOnDailyAt": "2025-05-23T00:49:30.349Z",
      "title": "スケーリング・レジョン、制御の失われ：大規模な理由論モデルでの指示従いの評価",
      "submittedOnDailyBy": {
        "_id": "64cb54da1af278541d663708",
        "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
        "isPro": false,
        "fullname": "Xiaoye Qu",
        "user": "Xiaoye08",
        "type": "user"
      },
      "summary": "指令従いは、大規模言語モデル（LLMs）とユーザーの意図を合わせるために重要です。最近の理由取り向けモデルは複雑な数学問題に優れた性能を示していますが、自然言語の指示に従う能力はまだ調査不足です。本論文では、数学的理由任務での指示従いを評価するための専門的ベンチマーク「MathIF」を紹介します。実験的な分析では、理由の能力を上げるときにコントローラブル性を維持するのは困難であることが明らかになりました。理由能力の高いモデルはユーザーの指示に従うことが難しいことがわかりました。モデルを長いチャインオフステーンで絞り込ませたり、理由取り向けの強化学習で訓練したものは、特に生成文長が長くなると指示従いの性能が低下します。また、簡単な干渉でも指示従いの復元が部分的に可能ですが、理由性能に代わりにコストがかかります。これらの発見は現在のLLM訓練パラダイムにおける基本的な対立を明らかにし、指示に関心のある理由モデルの必要性を強調します。コードとデータはhttps://github.com/TingchenFu/MathIFからリリースしています。",
      "upvotes": 37,
      "discussionId": "682ea2b550671dc82688b8e2",
      "githubRepo": "https://github.com/TingchenFu/MathIF",
      "ai_summary": "An empirical analysis of MathIF identifies a tension between enhancing reasoning capacity and maintaining instruction adherence in large language models.",
      "ai_keywords": [
        "instruction-following",
        "reasoning-oriented models",
        "benchmarks",
        "chains-of-thought",
        "reinforcement learning",
        "instruction adherence"
      ]
    },
    "publishedAt": "2025-05-20T14:18:01.000Z",
    "title": "Scaling Reasoning, Losing Control: Evaluating Instruction Following in\n  Large Reasoning Models",
    "summary": "Instruction-following is essential for aligning large language models (LLMs)\nwith user intent. While recent reasoning-oriented models exhibit impressive\nperformance on complex mathematical problems, their ability to adhere to\nnatural language instructions remains underexplored. In this work, we introduce\nMathIF, a dedicated benchmark for evaluating instruction-following in\nmathematical reasoning tasks. Our empirical analysis reveals a consistent\ntension between scaling up reasoning capacity and maintaining controllability,\nas models that reason more effectively often struggle to comply with user\ndirectives. We find that models tuned on distilled long chains-of-thought or\ntrained with reasoning-oriented reinforcement learning often degrade in\ninstruction adherence, especially when generation length increases.\nFurthermore, we show that even simple interventions can partially recover\nobedience, though at the cost of reasoning performance. These findings\nhighlight a fundamental tension in current LLM training paradigms and motivate\nthe need for more instruction-aware reasoning models. We release the code and\ndata at https://github.com/TingchenFu/MathIF.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14810.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb54da1af278541d663708",
      "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
      "fullname": "Xiaoye Qu",
      "name": "Xiaoye08",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16707",
      "authors": [
        {
          "_id": "682fdd77e3102e71872d9b00",
          "name": "Yongliang Wu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b01",
          "name": "Zonghui Li",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b02",
          "name": "Xinting Hu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b03",
          "name": "Xinyu Ye",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b04",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b05",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b06",
          "name": "Wenbo Zhu",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b07",
          "name": "Bernt Schiele",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b08",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        },
        {
          "_id": "682fdd77e3102e71872d9b09",
          "name": "Xu Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T14:08:59.000Z",
      "submittedOnDailyAt": "2025-05-23T00:59:23.402Z",
      "title": "KRIS-Bench: 次世代インテリジェント画像編集モデルのベンチマーク",
      "submittedOnDailyBy": {
        "_id": "66f6bc97980d52c75c300511",
        "avatarUrl": "/avatars/f7c23c4b09701580b533212ec9b6e306.svg",
        "isPro": false,
        "fullname": "Yongliang",
        "user": "Liang0223",
        "type": "user"
      },
      "summary": "最近の多モデル生成モデルの進展は、指示基準画像編集における顕著な進歩を実現しました。しかし、これらのモデルは視覚的に妥当な出力を生成するだけではなく、知識基準的な理由論的編集タスクの機能を詳しく調査していません。本論文では、知識基準的な理由論を通じてモデルを評価するための診断ベンチマーク、KRIS-Bench（画像編集システムにおける知識基準的な理由論のベンチマーク）を介して紹介します。教育理論から受け継いだもので、KRIS-Benchは編集タスクを事実的、概念的、プロセス的な3つの基盤的な知識タイプに分類しています。このタクニカルノミカルに基づき、7つの理由論的次元を範囲に22つの代表的なタスクを設計し、1,267件の高品質の標識された編集インスタンスをリリースしました。細かい評価を支援するために、知識の妥当性を評価する新しいメトリックを提案し、知識のヒントを加えて人間の研究によって調整しました。10つの最先端のモデルに対する実験結果から、理由論的性能の大きな間違いが明らかになり、知識センタリックなベンチマークの必要性を強調し、脳筋画像編集システムの開発に向けての進歩を促進することを求めます。",
      "upvotes": 33,
      "discussionId": "682fdd79e3102e71872d9b79",
      "projectPage": "https://yongliang-wu.github.io/kris_bench_project_page/",
      "githubRepo": "https://github.com/mercurystraw/Kris_Bench",
      "ai_summary": "KRIS-Bench assesses generative models' knowledge-based reasoning in image editing through a taxonomy of editing tasks and a Knowledge Plausibility metric.",
      "ai_keywords": [
        "multi-modal generative models",
        "instruction-based image editing",
        "knowledge-based reasoning",
        "KRIS-Bench",
        "cognitive assessment",
        "foundational knowledge types",
        "Factual",
        "Conceptual",
        "Procedural",
        "reasoning dimensions",
        "Knowledge Plausibility metric"
      ]
    },
    "publishedAt": "2025-05-22T10:08:59.000Z",
    "title": "KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models",
    "summary": "Recent advances in multi-modal generative models have enabled significant\nprogress in instruction-based image editing. However, while these models\nproduce visually plausible outputs, their capacity for knowledge-based\nreasoning editing tasks remains under-explored. In this paper, we introduce\nKRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), a\ndiagnostic benchmark designed to assess models through a cognitively informed\nlens. Drawing from educational theory, KRIS-Bench categorizes editing tasks\nacross three foundational knowledge types: Factual, Conceptual, and Procedural.\nBased on this taxonomy, we design 22 representative tasks spanning 7 reasoning\ndimensions and release 1,267 high-quality annotated editing instances. To\nsupport fine-grained evaluation, we propose a comprehensive protocol that\nincorporates a novel Knowledge Plausibility metric, enhanced by knowledge hints\nand calibrated through human studies. Empirical results on 10 state-of-the-art\nmodels reveal significant gaps in reasoning performance, highlighting the need\nfor knowledge-centric benchmarks to advance the development of intelligent\nimage editing systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16707.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66f6bc97980d52c75c300511",
      "avatarUrl": "/avatars/f7c23c4b09701580b533212ec9b6e306.svg",
      "fullname": "Yongliang",
      "name": "Liang0223",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15966",
      "authors": [
        {
          "_id": "682fe6bd5f80e910085b5116",
          "name": "Alex Su",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b5117",
          "name": "Haozhe Wang",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b5118",
          "name": "Weimin Ren",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b5119",
          "name": "Fangzhen Lin",
          "hidden": false
        },
        {
          "_id": "682fe6bd5f80e910085b511a",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:08:46.964Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/pPkAUFC7KKDS7Q9fhZKmM.png"
      ],
      "publishedAt": "2025-05-21T19:35:08.000Z",
      "submittedOnDailyAt": "2025-05-23T01:39:51.922Z",
      "title": "Pixel Reasoner: カイカルドリブレインドリニューロン学を使ったピクセルスペースの理由論を奨励する",
      "submittedOnDailyBy": {
        "_id": "6313a86154e6e5d9f0f94e04",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
        "isPro": false,
        "fullname": "Wenhu Chen",
        "user": "wenhu",
        "type": "user"
      },
      "summary": "鏈式思考推理在各種領域的大語言模型（LLMs）的性能上取得了顯著提升。然而，這種推理過程僅限於文本空間，限制了其在視覺密集任務中的有效性。為了解決這一局限，我們提出了在像素空間進行推理的概念。在這一新鮮的框架中，視覺語言模型（VLMs）配備了一套視覺推理操作，例如放大和選擇幀。這些操作使VLMs能夠直接檢查、詢問和從視覺證據中推斷，從而提高視覺任務的推理忠實度。在VLMs中培養這樣的像素空間推理能力面臨著明顯的挑戰，包括模型的初始不平衡能力和其對新引入的像素空間操作的抵觸。我們通過一個兩階段訓練方法來解決這些挑戰。第一階段在合成推理踪跡上進行指導調整，使模型熟悉新的視覺操作。接著，一個強化學習（RL）階段利用好奇心驅動的獎勵計劃來平衡像素空間推理和文本推理之間的探索。通過這些視覺操作，VLMs可以與複雜的視覺輸入進行交互，例如信息豐富的圖像或視頻，主動收集必要的信息。我們展示了這種方法在多種視覺推理測試標準上的性能有顯著提升。我們的70億參數模型，\\model，在V*測試標準上達到84%，在TallyQA-Complex上達到74%，在InfographicsVQA上達到84%，這是任何開源模型至今取得的最高準確率。這些結果突顯了像素空間推理的重要性以及我們框架的有效性。",
      "upvotes": 25,
      "discussionId": "682fe6bf5f80e910085b51ae",
      "ai_summary": "Introducing pixel-space reasoning in Vision-Language Models (VLMs) through visual operations like zoom-in and select-frame enhances their performance on visual tasks.",
      "ai_keywords": [
        "Vision-Language Models",
        "VLMs",
        "pixel-space reasoning",
        "visual reasoning operations",
        "zoom-in",
        "select-frame",
        "reinforcement learning",
        "RL",
        "curiosity-driven reward scheme",
        "V* bench",
        "TallyQA-Complex",
        "InfographicsVQA"
      ]
    },
    "publishedAt": "2025-05-21T15:35:08.000Z",
    "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with\n  Curiosity-Driven Reinforcement Learning",
    "summary": "Chain-of-thought reasoning has significantly improved the performance of\nLarge Language Models (LLMs) across various domains. However, this reasoning\nprocess has been confined exclusively to textual space, limiting its\neffectiveness in visually intensive tasks. To address this limitation, we\nintroduce the concept of reasoning in the pixel-space. Within this novel\nframework, Vision-Language Models (VLMs) are equipped with a suite of visual\nreasoning operations, such as zoom-in and select-frame. These operations enable\nVLMs to directly inspect, interrogate, and infer from visual evidences, thereby\nenhancing reasoning fidelity for visual tasks. Cultivating such pixel-space\nreasoning capabilities in VLMs presents notable challenges, including the\nmodel's initially imbalanced competence and its reluctance to adopt the newly\nintroduced pixel-space operations. We address these challenges through a\ntwo-phase training approach. The first phase employs instruction tuning on\nsynthesized reasoning traces to familiarize the model with the novel visual\noperations. Following this, a reinforcement learning (RL) phase leverages a\ncuriosity-driven reward scheme to balance exploration between pixel-space\nreasoning and textual reasoning. With these visual operations, VLMs can\ninteract with complex visual inputs, such as information-rich images or videos\nto proactively gather necessary information. We demonstrate that this approach\nsignificantly improves VLM performance across diverse visual reasoning\nbenchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on\nTallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy\nachieved by any open-source model to date. These results highlight the\nimportance of pixel-space reasoning and the effectiveness of our framework.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6313a86154e6e5d9f0f94e04/pPkAUFC7KKDS7Q9fhZKmM.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15966.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6313a86154e6e5d9f0f94e04",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
      "fullname": "Wenhu Chen",
      "name": "wenhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 38
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16175",
      "authors": [
        {
          "_id": "682fd91a1ffb93faf139d288",
          "name": "Benjamin Schneider",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d289",
          "name": "Dongfu Jiang",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d28a",
          "name": "Chao Du",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d28b",
          "name": "Tianyu Pang",
          "hidden": false
        },
        {
          "_id": "682fd91a1ffb93faf139d28c",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:26:50.000Z",
      "submittedOnDailyAt": "2025-05-23T00:43:01.292Z",
      "title": "クイックビデオ：システムアルゴリズムによる時間帯長いビデオの実時間理解\nコードシェイプ",
      "submittedOnDailyBy": {
        "_id": "62567c86d444a9b5a0ec51c1",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
        "isPro": false,
        "fullname": "Dongfu Jiang",
        "user": "DongfuJiang",
        "type": "user"
      },
      "summary": "長ビデオ理解は、実世界的なアプリケーションにおいて重要な能力として現れてきました。例えば、ビデオサイバンジャー、会議要約、教育講義分析、スポーツブロードキャストなどです。しかし、ビデオLLMにおいては、2つのボトルネックが主な原因で計算量が高く、実行できない状態になっています。1) 順次なビデオデコーディング、ビデオのバイトストリームをRGBフレームに変換する過程で、1時間のビデオ入力に対して1分程度かかることがあります。2) LLM推論のために、GPUメモリを計10万ターゲットのトークンを事前読み込むことで、ラテンシーとメモリ使用が高くなります。これらの課題に対処するために、QuickVideoというシステムアルゴリズム共通設計を提案しています。これは、長ビデオ理解を大幅に加速し、リアルタイムの下流アプリケーションをサポートすることを目的としています。QuickVideoは、3つのキーイノベーションを構成しています。QuickDecoderは、キーフレームに対応した間隔でビデオを分割し、並列化されたCPUビーコーディナーで2-3倍のスピードアップを達成します。QuickPrefillは、KVキャッシュ削減を用いたメモリ効率的な事前読み込み方法で、GPUメモリを少なくてもより多くのフレームをサポートします。また、CPUビデオデコーディングとGPU推論を重ならせるオーバーラップシナプスです。これらの構成要素は、長ビデオ入力に対して推論時間を1分程度減らし、限られたハードウェア上でもスケーラブルな高品質なビデオ理解を可能にします。実験により、QuickVideoは時間とサンプリングレートの拡張性を示し、実用的に長ビデオ処理が可能になることを示しています。",
      "upvotes": 24,
      "discussionId": "682fd91b1ffb93faf139d2d0",
      "ai_summary": "QuickVideo accelerates long-video understanding by combining a parallelized video decoder, memory-efficient prefilling, and overlapping video decoding with inference, enabling real-time performance.",
      "ai_keywords": [
        "QuickDecoder",
        "parallelized CPU-based video decoder",
        "keyframe-aligned intervals",
        "QuickPrefill",
        "memory-efficient prefilling",
        "KV-cache pruning",
        "overlapping scheme"
      ]
    },
    "publishedAt": "2025-05-21T23:26:50.000Z",
    "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
    "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16175.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62567c86d444a9b5a0ec51c1",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
      "fullname": "Dongfu Jiang",
      "name": "DongfuJiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 22
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17022",
      "authors": [
        {
          "_id": "682ffa9a6e906040a3bb7160",
          "name": "Chengqi Duan",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7161",
          "name": "Rongyao Fang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7162",
          "name": "Yuqing Wang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7163",
          "name": "Kun Wang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7164",
          "name": "Linjiang Huang",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7165",
          "name": "Xingyu Zeng",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7166",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "682ffa9a6e906040a3bb7167",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:58.000Z",
      "submittedOnDailyAt": "2025-05-23T03:06:29.578Z",
      "title": "GoT-R1: 視覚生成に向けたMLLMの理由論能力の解放を強化学習によって実現する",
      "submittedOnDailyBy": {
        "_id": "64a2b496e2e19de17db7de65",
        "avatarUrl": "/avatars/241448ca487833d6cc5d57bb1fdb6ee5.svg",
        "isPro": false,
        "fullname": "Duan Chengqi",
        "user": "gogoduan",
        "type": "user"
      },
      "summary": "ビジュアル生成モデルは、テキストプロンプトから写実的な画像を生成するために驚異的な進歩を達していますが、複数の物体を特定し、精密な空間関係と属性を指定する複雑なプロンプトに対しては困難を見せています。そんなプロンプトの有効な処理には、語義的な内容と空間配置の明確な理由論が必要です。私たちは、ビジュアル生成に対する理由論を強化学習を用いて向上させるフレームワークを紹介します。それは、Generation Chain-of-Thoughtアプローチに基づいて、複雑なプロンプトに対する語義的・空間的理由論を可能にするものです。これを達成するために、私たちは、MLLMを活用して理由論の過程と最終的な出力を評価する二段階多次元報酬フレームワークを提案します。これにより、生成パイプライン全体で有効なサポートが可能になります。報酬システムは、語義的な一致性、空間的な精度、そして視覚的な品質を統一的に評価します。実験結果によると、T2I-CompBenchベンチマークでは、特に精密な空間関係と属性の結合を含む組成タスクにおいて显著な向上が見られます。GoT-R1は、複雑な理由論能力をビジュアル生成ディレクトリに成功に移し、画像生成の最先端を進めます。将来の研究を促進するために、私たちは、https://github.com/gogoduan/GoT-R1でコードと事前学習モデルを公開します。",
      "upvotes": 21,
      "discussionId": "682ffa9b6e906040a3bb71ba",
      "githubRepo": "https://github.com/gogoduan/GoT-R1",
      "ai_summary": "GoT-R1 enhances visual generation by using reinforcement learning to improve semantic-spatial reasoning, outperforming existing models on complex compositional tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "Generation Chain-of-Thought",
        "MLLMs",
        "dual-stage multi-dimensional reward framework",
        "semantic alignment",
        "spatial accuracy",
        "visual quality",
        "T2I-CompBench"
      ]
    },
    "publishedAt": "2025-05-22T13:59:58.000Z",
    "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation\n  with Reinforcement Learning",
    "summary": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a2b496e2e19de17db7de65",
      "avatarUrl": "/avatars/241448ca487833d6cc5d57bb1fdb6ee5.svg",
      "fullname": "Duan Chengqi",
      "name": "gogoduan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16933",
      "authors": [
        {
          "_id": "682fe37bb998c9f79463b563",
          "name": "Zebin You",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b564",
          "name": "Shen Nie",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b565",
          "name": "Xiaolu Zhang",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b566",
          "name": "Jun Hu",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b567",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b568",
          "name": "Zhiwu Lu",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b569",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "682fe37bb998c9f79463b56a",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:23:26.000Z",
      "submittedOnDailyAt": "2025-05-23T01:26:20.847Z",
      "title": "LLaDA-V: 大規模言語拡散モデルと視覚インストラクションチューニング",
      "submittedOnDailyBy": {
        "_id": "624f909eac5dd186b01ac3f5",
        "avatarUrl": "/avatars/0aafdb1cbb492fda52a0303031cc6c14.svg",
        "isPro": false,
        "fullname": "Zebin You",
        "user": "yyyou",
        "type": "user"
      },
      "summary": "この研究では、LLaDA-Vという、純粋なディフュージョンベースの多モデル大語言モデル（MLLM）を紹介します。これは、現在の多モデルアプローチで主導する自動回帰パラダイムから離れ、視覚インストラクションチューニングとマスクディフュージョンモデルを統合しています。LLaDA（代表的な大語言ディフュージョンモデル）に基づいて構築されているLLaDA-Vは、視覚エンコーダとMLPコネクタを含み、視覚特徴を語言埋め込み空間に投射し、効果的な多モデルアライメントを可能にします。実験的な調査により、以下の興味深い結果が明らかになりました。最初に、LLaDA-Vは、語言モデルが純粋な文字タスクではLLaMA3-8BやQwen2-7Bと比較して弱いが、多モデル性能には望ましい結果を示しています。同じインストラクションデータで訓練された場合、LLaDA-Vは多モデルタスクでLLaMA3-Vと高度に競争的であり、データスケーラビリティも良いです。また、Qwen2-VLとの性能間隔を狭め、この構造の効果性を示しています。第二に、現在の組み合わせ自動回帰ディフュージョンと純粋なディフュージョンベースのMLLMと比較して、多モデル理解の最先端の性能を達成しています。我々の発見は、大語言ディフュージョンモデルは多モデルコンテキストで望ましいことを示し、将来の研究で進めるべきであることを示しています。プロジェクトページとコードは以下のURLにあります。",
      "upvotes": 20,
      "discussionId": "682fe37cb998c9f79463b5ae",
      "ai_summary": "A diffusion-based Multimodal Large Language Model (LLaDA-V) with integrated visual instruction tuning performs competitively on multimodal tasks and outperforms existing models in multimodal understanding.",
      "ai_keywords": [
        "diffusion-based",
        "Multimodal Large Language Model (MLLM)",
        "visual instruction tuning",
        "masked diffusion models",
        "autoregressive paradigms",
        "vision encoder",
        "MLP connector",
        "language embedding space",
        "multimodal performance",
        "LLaDA",
        "LLaMA3-8B",
        "Qwen2-7B",
        "LLaMA3-V",
        "Qwen2-VL",
        "multimodal understanding",
        "hybrid autoregressive-diffusion"
      ]
    },
    "publishedAt": "2025-05-22T13:23:26.000Z",
    "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning",
    "summary": "In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large\nLanguage Model (MLLM) that integrates visual instruction tuning with masked\ndiffusion models, representing a departure from the autoregressive paradigms\ndominant in current multimodal approaches. Built upon LLaDA, a representative\nlarge language diffusion model, LLaDA-V incorporates a vision encoder and MLP\nconnector that projects visual features into the language embedding space,\nenabling effective multimodal alignment. Our empirical investigation reveals\nseveral intriguing results: First, LLaDA-V demonstrates promising multimodal\nperformance despite its language model being weaker on purely textual tasks\nthan counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same\ninstruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal\ntasks with better data scalability. It also narrows the performance gap to\nQwen2-VL, suggesting the effectiveness of its architecture for multimodal\ntasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal\nunderstanding compared to existing hybrid autoregressive-diffusion and purely\ndiffusion-based MLLMs. Our findings suggest that large language diffusion\nmodels show promise in multimodal contexts and warrant further investigation in\nfuture research. Project page and codes:\nhttps://ml-gsai.github.io/LLaDA-V-demo/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16933.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "624f909eac5dd186b01ac3f5",
      "avatarUrl": "/avatars/0aafdb1cbb492fda52a0303031cc6c14.svg",
      "fullname": "Zebin You",
      "name": "yyyou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16925",
      "authors": [
        {
          "_id": "68302d01b85e3ed6a61e6476",
          "user": {
            "_id": "67a33f0e36fccbd55d6e8f7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a33f0e36fccbd55d6e8f7f/o_2UU5bxtnPmPdKo-jg7x.png",
            "isPro": false,
            "fullname": "Igor Udovichenko",
            "user": "i-udovichenko",
            "type": "user"
          },
          "name": "Igor Udovichenko",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T08:16:34.263Z",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e6477",
          "name": "Olivier Croissant",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e6478",
          "name": "Anita Toleutaeva",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e6479",
          "name": "Evgeny Burnaev",
          "hidden": false
        },
        {
          "_id": "68302d01b85e3ed6a61e647a",
          "name": "Alexander Korotin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:18:07.000Z",
      "submittedOnDailyAt": "2025-05-23T06:41:43.892Z",
      "title": "リスク回避型の強化学習におけるイタクラ-サイトスコープ損失",
      "submittedOnDailyBy": {
        "_id": "67a33f0e36fccbd55d6e8f7f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a33f0e36fccbd55d6e8f7f/o_2UU5bxtnPmPdKo-jg7x.png",
        "isPro": false,
        "fullname": "Igor Udovichenko",
        "user": "i-udovichenko",
        "type": "user"
      },
      "summary": "リスク回避型強化学習は、多くの高風險領域で応用されています。古典的な強化学習と異なり、期待値を最大化することではなく、リスクを最小化するために策を選択し、期待値を一時的に許容することもあります。これらの好みは、ユーティリティ理論によって構成できます。我々は、指数型ユーティリティ関数の特別な場合に焦点を当て、ここではBellman方程式を得ることができ、変更が少なくとも強化学習アルゴリズムを適用できます。しかし、これらの方法は、全過程で指数計算が必要となるため、数値不穩定に苦戦します。これを解決するために、我々は、状態値と行動値関数の学習において数値的に安定し、数学的に正当な損失関数を、Itakura-Saitoダイバージェンスに基づいて導入しました。我々の提案された損失関数は、理論的にも実験的にも、既存の代替と比較して評価しました。実験セクションでは、複数の財務シナリオを検討し、一部は分析的な解が既知の場合も含むことで、我々の損失関数が代替と比べて優れていることを示しました。",
      "upvotes": 16,
      "discussionId": "68302d02b85e3ed6a61e64db",
      "ai_summary": "Proposed Itakura-Saito divergence-based loss function enhances numerical stability in risk-averse reinforcement learning using exponential utility functions.",
      "ai_keywords": [
        "reinforcement learning",
        "risk-averse",
        "utility theory",
        "exponential utility function",
        "Bellman equations",
        "numerical instability",
        "Itakura-Saito divergence",
        "state-value functions",
        "action-value functions"
      ]
    },
    "publishedAt": "2025-05-22T13:18:07.000Z",
    "title": "Risk-Averse Reinforcement Learning with Itakura-Saito Loss",
    "summary": "Risk-averse reinforcement learning finds application in various high-stakes\nfields. Unlike classical reinforcement learning, which aims to maximize\nexpected returns, risk-averse agents choose policies that minimize risk,\noccasionally sacrificing expected value. These preferences can be framed\nthrough utility theory. We focus on the specific case of the exponential\nutility function, where we can derive the Bellman equations and employ various\nreinforcement learning algorithms with few modifications. However, these\nmethods suffer from numerical instability due to the need for exponent\ncomputation throughout the process. To address this, we introduce a numerically\nstable and mathematically sound loss function based on the Itakura-Saito\ndivergence for learning state-value and action-value functions. We evaluate our\nproposed loss function against established alternatives, both theoretically and\nempirically. In the experimental section, we explore multiple financial\nscenarios, some with known analytical solutions, and show that our loss\nfunction outperforms the alternatives.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16925.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a33f0e36fccbd55d6e8f7f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a33f0e36fccbd55d6e8f7f/o_2UU5bxtnPmPdKo-jg7x.png",
      "fullname": "Igor Udovichenko",
      "name": "i-udovichenko",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15270",
      "authors": [
        {
          "_id": "682e907a24b2bb08885b94dc",
          "user": {
            "_id": "682e8e6d007cd8c2f2cd0afd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/az0B5USOPlLR3vq872JeN.png",
            "isPro": false,
            "fullname": "Chenyu Zheng",
            "user": "ChenyuZheng",
            "type": "user"
          },
          "name": "Chenyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:10.939Z",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94dd",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94de",
          "name": "Rongzhen Wang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94df",
          "name": "Wei Huang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e0",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e1",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e2",
          "name": "Jun Zhu",
          "hidden": false
        },
        {
          "_id": "682e907a24b2bb08885b94e3",
          "name": "Chongxuan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T08:49:03.000Z",
      "submittedOnDailyAt": "2025-05-23T00:59:19.168Z",
      "title": "μPを用いて効率的に拡大するDiffusion Transformers",
      "submittedOnDailyBy": {
        "_id": "682e8e6d007cd8c2f2cd0afd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/az0B5USOPlLR3vq872JeN.png",
        "isPro": false,
        "fullname": "Chenyu Zheng",
        "user": "ChenyuZheng",
        "type": "user"
      },
      "summary": "Diffusion Transformersは視覚生成モデルの基礎として現れたが、大規模でのパラメータ調整の高額さによりスケーラビリティが限られている。最近、Maximal Update Parametrization (muP)はベジェフトランジャーマイナスの場合に提案され、小さなモデルから大規模な言語モデルへの安定したパラメータ調整を可能にし、調整コストを大幅に減少させることができる。しかし、muPのベジェフトランジャーマイナス版がディフュージョントランジャーマイナスにも適用できるかどうかは不明である。本稿では、muPをディフュージョントランジャーマイナスに一般化し、大規模な実験でその効果性を証明する。まず、主流のディフュージョントランジャーマイナスのmuP（DiT、U-ViT、PixArt-alpha、MMDiT）がベジェフトランジャーマイナスのmuPと一致することを厳密に証明し、既存のmuP手法を直接適用することができることを示す。この結果を活用し、DiT-muPのパラメータ調整の強固な適用可能性をシステマティックに示す。特に、学習率を適用したDiT-XL-2-muPは元のDiT-XL-2より2.9倍速く収束することが証明される。最後に、PixArt-alphaとMMDiTのテキストから画像生成におけるmuPの効果性を検証する。PixArt-alphaは0.04Bから0.61B、MMDiTは0.18Bから18Bにスケールする。両方では、muPのモデルは小規模な調整コストで基準モデルを超える。PixArt-alphaの1回の学習コストの5.5%だけで、MMDiT-18Bの人間の専門家の消費の3%だけで実現できる。これらの結果はmuPがディフュージョントランジャーマイナスのスケーリングにおける原理的で効率的なフレームワークとして確立する。",
      "upvotes": 16,
      "discussionId": "682e907b24b2bb08885b952c",
      "projectPage": "https://github.com/ML-GSAI/Scaling-Diffusion-Transformers-muP",
      "githubRepo": "https://github.com/ML-GSAI/Scaling-Diffusion-Transformers-muP",
      "ai_summary": "Maximal Update Parametrization (μP) is extended to diffusion Transformers, demonstrating efficient hyperparameter transferability and reduced tuning costs across various models and tasks.",
      "ai_keywords": [
        "Diffusion Transformers",
        "Maximal Update Parametrization",
        "μP",
        "hyperparameter tuning",
        "DiT",
        "U-ViT",
        "PixArt-α",
        "MMDiT",
        "text-to-image generation",
        "transfer learning",
        "convergence",
        "training run"
      ]
    },
    "publishedAt": "2025-05-21T04:49:03.000Z",
    "title": "Scaling Diffusion Transformers Efficiently via μP",
    "summary": "Diffusion Transformers have emerged as the foundation for vision generative\nmodels, but their scalability is limited by the high cost of hyperparameter\n(HP) tuning at large scales. Recently, Maximal Update Parametrization (muP)\nwas proposed for vanilla Transformers, which enables stable HP transfer from\nsmall to large language models, and dramatically reduces tuning costs. However,\nit remains unclear whether muP of vanilla Transformers extends to diffusion\nTransformers, which differ architecturally and objectively. In this work, we\ngeneralize standard muP to diffusion Transformers and validate its\neffectiveness through large-scale experiments. First, we rigorously prove that\nmuP of mainstream diffusion Transformers, including DiT, U-ViT,\nPixArt-alpha, and MMDiT, aligns with that of the vanilla Transformer,\nenabling the direct application of existing muP methodologies. Leveraging\nthis result, we systematically demonstrate that DiT-muP enjoys robust HP\ntransferability. Notably, DiT-XL-2-muP with transferred learning rate\nachieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we\nvalidate the effectiveness of muP on text-to-image generation by scaling\nPixArt-alpha from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,\nmodels under muP outperform their respective baselines while requiring small\ntuning cost, only 5.5% of one training run for PixArt-alpha and 3% of\nconsumption by human experts for MMDiT-18B. These results establish muP as a\nprincipled and efficient framework for scaling diffusion Transformers.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "682e8e6d007cd8c2f2cd0afd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/az0B5USOPlLR3vq872JeN.png",
      "fullname": "Chenyu Zheng",
      "name": "ChenyuZheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.14604",
      "authors": [
        {
          "_id": "682f34b52b9fdc24ae9de371",
          "name": "Haoran Zhao",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de372",
          "name": "Yuchen Yan",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de373",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de374",
          "name": "Haolei Xu",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de375",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de376",
          "name": "Kaitao Song",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de377",
          "name": "Jian Shao",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de378",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de379",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "682f34b52b9fdc24ae9de37a",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T16:53:40.000Z",
      "submittedOnDailyAt": "2025-05-23T03:36:02.584Z",
      "title": "LLMsは自分自身による制限を通じて、過度な考えを抑えることで自由になることを提唱します。",
      "submittedOnDailyBy": {
        "_id": "5e1058e9fcf41d740b69966d",
        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
        "isPro": false,
        "fullname": "Yongliang Shen",
        "user": "tricktreat",
        "type": "user"
      },
      "summary": "大論理モデル（LRMs）の例としてOpenAI o1とDeepSeek-R1などが、思索の長さを増やして多様なタスクで上位の評価を果たし、理由能力を大幅に向上させています。しかし、この性能の向上は、生成プロセス中の冗長な理由の大幅な増加に伴い、高い計算オーバーヘッドと過度思い込みの問題を悪化させています。現在の多数のアプローチは、過度思い込みの問題を解決するために外部の干渉を依存していますが、これらは通常外部の制御機構に依存しています。本論文では、モデルが自ら理由プロセスを調節できるような新しいフレームワークを提案しています。これは、外部の制御機構を依存せず、過度思い込みを解決するためにモデルが自ら理由プロセスを調節できるようにします。標準的な答えに基づいた過度思い込みの識別測定基準を用いて、冗長な理由を検出する系統的な方法を設計し、理由の軌跡内の不必要なステップを正確に識別し、学習をするためのトレーニングシグナルを生成します。この基盤により、適応的な理由の長さを持つデータの構築の完全な戦略を開発し、モデルが適切な点で理由を終了する時間を自然に学習するための革新的なブレイクプロンプト機構を導入します。数学ベンチマーク（AIME、AMC、MATH500、GSM8K）の実験は、この方法は、60%以上のトークン消費を減らしながら、制約なしモデルと比較的な精度を維持することを示しています。",
      "upvotes": 16,
      "discussionId": "682f34b62b9fdc24ae9de3be",
      "ai_summary": "A novel Self-Braking Tuning framework reduces overthinking and unnecessary computational overhead in large reasoning models by enabling the model to self-regulate its reasoning process.",
      "ai_keywords": [
        "large reasoning models",
        "self-braking tuning",
        "overthinking",
        "reasoning capabilities",
        "mathematical benchmarks",
        "adaptive reasoning lengths",
        "braking prompt mechanism"
      ]
    },
    "publishedAt": "2025-05-20T12:53:40.000Z",
    "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning",
    "summary": "Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have\nsignificantly enhanced their reasoning capabilities by generating longer chains\nof thought, demonstrating outstanding performance across a variety of tasks.\nHowever, this performance gain comes at the cost of a substantial increase in\nredundant reasoning during the generation process, leading to high\ncomputational overhead and exacerbating the issue of overthinking. Although\nnumerous existing approaches aim to address the problem of overthinking, they\noften rely on external interventions. In this paper, we propose a novel\nframework, Self-Braking Tuning (SBT), which tackles overthinking from the\nperspective of allowing the model to regulate its own reasoning process, thus\neliminating the reliance on external control mechanisms. We construct a set of\noverthinking identification metrics based on standard answers and design a\nsystematic method to detect redundant reasoning. This method accurately\nidentifies unnecessary steps within the reasoning trajectory and generates\ntraining signals for learning self-regulation behaviors. Building on this\nfoundation, we develop a complete strategy for constructing data with adaptive\nreasoning lengths and introduce an innovative braking prompt mechanism that\nenables the model to naturally learn when to terminate reasoning at an\nappropriate point. Experiments across mathematical benchmarks (AIME, AMC,\nMATH500, GSM8K) demonstrate that our method reduces token consumption by up to\n60% while maintaining comparable accuracy to unconstrained models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14604.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e1058e9fcf41d740b69966d",
      "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
      "fullname": "Yongliang Shen",
      "name": "tricktreat",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16916",
      "authors": [
        {
          "_id": "682fdcfc2c98b5e99660561e",
          "name": "Xuankun Rong",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e99660561f",
          "name": "Wenke Huang",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605620",
          "name": "Jian Liang",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605621",
          "name": "Jinhe Bi",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605622",
          "name": "Xun Xiao",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605623",
          "name": "Yiming Li",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605624",
          "name": "Bo Du",
          "hidden": false
        },
        {
          "_id": "682fdcfc2c98b5e996605625",
          "name": "Mang Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:11:58.000Z",
      "submittedOnDailyAt": "2025-05-23T00:58:25.177Z",
      "title": "マルチモデルフィーチューニングにおける外部ガイドラインを必要とさせないバックドアクリーニング",
      "submittedOnDailyBy": {
        "_id": "66c014820836dd7a55be3fde",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OuqAxEFq1Ny2CHbl9HOm.jpeg",
        "isPro": false,
        "fullname": "Xuankun Rong",
        "user": "XuankunRong",
        "type": "user"
      },
      "summary": "多モダル大語言モデル（MLLMs）は、ユーザーが提供するデータセットを用いて一般的なモデルを下流タスクに適用するファインチューニングアソシアティング（FTaaS）設定で日適用されています。この柔軟性は、悪意のあるファインチューニングがMLLMにバックドアを植え込むための最小限の努力で可能であることにより、厳しい安全リスクを引き起こします。本論文では、バックドアトリガーがシステマ的に、非語意領域に対する非通常のアテンション濃度を引き起こし、クロスモーダル処理を破壊することを観察しました。この観察に基づいて、我々は、アテンションエントロピーパターンを自動軽視信号として利用し、バックドアサンプルを識別してフィルタリングするデータフィルタリングフレームワーク「Believe Your Eyes（BYE）」を提案します。BYEは、以下の3ステップのパイプラインを通じて動作します：1）ファインチューニングモデルを用いてアテンションマップを抽出、2）バイモード分離によりエントロピースコアを計算し、過敏層をプロファイリング、3）非サブジューデーションクラスタリングを行い、疑似サンプルを除去します。先行の防御と違い、BYEはクリーンサブジューデーション、アシステントラベル、またはモデルの改修が必要とならないです。多様なデータセット、モデル、ドリフェント種類の様々な実験で、BYEの効果性を検証しました：それはクリーンタスク性能を維持する同時に近似ゼロの攻撃成功率を達成し、MLLMにおけるバックドアリスクに対する強固な一般化可能な解決策を提供します。",
      "upvotes": 14,
      "discussionId": "682fdcfd2c98b5e99660567b",
      "ai_summary": "A novel defense framework, Believe Your Eyes (BYE), identifies and filters backdoor samples in fine-tuned multimodal large language models by analyzing attention entropy patterns, preventing trigger activation without requiring additional labels or model changes.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "fine-tuning-as-a-service",
        "FTaaS",
        "backdoors",
        "attention collapse",
        "attention entropy",
        "self-supervised",
        "bimodal separation",
        "unsupervised clustering",
        "clean-task performance"
      ]
    },
    "publishedAt": "2025-05-22T13:11:58.000Z",
    "title": "Backdoor Cleaning without External Guidance in MLLM Fine-tuning",
    "summary": "Multimodal Large Language Models (MLLMs) are increasingly deployed in\nfine-tuning-as-a-service (FTaaS) settings, where user-submitted datasets adapt\ngeneral-purpose models to downstream tasks. This flexibility, however,\nintroduces serious security risks, as malicious fine-tuning can implant\nbackdoors into MLLMs with minimal effort. In this paper, we observe that\nbackdoor triggers systematically disrupt cross-modal processing by causing\nabnormal attention concentration on non-semantic regions--a phenomenon we term\nattention collapse. Based on this insight, we propose Believe Your Eyes (BYE),\na data filtering framework that leverages attention entropy patterns as\nself-supervised signals to identify and filter backdoor samples. BYE operates\nvia a three-stage pipeline: (1) extracting attention maps using the fine-tuned\nmodel, (2) computing entropy scores and profiling sensitive layers via bimodal\nseparation, and (3) performing unsupervised clustering to remove suspicious\nsamples. Unlike prior defenses, BYE equires no clean supervision, auxiliary\nlabels, or model modifications. Extensive experiments across various datasets,\nmodels, and diverse trigger types validate BYE's effectiveness: it achieves\nnear-zero attack success rates while maintaining clean-task performance,\noffering a robust and generalizable solution against backdoor threats in MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16916.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c014820836dd7a55be3fde",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OuqAxEFq1Ny2CHbl9HOm.jpeg",
      "fullname": "Xuankun Rong",
      "name": "XuankunRong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14684",
      "authors": [
        {
          "_id": "682f474c9ee0bb0cc953b885",
          "name": "Haolei Xu",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b886",
          "name": "Yuchen Yan",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b887",
          "name": "Yongliang Shen",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b888",
          "name": "Wenqi Zhang",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b889",
          "name": "Guiyang Hou",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88a",
          "name": "Shengpei Jiang",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88b",
          "name": "Kaitao Song",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88c",
          "name": "Weiming Lu",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88d",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "682f474c9ee0bb0cc953b88e",
          "name": "Yueting Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:59:31.000Z",
      "submittedOnDailyAt": "2025-05-23T03:43:43.765Z",
      "title": "隙を閉じる：考えの飛躍を結びつけて、Chain-of-Thought Tuningを向上させる",
      "submittedOnDailyBy": {
        "_id": "64098738342c26884c792c93",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
        "isPro": false,
        "fullname": "Yuchen Yan",
        "user": "yanyc",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、Chain-of-Thought（CoT）論理により数学タスクにおいて驚異的な進歩を達成しました。しかし、現在の数学CoTデータセットは、専門家が中間ステップを省略したためにThought Leapsによって苦戦しています。これはモデルの学習と一般化に負面影響を及ぼしています。私たちは、CoT Thought Leap Bridge Taskを提案し、自動的にLeapsを検出し、欠損した中間論理ステップを生成してCoTの完全性と一貫性を復元することを目指しています。これを促進するために、構造化されたScaleQuestMathデータセットに基づいた特別なトレーニングデータセットを構築し、CoT-Bridgeをトレーニングしました。数學論理ベンチマークにおいて詳細な実験を通じて、ブリッジされたデータセットによって微調節されたモデルは、元のデータセットによってトレーニンされたモデルを超え、NuminaMathでは+5.87%の改善を示しました。私たちのアプローチは、結合されたデータの精約化（+3.02%）を効果的に増強し、強化学習のより良い始点を提供し、現有の最適化手法とのプラグインとプレイングモジュールとして機能します。また、CoT-Bridgeは、外れドメインの論理論理タスクにおける一般化を改善し、理由論の完全性を強化することが広範囲的な利益を与えることを確認しました。",
      "upvotes": 14,
      "discussionId": "682f474d9ee0bb0cc953b8c7",
      "projectPage": "https://zju-real.github.io/CoT-Bridge/",
      "githubRepo": "https://github.com/ZJU-REAL/Mind-the-Gap",
      "ai_summary": "A model for detecting and generating missing intermediate steps in mathematical Chain-of-Thought reasoning improves performance and generalization on mathematical and logical reasoning tasks.",
      "ai_keywords": [
        "Chain-of-Thought",
        "reasoning",
        "thought leaps",
        "ScaleQM+",
        "ScaleQuestMath",
        "NuminaMath",
        "distilled data",
        "reinforcement learning",
        "generalization",
        "logical reasoning tasks"
      ]
    },
    "publishedAt": "2025-05-20T13:59:31.000Z",
    "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
    "summary": "Large language models (LLMs) have achieved remarkable progress on\nmathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14684.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64098738342c26884c792c93",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
      "fullname": "Yuchen Yan",
      "name": "yanyc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16990",
      "authors": [
        {
          "_id": "682fdd034640a9db4d1cc04d",
          "name": "Runpeng Yu",
          "hidden": false
        },
        {
          "_id": "682fdd034640a9db4d1cc04e",
          "name": "Xinyin Ma",
          "hidden": false
        },
        {
          "_id": "682fdd034640a9db4d1cc04f",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:55:04.000Z",
      "submittedOnDailyAt": "2025-05-23T01:04:28.755Z",
      "title": "ディンプル: 離散ディフューション多モデル大語言モデル平行解確認",
      "submittedOnDailyBy": {
        "_id": "635364b3c41f548fe39db945",
        "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
        "isPro": false,
        "fullname": "Runpeng Yu",
        "user": "rp-yu",
        "type": "user"
      },
      "summary": "この研究では、最初の離散ディフューション多モデル大語言モデル（DMLLM）のDimpleを提案します。離散ディフューションの単一アプローチでの訓練は、大きな訓練不穩定、最適な性能、長さバイアス問題による厳しい課題を伴います。これらの課題に対処するために、初期の自動回帰フェーズと後続のディフューションフェーズを組み合わせた新しい訓練パラダイムを設計しました。このアプローチにより、Dimple-7Bモデルを構築し、LLaVA-NEXTと同じデータセットと類似した訓練パイプラインを使用して訓練されました。Dimple-7Bは、LLaVA-NEXTの性能を3.9%より上回り、DMLLMが自動回帰モデルと同等の性能を達成できることを示します。推論の効率化を促進するために、自信デコーディングという名前のデコーディングステラテジを提案し、各ステップで生成するトークンの数を動的に調整し、生成イテレーション数を大幅に減少させます。自動回帰モデルでは、生成時の正向イテレーション数は応答の長さと等しいが、自信デコーディングでは、Dimpleの必要なイテレーション数は応答長さの3倍に抑えられます。また、自動回帰モデルの予り込み手法を再実装し、バージョン評価の多くのテストで性能に大きな影響を与えないことを示し、1.5倍から7倍のスピードアップを提供します。また、Dimpleの応答の精密な制御を可能にする構造プロファイルを使用し、これらのプロファイルはインストラクションベースまたはチャインオフショープロンティングと異なる構造化された応答を生成でき、応答のフォーマットと長さの細かい制御を可能にします。全体として、この研究はDMLLMの可能性と優れた点を証明し、推論の効率化と制御可能性を向上させます。コードとモデルは、https://github.com/yu-rp/Dimple から利用可能です。",
      "upvotes": 12,
      "discussionId": "682fdd044640a9db4d1cc0a1",
      "githubRepo": "https://github.com/yu-rp/Dimple",
      "ai_summary": "Dimple, a Discrete Diffusion Multimodal Large Language Model, achieves performance comparable to autoregressive models through a hybrid training approach and enhances inference efficiency with confident decoding and structure priors.",
      "ai_keywords": [
        "Discrete Diffusion Multimodal Large Language Model",
        "DMLLM",
        "autoregressive phase",
        "diffusion phase",
        "confident decoding",
        "prefilling technique",
        "structure priors"
      ]
    },
    "publishedAt": "2025-05-22T13:55:04.000Z",
    "title": "Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel\n  Decoding",
    "summary": "In this work, we propose Dimple, the first Discrete Diffusion Multimodal\nLarge Language Model (DMLLM). We observe that training with a purely discrete\ndiffusion approach leads to significant training instability, suboptimal\nperformance, and severe length bias issues. To address these challenges, we\ndesign a novel training paradigm that combines an initial autoregressive phase\nwith a subsequent diffusion phase. This approach yields the Dimple-7B model,\ntrained on the same dataset and using a similar training pipeline as\nLLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%,\ndemonstrating that DMLLM can achieve performance comparable to that of\nautoregressive models. To improve inference efficiency, we propose a decoding\nstrategy termed confident decoding, which dynamically adjusts the number of\ntokens generated at each step, significantly reducing the number of generation\niterations. In autoregressive models, the number of forward iterations during\ngeneration equals the response length. With confident decoding, however, the\nnumber of iterations needed by Dimple is even only text{response\nlength}{3}. We also re-implement the prefilling technique in autoregressive\nmodels and demonstrate that it does not significantly impact performance on\nmost benchmark evaluations, while offering a speedup of 1.5x to 7x.\nAdditionally, we explore Dimple's capability to precisely control its response\nusing structure priors. These priors enable structured responses in a manner\ndistinct from instruction-based or chain-of-thought prompting, and allow\nfine-grained control over response format and length, which is difficult to\nachieve in autoregressive models. Overall, this work validates the feasibility\nand advantages of DMLLM and enhances its inference efficiency and\ncontrollability. Code and models are available at\nhttps://github.com/yu-rp/Dimple.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635364b3c41f548fe39db945",
      "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
      "fullname": "Runpeng Yu",
      "name": "rp-yu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16864",
      "authors": [
        {
          "_id": "682fe14abafb480b9595da32",
          "name": "Yuechen Zhang",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da33",
          "name": "Jinbo Xing",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da34",
          "name": "Bin Xia",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da35",
          "name": "Shaoteng Liu",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da36",
          "name": "Bohao Peng",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da37",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da38",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da39",
          "name": "Eric Lo",
          "hidden": false
        },
        {
          "_id": "682fe14abafb480b9595da3a",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T16:21:32.000Z",
      "submittedOnDailyAt": "2025-05-23T01:22:45.264Z",
      "title": "トレーニングなしの効率的なビデオ生成をどうやって実現するか：動的トークン切り出し法",
      "submittedOnDailyBy": {
        "_id": "6418554a0956be7233a1023e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
        "isPro": false,
        "fullname": "zhang yuechen",
        "user": "julianjuaner",
        "type": "user"
      },
      "summary": "ビデオディフュージョントランスフォーマー（DiT）モデルの驚人的生成質量に違いなく、実用的な扱いによる実装は厳しい計算要求により厳しく制限されています。この不適切さは2つの主な課題に基づきます：トークン長に対する自動注意の二次元複雑さと、ディフュージョンモデルの多ステップ的な性質です。これらの制限を解決するために、ユニークな推論パイプライン「ジェンジ」を提案します。このパイプラインは動的な注意切り出しと進歩的な解像度生成を組み合わせています。私たちのアプローチは2つの主なヒントを基づきます：（1）早期のデノイズステップは高解像度の潜在変数が不要であり、（2）後期のステップは密集的な注意が不要です。ジェンジは3D空間埋め込み曲線を使用して関連するトークン相互作用を動的に選択するブロックごとの注意機構を導入し、生成中に潜在解像度を進歩的に増加させることを伴います。実験結果は、ジェンジは多くの最先端のビデオディフュージョンモデルで大幅なスピードアップを実現し、比較的な生成質量を維持します（VBenchでは8.83倍のスピードアップと0.01%の性能ドロップ）。プラグとパンのような解決策として、ジェンジはモデル再学習を必要としないモデルの推論時間を分かりやすい秒に抑えることを可能にし、現代のハードウェア上で実用的な高品質のビデオ生成を可能にします。コード：https://github.com/dvlab-research/Jenga",
      "upvotes": 11,
      "discussionId": "682fe14ebafb480b9595db1c",
      "projectPage": "https://julianjuaner.github.io/projects/jenga/",
      "githubRepo": "https://github.com/dvlab-research/Jenga",
      "ai_summary": "Jenga, a novel inference pipeline for video Diffusion Transformer models, combines dynamic attention carving and progressive resolution generation to significantly speed up video generation while maintaining high quality.",
      "ai_keywords": [
        "video Diffusion Transformer (DiT)",
        "self-attention",
        "diffusion models",
        "dynamic attention carving",
        "progressive resolution generation",
        "block-wise attention",
        "3D space-filling curves"
      ]
    },
    "publishedAt": "2025-05-22T12:21:32.000Z",
    "title": "Training-Free Efficient Video Generation via Dynamic Token Carving",
    "summary": "Despite the remarkable generation quality of video Diffusion Transformer\n(DiT) models, their practical deployment is severely hindered by extensive\ncomputational requirements. This inefficiency stems from two key challenges:\nthe quadratic complexity of self-attention with respect to token length and the\nmulti-step nature of diffusion models. To address these limitations, we present\nJenga, a novel inference pipeline that combines dynamic attention carving with\nprogressive resolution generation. Our approach leverages two key insights: (1)\nearly denoising steps do not require high-resolution latents, and (2) later\nsteps do not require dense attention. Jenga introduces a block-wise attention\nmechanism that dynamically selects relevant token interactions using 3D\nspace-filling curves, alongside a progressive resolution strategy that\ngradually increases latent resolution during generation. Experimental results\ndemonstrate that Jenga achieves substantial speedups across multiple\nstate-of-the-art video diffusion models while maintaining comparable generation\nquality (8.83times speedup with 0.01\\% performance drop on VBench). As a\nplug-and-play solution, Jenga enables practical, high-quality video generation\non modern hardware by reducing inference time from minutes to seconds --\nwithout requiring model retraining. Code:\nhttps://github.com/dvlab-research/Jenga",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16864.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6418554a0956be7233a1023e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6418554a0956be7233a1023e/9EKN0GoOpcDbvBDmAQEJf.png",
      "fullname": "zhang yuechen",
      "name": "julianjuaner",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16181",
      "authors": [
        {
          "_id": "682fddbd2b4a4d1ce53c5afb",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Reza Taesiri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T03:18:33.876Z",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5afc",
          "user": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "isPro": false,
            "fullname": "Franck Dernoncourt",
            "user": "Franck-Dernoncourt",
            "type": "user"
          },
          "name": "Brandon Collins",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T02:32:10.307Z",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5afd",
          "user": {
            "_id": "668c8e8c142f9b26a49f03cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668c8e8c142f9b26a49f03cc/YNmPCrlsi6iwSeNfh1iID.png",
            "isPro": false,
            "fullname": "Logan Bolton",
            "user": "loganbolton",
            "type": "user"
          },
          "name": "Logan Bolton",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T02:33:14.731Z",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5afe",
          "name": "Viet Dac Lai",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5aff",
          "name": "Franck Dernoncourt",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5b00",
          "name": "Trung Bui",
          "hidden": false
        },
        {
          "_id": "682fddbd2b4a4d1ce53c5b01",
          "name": "Anh Totti Nguyen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:35:15.000Z",
      "submittedOnDailyAt": "2025-05-23T01:00:45.248Z",
      "title": "日常画像編集タスクでの生成型AIの能力を理解する",
      "submittedOnDailyBy": {
        "_id": "62c5947524171688a9feb992",
        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
        "isPro": false,
        "fullname": "Franck Dernoncourt",
        "user": "Franck-Dernoncourt",
        "type": "user"
      },
      "summary": "生成的AI（GenAI）在自动化日常图像编辑任务方面展现出巨大的潜力，尤其是在2025年3月25日GPT-4o发布之后。然而，人们最常希望编辑的主题是什么？他们希望进行什么样的编辑操作（例如，移除或风格化主题）？人们是否更喜欢精确且可预测的编辑，还是高度创意的编辑？通过理解现实世界请求的特征以及自由职业摄影编辑大师所做的相应编辑，我们是否可以为改进基于AI的编辑器提供教训，并确定哪些类型的请求目前可以由AI编辑器成功处理？本文通过分析过去12年（2013-2025年）Reddit社区收集的83k请求和305k PSR-wizard编辑，对这些问题进行了独特的研究。根据人类评分，大约只有33%的请求可以由最佳AI编辑器（包括GPT-4o、Gemini-2.0-Flash、SeedEdit）实现。有趣的是，AI编辑器在需要精确编辑的低创意请求上表现不如在开放式任务上。他们经常难以保持人物和动物的身份，并经常进行非请求的修饰。另一方面，VLM法官（例如o1）的表现与人类法官不同，可能更倾向于AI编辑而非人类编辑。代码和定性示例可在https://psrdataset.github.io上找到。",
      "upvotes": 11,
      "discussionId": "682fddc32b4a4d1ce53c5c60",
      "projectPage": "https://psrdataset.github.io",
      "ai_summary": "Analysis of 83k image editing requests reveals that AI editors, including GPT-4o, struggle with low-creativity tasks and precise editing, while performing better on open-ended tasks, and human and VLM judges differ in their preferences for AI versus human edits.",
      "ai_keywords": [
        "GPT-4o",
        "Gemini-2.0-Flash",
        "SeedEdit",
        "VLM judges"
      ]
    },
    "publishedAt": "2025-05-21T23:35:15.000Z",
    "title": "Understanding Generative AI Capabilities in Everyday Image Editing Tasks",
    "summary": "Generative AI (GenAI) holds significant promise for automating everyday image\nediting tasks, especially following the recent release of GPT-4o on March 25,\n2025. However, what subjects do people most often want edited? What kinds of\nediting actions do they want to perform (e.g., removing or stylizing the\nsubject)? Do people prefer precise edits with predictable outcomes or highly\ncreative ones? By understanding the characteristics of real-world requests and\nthe corresponding edits made by freelance photo-editing wizards, can we draw\nlessons for improving AI-based editors and determine which types of requests\ncan currently be handled successfully by AI editors? In this paper, we present\na unique study addressing these questions by analyzing 83k requests from the\npast 12 years (2013-2025) on the Reddit community, which collected 305k\nPSR-wizard edits. According to human ratings, approximately only 33% of\nrequests can be fulfilled by the best AI editors (including GPT-4o,\nGemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on\nlow-creativity requests that require precise editing than on more open-ended\ntasks. They often struggle to preserve the identity of people and animals, and\nfrequently make non-requested touch-ups. On the other side of the table, VLM\njudges (e.g., o1) perform differently from human judges and may prefer AI edits\nmore than human edits. Code and qualitative examples are available at:\nhttps://psrdataset.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16181.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c5947524171688a9feb992",
      "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
      "fullname": "Franck Dernoncourt",
      "name": "Franck-Dernoncourt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.15952",
      "authors": [
        {
          "_id": "682fe833f39f561d1d8cd5d1",
          "user": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "isPro": true,
            "fullname": "taesiri",
            "user": "taesiri",
            "type": "user"
          },
          "name": "Mohammad Reza Taesiri",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T03:18:36.072Z",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d2",
          "name": "Abhijay Ghildyal",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d3",
          "name": "Saman Zadtootaghaj",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d4",
          "name": "Nabajeet Barman",
          "hidden": false
        },
        {
          "_id": "682fe833f39f561d1d8cd5d5",
          "user": {
            "_id": "644feede17b6189cda58575d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WVo1Ah7xmHEeBOUQpkYgS.png",
            "isPro": false,
            "fullname": "Cor-Paul",
            "user": "corpaul",
            "type": "user"
          },
          "name": "Cor-Paul Bezemer",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:15:06.040Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T19:08:38.000Z",
      "submittedOnDailyAt": "2025-05-23T01:45:28.315Z",
      "title": "VideoGameQA-Bench: ゲームの視覚言語モデルの質量保証評価",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "ビデオゲームがエンターテインメント業界で最高の収入を発生している現在、ゲーム開発ワークフローの最適化は、業界の継続的な成長にとって重要である。ビジョン言語モデル（VLMs）の最近の進歩は、ゲーム開発の様々な面で自動化されることと強化されることができることを示している、特にゲームの品質保証（QA）において、これは現在の自動化の機会が限られているような、業界で最も労働力の豊かなプロセスの一つである。VLMsがゲームQAタスクの性能を正確に評価し、実世界的なシナリオを扱う効果性を評価するために、標準化されたベンチマークの必要がある。現在のベンチマークは、このドメインの特定の要求に対応していることができないため、この空間を埋めるために、VideoGameQA-Benchという、ゲームQA活動の幅広い範囲を被覆する詳細なベンチマークを紹介します。このベンチマークは、視覚ユニットテスト、視覚的なリセットテスト、ハイスタックのニードルタスク、グリッチ検出、そして各種ゲームの画像や映像のバグ報告の生成など、様々なゲームQAアクティビティを含むものです。コードとデータは以下のURLから利用できます：https://asgaardlab.github.io/videogameqa-bench/",
      "upvotes": 11,
      "discussionId": "682fe83af39f561d1d8cd7e5",
      "projectPage": "https://asgaardlab.github.io/videogameqa-bench/",
      "ai_summary": "A benchmark called VideoGameQA-Bench is introduced to assess Vision-Language Models in video game quality assurance tasks.",
      "ai_keywords": [
        "Vision-Language Models",
        "VideoGameQA-Bench",
        "visual unit testing",
        "visual regression testing",
        "needle-in-a-haystack tasks",
        "glitch detection",
        "bug report generation"
      ]
    },
    "publishedAt": "2025-05-21T15:08:38.000Z",
    "title": "VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game\n  Quality Assurance",
    "summary": "With video games now generating the highest revenues in the entertainment\nindustry, optimizing game development workflows has become essential for the\nsector's sustained growth. Recent advancements in Vision-Language Models (VLMs)\noffer considerable potential to automate and enhance various aspects of game\ndevelopment, particularly Quality Assurance (QA), which remains one of the\nindustry's most labor-intensive processes with limited automation options. To\naccurately evaluate the performance of VLMs in video game QA tasks and\ndetermine their effectiveness in handling real-world scenarios, there is a\nclear need for standardized benchmarks, as existing benchmarks are insufficient\nto address the specific requirements of this domain. To bridge this gap, we\nintroduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array\nof game QA activities, including visual unit testing, visual regression\ntesting, needle-in-a-haystack tasks, glitch detection, and bug report\ngeneration for both images and videos of various games. Code and data are\navailable at: https://asgaardlab.github.io/videogameqa-bench/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15952.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 83
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17018",
      "authors": [
        {
          "_id": "682fe2b865bac3ec3556c016",
          "name": "Kaixuan Fan",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c017",
          "name": "Kaituo Feng",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c018",
          "name": "Haoming Lyu",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c019",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "682fe2b865bac3ec3556c01a",
          "name": "Xiangyu Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:53.000Z",
      "submittedOnDailyAt": "2025-05-23T01:28:23.338Z",
      "title": "SophiaVL-R1: テキスト理解モデルの計算能力を強化するための考える報酬",
      "submittedOnDailyBy": {
        "_id": "67079840a9bcb7459b8d2a46",
        "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
        "isPro": false,
        "fullname": "Kaituo Feng",
        "user": "KaituoFeng",
        "type": "user"
      },
      "summary": "最近の進展は、ルールベースの強化学習（RL）により多模態大語言モデル（MLLMs）で強い理由論能力を引き出す成功を示しています。しかし、このパラダイムは、最終的な結果を導く思考過程の観察に欠損しているため、モデルは最適な理由論戦略を学習することができない可能性があります。この点に対して、私たちはSophiaVL-R1を提案し、このパラダイムで思考過程に報酬信号を追加する試みです。これを達成するために、最初に、思考報酬モデルを訓練し、モデル全体の思考過程の質を評価するものです。思考報酬が特定のサンプルに対して信頼性が低い可能性があるため、私たちはTrust-GRPOメソッドを提案し、訓練中に思考報酬に信頼性重みを割り当てるものです。この重みは、正解の答えに導く回答と不正解の答えに導く回答の思考報酬の比較に基づいて計算され、潜在的に不信頼な思考報酬の影響を軽減することを助けます。また、私たちは、時間によって思考報酬を徐々に減少させる軽減訓練戦略を設計し、モデルが後期の訓練ステージで正確なルールベースの結果報酬により多く依存するようにします。実験結果は、私たちのSophiaVL-R1は、MathVisita、MMMUなどの様々なベンチマークで理由論モデルの系列を超え、強い理由論と一般化能力を示しています。特に、私たちのSophiaVL-R1-7Bは、後者が10倍のパラメータを持つLLaVA-OneVision-72Bを超えています。すべてのコード、モデル、データセットは、https://github.com/kxfan2002/SophiaVL-R1に公開されています。",
      "upvotes": 10,
      "discussionId": "682fe2b965bac3ec3556c066",
      "projectPage": "https://github.com/kxfan2002/SophiaVL-R1",
      "githubRepo": "https://github.com/kxfan2002/SophiaVL-R1",
      "ai_summary": "An enhanced multimodal language model incorporates thinking process rewards to improve reasoning and generalization, achieving superior performance on benchmarks compared to larger models.",
      "ai_keywords": [
        "multimodal large language models",
        "rule-based reinforcement learning",
        "reward signals",
        "thinking reward model",
        "Trust-GRPO method",
        "thinking reward comparison",
        "annealing training strategy",
        "reasoning MLLMs",
        "MathVisita",
        "MMMU"
      ]
    },
    "publishedAt": "2025-05-22T13:59:53.000Z",
    "title": "SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward",
    "summary": "Recent advances have shown success in eliciting strong reasoning abilities in\nmultimodal large language models (MLLMs) through rule-based reinforcement\nlearning (RL) with outcome rewards. However, this paradigm typically lacks\nsupervision over the thinking process leading to the final outcome.As a result,\nthe model may learn sub-optimal reasoning strategies, which can hinder its\ngeneralization ability. In light of this, we propose SophiaVL-R1, as an attempt\nto add reward signals for the thinking process in this paradigm. To achieve\nthis, we first train a thinking reward model that evaluates the quality of the\nentire thinking process. Given that the thinking reward may be unreliable for\ncertain samples due to reward hacking, we propose the Trust-GRPO method, which\nassigns a trustworthiness weight to the thinking reward during training. This\nweight is computed based on the thinking reward comparison of responses leading\nto correct answers versus incorrect answers, helping to mitigate the impact of\npotentially unreliable thinking rewards. Moreover, we design an annealing\ntraining strategy that gradually reduces the thinking reward over time,\nallowing the model to rely more on the accurate rule-based outcome reward in\nlater training stages. Experiments show that our SophiaVL-R1 surpasses a series\nof reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU),\ndemonstrating strong reasoning and generalization capabilities. Notably, our\nSophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite\nthe latter having 10 times more parameters. All code, models, and datasets are\nmade publicly available at https://github.com/kxfan2002/SophiaVL-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17018.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67079840a9bcb7459b8d2a46",
      "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
      "fullname": "Kaituo Feng",
      "name": "KaituoFeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.17012",
      "authors": [
        {
          "_id": "682fde942f8f73559fcbc5da",
          "name": "Haoning Wu",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5db",
          "name": "Xiao Huang",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5dc",
          "name": "Yaohui Chen",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5dd",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5de",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "682fde942f8f73559fcbc5df",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632c7a0d1d303f5f9acf01b8/3GQcp-q-BBj5bX_DRmB6S.jpeg"
      ],
      "publishedAt": "2025-05-22T17:59:03.000Z",
      "submittedOnDailyAt": "2025-05-23T01:12:41.090Z",
      "title": "スペクトルスコア：多モーダルスペクトル理解の統一的評価へ",
      "submittedOnDailyBy": {
        "_id": "632c7a0d1d303f5f9acf01b8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
        "isPro": false,
        "fullname": "Haoning Wu",
        "user": "haoningwu",
        "type": "user"
      },
      "summary": "多模态大语言模型（MLLMs）在问答任务中取得了令人印象深刻的成功，但它们在空间理解方面的能力却鲜有探索。本文研究了一个关键问题：现有的MLLMs是否具备3D空间感知和理解能力？具体来说，本文在以下几个方面做出了贡献：（i）我们引入了VGBench，这是一个专门设计用于评估MLLMs在视觉几何感知方面的基准，例如相机姿态和运动估计；（ii）我们提出了SpatialScore，这是迄今为止最全面和多样化的多模态空间理解基准，它整合了VGBench以及其他11个现有数据集的相关数据。该基准包含了28K个样本，涵盖了各种空间理解任务、模态和QA格式，以及一个精心挑选的具有挑战性的子集，即SpatialScore-Hard；（iii）我们开发了SpatialAgent，这是一个结合了9个专门工具的新型多代理系统，支持Plan-Execute和ReAct推理范式；（iv）我们进行了广泛的评估，揭示了空间推理中持续存在的挑战，并展示了SpatialAgent的有效性。我们相信SpatialScore将提供有价值的见解，并作为MLLMs下一次进化的严格基准。",
      "upvotes": 9,
      "discussionId": "682fde952f8f73559fcbc616",
      "projectPage": "https://haoningwu3639.github.io/SpatialScore/",
      "githubRepo": "https://github.com/haoningwu3639/SpatialScore/",
      "ai_summary": "SpatialScore benchmarks multimodal large language models for 3D spatial understanding, revealing challenges and showcasing the effectiveness of SpatialAgent with specialized tools.",
      "ai_keywords": [
        "Multimodal large language models",
        "MLLMs",
        "VGBench",
        "SpatialScore",
        "spatial understanding",
        "visual geometry perception",
        "camera pose",
        "motion estimation",
        "multi-agent system",
        "SpatialAgent",
        "Plan-Execute",
        "ReAct reasoning paradigms"
      ]
    },
    "publishedAt": "2025-05-22T13:59:03.000Z",
    "title": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial\n  Understanding",
    "summary": "Multimodal large language models (MLLMs) have achieved impressive success in\nquestion-answering tasks, yet their capabilities for spatial understanding are\nless explored. This work investigates a critical question: do existing MLLMs\npossess 3D spatial perception and understanding abilities? Concretely, we make\nthe following contributions in this paper: (i) we introduce VGBench, a\nbenchmark specifically designed to assess MLLMs for visual geometry perception,\ne.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most\ncomprehensive and diverse multimodal spatial understanding benchmark to date,\nintegrating VGBench with relevant data from the other 11 existing datasets.\nThis benchmark comprises 28K samples across various spatial understanding\ntasks, modalities, and QA formats, along with a carefully curated challenging\nsubset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent\nsystem incorporating 9 specialized tools for spatial understanding, supporting\nboth Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive\nevaluations to reveal persistent challenges in spatial reasoning while\ndemonstrating the effectiveness of SpatialAgent. We believe SpatialScore will\noffer valuable insights and serve as a rigorous benchmark for the next\nevolution of MLLMs.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632c7a0d1d303f5f9acf01b8/3GQcp-q-BBj5bX_DRmB6S.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632c7a0d1d303f5f9acf01b8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c7a0d1d303f5f9acf01b8/T010IFuCp6UaOeIyWhbCk.jpeg",
      "fullname": "Haoning Wu",
      "name": "haoningwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16839",
      "authors": [
        {
          "_id": "682fd5758d2fd6fc7cd5c9f7",
          "name": "Shufan Li",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9f8",
          "name": "Konstantinos Kallidromitis",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9f9",
          "name": "Hritik Bansal",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fa",
          "name": "Akash Gokul",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fb",
          "name": "Yusuke Kato",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fc",
          "name": "Kazuki Kozuka",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fd",
          "name": "Jason Kuen",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9fe",
          "name": "Zhe Lin",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5c9ff",
          "name": "Kai-Wei Chang",
          "hidden": false
        },
        {
          "_id": "682fd5758d2fd6fc7cd5ca00",
          "name": "Aditya Grover",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6310531914aa81e1044363ed/85JcNXnpZ6f0XvO4vZ5_s.gif"
      ],
      "publishedAt": "2025-05-22T16:07:12.000Z",
      "submittedOnDailyAt": "2025-05-23T00:30:19.437Z",
      "title": "LaViDa: マルチモーダル理解向けの大規模なDiffusion言語モデル",
      "submittedOnDailyBy": {
        "_id": "6310531914aa81e1044363ed",
        "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
        "isPro": false,
        "fullname": "Shufan Li",
        "user": "jacklishufan",
        "type": "user"
      },
      "summary": "現代のビジョン言語モデル（VLMs）は、複数のタスクを解決することができます。実世界的なシナリオでは、VLMsに望ましい性質として、高速な推論と生成の制御（例：出力を特定のフォーマットに従うように制限する）が望ましいです。しかし、現在の自動復元（AR）VLMsの例であるLLaVAは、これらの面で困難を見せます。離散なディフフェーションモデル（DMs）は、並列的な解確を可能にして高速な推論を実現し、文脈を逆方向に利用して生成の制御を可能にします。DMsは言語だけの設定では効果的ですが、多タイプタスクの可能性は調査不足です。我々は、DMsに基づくVLMsの家族を紹介します。我々は、DMsに視覚エンコーダーを付け加え、結合した部分を多タイプのインストラクションフォローを共に微調節して、LaViDaを構築しました。このような課題に対処するために、LaViDaは、補間マスク、プレフィックスKVキャッシュ、時間ステップシフトなどの新しい技術を採用します。実験は、LaViDaはAR VLMsとの比較で、MMMUなどの多タイプベンチマークで優れた性能を示し、DMsの特有の優れた性能を提供します。コードとモデルは、カメララインバージョンで公開します。",
      "upvotes": 9,
      "discussionId": "682fd5768d2fd6fc7cd5ca3c",
      "projectPage": " https://homepage.jackli.org/projects/lavida/index.html",
      "githubRepo": "https://github.com/jacklishufan/LaViDa",
      "ai_summary": "LaViDa, a family of vision-language models built on discrete diffusion models, offers competitive performance on multimodal benchmarks with advantages in speed, controllability, and bidirectional reasoning.",
      "ai_keywords": [
        "autoregressive (AR) VLMs",
        "discrete diffusion models (DMs)",
        "parallel decoding",
        "bidirectional context",
        "text-infilling",
        "multimodal instruction following",
        "complementary masking",
        "prefix KV cache",
        "timestep shifting",
        "MMMU",
        "COCO captioning",
        "Constrained Poem Completion",
        "Open-LLaVa-Next-8B",
        "CIDEr"
      ]
    },
    "publishedAt": "2025-05-22T12:07:12.000Z",
    "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
    "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6310531914aa81e1044363ed/85JcNXnpZ6f0XvO4vZ5_s.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16839.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310531914aa81e1044363ed",
      "avatarUrl": "/avatars/ae7767e591cb7199ea2f62d2db89fc7f.svg",
      "fullname": "Shufan Li",
      "name": "jacklishufan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14625",
      "authors": [
        {
          "_id": "682fdb318df2d5446a1cf30b",
          "name": "Zhangchen Xu",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30c",
          "name": "Yuetai Li",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30d",
          "name": "Fengqing Jiang",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30e",
          "name": "Bhaskar Ramasubramanian",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf30f",
          "name": "Luyao Niu",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf310",
          "name": "Bill Yuchen Lin",
          "hidden": false
        },
        {
          "_id": "682fdb318df2d5446a1cf311",
          "name": "Radha Poovendran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-20T17:16:44.000Z",
      "submittedOnDailyAt": "2025-05-23T00:50:16.785Z",
      "title": "TinyV: 検証での過漏捨いの減少がLLMのRL向上に貢献します\n  理由",
      "submittedOnDailyBy": {
        "_id": "653df1323479e9ebbe3eb6cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
        "isPro": true,
        "fullname": "Zhangchen Xu",
        "user": "zhangchenxu",
        "type": "user"
      },
      "summary": "強化学習（RL）は、報酬信号でポリシーを最適化して大規模な言語モデル（LLMs）の論理能力を向上させる強力なツールとなっています。しかし、RLの成功は報酬の信頼性に依存し、それらは証明者が提供します。本文では、証明者が正しいモデル出力を間違って拒否する「誤りの否定」（false negatives）の広く散在する問題を暴露し、分析しています。Big-Math-RL-Verifiedデータセットの詳細な研究により、38%以上のモデル生成のレスポンスが証明者が正しい答えを認識できない「誤りの否定」に受けていることが明らかになりました。実験的および理論的にも示し、これらの「誤りの否定」は、情報的な勾配信号をモデルから奪い去り、収束を遅らせるようにRLの訓練を悪化させています。これを軽減するために、簡素なLLMベースの証明者tinyVを提案しています。現在のルールベースの方法を増強し、潜在的な「誤りの否定」を動的に識別し、正しいレスポンスを復元して、より正確な報酬の推定を行います。複数の数学論理ベンチマークでは、TinyVの統合により、合格率が10%以上上昇し、基準と比較して収束を加速します。我々の見つけたことは、証明者の「誤りの否定」を解決する重要性を強調し、LLMsのRLベースの微調節を改善する実用的なアプローチを提供します。我々のコードは、https://github.com/uw-nsl/TinyV に公開されています。",
      "upvotes": 9,
      "discussionId": "682fdb328df2d5446a1cf377",
      "githubRepo": "https://github.com/uw-nsl/TinyV",
      "ai_summary": "TinyV, a lightweight LLM-based verifier, improves RL training of large language models by addressing false negatives from existing rule-based verifiers, enhancing reward accuracy and convergence speed.",
      "ai_keywords": [
        "Reinforcement Learning",
        "large language models",
        "policies",
        "reward signals",
        "verifiers",
        "false negatives",
        "Big-Math-RL-Verified dataset",
        "gradient signals",
        "convergence",
        "TinyV",
        "rule-based methods",
        "math-reasoning benchmarks",
        "pass rates"
      ]
    },
    "publishedAt": "2025-05-20T13:16:44.000Z",
    "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning",
    "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "zhangchenxu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16151",
      "authors": [
        {
          "_id": "682fd61601208348fffaa62e",
          "name": "Hongchen Wei",
          "hidden": false
        },
        {
          "_id": "682fd61601208348fffaa62f",
          "name": "Zhenzhong Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T02:51:12.000Z",
      "submittedOnDailyAt": "2025-05-23T00:34:39.659Z",
      "title": "トレーニング無しの理由論と反省機能のMLLM",
      "submittedOnDailyBy": {
        "_id": "63f96e99ade090bc87bc2f81",
        "avatarUrl": "/avatars/0dd0807e5b2cec011e97c8d6a3c61bae.svg",
        "isPro": false,
        "fullname": "hcwei",
        "user": "hcwei",
        "type": "user"
      },
      "summary": "最近のReasoning LLMs（例：DeepSeek-R1とOpenAI-o1）の進展は、強化学習による魅力的な推理能力を示しています。しかし、これらの能力をMultimodal LLMs（MLLMs）に拡張することは、再訓練の高額コストと高品質な、確認可能な多タイプ的推理データセットの不足により難しくなっています。本論文では、FRANK Modelを紹介します。これは、グラデイント更新や追加のステラバイオンを不要にして、オフショッルのMLLMに推理と反省能力を与える、訓練無料であるANd r1-like MLLMです。私たちの主なアイデアは、MLLMのデコーダー層での観察と理由を分離することです。特に、深いデコーダー層と比較して、浅いデコーダー層はビジュアルトークンに多くのアテンションを配布し、深いデコーダー層は文字的意味に集中していることを観察しました。この観察は、ビジュアルプレトラインされたMLLMと理由に特化されたLLMの重み結合の分階段的アプローチによる結合を促進します。これにより、深いデコーダー層に理由の能力を統合し、浅いデコーダー層でビジュアルグラウンドを保持するためのレイヤーごとのタイヤードerived閉じた形の融合機構を提案します。難しい多タイプ的推理ベンチマーク上での拡張的な実験は、私たちのアプローチの効果を示しています。MMMUベンチマーク上では、私たちのモデル、FRANK-38Bは、69.2の精度を達成し、強力なベースラインInternVL2.5-38Bを+5.3超え、さらには、プロプライターGPT-4oモデルを超えました。私たちのプロジェクトホームページは以下のURLを参照してください：http://iip.whu.edu.cn/frank/index.html",
      "upvotes": 6,
      "discussionId": "682fd61701208348fffaa654",
      "ai_summary": "The FRANK Model enhances multimodal LLMs with reasoning and reflection abilities without retraining, using a hierarchical weight merging approach that merges visual-pretrained and reasoning-specialized models.",
      "ai_keywords": [
        "Reasoning LLMs",
        "Multimodal LLMs (MLLMs)",
        "FRANK Model",
        "reinforcement learning",
        "multimodal reasoning datasets",
        "hierarchical weight merging",
        "Taylor-derived closed-form fusion mechanism",
        "MMMU benchmark",
        "visual tokens",
        "textual semantics",
        "deep decoder layers",
        "shallow decoder layers"
      ]
    },
    "publishedAt": "2025-05-21T22:51:12.000Z",
    "title": "Training-Free Reasoning and Reflection in MLLMs",
    "summary": "Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have\nshowcased impressive reasoning capabilities via reinforcement learning.\nHowever, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by\nthe prohibitive costs of retraining and the scarcity of high-quality,\nverifiable multimodal reasoning datasets. This paper introduces FRANK Model, a\ntraining-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning\nand reflection abilities, without any gradient updates or extra supervision.\nOur key insight is to decouple perception and reasoning across MLLM decoder\nlayers. Specifically, we observe that compared to the deeper decoder layers,\nthe shallow decoder layers allocate more attention to visual tokens, while the\ndeeper decoder layers concentrate on textual semantics. This observation\nmotivates a hierarchical weight merging approach that combines a\nvisual-pretrained MLLM with a reasoning-specialized LLM. To this end, we\npropose a layer-wise, Taylor-derived closed-form fusion mechanism that\nintegrates reasoning capacity into deep decoder layers while preserving visual\ngrounding in shallow decoder layers. Extensive experiments on challenging\nmultimodal reasoning benchmarks demonstrate the effectiveness of our approach.\nOn the MMMU benchmark, our model FRANK-38B achieves an accuracy of 69.2,\noutperforming the strongest baseline InternVL2.5-38B by +5.3, and even\nsurpasses the proprietary GPT-4o model. Our project homepage is at:\nhttp://iip.whu.edu.cn/frank/index.html",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16151.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "63f96e99ade090bc87bc2f81",
      "avatarUrl": "/avatars/0dd0807e5b2cec011e97c8d6a3c61bae.svg",
      "fullname": "hcwei",
      "name": "hcwei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15879",
      "authors": [
        {
          "_id": "682fecc3fd3719dbe6fbb84b",
          "name": "Yue Fan",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84c",
          "name": "Xuehai He",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84d",
          "name": "Diji Yang",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84e",
          "name": "Kaizhi Zheng",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb84f",
          "name": "Ching-Chen Kuo",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb850",
          "name": "Yuting Zheng",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb851",
          "name": "Sravana Jyothi Narayanaraju",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb852",
          "name": "Xinze Guan",
          "hidden": false
        },
        {
          "_id": "682fecc3fd3719dbe6fbb853",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T17:54:49.000Z",
      "submittedOnDailyAt": "2025-05-23T02:05:02.804Z",
      "title": "GRIT: 画像を使って思考するMLLMの教え方",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "最近の研究は、Reinforcement Learning（RL）を使用して理由を述べるモデルを構築する効果性を示しています。しかし、視覚語言タスクの理由を可能にするための進歩が進む中であるが、現在の開放ソースの視覚理由モデルは通常、理由の内容を純粋な自然言語で生成し、明示的な視覚情報の統合を欠けています。これは、明確な理由の連鎖を生成する能力を制限しています。この点に対して、我々は、画像とテキストを用いた基礎的な理由（GRIT）を提案します。GRITは、画像を用いて考えるMLLMの訓練の新しい方法です。GRITは、基礎的な理由パラダイムを導入し、モデルが自然言語と明示的なボウンディングボックス座標を交じり合った理由の連鎖を生成するようにします。これらの座標は、モデルが理由の過程で参照する入力画像の領域を指定します。また、GRITは、GRPOアルゴリズムに基づくGRPO-GRの強化学習アプローチを採用しています。GRPO-GRは、最終的な答えの精度と基礎的な理由の出力の形式に焦点を当てた強力的な報酬を用いています。これは、理由の連鎖の証明および明示的なボウンディングボックスラベルのデータを必要としないようにします。このもとに、GRITは、例外的なデータ効率を達成し、現在のデータセットから20の画像-問題-答えのタプルを必要とします。詳細な評価は、GRITがMLLMを理由の連鎖を生成することを効果的に訓練し、理由と基礎的な理由の能力の成功な統合を示しています。",
      "upvotes": 6,
      "discussionId": "682fecc4fd3719dbe6fbb8ac",
      "projectPage": "https://grounded-reasoning.github.io",
      "githubRepo": "https://github.com/eric-ai-lab/GRIT",
      "ai_summary": "A novel method called GRIT enhances visual reasoning in MLLMs by generating reasoning chains that integrate both natural language and bounding box coordinates, guided by a reinforcement learning approach for high data efficiency.",
      "ai_keywords": [
        "Reinforcement Learning",
        "RL",
        "MLLMs",
        "reasoning chains",
        "interleave natural language",
        "bounding box coordinates",
        "GRPO-GR",
        "GRPO",
        "grounded reasoning output",
        "reasoning and grounding abilities"
      ]
    },
    "publishedAt": "2025-05-21T13:54:49.000Z",
    "title": "GRIT: Teaching MLLMs to Think with Images",
    "summary": "Recent studies have demonstrated the efficacy of using Reinforcement Learning\n(RL) in building reasoning models that articulate chains of thoughts prior to\nproducing final answers. However, despite ongoing advances that aim at enabling\nreasoning for vision-language tasks, existing open-source visual reasoning\nmodels typically generate reasoning content with pure natural language, lacking\nexplicit integration of visual information. This limits their ability to\nproduce clearly articulated and visually grounded reasoning chains. To this\nend, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method\nfor training MLLMs to think with images. GRIT introduces a grounded reasoning\nparadigm, in which models generate reasoning chains that interleave natural\nlanguage and explicit bounding box coordinates. These coordinates point to\nregions of the input image that the model consults during its reasoning\nprocess. Additionally, GRIT is equipped with a reinforcement learning approach,\nGRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused\non the final answer accuracy and format of the grounded reasoning output, which\neliminates the need for data with reasoning chain annotations or explicit\nbounding box labels. As a result, GRIT achieves exceptional data efficiency,\nrequiring as few as 20 image-question-answer triplets from existing datasets.\nComprehensive evaluations demonstrate that GRIT effectively trains MLLMs to\nproduce coherent and visually grounded reasoning chains, showing a successful\nunification of reasoning and grounding abilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16944",
      "authors": [
        {
          "_id": "683023848d2fd6fc7ce9b99a",
          "name": "Yunjia Qi",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99b",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99c",
          "name": "Xiaozhi Wang",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99d",
          "name": "Amy Xin",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99e",
          "name": "Youfeng Liu",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b99f",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b9a0",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "683023848d2fd6fc7ce9b9a1",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:31:10.000Z",
      "submittedOnDailyAt": "2025-05-23T05:58:53.468Z",
      "title": "AGENTIF: アガントシナリオでの大規模言語モデルの指示従いのベンチマーク",
      "submittedOnDailyBy": {
        "_id": "6556cf2cee35f7d8bcf13bb3",
        "avatarUrl": "/avatars/0a945054cb4732c2ee7a5502c42628bf.svg",
        "isPro": false,
        "fullname": "Qi Yunjia",
        "user": "Kikkk",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、実世界的効果的なアガントシステムの開発において高度な能力を示しています。研究の進歩は、実用的な要求を満たすためのLLMベースのアガントの開発を目指していますが、新たな挑戦が生じました。アガントシナリオは、長い指示と複雑な制約を含むシステムプロンプトや詳細なツール規格などを含むものです。アガントシステムの適用において指示の遵守は重要であるが、LLMsがそれらを信頼的に追うことができるかどうかは、まだ調査が不足しています。本論文では、アガントシナリオでのLLMsの指示追跡能力をシステマティックに評価するための最初のベンチマーク「AgentIF」を紹介します。AgentIFは以下の3つの特徴を持ちます：1. リアルティスティックで、50の実世界的アガントシステムから構築されています。2. 長い、平均1,723文字で最大15,630文字を超えます。3. 複雑で、指示における制約の平均数は11.9で、ツール規格や条件制約などの多様な制約タイプを含みます。AgentIFの構築には、50のアガントタスクからの707件の人間アノテーションされた指示を収集しました。各指示について、関連する制約と評価指標をアノテートし、コードベース評価、LLMベース評価、ハイブリッドコード-LLM評価を含む評価指標を含みます。AgentIFを用いて、既存の先進的なLLMsをシステマティックに評価しました。現在のモデルは、特に複雑な制約構造やツール規格の処理において一般的に悪い性能を示しています。さらに、指示の長さとメタ制約について誤り分析と解析的実験を行い、既存のLLMsの失敗モードについての見つけを提供しました。コードおよびデータを公開し、将来の研究にサポートすることを目的としています。",
      "upvotes": 5,
      "discussionId": "683023858d2fd6fc7ce9b9e4",
      "githubRepo": "https://github.com/THU-KEG/AgentIF",
      "ai_summary": "A new benchmark, AgentIF, evaluates Large Language Models' ability to follow complex instructions in realistic agentic scenarios, revealing performance limitations in handling constraints and tool specifications.",
      "ai_keywords": [
        "AgentIF",
        "Large Language Models (LLMs)",
        "agentic applications",
        "system prompts",
        "tool specifications",
        "instruction following",
        "constraint structures",
        "error analysis"
      ]
    },
    "publishedAt": "2025-05-22T13:31:10.000Z",
    "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in\n  Agentic Scenarios",
    "summary": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16944.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6556cf2cee35f7d8bcf13bb3",
      "avatarUrl": "/avatars/0a945054cb4732c2ee7a5502c42628bf.svg",
      "fullname": "Qi Yunjia",
      "name": "Kikkk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16854",
      "authors": [
        {
          "_id": "682fdd8691757629e1d58e16",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "682fdd8691757629e1d58e17",
          "name": "Kevin Qinghong Lin",
          "hidden": false
        },
        {
          "_id": "682fdd8691757629e1d58e18",
          "name": "James Cheng",
          "hidden": false
        },
        {
          "_id": "682fdd8691757629e1d58e19",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T16:13:29.000Z",
      "submittedOnDailyAt": "2025-05-23T01:09:25.197Z",
      "title": "Think or Not? 選択的な理由論を行うリノース学習を用いたビジョン-言語モデル",
      "submittedOnDailyBy": {
        "_id": "64440be5af034cdfd69ca3a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
        "isPro": false,
        "fullname": "Qinghong (Kevin) Lin",
        "user": "KevinQHLin",
        "type": "user"
      },
      "summary": "強化学習（RL）は、視覚言語モデル（VLMs）の理由論を向上させる効果的なトレーニング後の戦略であることが証明されています。グループ相対策最適化（GRPO）は、モデルが回答する前に完全な理由論トレースを生成させるよう促す最近の重要な方法で、トークン使用量と計算コストが増加することにより、理由論の効率化を目指しています。人間のような思考プロセスをモデル化し、簡単な問題に対して理由論を省略し、必要な場合には慎重に考えるように促すことをモデルに学ぶ方法を調べ、VLMsが理由論が必要かどうかを最初に判断することを可能にする方法を調べました。これを実現するために、TON（Think or Not）を提案しました。TONは2段階トレーニング戦略で、(i) 簡単で効果的な「thought dropout」操作を含む規範化微調（SFT）ステップで、理由論トレースがランダムに空の思い出に置き換えられるようなthink-or-not形式を導入し、選択的な理由論の冷やめのスタートを提供します。 (ii) GRPOステップでは、モデルが自由に考えるかどうかを決定することを可能にし、タスクに関連した結果報酬を最大化することを目指します。実験結果は、TONはベージャーGRPOに比べて完了長を90%減少させることができ、性能を失わず、それに加えて改善することができます。3Bと7Bモデルで理由論の難易度の範囲を広げた多様な視覚言語タスクの幅広い評価で、トレーニングが進みながら、モデルが不必要な理由論ステップをスキップすることを進歩的に学ぶことが経験的に明らかにされました。これらの発見は、強化学習アプローチでの人間のような理由論パターンへの道を照らしています。コードは、https://github.com/kokolerk/TON に公開されています。",
      "upvotes": 5,
      "discussionId": "682fdd8791757629e1d58e77",
      "projectPage": "https://huggingface.co/collections/kolerk/ton-682ad9038395c21e228a645b",
      "githubRepo": "https://github.com/kokolerk/TON",
      "ai_summary": "TON, a two-stage training strategy combining supervised fine-tuning with thought dropout and Group Relative Policy Optimization, reduces unnecessary reasoning steps in vision-language models without sacrificing performance.",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "vision-language models (VLMs)",
        "Group Relative Policy Optimization (GRPO)",
        "thought dropout",
        "selective reasoning"
      ]
    },
    "publishedAt": "2025-05-22T12:13:29.000Z",
    "title": "Think or Not? Selective Reasoning via Reinforcement Learning for\n  Vision-Language Models",
    "summary": "Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across diverse vision-language tasks-covering\na range of reasoning difficulties under both 3B and 7B models-consistently\nreveal that the model progressively learns to bypass unnecessary reasoning\nsteps as training advances. These findings shed light on the path toward\nhuman-like reasoning patterns in reinforcement learning approaches. Our code is\navailable at https://github.com/kokolerk/TON.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16854.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64440be5af034cdfd69ca3a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
      "fullname": "Qinghong (Kevin) Lin",
      "name": "KevinQHLin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 31
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16400",
      "authors": [
        {
          "_id": "682fe6644640a9db4d1f31d9",
          "name": "Yang Chen",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31da",
          "user": {
            "_id": "67d75b0117c2acac528f47b6",
            "avatarUrl": "/avatars/619aacd1a619aab64de3499ac3ee2229.svg",
            "isPro": false,
            "fullname": "Zhuolin Yang",
            "user": "zhuoliny",
            "type": "user"
          },
          "name": "Zhuolin Yang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:07:17.257Z",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31db",
          "name": "Zihan Liu",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31dc",
          "name": "Chankyu Lee",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31dd",
          "name": "Peng Xu",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31de",
          "name": "Mohammad Shoeybi",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31df",
          "name": "Bryan Catanzaro",
          "hidden": false
        },
        {
          "_id": "682fe6644640a9db4d1f31e0",
          "user": {
            "_id": "663ee43bfeeb49803537da98",
            "avatarUrl": "/avatars/17c3e9c435cc36fb04b4589e6176a243.svg",
            "isPro": false,
            "fullname": "Wei Ping",
            "user": "wping",
            "type": "user"
          },
          "name": "Wei Ping",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-23T03:07:17.257Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T08:50:47.000Z",
      "submittedOnDailyAt": "2025-05-23T01:38:02.331Z",
      "title": "AceReason-Nemotron: 数学とコードの理解を進めるリノース学習",
      "submittedOnDailyBy": {
        "_id": "62bc9d90e81dfd65cced9316",
        "avatarUrl": "/avatars/05df14cd1fdbc7d6a80d2960a05a94f0.svg",
        "isPro": false,
        "fullname": "Yang Chen",
        "user": "ychenNLP",
        "type": "user"
      },
      "summary": "最近の大規模な強化学習（RL）における論理モデルの開発については、高性能の論理モデルを構築するための訓練ドリティングがまだ不明である。先鋒モデルの実装詳細、例えばDeepSeek-R1のデータカレーティオンステラテジーとRL訓練ドリティングが、通常省略されている。また、最近の研究は、小さなモデルに対しては、転写はRLよりもより効果的であることを示している。本研究では、大規模なRLが強力な小さきや中サイズのモデルの論理能力を大幅に向上させ、最先端の転写ベースのモデルよりも優れた結果を実現できることを示している。RL訓練プロセスを構築し、拡散テストを実施し、簡単で効果的なアプローチを提案している。最初に数学だけのプロンプトで訓練し、次にコードだけのプロンプトで訓練する。特に、数学だけのRLは、強力な転写モデルの数学ベンチマーク（例えば、7B/14BモデルのAIME 2025ではそれぞれ+14.6%/+17.2%）の性能を大幅に向上させ、コード論理タスク（例えば、7B/14BモデルのLiveCodeBenchではそれぞれ+6.8%/+5.8%）も向上させることを見出している。また、拡張されたコードだけのRL訓練は、数学結果の最小限またはデカリングがない状態でコードベンチマークの性能を進めることを示している。高品質で確認可能な答えとテストケースを含む難しいプロンプトを集めるための強力なデータカレーティオンパイプラインを開発し、両分野での証明ベースのRLを可能にしている。最後に、キーの実験のフィードバックを特定し、進歩的に増加するレスポンス長のカレキュラム学習とオンプラインパラメータ更新の安定化効果を含む。RLは、予ち学習および規範化調整（例えば、転写）の際に獲得した基盤的な論理能力を引き出し、モデルの論理能力の限界を超え、以前に解決できなかった問題を解決することを可能にしている。",
      "upvotes": 5,
      "discussionId": "682fe6654640a9db4d1f3229",
      "ai_summary": "Large-scale reinforcement learning enhances reasoning capabilities in small and mid-sized models more effectively than distillation, achieving superior results in both math and code benchmarks.",
      "ai_keywords": [
        "reinforcement learning",
        "RL",
        "DeepSeek-R1",
        "data curation",
        "distillation",
        "math-only prompts",
        "code-only prompts",
        "AIME 2025",
        "LiveCodeBench",
        "curriculum learning",
        "on-policy parameter updates"
      ]
    },
    "publishedAt": "2025-05-22T04:50:47.000Z",
    "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through\n  Reinforcement Learning",
    "summary": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16400.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62bc9d90e81dfd65cced9316",
      "avatarUrl": "/avatars/05df14cd1fdbc7d6a80d2960a05a94f0.svg",
      "fullname": "Yang Chen",
      "name": "ychenNLP",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16192",
      "authors": [
        {
          "_id": "68302db3b73ef22aebdce9c2",
          "name": "Chaoya Jiang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c3",
          "name": "Yongrui Heng",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c4",
          "name": "Wei Ye",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c5",
          "name": "Han Yang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c6",
          "name": "Haiyang Xu",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c7",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c8",
          "name": "Ji Zhang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9c9",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "68302db3b73ef22aebdce9ca",
          "name": "Shikun Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:50:13.000Z",
      "submittedOnDailyAt": "2025-05-23T06:42:34.763Z",
      "title": "VLM-R^3: 領域認識、理由論、と改良のための拡張マルチモデルコンティンウェーザー",
      "submittedOnDailyBy": {
        "_id": "645b10e80c73ea27d13f7aca",
        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
        "isPro": false,
        "fullname": "xuhaiyang",
        "user": "xhyandwyy",
        "type": "user"
      },
      "summary": "最近、理由ベースのMLLMは長文の理由連鎖を生成するために一定程度の成功を達しました。しかし、複雑なタスクでは、画像のビジュアル領域を動的にそして再読み込みする必要がある理由の文脈をビジュアル証拠に正確に当てはめるのに難しくなります。我々は、ビジュアル言語モデルとして領域認識と理由に対応したVLM-R^3（Visual Language Model with Region Recognition and Reasoning）フレームワークを紹介します。このフレームワークは、MLLMに次の能力を提供します。 （i）追加のビジュアル証拠が必要かどうかを判断する、 （ii）画像のどこに理由を当てはめるかを決定する、 （iii）関連するサブ画像の内容を連続した理由連鎖にセインドします。我々の方法の核心は、情報的な領域を選択し、適切な変換（例：切り取り、拡大）を構成し、その結果のビジュアルコンテキストを後続の理由ステップに統合するための領域条件付きの強化ポリシー最適化（R-GRPO）です。このポリシーを起動するために、領域選択と文脈理由のステップレベルのディレクトリベースのビジュアル言語インターライトレジオン（VLIR）コーパスを製作します。このコーパスは、MathVista、ScienceQA、その他のベンチマークでの拡張試験により、VLM-R^3はゼロショットとフィーチャショット設定で新しい状態の最高レベルを設定し、複雑な空間理由や細かなビジュアルカットを要求する問題で特に大きな進歩を示しています。",
      "upvotes": 4,
      "discussionId": "68302db5b73ef22aebdcea32",
      "ai_summary": "VLM-R3 enhances multi-modal language models with region recognition and reasoning, achieving state-of-the-art performance on visual question answering tasks through region-conditioned reinforcement policy optimization.",
      "ai_keywords": [
        "VLM-R3",
        "Region-Conditioned Reinforcement Policy Optimization (R-GRPO)",
        "Visuo-Lingual Interleaved Rationale (VLIR) corpus",
        "MathVista",
        "ScienceQA"
      ]
    },
    "publishedAt": "2025-05-21T23:50:13.000Z",
    "title": "VLM-R^3: Region Recognition, Reasoning, and Refinement for Enhanced\n  Multimodal Chain-of-Thought",
    "summary": "Recently, reasoning-based MLLMs have achieved a degree of success in\ngenerating long-form textual reasoning chains. However, they still struggle\nwith complex tasks that necessitate dynamic and iterative focusing on and\nrevisiting of visual regions to achieve precise grounding of textual reasoning\nin visual evidence. We introduce VLM-R^3 (Visual\nLanguage Model with Region Recognition and\nReasoning), a framework that equips an MLLM with the ability to (i)\ndecide when additional visual evidence is needed, (ii) determine\nwhere to ground within the image, and (iii) seamlessly weave the\nrelevant sub-image content back into an interleaved chain-of-thought. The core\nof our method is Region-Conditioned Reinforcement Policy Optimization\n(R-GRPO), a training paradigm that rewards the model for selecting informative\nregions, formulating appropriate transformations (e.g.\\ crop, zoom), and\nintegrating the resulting visual context into subsequent reasoning steps. To\nbootstrap this policy, we compile a modest but carefully curated Visuo-Lingual\nInterleaved Rationale (VLIR) corpus that provides step-level supervision on\nregion selection and textual justification. Extensive experiments on MathVista,\nScienceQA, and other benchmarks show that VLM-R^3 sets a new state of the art\nin zero-shot and few-shot settings, with the largest gains appearing on\nquestions demanding subtle spatial reasoning or fine-grained visual cue\nextraction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16192.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b10e80c73ea27d13f7aca",
      "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
      "fullname": "xuhaiyang",
      "name": "xhyandwyy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15963",
      "authors": [
        {
          "_id": "682fdcc0087ea62f1663df96",
          "name": "Shujun Liu",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df97",
          "name": "Siyuan Wang",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df98",
          "name": "Zejun Li",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df99",
          "name": "Jianxiang Wang",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df9a",
          "name": "Cheng Zeng",
          "hidden": false
        },
        {
          "_id": "682fdcc0087ea62f1663df9b",
          "name": "Zhongyu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T19:26:09.000Z",
      "submittedOnDailyAt": "2025-05-23T00:57:50.603Z",
      "title": "OViP: オンラインビジョン言語好み学習",
      "submittedOnDailyBy": {
        "_id": "6534c1fc23e0af0e0d7e8ebd",
        "avatarUrl": "/avatars/d65237d82aea19328f28b5a0cc93b271.svg",
        "isPro": false,
        "fullname": "Siyuan Wang",
        "user": "Siyuanyuan",
        "type": "user"
      },
      "summary": "大視覚言語モデル（LVLMs）は、ホライゼーションに脆弱であり、視覚入力との内容が一致しないような内容を生成することが多い。最近のアプローチは、ホライゼーションを軽減するために、多タイプの直接好み最適化（DPO）を進めているが、通常、事前定義されたもしくはランダムに編集された負のサンプルを使用していることで、実際のモデルエラーを反映しないことが多いため、トレーニングの効果が限定されている。本研究では、モデル自身がホライゼーションした出力に基づいて対比的なトレーニングデータを動的に構築するオンライン視覚言語好み学習（OViP）フレームワークを提案しています。サンプリングされたレスポンスペアの語意的差異を識別し、拡散モデルを用いて負の画像を合成することで、OViPは実時間に関連性のあるサブジェクト信号を生成します。このエラー駆動トレーニングは、両方の語文と視覚好みの適応的な調整を可能にします。また、現在の評価プロトコルを改良し、ホライゼーションの抑制と表現性のトレードオフをより良く捉えることができます。ホライゼーションおよび一般的なベンチマークの実験では、OViPはホライゼーションを有効に減少しながら、核心の多タイプ能力を保持することを示しています。",
      "upvotes": 4,
      "discussionId": "682fdcc1087ea62f1663dfcb",
      "ai_summary": "OViP dynamically generates contrastive training data using a diffusion model to reduce hallucinations in large vision-language models while maintaining their multi-modal capabilities.",
      "ai_keywords": [
        "large vision-language models",
        "hallucination",
        "multi-modal Direct Preference Optimization",
        "OViP",
        "diffusion model",
        "contrastive training",
        "semantic differences",
        "failure-driven training"
      ]
    },
    "publishedAt": "2025-05-21T15:26:09.000Z",
    "title": "OViP: Online Vision-Language Preference Learning",
    "summary": "Large vision-language models (LVLMs) remain vulnerable to hallucination,\noften generating content misaligned with visual inputs. While recent approaches\nadvance multi-modal Direct Preference Optimization (DPO) to mitigate\nhallucination, they typically rely on predefined or randomly edited negative\nsamples that fail to reflect actual model errors, limiting training efficacy.\nIn this work, we propose an Online Vision-language Preference Learning (OViP)\nframework that dynamically constructs contrastive training data based on the\nmodel's own hallucinated outputs. By identifying semantic differences between\nsampled response pairs and synthesizing negative images using a diffusion\nmodel, OViP generates more relevant supervision signals in real time. This\nfailure-driven training enables adaptive alignment of both textual and visual\npreferences. Moreover, we refine existing evaluation protocols to better\ncapture the trade-off between hallucination suppression and expressiveness.\nExperiments on hallucination and general benchmarks demonstrate that OViP\neffectively reduces hallucinations while preserving core multi-modal\ncapabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15963.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6534c1fc23e0af0e0d7e8ebd",
      "avatarUrl": "/avatars/d65237d82aea19328f28b5a0cc93b271.svg",
      "fullname": "Siyuan Wang",
      "name": "Siyuanyuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.11711",
      "authors": [
        {
          "_id": "682e0d9540c6417d9962227a",
          "user": {
            "_id": "6255a34d7dacca56ac2b04e4",
            "avatarUrl": "/avatars/3e7751aa6ef7c880e3e36ac995c9a191.svg",
            "isPro": false,
            "fullname": "sagnik mukherjee",
            "user": "sagnikM",
            "type": "user"
          },
          "name": "Sagnik Mukherjee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:17:26.467Z",
          "hidden": false
        },
        {
          "_id": "682e0d9540c6417d9962227b",
          "name": "Lifan Yuan",
          "hidden": false
        },
        {
          "_id": "682e0d9540c6417d9962227c",
          "name": "Dilek Hakkani-Tur",
          "hidden": false
        },
        {
          "_id": "682e0d9540c6417d9962227d",
          "name": "Hao Peng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-16T21:42:28.000Z",
      "submittedOnDailyAt": "2025-05-23T00:08:00.344Z",
      "title": "強化学習では、大規模な言語モデルの小さなサブネットワークを微調節する",
      "submittedOnDailyBy": {
        "_id": "6255a34d7dacca56ac2b04e4",
        "avatarUrl": "/avatars/3e7751aa6ef7c880e3e36ac995c9a191.svg",
        "isPro": false,
        "fullname": "sagnik mukherjee",
        "user": "sagnikM",
        "type": "user"
      },
      "summary": "強化学習（RL）は、大規模言語モデル（LLMs）の下流タスク性能と人間の価値観の一致に大幅な向上を収めます。驚き的なことに、この大きな向上は、パラメータの5％から30％の小さな部分ネットワークの更新によって実現され、残りは効果的に変更されていません。この現象をRLによるパラメータ更新のスパースさと呼びます。このスパースさは、7つの広く使用されているRLアルゴリズム（例：PPO、GRPO、DPO）と、実験で使用された10つの異なるファミリーのLLMsにおいて見られます。このスパースさは、明示的なスパース性促進の正則化やアーキテクチャの制約を含めても発生します。この部分ネットワークのみの微調節で検出精度が回復し、驚くことに、全ネットワークの微調節と近似したモデルが得られます。ランダムシード、学習データ、そしてRLアルゴリズムのネットワークは、予期せぬよりも大幅に重なり合います。分析によると、このスパースさは、一部のレイヤーのみの更新ではなく、近似したスパースな更新がほとんどすべてのパラメータ行列に与えられていることを示しています。また、パラメータ行列のほとんどすべての更新は、近似した全次元更新となり、RLの更新は、それらのパラメータ行列が表現できる近似した全次元を覆う小さなパラメータのセットを更新していることを示しています。このスパースさは、ポリシー分布に近いデータの学習、ポリシーが予め学習されたモデルに近いようにする技術（例：KL正則化、勾配クリッピング）の影響が限定的であることを仮定しています。",
      "upvotes": 4,
      "discussionId": "682e0d9540c6417d996222d7",
      "ai_summary": "Reinforcement learning improves large language models with minimal parameter updates, affecting only a small subnetwork without explicit sparsity techniques.",
      "ai_keywords": [
        "reinforcement learning",
        "large language models",
        "parameter update sparsity",
        "PPO",
        "GRPO",
        "DPO",
        "policy distribution",
        "KL regularization",
        "gradient clipping"
      ]
    },
    "publishedAt": "2025-05-16T17:42:28.000Z",
    "title": "Reinforcement Learning Finetunes Small Subnetworks in Large Language\n  Models",
    "summary": "Reinforcement learning (RL) yields substantial improvements in large language\nmodels (LLMs) downstream task performance and alignment with human values.\nSurprisingly, such large gains result from updating only a small subnetwork\ncomprising just 5 percent to 30 percent of the parameters, with the rest\neffectively unchanged. We refer to this phenomenon as parameter update sparsity\ninduced by RL. It is observed across all 7 widely used RL algorithms (e.g.,\nPPO, GRPO, DPO) and all 10 LLMs from different families in our experiments.\nThis sparsity is intrinsic and occurs without any explicit sparsity promoting\nregularizations or architectural constraints. Finetuning the subnetwork alone\nrecovers the test accuracy, and, remarkably, produces a model nearly identical\nto the one obtained via full finetuning. The subnetworks from different random\nseeds, training data, and even RL algorithms show substantially greater overlap\nthan expected by chance. Our analysis suggests that this sparsity is not due to\nupdating only a subset of layers, instead, nearly all parameter matrices\nreceive similarly sparse updates. Moreover, the updates to almost all parameter\nmatrices are nearly full-rank, suggesting RL updates a small subset of\nparameters that nevertheless span almost the full subspaces that the parameter\nmatrices can represent. We conjecture that the this update sparsity can be\nprimarily attributed to training on data that is near the policy distribution,\ntechniques that encourage the policy to remain close to the pretrained model,\nsuch as the KL regularization and gradient clipping, have limited impact.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11711.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6255a34d7dacca56ac2b04e4",
      "avatarUrl": "/avatars/3e7751aa6ef7c880e3e36ac995c9a191.svg",
      "fullname": "sagnik mukherjee",
      "name": "sagnikM",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16186",
      "authors": [
        {
          "_id": "683005352b4a4d1ce546568b",
          "name": "Kaiwen Zhou",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568c",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568d",
          "name": "Gaowen Liu",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568e",
          "name": "Jayanth Srinivasa",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce546568f",
          "name": "Aosong Feng",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce5465690",
          "name": "Dawn Song",
          "hidden": false
        },
        {
          "_id": "683005352b4a4d1ce5465691",
          "name": "Xin Eric Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/Qu6Z3hnjamfkpCZ9DuRkT.png"
      ],
      "publishedAt": "2025-05-22T03:46:03.000Z",
      "submittedOnDailyAt": "2025-05-23T03:51:00.196Z",
      "title": "SafeKey: 安全理由のアヒアモーメントインサイトを強化する",
      "submittedOnDailyBy": {
        "_id": "64679a226192d39142245e5e",
        "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
        "isPro": false,
        "fullname": "Xin Eric Wang",
        "user": "xw-eric",
        "type": "user"
      },
      "summary": "大型推理模型（LRMs）は、明示的に理由を与えて回答する新しい世代パラダイムを導入し、複雑なタスクにおいて驚異的な向上を収めることができます。しかし、有害なクエリと敵対的攻撃に対しては大きな安全性リスクを伴います。最近の主流のLRMsの安全性向上の試みである、規範制御の微調節（SFT）は安全性性能を向上させることができますが、SFTに沿ったモデルは未見のジャイルブレイクプロンプトに対して拡張性が低いことが見られます。LRMsの生成を詳細に調査し、安全性の「アハハ」モーメントを特定し、これが安全性の理由を活性化し、安全な応答を引き出すことができることを認識しました。この「アハハ」モーメントは、モデルのクエリ理解プロセスに続く`鍵文'で通常に現れ、モデルが安全に進むかどうかを示すことができます。これらの見解に基づいて、SafeKeyを提案し、鍵文での安全性の「アハハ」モーメントをより良く活性化するための2つの補間的な目標を含むものです。1）鍵文前のモデルの内部表現の安全性信号を強化する双重パス安全ヘッド、2）クエリ理解の注意を改善するクエリマスクモデリングの目標です。複数の安全性ベンチマークでの実験は、我々の方法が広範囲のジャイルブレイク攻撃と分布外の有害なプロンプトに対する安全性拡張性を大幅に向上させ、平均的有害性率を9.6%下げ、一般的な能力を維持することを示しました。分析は、SafeKeyが安全性を向上させることをどのように行うかを明らかにし、内部の注意を変形し、隠れ表現の質を向上させることを示しました。",
      "upvotes": 3,
      "discussionId": "683005362b4a4d1ce54656b1",
      "ai_summary": "SafeKey enhances the safety of large reasoning models by focusing on activating a safety aha moment in the key sentence through dual-path safety head and query-mask modeling, thereby improving generalization to harmful prompts.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRMs",
        "explicit reasoning",
        "safety risks",
        "adversarial attacks",
        "supervised fine-tuning",
        "SFT",
        "safety aha moment",
        "key sentence",
        "query understanding process",
        "Dual-Path Safety Head",
        "Query-Mask Modeling",
        "safety generalization",
        "jailbreak attacks",
        "out-of-distribution harmful prompts",
        "average harmfulness rate",
        "hidden representations"
      ]
    },
    "publishedAt": "2025-05-21T23:46:03.000Z",
    "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning",
    "summary": "Large Reasoning Models (LRMs) introduce a new generation paradigm of\nexplicitly reasoning before answering, leading to remarkable improvements in\ncomplex tasks. However, they pose great safety risks against harmful queries\nand adversarial attacks. While recent mainstream safety efforts on LRMs,\nsupervised fine-tuning (SFT), improve safety performance, we find that\nSFT-aligned models struggle to generalize to unseen jailbreak prompts. After\nthorough investigation of LRMs' generation, we identify a safety aha moment\nthat can activate safety reasoning and lead to a safe response. This aha moment\ntypically appears in the `key sentence', which follows models' query\nunderstanding process and can indicate whether the model will proceed safely.\nBased on these insights, we propose SafeKey, including two complementary\nobjectives to better activate the safety aha moment in the key sentence: (1) a\nDual-Path Safety Head to enhance the safety signal in the model's internal\nrepresentations before the key sentence, and (2) a Query-Mask Modeling\nobjective to improve the models' attention on its query understanding, which\nhas important safety hints. Experiments across multiple safety benchmarks\ndemonstrate that our methods significantly improve safety generalization to a\nwide range of jailbreak attacks and out-of-distribution harmful prompts,\nlowering the average harmfulness rate by 9.6\\%, while maintaining general\nabilities. Our analysis reveals how SafeKey enhances safety by reshaping\ninternal attention and improving the quality of hidden representations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64679a226192d39142245e5e/Qu6Z3hnjamfkpCZ9DuRkT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16186.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64679a226192d39142245e5e",
      "avatarUrl": "/avatars/05abee0b6317f100923936ca2099e9eb.svg",
      "fullname": "Xin Eric Wang",
      "name": "xw-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15517",
      "authors": [
        {
          "_id": "682e8bc5b38184d0edcd1671",
          "user": {
            "_id": "66d2af23f040611f7cea1b1b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d2af23f040611f7cea1b1b/vCToO_XgrLD0nfR8rzjZd.jpeg",
            "isPro": false,
            "fullname": "Kaiyuan Eric Chen",
            "user": "keplerccc",
            "type": "user"
          },
          "name": "Kaiyuan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-22T07:16:21.981Z",
          "hidden": false
        },
        {
          "_id": "682e8bc5b38184d0edcd1672",
          "name": "Shuangyu Xie",
          "hidden": false
        },
        {
          "_id": "682e8bc5b38184d0edcd1673",
          "name": "Zehan Ma",
          "hidden": false
        },
        {
          "_id": "682e8bc5b38184d0edcd1674",
          "name": "Ken Goldberg",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66d2af23f040611f7cea1b1b/f5gpZzVb5pLF6IXMt0_xZ.png"
      ],
      "publishedAt": "2025-05-21T13:42:52.000Z",
      "submittedOnDailyAt": "2025-05-23T03:39:07.197Z",
      "title": "Robo2VLM: 野外ロボット操作データセットからのビジュアルクエストアンサー",
      "submittedOnDailyBy": {
        "_id": "66d2af23f040611f7cea1b1b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d2af23f040611f7cea1b1b/vCToO_XgrLD0nfR8rzjZd.jpeg",
        "isPro": false,
        "fullname": "Kaiyuan Eric Chen",
        "user": "keplerccc",
        "type": "user"
      },
      "summary": "ビジョン・ラングワードモデル（VLMs）は、インターネットサイズの画像・テキストコーパスからリアルワールドの知識と一般的な理由論能力を獲得します。それらは、シーン理解とタスクプランニングを増強し、ロボットトラジェットデータに基づいて訓練された視覚モータールポリシーを支援します。私たちは逆のパラダイムを検討しています - 豊富な、本物の、多タイプのロボットトラジェットデータを使用してVLMsを強化し、評価することです。この論文では、VLMs向けのVisual Question Answering（VQA）データセット生成フレームワークであるRobo2VLMを紹介します。人間のテレオプレーティッドロボットトラジェットを与えると、Robo2VLMは非可視的なおよび非説明的なセンサーモデライターからの真実データを取得します。これらのモデライターに基づいて、ロボットトラジェットを操作フェーズの列に分割します。各フェーズでは、Robo2VLMはシーンと相互作用の理解を用いて、ロボット、タスクゴール、および目標物の3D属性を識別します。これらの属性を使用して、空間的、ゴール条件付き、相互作用の理由論のクエストテンプレートに基づいた代表的なVQAクエリーを生成します。Robo2VLM-1という大規模なロボットトラジェットデータからのデータセットをカレーレートします。このデータセットは176kの本物のロボットトラジェットからの463種類のシーンと3,396種類のロボット操作タスクを含み、684,710問題を収録しています。結果は、Robo2VLM-1が空間的および相互作用の理由論のVLM能力をベンチマークし、向上させることを示しています。",
      "upvotes": 2,
      "discussionId": "682e8bc6b38184d0edcd16bc",
      "githubRepo": "https://github.com/KeplerC/robo2VLM",
      "ai_summary": "Robo2VLM, a framework for generating Visual Question Answering datasets using robot trajectory data, enhances and evaluates Vision-Language Models by leveraging sensory modalities and 3D property understanding.",
      "ai_keywords": [
        "Visual-Language Models",
        "Visual Question Answering",
        "VQA",
        "robot trajectory data",
        "end-effector pose",
        "gripper aperture",
        "force sensing",
        "manipulation phases",
        "3D properties",
        "task goal",
        "target object",
        "spatial reasoning",
        "goal-conditioned reasoning",
        "interaction reasoning",
        "Robo2VLM-1"
      ]
    },
    "publishedAt": "2025-05-21T09:42:52.000Z",
    "title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot\n  Manipulation Datasets",
    "summary": "Vision-Language Models (VLMs) acquire real-world knowledge and general\nreasoning ability through Internet-scale image-text corpora. They can augment\nrobotic systems with scene understanding and task planning, and assist\nvisuomotor policies that are trained on robot trajectory data. We explore the\nreverse paradigm - using rich, real, multi-modal robot trajectory data to\nenhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual\nQuestion Answering (VQA) dataset generation framework for VLMs. Given a human\ntele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual\nand non-descriptive sensory modalities, such as end-effector pose, gripper\naperture, and force sensing. Based on these modalities, it segments the robot\ntrajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses\nscene and interaction understanding to identify 3D properties of the robot,\ntask goal, and the target object. The properties are used to generate\nrepresentative VQA queries - images with textural multiple-choice questions -\nbased on spatial, goal-conditioned, and interaction reasoning question\ntemplates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710\nquestions covering 463 distinct scenes and 3,396 robotic manipulation tasks\nfrom 176k real robot trajectories. Results suggest that Robo2VLM-1 can\nbenchmark and improve VLM capabilities in spatial and interaction reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66d2af23f040611f7cea1b1b/f5gpZzVb5pLF6IXMt0_xZ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15517.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d2af23f040611f7cea1b1b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d2af23f040611f7cea1b1b/vCToO_XgrLD0nfR8rzjZd.jpeg",
      "fullname": "Kaiyuan Eric Chen",
      "name": "keplerccc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.17019",
      "authors": [
        {
          "_id": "683041f868160a3c0e525cae",
          "name": "Chenhao Zhang",
          "hidden": false
        },
        {
          "_id": "683041f868160a3c0e525caf",
          "name": "Yazhe Niu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T17:59:53.000Z",
      "submittedOnDailyAt": "2025-05-23T08:11:12.176Z",
      "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework",
      "submittedOnDailyBy": {
        "_id": "647daf00cfca67bc50f9a99f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647daf00cfca67bc50f9a99f/8Snmk1V6lZ8ecdTW3POfm.jpeg",
        "isPro": false,
        "fullname": "Chenhao(Leo) Zhang",
        "user": "MING-ZCH",
        "type": "user"
      },
      "summary": "イメージのメタフォール理解は、AIシステムにとって重要な課題であり、現在のモデルは、イメージ内容に含まれる微妙な文化的、感情的、およびコンテキスト的な意味を理解することが難しい。多モーダル大語言モデル（MLLMs）は、基本的なVisual Question Answer（VQA）タスクでは優れているが、イメージの意味を捉えるタスクには、コンテキスト的な欠陥により基本的な制限がある。人間の認知プロセスにヒントを得て、「Let Androids Dream（LAD）」という新しいフレームワークを提案します。LADは、コンテキスト的な欠陥を解決するための3ステップフレームワークを採用しています：（1）視覚情報を豊富な多レベルの文字表現に変換する、「Perception」、（2）間接的にキャッスルを解決し、コンテキスト的な知識を統合する、「Search」、（3）明示的な理由により、コンテキスト的なイメージの意味を生成する、「Reasoning」。ライトウェイトGPT-4o-miniモデルを使用したフレームワークは、英語のイメージ意味ベンチマークで15+MLLMより最先端の性能を収め、中国のベンチマークでは巨大な向上を収め、Multiple-Choice Question（MCQ）ではGPT-4oと同等の性能を示し、Open-Style Question（OSQ）では36.7%以上の改善を収めました。また、本プロジェクトは、視覚言語推理と人間-AIインタラクションの分野にどのようにAIがイメージの意味を理解することができるかについて新しいヒントを提供し、公開的に利用できます（https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep）。",
      "upvotes": 1,
      "discussionId": "683041f968160a3c0e525cf2",
      "githubRepo": "https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep",
      "ai_summary": "LAD, a three-stage framework using GPT-4o-mini, achieves state-of-the-art performance in image implication understanding and reasoning tasks across different languages and question types.",
      "ai_keywords": [
        "Visual Question Answering (VQA)",
        "image implication understanding",
        "reasoning",
        "multimodal large language models (MLLMs)",
        "cross-domain knowledge",
        "context-alignment",
        "Multiple-Choice Question (MCQ)",
        "Open-Style Question (OSQ)"
      ]
    },
    "publishedAt": "2025-05-22T13:59:53.000Z",
    "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication\n  Understanding and Reasoning Framework",
    "summary": "Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17019.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "647daf00cfca67bc50f9a99f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647daf00cfca67bc50f9a99f/8Snmk1V6lZ8ecdTW3POfm.jpeg",
      "fullname": "Chenhao(Leo) Zhang",
      "name": "MING-ZCH",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.16612",
      "authors": [
        {
          "_id": "6830410d37efd0e958fcaf9c",
          "name": "Daniel Scalena",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcaf9d",
          "user": {
            "_id": "5e7749883d77a72421292d07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
            "isPro": false,
            "fullname": "Gabriele Sarti",
            "user": "gsarti",
            "type": "user"
          },
          "name": "Gabriele Sarti",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T09:35:27.088Z",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcaf9e",
          "name": "Arianna Bisazza",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcaf9f",
          "name": "Elisabetta Fersini",
          "hidden": false
        },
        {
          "_id": "6830410d37efd0e958fcafa0",
          "name": "Malvina Nissim",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/NW7x7zlN8SRZybdNR_ad1.png",
        "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/Y5HbXIQzjGZjHUu99IGGT.png"
      ],
      "publishedAt": "2025-05-22T12:47:16.000Z",
      "submittedOnDailyAt": "2025-05-23T08:07:20.310Z",
      "title": "ステアリング・ラーグ・ラング・ラング・モデルフォラマッションマシントレンストレーションパーソナリゼーション",
      "submittedOnDailyBy": {
        "_id": "5e7749883d77a72421292d07",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
        "isPro": false,
        "fullname": "Gabriele Sarti",
        "user": "gsarti",
        "type": "user"
      },
      "summary": "高品質の大規模言語モデル（LLMs）に基づく機械翻訳システムは、特定のスタイリスティクス制約を反映するポータリゼット翻訳の生産を簡単にしました。しかし、スタイリスティクスの要求が不明確で提示によって伝えやすい場合には、これらのシステムが難しい問題を見落とします。私たちは、低リソース設定でLLM生成のポータリゼット化を個別化するための様々な戦略を検討し、難しい文学翻訳領域を焦点としています。私たちは、個別化スタイルに向けたモデル生成を制御するための提示戦略と推論時の介入を検討し、稀疏自動エンコーダーから抽出される潜在的な概念を利用した比較的フレームワークを提案します。私たちの結果によると、制御は強い個別化を実現しながら翻訳品質を保っています。また、制御効果によるLLM表現の影響を検討し、個別化に関係のあるモデル層が、多スライド提示と私たちの制御方法によって同様に影響されることを見出しました。これは、同じ機構が運営されていることを示しています。",
      "upvotes": 1,
      "discussionId": "6830410d37efd0e958fcafd5",
      "ai_summary": "Strategies including prompting and contrastive frameworks using latent concepts from sparse autoencoders effectively personalize LLM translations in low-resource settings while maintaining quality.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "literary translation",
        "prompting strategies",
        "inference-time interventions",
        "steering",
        "contrastive framework",
        "latent concepts",
        "sparse autoencoders",
        "personalization properties",
        "translation quality",
        "multi-shot prompting"
      ]
    },
    "publishedAt": "2025-05-22T08:47:16.000Z",
    "title": "Steering Large Language Models for Machine Translation Personalization",
    "summary": "High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/NW7x7zlN8SRZybdNR_ad1.png",
      "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/Y5HbXIQzjGZjHUu99IGGT.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16612.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5e7749883d77a72421292d07",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
      "fullname": "Gabriele Sarti",
      "name": "gsarti",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 224
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16170",
      "authors": [
        {
          "_id": "68301340cc0d12cd873342e1",
          "user": {
            "_id": "65c0de12efbb14b39c97f78e",
            "avatarUrl": "/avatars/18485f79427a35bd9e19f71b67c88dce.svg",
            "isPro": false,
            "fullname": "Yuqing Yang",
            "user": "ayyyq",
            "type": "user"
          },
          "name": "Yuqing Yang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T06:35:23.129Z",
          "hidden": false
        },
        {
          "_id": "68301340cc0d12cd873342e2",
          "name": "Robin Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T03:16:00.000Z",
      "submittedOnDailyAt": "2025-05-23T05:10:31.730Z",
      "title": "モデルの信頼性とリトラクションの役割を理解する「LLMsがどのように誤りを認めるか」",
      "submittedOnDailyBy": {
        "_id": "65c0de12efbb14b39c97f78e",
        "avatarUrl": "/avatars/18485f79427a35bd9e19f71b67c88dce.svg",
        "isPro": false,
        "fullname": "Yuqing Yang",
        "user": "ayyyq",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、自分が知っているべきよりも良く知っているときに誤りを承認できるか？本研究では、前に生成した回答の誤りを承認する行動を「後退」と定義し、LLMsがどのような時や理由で後退するかを理解しようとする。まずは、モデルに特有のデータセットを構築し、モデルが自分のパラメータ知識に反する誤りの回答を後退するかを評価する。LLMsは後退の能力を持っているが、それは頻繁に行われない。後退は、モデルの内部的な信念に関連していることを示し、「信じている」と見なされる誤りの回答を後退さないことを示している。導航実験は、内部的な信念が後退を影響する因果関係を示し、モデルが自分の回答を信じないときは、それが回答の確認を試みることを促し、自覚認識の際の注意行動を変えることを示している。最後に、簡単なスーパーバイザー微調調整は、モデルが更に正確な内部的な信念を学ぶことで後退の性能を大幅に向上させることを示している。コードとデータセットは、https://github.com/ayyyq/llm-retraction に公開されている。",
      "upvotes": 1,
      "discussionId": "68301341cc0d12cd87334304",
      "githubRepo": "https://github.com/ayyyq/llm-retraction",
      "ai_summary": "LLMs rarely retract incorrect answers they believe to be factually correct, but supervised fine-tuning can improve their retraction performance by refining their internal beliefs.",
      "ai_keywords": [
        "retraction",
        "model-specific datasets",
        "parametric knowledge",
        "internal belief",
        "self-verification",
        "attention behavior",
        "supervised fine-tuning"
      ]
    },
    "publishedAt": "2025-05-21T23:16:00.000Z",
    "title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model\n  Belief in Retraction",
    "summary": "Can large language models (LLMs) admit their mistakes when they should know\nbetter? In this work, we define the behavior of acknowledging errors in\npreviously generated answers as \"retraction\" and aim to understand when and why\nLLMs choose to retract. We first construct model-specific datasets to evaluate\nwhether a model will retract an incorrect answer that contradicts its own\nparametric knowledge. While LLMs are capable of retraction, they do so only\ninfrequently. We demonstrate that retraction is closely tied to previously\nidentified indicators of models' internal belief: models fail to retract wrong\nanswers that they \"believe\" to be factually correct. Steering experiments\nfurther demonstrate that internal belief causally influences model retraction.\nIn particular, when the model does not believe its answer, this not only\nencourages the model to attempt to verify the answer, but also alters attention\nbehavior during self-verification. Finally, we demonstrate that simple\nsupervised fine-tuning significantly improves retraction performance by helping\nthe model learn more accurate internal beliefs. Code and datasets are available\non https://github.com/ayyyq/llm-retraction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16170.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c0de12efbb14b39c97f78e",
      "avatarUrl": "/avatars/18485f79427a35bd9e19f71b67c88dce.svg",
      "fullname": "Yuqing Yang",
      "name": "ayyyq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16088",
      "authors": [
        {
          "_id": "68302befc518550cc0c2e505",
          "name": "Gagan Bhatia",
          "hidden": false
        },
        {
          "_id": "68302befc518550cc0c2e506",
          "name": "Maxime Peyrard",
          "hidden": false
        },
        {
          "_id": "68302befc518550cc0c2e507",
          "name": "Wei Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-22T00:06:29.000Z",
      "submittedOnDailyAt": "2025-05-23T06:35:22.205Z",
      "title": "データフレームワーク: 時間的推論の隠れたトークナイゼーションバックロック",
      "submittedOnDailyBy": {
        "_id": "60394599033b61166496163b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614366097007-noauth.jpeg",
        "isPro": false,
        "fullname": "Gagan Bhatia",
        "user": "gagan3012",
        "type": "user"
      },
      "summary": "現代のBPEトーキナイザーは、日付を意味のないフラグメントに分割してしまうことが多い、例えば20250312→202,503,12となり、トーキングカウントを膨らませ、強力的な時間的論理に必要な固有の構造を隠す。本研究では、（1）単純で解釈可能な指標を紹介し、これを日付フラグメント比と呼び、これがマルチデジットの日付コンポーネントをどのように忠実に保存しているかを評価する；（2）DateAugBenchをリリースし、6500例を含むシステムを提供し、3つの時間的論理タスクに対応しています：コンテキストベースの日付解決、フォーマット不変性パズル、歴史的、現代的、将来的なレジムの日付演算；（3）層ごとのプローブと因果注意ホップ分析を通じて、大規模言語モデルが月、日、年のコンポーネントのフラグメントを結合して時間的論理を行うエミジングデータ抽象機構を明らかにします。実験結果によると、過度なフラグメント化は、歴史的や将来的な日付のような不傍見な日付の精度低下を10点程度に関連付けることができます。また、モデルの大きさが大きいほど、日付フラグメントを治療するエミジングデータ抽象が速まることがわかります。最後に、LLMがフラグメントを組み立てる推理パスを観察し、通常は人間の解釈と異なる（年→月→日）。",
      "upvotes": 1,
      "discussionId": "68302bf0c518550cc0c2e52c",
      "ai_summary": "New DateAugBench benchmarks reveal how modern tokenizers fragment dates, impacting the accuracy of temporal reasoning in large language models, which compensate for fragmentation more effectively as they grow larger.",
      "ai_keywords": [
        "BPE tokenizers",
        "date fragmentation ratio",
        "DateAugBench",
        "temporal reasoning tasks",
        "context-based date resolution",
        "format-invariance puzzles",
        "date arithmetic",
        "layer-wise probing",
        "causal attention-hop analyses",
        "date-abstraction mechanism",
        "large language models",
        "historical dates",
        "futuristic dates"
      ]
    },
    "publishedAt": "2025-05-21T20:06:29.000Z",
    "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal\n  Reasoning",
    "summary": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 rightarrow 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future\nregimes; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year rightarrow month rightarrow\nday).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16088.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60394599033b61166496163b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1614366097007-noauth.jpeg",
      "fullname": "Gagan Bhatia",
      "name": "gagan3012",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.15865",
      "authors": [
        {
          "_id": "68301a21694f7a58a32919a8",
          "name": "Ingeol Baek",
          "hidden": false
        },
        {
          "_id": "68301a21694f7a58a32919a9",
          "name": "Hwan Chang",
          "hidden": false
        },
        {
          "_id": "68301a21694f7a58a32919aa",
          "name": "Sunghyun Ryu",
          "hidden": false
        },
        {
          "_id": "68301a21694f7a58a32919ab",
          "name": "Hwanhee Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T10:53:41.000Z",
      "submittedOnDailyAt": "2025-05-23T05:19:58.152Z",
      "title": "ビジョン-言語モデルは画像中のテキストをどのように見るか？OCRヘッドの特徴的な役割を明らかにする",
      "submittedOnDailyBy": {
        "_id": "63f6f245e94ed998c46316df",
        "avatarUrl": "/avatars/9c0ec8682d4a85b96d2180602b1bbe6c.svg",
        "isPro": false,
        "fullname": "ingeolbaek",
        "user": "ingeol",
        "type": "user"
      },
      "summary": "ラージービジョン・ラングゲージ・モデル（LVLMs）において顕著な進歩があるにも関わらず、特に解釈性と画像内の文脈情報の検出と解釈に関する間違いが残っています。本論文では、これらのLVLMsを検討し、画像から文字を認識するために責任を持つ特定のヘッドを識別します。これらのヘッドをOCRヘッドと呼びます。これらのヘッドについての発見は以下の通りです：1）少なくとも稀疏ではない：前回の検索ヘッドと異なり、画像から文脈情報を抽出するために多くのヘッドが活性化されます。2）質的に異なる：OCRヘッドは一般的な検索ヘッドと違って、特性についての低い類似性を示します。3）静的に活性化される：これらのヘッドの活性化頻度はOCRスコアと密接に一致します。これらの発見は、Chain-of-Thought（CoT）をOCRおよび通常の検索ヘッドに適用し、これらのヘッドをマスクして下流タスクで検証しました。また、OCRヘッド内でシンクトークンの値を再分配することで性能向上が見られました。これらの洞察力は、LVLMsが画像内の埋め込み文字情報を処理する際に使用する内部機構についてより深い理解を提供します。",
      "upvotes": 1,
      "discussionId": "68301a22694f7a58a32919ef",
      "ai_summary": "The study identifies and analyzes OCR Heads within Large Vision Language Models, revealing their unique activation patterns and roles in interpreting text within images.",
      "ai_keywords": [
        "Large Vision Language Models",
        "LVLMs",
        "Optical Character Recognition Head",
        "OCR Head",
        "retrieval heads",
        "Chain-of-Thought",
        "CoT",
        "sink-token values"
      ]
    },
    "publishedAt": "2025-05-21T06:53:41.000Z",
    "title": "How Do Large Vision-Language Models See Text in Image? Unveiling the\n  Distinctive Role of OCR Heads",
    "summary": "Despite significant advancements in Large Vision Language Models (LVLMs), a\ngap remains, particularly regarding their interpretability and how they locate\nand interpret textual information within images. In this paper, we explore\nvarious LVLMs to identify the specific heads responsible for recognizing text\nfrom images, which we term the Optical Character Recognition Head (OCR Head).\nOur findings regarding these heads are as follows: (1) Less Sparse: Unlike\nprevious retrieval heads, a large number of heads are activated to extract\ntextual information from images. (2) Qualitatively Distinct: OCR heads possess\nproperties that differ significantly from general retrieval heads, exhibiting\nlow similarity in their characteristics. (3) Statically Activated: The\nfrequency of activation for these heads closely aligns with their OCR scores.\nWe validate our findings in downstream tasks by applying Chain-of-Thought (CoT)\nto both OCR and conventional retrieval heads and by masking these heads. We\nalso demonstrate that redistributing sink-token values within the OCR heads\nimproves performance. These insights provide a deeper understanding of the\ninternal mechanisms LVLMs employ in processing embedded textual information in\nimages.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15865.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f6f245e94ed998c46316df",
      "avatarUrl": "/avatars/9c0ec8682d4a85b96d2180602b1bbe6c.svg",
      "fullname": "ingeolbaek",
      "name": "ingeol",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.14395",
      "authors": [
        {
          "_id": "682db49f167398cff979ec27",
          "user": {
            "_id": "654f3cca8cc59d5b490b805b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654f3cca8cc59d5b490b805b/WAZZjN4q8VGp3VS2lAWyy.png",
            "isPro": false,
            "fullname": "Seyoung Song",
            "user": "seyoungsong",
            "type": "user"
          },
          "name": "Seyoung Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-21T12:27:01.867Z",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec28",
          "name": "Seogyeong Jeong",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec29",
          "name": "Eunsu Kim",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2a",
          "name": "Jiho Jin",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2b",
          "name": "Dongkwan Kim",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2c",
          "name": "Jay Shin",
          "hidden": false
        },
        {
          "_id": "682db49f167398cff979ec2d",
          "user": {
            "_id": "60e0251ea9b5d8282481f2b7",
            "avatarUrl": "/avatars/43441373af054a6184c22097bfeb97e4.svg",
            "isPro": false,
            "fullname": "Alice Oh",
            "user": "aliceoh",
            "type": "user"
          },
          "name": "Alice Oh",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-21T19:06:44.285Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/654f3cca8cc59d5b490b805b/cxsyLLbll1-fv-itFCzp_.png"
      ],
      "publishedAt": "2025-05-20T14:14:00.000Z",
      "submittedOnDailyAt": "2025-05-23T07:02:40.832Z",
      "title": "MUG-Eval: 多言語生成の仮想評価フレームワーク\n  どの言語でも可能な能力",
      "submittedOnDailyBy": {
        "_id": "654f3cca8cc59d5b490b805b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654f3cca8cc59d5b490b805b/WAZZjN4q8VGp3VS2lAWyy.png",
        "isPro": false,
        "fullname": "Seyoung Song",
        "user": "seyoungsong",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）のテキスト生成能力の評価は難しい、特に直接評価の方法が少ない低資源言語では特にそうです。私たちは、MUG-Evalという新しいフレームワークを提案します。これは、既存のベンチマークを会話タスクに変換し、LLMsのそれらのタスク上の精度を評価することで、LLMsの多言語生成能力を評価します。特に、これらの会話タスクは、目標言語での効果的なコミュニケーションが必要とするように設計されています。そして、これらのタスクの成功率を、会話生成の成功としての代理として使用します。私たちのアプローチは、2つの主な優点を提供します。1つは、言語特有のNLPツールや注釈されたデータセットに依存しないことで、これらは多くの言語で限られています。2つは、LLMs-as-judgesに依存しないことで、その評価の品質は少数の高資源言語の範囲外で低下します。私たちは、8つのLLMsを30言語の幅で評価し、高資源、中資源、低資源のカテゴリに含むものです。私たちは、MUG-Evalは既存のベンチマークと強烈に相関し、言語やモデルの標準化での比較を可能にします。私たちのフレームワークは、多言語生成の評価に強力で資源効率的な解決策を提供し、数千の言語に拡張可能です。",
      "upvotes": 1,
      "discussionId": "682db4a0167398cff979ec67",
      "ai_summary": "MUG-Eval assesses LLMs' multilingual generation by transforming benchmarks into conversational tasks, offering a language-independent and NLP tool-free method that correlates well with established benchmarks.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "multilingual generation",
        "conversational tasks",
        "task success rate",
        "low-resource languages",
        "high-resource languages",
        "NLP tools",
        "annotated datasets",
        "MUG-Eval",
        "standardized comparisons"
      ]
    },
    "publishedAt": "2025-05-20T10:14:00.000Z",
    "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language",
    "summary": "Evaluating text generation capabilities of large language models (LLMs) is\nchallenging, particularly for low-resource languages where methods for direct\nassessment are scarce. We propose MUG-Eval, a novel framework that evaluates\nLLMs' multilingual generation capabilities by transforming existing benchmarks\ninto conversational tasks and measuring the LLMs' accuracies on those tasks. We\nspecifically designed these conversational tasks to require effective\ncommunication in the target language. Then, we simply use task success rate as\na proxy of successful conversation generation. Our approach offers two key\nadvantages: it is independent of language-specific NLP tools or annotated\ndatasets, which are limited for most languages, and it does not rely on\nLLMs-as-judges, whose evaluation quality degrades outside a few high-resource\nlanguages. We evaluate 8 LLMs across 30 languages spanning high, mid, and\nlow-resource categories, and we find that MUG-Eval correlates strongly with\nestablished benchmarks (r > 0.75) while enabling standardized comparisons\nacross languages and models. Our framework provides a robust and\nresource-efficient solution for evaluating multilingual generation that can be\nextended to thousands of languages.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/654f3cca8cc59d5b490b805b/cxsyLLbll1-fv-itFCzp_.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14395.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654f3cca8cc59d5b490b805b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654f3cca8cc59d5b490b805b/WAZZjN4q8VGp3VS2lAWyy.png",
      "fullname": "Seyoung Song",
      "name": "seyoungsong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.16048",
      "authors": [
        {
          "_id": "68302100d260f25aad14b1c7",
          "user": {
            "_id": "62a1e17591f85abff79c2cdf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
            "isPro": false,
            "fullname": "Philipp Siedler",
            "user": "philippds",
            "type": "user"
          },
          "name": "Philipp D. Siedler",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-23T07:26:08.014Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-21T22:00:20.000Z",
      "submittedOnDailyAt": "2025-05-23T05:49:26.287Z",
      "title": "SPhyR: 物質分布に関する空間物理説明ベンチマーク",
      "submittedOnDailyBy": {
        "_id": "62a1e17591f85abff79c2cdf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
        "isPro": false,
        "fullname": "Philipp Siedler",
        "user": "philippds",
        "type": "user"
      },
      "summary": "ここでは、トポロジー最適化の方法を基にした新しいデータセットを紹介します。トポロジー最適化は、規定された負荷と支点の条件下で設計空間内の最適な材料分布を計算する手法です。このデータセットでは、LLM（大規模言語モデル）に2次元の境界条件、適用された力と支点などの条件を提供し、それによって発生する最適な材料分布について理由をつける必要があります。このデータセットには、部分構造内のマスクされた領域を埋め込むタスクから、完全な材料分布を予測するタスクまで様々なタスクが含まれています。これらのタスクを解決するには、与えられた制約下での力の流れと必要な材料分布を理解する必要があり、シミュレーションツールや明記の物理モデルに依存しないように理由をつける必要があります。これにより、構造の安定性と空間の組織を理由をつけることを課題にします。我々のデータセットは、2次元設定での空間的および物理的な理由能力の評価を目指し、傳統的な言語および論理ベンチマークに対する補間的な視点を提供します。",
      "upvotes": 0,
      "discussionId": "68302101d260f25aad14b215",
      "githubRepo": "https://github.com/philippds/SPhyR",
      "ai_summary": "A dataset benchmarks spatial and physical reasoning of LLMs using topology optimization tasks without simulation tools.",
      "ai_keywords": [
        "Large Language Models (LLM)",
        "topology optimization",
        "material distribution",
        "structural stability",
        "spatial reasoning",
        "physical reasoning",
        "boundary conditions",
        "applied forces",
        "supports"
      ]
    },
    "publishedAt": "2025-05-21T18:00:20.000Z",
    "title": "SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution",
    "summary": "We introduce a novel dataset designed to benchmark the physical and spatial\nreasoning capabilities of Large Language Models (LLM) based on topology\noptimization, a method for computing optimal material distributions within a\ndesign space under prescribed loads and supports. In this dataset, LLMs are\nprovided with conditions such as 2D boundary, applied forces and supports, and\nmust reason about the resulting optimal material distribution. The dataset\nincludes a variety of tasks, ranging from filling in masked regions within\npartial structures to predicting complete material distributions. Solving these\ntasks requires understanding the flow of forces and the required material\ndistribution under given constraints, without access to simulation tools or\nexplicit physical models, challenging models to reason about structural\nstability and spatial organization. Our dataset targets the evaluation of\nspatial and physical reasoning abilities in 2D settings, offering a\ncomplementary perspective to traditional language and logic benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16048.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a1e17591f85abff79c2cdf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654776114657-noauth.jpeg",
      "fullname": "Philipp Siedler",
      "name": "philippds",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]