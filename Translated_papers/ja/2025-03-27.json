[
  {
    "paper": {
      "id": "2503.19757",
      "authors": [
        {
          "_id": "67e3e1e20706b07bfb2713d6",
          "user": {
            "_id": "643fa1c318afbc4d1f3e5e59",
            "avatarUrl": "/avatars/f8f35355902b4cda72e9c6d768322fae.svg",
            "isPro": false,
            "fullname": "Zhi Hou",
            "user": "zhihou",
            "type": "user"
          },
          "name": "Zhi Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:43:49.995Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713d7",
          "user": {
            "_id": "64c9e86a6a26cddbecd9bae2",
            "avatarUrl": "/avatars/61a84989dbbc1898ebcba3236dbed039.svg",
            "isPro": false,
            "fullname": "Tianyi Zhang",
            "user": "TianyiZhang0213",
            "type": "user"
          },
          "name": "Tianyi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:44:21.118Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713d8",
          "name": "Yuwen Xiong",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713d9",
          "user": {
            "_id": "66ab30dfd456f0408b93f27b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ab30dfd456f0408b93f27b/nps4Kni_eOExO5Z92RiiF.jpeg",
            "isPro": false,
            "fullname": "Haonan Duan",
            "user": "robot-haonan",
            "type": "user"
          },
          "name": "Haonan Duan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:04:53.498Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713da",
          "user": {
            "_id": "648a1e44fe11ebd7489c289c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WwMcD9PK0gxIu2I0n0QyD.jpeg",
            "isPro": false,
            "fullname": "Hengjun Pu",
            "user": "MIASANMIA",
            "type": "user"
          },
          "name": "Hengjun Pu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:01.725Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713db",
          "user": {
            "_id": "66b9a5bb32be421cd8538cd6",
            "avatarUrl": "/avatars/a1f7c0fe3ed4741017db713b4e6d47c8.svg",
            "isPro": false,
            "fullname": "Ronglei Tong",
            "user": "TTTTTony",
            "type": "user"
          },
          "name": "Ronglei Tong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:07.673Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713dc",
          "user": {
            "_id": "679165b9c7f527ef3619504e",
            "avatarUrl": "/avatars/f3e6ce5fc3d05c8632d8b208f55c2987.svg",
            "isPro": false,
            "fullname": "Chengyang Zhao",
            "user": "chengyzhao",
            "type": "user"
          },
          "name": "Chengyang Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:13.906Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713dd",
          "user": {
            "_id": "64ae2359179421d320b1694b",
            "avatarUrl": "/avatars/c387a75191005bcaa473091de5383a10.svg",
            "isPro": false,
            "fullname": "Xizhou Zhu",
            "user": "Einsiedler",
            "type": "user"
          },
          "name": "Xizhou Zhu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:20.614Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713de",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713df",
          "user": {
            "_id": "64686f7172d9180d4ac8b4e4",
            "avatarUrl": "/avatars/db67dd6c4b2b41054ddcce5a18ade6f8.svg",
            "isPro": false,
            "fullname": "Jifeng Dai",
            "user": "daijifeng",
            "type": "user"
          },
          "name": "Jifeng Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:27.039Z",
          "hidden": false
        },
        {
          "_id": "67e3e1e20706b07bfb2713e0",
          "user": {
            "_id": "632dab84fdb35759ea6646a0",
            "avatarUrl": "/avatars/857b0b4d115aa5ab2f143e60b0e4edc6.svg",
            "isPro": false,
            "fullname": "Yuntao Chen",
            "user": "YuntaoChen",
            "type": "user"
          },
          "name": "Yuntao Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:45:40.200Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/643fa1c318afbc4d1f3e5e59/T0twf6ibM7Htfq525gf3L.mp4"
      ],
      "publishedAt": "2025-03-25T15:19:56.000Z",
      "submittedOnDailyAt": "2025-03-27T01:27:55.746Z",
      "title": "Dita: 拡大ディフュージョン・トランジャーフォージャニストへの一般主義的な視覚言語アクションポリシー",
      "submittedOnDailyBy": {
        "_id": "643fa1c318afbc4d1f3e5e59",
        "avatarUrl": "/avatars/f8f35355902b4cda72e9c6d768322fae.svg",
        "isPro": false,
        "fullname": "Zhi Hou",
        "user": "zhihou",
        "type": "user"
      },
      "summary": "最近、多様なロボットデータセットで訓練された視覚言語アクションモデルは、限られた領域データでも望ましい一般化能力を示しているが、簡略化されたアクションヘッドを使用して、離散化されたまたは続けているアクションを予測することにより、異なるアクションスペースに適応することができない。ここでは、Transformerアーキテクチャを活用して、一貫した多模態ディフフェレンシャインプロセスを通じて、連続的なアクションシーケンスを直接デノイズするスケーラブルなフレームワークDitaを紹介します。先行の方法と異なり、浅いネットワークで結合されたエンベディングに基づいたデノイズを条件にしないもので、Ditaはインコンテキスト条件付きでデノイズを行い、歴史的な観察からの裸の視覚トークンとデノイズされたアクションの微妙な対応を可能にします。この設計は、アクションの差と環境の微妙な点を明確にモデル化します。Transformerのスケーラブルさとともに、Ditaは撮影角度、観察シーン、タスク、アクションスペースの多様性を含む異なる機体のデータセットを間接的に統合し、その連携は、多様性の対応性を強化し、長期タスクの成功実行を促進します。様々なベンチマークでの評価は、シミュレーションで最先端的な性能または比較的な性能を示します。特に、Ditaは、10ショットの微調節で、そのみ第三人称のカメラ入力を使用して、環境の変化と複雑な長期タスクに対する強固な実世界応用を実現します。このアーキテクチャは、一般的なロボットポリシー学習のための機能的な、軽量級で開放ソースのベースラインを提供します。プロジェクトページ：https://robodita.github.io。",
      "upvotes": 32,
      "discussionId": "67e3e1e40706b07bfb2714cd",
      "projectPage": "https://robodita.github.io",
      "githubRepo": "https://github.com/RoboDita/Dita",
      "ai_keywords": [
        "Transformer architectures",
        "multimodal diffusion process",
        "in-context conditioning",
        "action deltas",
        "environmental nuances",
        "cross-embodiment datasets",
        "long-horizon tasks",
        "10-shot finetuning",
        "third-person camera inputs",
        "generalist robot policy learning"
      ]
    },
    "publishedAt": "2025-03-25T11:19:56.000Z",
    "title": "Dita: Scaling Diffusion Transformer for Generalist\n  Vision-Language-Action Policy",
    "summary": "While recent vision-language-action models trained on diverse robot datasets\nexhibit promising generalization capabilities with limited in-domain data,\ntheir reliance on compact action heads to predict discretized or continuous\nactions constrains adaptability to heterogeneous action spaces. We present\nDita, a scalable framework that leverages Transformer architectures to directly\ndenoise continuous action sequences through a unified multimodal diffusion\nprocess. Departing from prior methods that condition denoising on fused\nembeddings via shallow networks, Dita employs in-context conditioning --\nenabling fine-grained alignment between denoised actions and raw visual tokens\nfrom historical observations. This design explicitly models action deltas and\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\nTransformer's scalability, Dita effectively integrates cross-embodiment\ndatasets across diverse camera perspectives, observation scenes, tasks, and\naction spaces. Such synergy enhances robustness against various variances and\nfacilitates the successful execution of long-horizon tasks. Evaluations across\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\nsimulation. Notably, Dita achieves robust real-world adaptation to\nenvironmental variances and complex long-horizon tasks through 10-shot\nfinetuning, using only third-person camera inputs. The architecture establishes\na versatile, lightweight and open-source baseline for generalist robot policy\nlearning. Project Page: https://robodita.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/643fa1c318afbc4d1f3e5e59/T0twf6ibM7Htfq525gf3L.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19757.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "643fa1c318afbc4d1f3e5e59",
      "avatarUrl": "/avatars/f8f35355902b4cda72e9c6d768322fae.svg",
      "fullname": "Zhi Hou",
      "name": "zhihou",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.19990",
      "authors": [
        {
          "_id": "67e4d3df7e97884ba4150ec0",
          "user": {
            "_id": "662516d72419feed62fb3a0a",
            "avatarUrl": "/avatars/24c4157829e70a4e346aa984885aa5ad.svg",
            "isPro": false,
            "fullname": "Dian",
            "user": "KexianTang",
            "type": "user"
          },
          "name": "Kexian Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:46:12.696Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec1",
          "user": {
            "_id": "64a6ae0e0437599198cf3a98",
            "avatarUrl": "/avatars/6635432cc0589ba12dc170cad6986d6d.svg",
            "isPro": false,
            "fullname": "Junyao Gao",
            "user": "favourisnotyou",
            "type": "user"
          },
          "name": "Junyao Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:15.061Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec2",
          "user": {
            "_id": "63d4b843df01ef426a0f79fb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676365795587-63d4b843df01ef426a0f79fb.jpeg",
            "isPro": false,
            "fullname": "Yanhong Zeng",
            "user": "zengyh1900",
            "type": "user"
          },
          "name": "Yanhong Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:46:24.775Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec3",
          "user": {
            "_id": "63ee1379190ddd6214efd73a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
            "isPro": false,
            "fullname": "HAODONG DUAN",
            "user": "KennyUTC",
            "type": "user"
          },
          "name": "Haodong Duan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:00:33.733Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec4",
          "name": "Yanan Sun",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec5",
          "user": {
            "_id": "62fb2a9dc95d426ff8f74c8d",
            "avatarUrl": "/avatars/25c1a68ee7b7d0cc7e9f56bde37f4914.svg",
            "isPro": false,
            "fullname": "Zhening Xing",
            "user": "Leoxing",
            "type": "user"
          },
          "name": "Zhening Xing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:00:36.341Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec6",
          "user": {
            "_id": "6385f8598b5acae8d24caf16",
            "avatarUrl": "/avatars/9d261f95d24e882157b987b8827098be.svg",
            "isPro": false,
            "fullname": "liuwenran",
            "user": "lwrshi1965",
            "type": "user"
          },
          "name": "Wenran Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:46:45.866Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec7",
          "user": {
            "_id": "6414230a0fcefcf72e5085dd",
            "avatarUrl": "/avatars/3a38dc8c84b0f27af846184d1c19f6ef.svg",
            "isPro": false,
            "fullname": "Kaifeng Lyu",
            "user": "vfleaking",
            "type": "user"
          },
          "name": "Kaifeng Lyu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:46:52.341Z",
          "hidden": false
        },
        {
          "_id": "67e4d3df7e97884ba4150ec8",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/kjq_V0to2uTR2RSR3TyfV.png"
      ],
      "publishedAt": "2025-03-25T18:21:07.000Z",
      "submittedOnDailyAt": "2025-03-27T03:00:01.698Z",
      "title": "LEGO-Puzzles: マルチステップ空間的論理におけるMLLMの性能",
      "submittedOnDailyBy": {
        "_id": "63ee1379190ddd6214efd73a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
        "isPro": false,
        "fullname": "HAODONG DUAN",
        "user": "KennyUTC",
        "type": "user"
      },
      "summary": "多ステップ空間的な理由論は、複数の順序的ステップにおける空間関係の理解と理由論を含むものであり、ロボット操作、自動ナビゲーション、自動装配などの複雑な実世界のアプリケーションを解決するために重要である。現在のMultimodal Large Language Models (MLLMs)がこの基本的な能力をどの程度習得しているかを評価するために、LEGO-Puzzlesというスケーラブルなベンチマークを介して、MLLMsの空間的理解と順序的な理由論を評価するためのLEGOベースのタスクを用いて設計しました。LEGO-Puzzlesは1,100点のよく選ばれた視覚的な問答（VQA）サンプルを含み、11種類の異なるタスクを範囲に広げ、基本的な空間的理解から複雑な多ステップの理由論まで幅広く取り扱います。LEGO-Puzzlesに基づいて、最先端のMLLMsを評価し、空間的な理由論能力における重大な制限を明らかにしました：最も強力なMLLMsでもテストケースの半分を答えることができることに限り、人間参加者は90%以上の正確性を達成します。VQAタスクのほかに、MLLMsが装配イラストをフォローしてLEGO画像を生成する能力も評価しました。実験により、その他のMLLMsは入力画像を再現したかい、完全に関係ない出力を生成したことが見られました。全体として、LEGO-Puzzlesは現在のMLLMsの空間的理解と順序的な理由論能力における重要な欠点を明らかにし、多モデル空間的な理由論の進歩の必要性を強調します。",
      "upvotes": 22,
      "discussionId": "67e4d3e07e97884ba4150f2b",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "LEGO-Puzzles",
        "visual question-answering (VQA)",
        "spatial understanding",
        "sequential reasoning",
        "Gemini-2.0-Flash",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-03-25T14:21:07.000Z",
    "title": "LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?",
    "summary": "Multi-step spatial reasoning entails understanding and reasoning about\nspatial relationships across multiple sequential steps, which is crucial for\ntackling complex real-world applications, such as robotic manipulation,\nautonomous navigation, and automated assembly. To assess how well current\nMultimodal Large Language Models (MLLMs) have acquired this fundamental\ncapability, we introduce LEGO-Puzzles, a scalable benchmark designed\nto evaluate both spatial understanding and sequential\nreasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100\ncarefully curated visual question-answering (VQA) samples spanning 11 distinct\ntasks, ranging from basic spatial understanding to complex multi-step\nreasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of\nstate-of-the-art MLLMs and uncover significant limitations in their spatial\nreasoning capabilities: even the most powerful MLLMs can answer only about half\nof the test cases, whereas human participants achieve over 90\\% accuracy. In\naddition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images\nfollowing assembly illustrations. Our experiments show that only\nGemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these\ninstructions, while other MLLMs either replicate the input image or generate\ncompletely irrelevant outputs. Overall, LEGO-Puzzles exposes critical\ndeficiencies in existing MLLMs' spatial understanding and sequential reasoning\ncapabilities, and underscores the need for further advancements in multimodal\nspatial reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63ee1379190ddd6214efd73a/kjq_V0to2uTR2RSR3TyfV.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ee1379190ddd6214efd73a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png",
      "fullname": "HAODONG DUAN",
      "name": "KennyUTC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20240",
      "authors": [
        {
          "_id": "67e4ae6787c92169aa3caa74",
          "user": {
            "_id": "66435efdc26b490acc85079b",
            "avatarUrl": "/avatars/16e0ee25734516d4295abe0fcc0e26a9.svg",
            "isPro": false,
            "fullname": "Prin Phunyaphibarn",
            "user": "prinphunya",
            "type": "user"
          },
          "name": "Prin Phunyaphibarn",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:34.055Z",
          "hidden": false
        },
        {
          "_id": "67e4ae6787c92169aa3caa75",
          "user": {
            "_id": "6342796a0875f2c99cfd313b",
            "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
            "isPro": false,
            "fullname": "Yuseung \"Phillip\" Lee",
            "user": "phillipinseoul",
            "type": "user"
          },
          "name": "Phillip Y. Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:32.144Z",
          "hidden": false
        },
        {
          "_id": "67e4ae6787c92169aa3caa76",
          "name": "Jaihoon Kim",
          "hidden": false
        },
        {
          "_id": "67e4ae6787c92169aa3caa77",
          "user": {
            "_id": "631f432b5ba8c026340a7890",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631f432b5ba8c026340a7890/9PK7A_TRMpugwYjCsNBf1.jpeg",
            "isPro": false,
            "fullname": "Minhyuk Sung",
            "user": "Minhyuk",
            "type": "user"
          },
          "name": "Minhyuk Sung",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:47:11.255Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T05:11:38.000Z",
      "submittedOnDailyAt": "2025-03-27T00:22:30.335Z",
      "title": "無条件先頭モデルが重要です！ 調整されたディフュージョンモデルの条件付き生成を改善する",
      "submittedOnDailyBy": {
        "_id": "6342796a0875f2c99cfd313b",
        "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
        "isPro": false,
        "fullname": "Yuseung \"Phillip\" Lee",
        "user": "phillipinseoul",
        "type": "user"
      },
      "summary": "クラスフレードガイド（CFG）は、条件付きディフフォーションモデルの訓練において基本的な技術です。CFGに基づく訓練の一般的な実践は、条件付きおよび非条件付きノイズ予測を学習するために1つのネットワークを使用し、条件付き学習のための小さなドロップアウト率を使用することです。しかし、私たちは、学習中の有限バンダイッド内での非条件付きノイズとの共に学習することにより、非条件付きケースのプロイアリズムが悪くなることを見出しました。より重要なことに、これらの悪い非条件付きノイズ予測が条件付き生成の品質を低下させる重大な原因となります。多数のCFGに基づく条件付きモデルが、ベースモデルの微調節で学習されていることをヒントに、私たちは最初に、CFGでの非条件付きノイズをベースモデルが予測したものに置き換えることで条件付き生成を大幅に向上させることができることを示します。また、微調節されたモデルが学習されたディフフォーションモデル以外のモデルを非条件付きノイズ置換に使用することができることを示します。画像と映像生成の両方において、Zero-1-to-3、Versatile Diffusion、DiT、DynamiCrafter、InstructPix2Pixなどの様々なCFGに基づく条件付きモデルで実験的にこの主張を検証しました。",
      "upvotes": 17,
      "discussionId": "67e4ae6a87c92169aa3cabc2",
      "ai_keywords": [
        "Classifier-Free Guidance (CFG)",
        "conditional diffusion models",
        "noise prediction",
        "dropout rate",
        "priors",
        "fine-tuning",
        "base model",
        "unconditional generation",
        "variance scaling",
        "Zero-1-to-3",
        "Versatile Diffusion",
        "DiT",
        "DynamiCrafter",
        "InstructPix2Pix"
      ]
    },
    "publishedAt": "2025-03-26T01:11:38.000Z",
    "title": "Unconditional Priors Matter! Improving Conditional Generation of\n  Fine-Tuned Diffusion Models",
    "summary": "Classifier-Free Guidance (CFG) is a fundamental technique in training\nconditional diffusion models. The common practice for CFG-based training is to\nuse a single network to learn both conditional and unconditional noise\nprediction, with a small dropout rate for conditioning. However, we observe\nthat the joint learning of unconditional noise with limited bandwidth in\ntraining results in poor priors for the unconditional case. More importantly,\nthese poor unconditional noise predictions become a serious reason for\ndegrading the quality of conditional generation. Inspired by the fact that most\nCFG-based conditional models are trained by fine-tuning a base model with\nbetter unconditional generation, we first show that simply replacing the\nunconditional noise in CFG with that predicted by the base model can\nsignificantly improve conditional generation. Furthermore, we show that a\ndiffusion model other than the one the fine-tuned model was trained on can be\nused for unconditional noise replacement. We experimentally verify our claim\nwith a range of CFG-based conditional models for both image and video\ngeneration, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and\nInstructPix2Pix.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20240.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6342796a0875f2c99cfd313b",
      "avatarUrl": "/avatars/98575092404c4197b20c929a6499a015.svg",
      "fullname": "Yuseung \"Phillip\" Lee",
      "name": "phillipinseoul",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20215",
      "authors": [
        {
          "_id": "67e4f2507e97884ba4205660",
          "name": "Jin Xu",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205661",
          "user": {
            "_id": "661e577cbac5d981f883b743",
            "avatarUrl": "/avatars/95e55e9707a6b55594c264081202d7f4.svg",
            "isPro": false,
            "fullname": "GuoZhifang",
            "user": "ZhifangGuo",
            "type": "user"
          },
          "name": "Zhifang Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:52:32.600Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205662",
          "user": {
            "_id": "6594f06ac04427eb38444bce",
            "avatarUrl": "/avatars/b13fbf589b25eff038deb3fa12d95871.svg",
            "isPro": false,
            "fullname": "Jinzheng He",
            "user": "jinzheng-he",
            "type": "user"
          },
          "name": "Jinzheng He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:52:48.707Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205663",
          "name": "Hangrui Hu",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205664",
          "name": "Ting He",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205665",
          "user": {
            "_id": "63451cf0a05b51f7ded25505",
            "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg",
            "isPro": false,
            "fullname": "shuai bai",
            "user": "bluelike",
            "type": "user"
          },
          "name": "Shuai Bai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:02.818Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205666",
          "user": {
            "_id": "6461d675681b2e19b6acb5a5",
            "avatarUrl": "/avatars/0d95d65d30f6672ec09dc92155324d7f.svg",
            "isPro": false,
            "fullname": "Keqin Chen",
            "user": "chenkq",
            "type": "user"
          },
          "name": "Keqin Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:19.770Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205667",
          "user": {
            "_id": "649a3ba9342f14148357c367",
            "avatarUrl": "/avatars/81a769fa38b7384f382ff3cc10d6d624.svg",
            "isPro": false,
            "fullname": "Jialin Wang",
            "user": "JialinWang",
            "type": "user"
          },
          "name": "Jialin Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:26.467Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205668",
          "name": "Yang Fan",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba4205669",
          "user": {
            "_id": "6712930f0fac3235c56edf5b",
            "avatarUrl": "/avatars/cafe7cb56ce7c3b2572f5f2d0b89357a.svg",
            "isPro": false,
            "fullname": "kai dang",
            "user": "1vk5i",
            "type": "user"
          },
          "name": "Kai Dang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:33.105Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566a",
          "name": "Bin Zhang",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566b",
          "name": "Xiong Wang",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566c",
          "user": {
            "_id": "62c6a751a71b40cf26f359a8",
            "avatarUrl": "/avatars/49abd2e71946035452c316d703baaac6.svg",
            "isPro": false,
            "fullname": "Yunfei Chu",
            "user": "faychu",
            "type": "user"
          },
          "name": "Yunfei Chu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:51.503Z",
          "hidden": false
        },
        {
          "_id": "67e4f2507e97884ba420566d",
          "user": {
            "_id": "620760a26e3b7210c2ff1943",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
            "isPro": false,
            "fullname": "Junyang Lin",
            "user": "JustinLin610",
            "type": "user"
          },
          "name": "Junyang Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:53:44.277Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/659e513ea9bc1f60189ac148/3pDelIehF3CWGPS0jrZvN.png"
      ],
      "publishedAt": "2025-03-26T04:17:55.000Z",
      "submittedOnDailyAt": "2025-03-27T05:14:09.555Z",
      "title": "Qwen2.5-Omni 技術報告",
      "submittedOnDailyBy": {
        "_id": "659e513ea9bc1f60189ac148",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659e513ea9bc1f60189ac148/DBDDqjGTQ0SvjWyuYu7py.jpeg",
        "isPro": false,
        "fullname": "YuanjunLv",
        "user": "Bakerbunker",
        "type": "user"
      },
      "summary": "このレポートでは、Qwen2.5-Omni、一つの端末から端末までの多様性を認識するモデルを紹介します。このモデルは、テキスト、画像、音声、映像などの多様なモデルを同時に認識し、ストリーミングモードでテキストと自然な音声レスポンスを生成することができます。多様性の情報入力をストリーミングさせるために、音声エンコーダと視覚エンコーダはブロックごとの処理アプローチを使用しています。映像入力のタイムスタンプと音声の同期を実現するために、音声と映像を交差して順番に配置し、TMRoPE（Time-aligned Multimodal RoPE）という新しい位置埋めアプローチを提案しています。テキストと音声を同時に生成し、それらのモデル間の干渉を避けるために、Thinker-Talkerアーキテクチャを提案しています。このフレームワークでは、Thinkerは大規模な言語モデルとしてテキスト生成を担当し、Talkerはデュアルトラック自動増加モデルであり、Thinkerからの隠れ表現を直接利用して音声トークンを出力することができます。ThinkerとTalkerモデルは、端末から端末までの学習と推論を可能にしています。ストリーミングモードで音声トークンを解碼するために、スライディングウィンドウDiTを導入し、受容領域を制限し、初期パッケージデライ延びを減らすことを目的としています。Qwen2.5-Omniは、同じサイズのQwen2.5-VLと比較的に等しく、Qwen2-Audioを上回る性能を示しています。また、Qwen2.5-Omniは、Omni-Benchなどの多様性ベンチマークで最先端の性能を達成しています。特に、MMLUやGSM8Kなどのベンチマークによる証拠により、Qwen2.5-Omniの端末から端末までの音声指示従行の性能は、テキスト入力と同様の性能を示しています。音声生成においては、Qwen2.5-OmniのストリーミングTalkerは、現在のストリーミングや非ストリーミングアlternativesを上回る強固性と自然性において、多くのものを上回っています。",
      "upvotes": 17,
      "discussionId": "67e4f2527e97884ba42056df",
      "projectPage": "https://qwenlm.github.io/blog/qwen2.5-omni/",
      "githubRepo": "https://github.com/QwenLM/Qwen2.5-Omni",
      "ai_keywords": [
        "multimodal model",
        "block-wise processing",
        "interleaved manner",
        "TMRoPE (Time-aligned Multimodal RoPE)",
        "position embedding",
        "Thinker-Talker architecture",
        "large language model",
        "dual-track autoregressive model",
        "end-to-end manner",
        "sliding-window DiT",
        "receptive field",
        "initial package delay",
        "Omni-Bench",
        "MMLU",
        "GSM8K",
        "end-to-end speech instruction following"
      ]
    },
    "publishedAt": "2025-03-26T00:17:55.000Z",
    "title": "Qwen2.5-Omni Technical Report",
    "summary": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model\ndesigned to perceive diverse modalities, including text, images, audio, and\nvideo, while simultaneously generating text and natural speech responses in a\nstreaming manner. To enable the streaming of multimodal information inputs,\nboth audio and visual encoders utilize a block-wise processing approach. To\nsynchronize the timestamps of video inputs with audio, we organize the audio\nand video sequentially in an interleaved manner and propose a novel position\nembedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently\ngenerate text and speech while avoiding interference between the two\nmodalities, we propose Thinker-Talker architecture. In this framework,\nThinker functions as a large language model tasked with text generation, while\nTalker is a dual-track autoregressive model that directly utilizes the hidden\nrepresentations from the Thinker to produce audio tokens as output. Both the\nThinker and Talker models are designed to be trained and inferred in an\nend-to-end manner. For decoding audio tokens in a streaming manner, we\nintroduce a sliding-window DiT that restricts the receptive field, aiming to\nreduce the initial package delay. Qwen2.5-Omni is comparable with the similarly\nsized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni\nachieves state-of-the-art performance on multimodal benchmarks like Omni-Bench.\nNotably, Qwen2.5-Omni's performance in end-to-end speech instruction following\nis comparable to its capabilities with text inputs, as evidenced by benchmarks\nsuch as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming\nTalker outperforms most existing streaming and non-streaming alternatives in\nrobustness and naturalness.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/659e513ea9bc1f60189ac148/3pDelIehF3CWGPS0jrZvN.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20215.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "659e513ea9bc1f60189ac148",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659e513ea9bc1f60189ac148/DBDDqjGTQ0SvjWyuYu7py.jpeg",
      "fullname": "YuanjunLv",
      "name": "Bakerbunker",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20314",
      "authors": [
        {
          "_id": "67e4b65a080a33e3955b340c",
          "name": "WanTeam",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b340e",
          "user": {
            "_id": "63f1f1727ddf724fbcbc9c7e",
            "avatarUrl": "/avatars/9e0516d9b1036c23c78f313c79872f55.svg",
            "isPro": false,
            "fullname": "Ang Wang",
            "user": "ang-annng",
            "type": "user"
          },
          "name": "Ang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:47:28.144Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b340f",
          "user": {
            "_id": "64755ff5a51711a3b59118af",
            "avatarUrl": "/avatars/2e899088902db94e785107c3ec2abe85.svg",
            "isPro": false,
            "fullname": "Baole Ai",
            "user": "baoleai",
            "type": "user"
          },
          "name": "Baole Ai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:47:49.260Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3410",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3411",
          "user": {
            "_id": "6458970cab9a44f42f620a80",
            "avatarUrl": "/avatars/f9779b0621c931f922440fec95342444.svg",
            "isPro": false,
            "fullname": "chaojie mao",
            "user": "chaojiemao",
            "type": "user"
          },
          "name": "Chaojie Mao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:02.730Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3412",
          "user": {
            "_id": "66592c72f4124d863fd55574",
            "avatarUrl": "/avatars/98f0d5e6ba3728e8a1164aa5188a3298.svg",
            "isPro": false,
            "fullname": "Chenwei Xie",
            "user": "chenweix7",
            "type": "user"
          },
          "name": "Chen-Wei Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:10.933Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3413",
          "name": "Di Chen",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3414",
          "name": "Feiwu Yu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3415",
          "user": {
            "_id": "67a73767282aa06f7bcaeeb1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/J28OVrPhD0xYulWMgICmW.png",
            "isPro": false,
            "fullname": "Haiming Zhao",
            "user": "HermanZ",
            "type": "user"
          },
          "name": "Haiming Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:26.135Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3416",
          "user": {
            "_id": "651441e92c5da979038df5ee",
            "avatarUrl": "/avatars/85cdafcccb522eced50dc9e4770b630a.svg",
            "isPro": false,
            "fullname": "Jianxiao Yang",
            "user": "Jianxiao0203",
            "type": "user"
          },
          "name": "Jianxiao Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:33.714Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3417",
          "user": {
            "_id": "6274b866f978441a764b30f6",
            "avatarUrl": "/avatars/953b1ff82f63e371a7358a85d68304cd.svg",
            "isPro": false,
            "fullname": "jianyuan.zengjy",
            "user": "filwsyl",
            "type": "user"
          },
          "name": "Jianyuan Zeng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:40.108Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3418",
          "name": "Jiayu Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3419",
          "user": {
            "_id": "66f0e0262aee3cb7e981bbac",
            "avatarUrl": "/avatars/f8f1e70469b5e047dc6e0e9dec6c5bc1.svg",
            "isPro": false,
            "fullname": "Jingfeng Zhang",
            "user": "jingfengzhang",
            "type": "user"
          },
          "name": "Jingfeng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:48:58.316Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341a",
          "user": {
            "_id": "602f88f5e8149a962412a667",
            "avatarUrl": "/avatars/b78f0e583df8e5d5e3365934fe5f4900.svg",
            "isPro": false,
            "fullname": "Zhou",
            "user": "Jingren",
            "type": "user"
          },
          "name": "Jingren Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:49:09.146Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341b",
          "user": {
            "_id": "627c93b2bec91eb1720b8bad",
            "avatarUrl": "/avatars/89c31c71aa5027543ed5be0471fe1109.svg",
            "isPro": false,
            "fullname": "Jinkai Wang",
            "user": "zwsjink",
            "type": "user"
          },
          "name": "Jinkai Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:49:15.680Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341c",
          "user": {
            "_id": "6465941d0e6c7618f615675b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6465941d0e6c7618f615675b/W4EHqlCucz_bojFLFEeV_.jpeg",
            "isPro": false,
            "fullname": "Jixuan Chen",
            "user": "Mayome",
            "type": "user"
          },
          "name": "Jixuan Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:49:25.437Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341d",
          "name": "Kai Zhu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341e",
          "name": "Kang Zhao",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b341f",
          "name": "Keyu Yan",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3420",
          "name": "Lianghua Huang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3421",
          "user": {
            "_id": "63b4ec15103617b0a5b3101e",
            "avatarUrl": "/avatars/e6faad833b31ad5d892faccf621e7a34.svg",
            "isPro": false,
            "fullname": "Mengyang Feng",
            "user": "archerfmy",
            "type": "user"
          },
          "name": "Mengyang Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:50:01.919Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3422",
          "user": {
            "_id": "66eae63f533fd44f8a8ca60b",
            "avatarUrl": "/avatars/38cecb4c80cc7a6e63028fcb572e3a22.svg",
            "isPro": false,
            "fullname": "Zhang Ningyi",
            "user": "ZhangNy",
            "type": "user"
          },
          "name": "Ningyi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:50:13.628Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3423",
          "name": "Pandeng Li",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3424",
          "user": {
            "_id": "64c5182771947b03ffee931c",
            "avatarUrl": "/avatars/478f4e06ac1bced092dde0f11963a975.svg",
            "isPro": false,
            "fullname": "Wupingyu",
            "user": "wpy1999",
            "type": "user"
          },
          "name": "Pingyu Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:50:38.625Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3425",
          "user": {
            "_id": "642e3bcb958faf258a40e89c",
            "avatarUrl": "/avatars/213501def37dc53032cee17e37fcc4c1.svg",
            "isPro": false,
            "fullname": "Ruihang Chu",
            "user": "Ruihang",
            "type": "user"
          },
          "name": "Ruihang Chu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:50:46.771Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3426",
          "user": {
            "_id": "6790e2b74932687e24024b4a",
            "avatarUrl": "/avatars/951f55648490e1f520483a3e425621dd.svg",
            "isPro": false,
            "fullname": "Ruili",
            "user": "RuiliFeng",
            "type": "user"
          },
          "name": "Ruili Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:03.191Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3427",
          "name": "Shiwei Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3428",
          "user": {
            "_id": "62bbf42ac9633b01802a6d45",
            "avatarUrl": "/avatars/0fee1462d228f5e7f22d5c240900a3ad.svg",
            "isPro": false,
            "fullname": "Siyang Sun",
            "user": "sunsiyang",
            "type": "user"
          },
          "name": "Siyang Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:10.461Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3429",
          "name": "Tao Fang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342a",
          "name": "Tianxing Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342b",
          "name": "Tianyi Gui",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342c",
          "name": "Tingyu Weng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342d",
          "name": "Tong Shen",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342e",
          "name": "Wei Lin",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b342f",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3430",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3431",
          "user": {
            "_id": "623c6253389748c9f72ca287",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654828369523-623c6253389748c9f72ca287.jpeg",
            "isPro": false,
            "fullname": "wenmeng zhou",
            "user": "wenmengzhou",
            "type": "user"
          },
          "name": "Wenmeng Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:38.310Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3432",
          "user": {
            "_id": "644240b1251730a7ee243ef3",
            "avatarUrl": "/avatars/c4ca99739e2b6f3d3d0ca83ecc54766a.svg",
            "isPro": false,
            "fullname": "wente.wang",
            "user": "shiftc",
            "type": "user"
          },
          "name": "Wente Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:46.041Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3433",
          "user": {
            "_id": "64af91eb5c17fe25cfcbebc3",
            "avatarUrl": "/avatars/ffc6e7b6a40300e05e66f544264dddbc.svg",
            "isPro": false,
            "fullname": "Wenting Shen",
            "user": "SeventeenSSS",
            "type": "user"
          },
          "name": "Wenting Shen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:53.298Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3434",
          "name": "Wenyuan Yu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3435",
          "user": {
            "_id": "642e19b26748dd4f8eea1321",
            "avatarUrl": "/avatars/a534e61c21d2fb3c7a4c4d4dba98fafb.svg",
            "isPro": false,
            "fullname": "Xianzhong Shi",
            "user": "itutor",
            "type": "user"
          },
          "name": "Xianzhong Shi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:51:19.514Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3436",
          "user": {
            "_id": "65105ab08c4b535a97052fe8",
            "avatarUrl": "/avatars/a97862045a26a74ca33d1a47b6a1f2b4.svg",
            "isPro": false,
            "fullname": "xiaominghuang",
            "user": "xiaominghuang",
            "type": "user"
          },
          "name": "Xiaoming Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:52:03.599Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3437",
          "name": "Xin Xu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3438",
          "name": "Yan Kou",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3439",
          "name": "Yangyu Lv",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343a",
          "name": "Yifei Li",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343b",
          "user": {
            "_id": "67d39e61943a965360fbbc0c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-JwILFmblPdd6Sv28c1J7.png",
            "isPro": false,
            "fullname": "yijing liu",
            "user": "86diphda",
            "type": "user"
          },
          "name": "Yijing Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:52:18.647Z",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343c",
          "name": "Yiming Wang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343d",
          "name": "Yingya Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343e",
          "name": "Yitong Huang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b343f",
          "name": "Yong Li",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3440",
          "name": "You Wu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3441",
          "name": "Yu Liu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3442",
          "name": "Yulin Pan",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3443",
          "name": "Yun Zheng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3444",
          "name": "Yuntao Hong",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3445",
          "name": "Yupeng Shi",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3446",
          "name": "Yutong Feng",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3447",
          "name": "Zeyinzi Jiang",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3448",
          "name": "Zhen Han",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b3449",
          "name": "Zhi-Fan Wu",
          "hidden": false
        },
        {
          "_id": "67e4b65a080a33e3955b344a",
          "name": "Ziyu Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T08:25:43.000Z",
      "submittedOnDailyAt": "2025-03-27T00:52:37.426Z",
      "title": "ワン: 開放とアdvancedの大規模ビデオ生成モデル",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "このレポートでは、Wanという、実用的で開放的なビデオベースモデルシートを紹介します。これらのモデルは、ビデオ生成の境界を超えるために設計されています。主流のディフュージョントランスフォーマーパラダイムに基づいて構築され、Wanは、新しいVAE、スケーラブルな事前学習戦略、大規模なデータコレクション、マイナスティック評価指標の自動計算などの系列のイノベーションを通じて、生成能力に関する重要な進歩を達成します。これらの貢献は、モデルの性能と多様性を向上させます。特に、Wanは以下の4つの特徴を持ちます：リーディングパフォーマンス：Wanの14Bモデルは、数万亿枚の画像とビデオを含む巨大なデータセットで訓練され、データおよびモデルサイズに対するビデオ生成のスケーリング法則を示します。内部と外部の複数のベンチマークで、現在の開放ソースモデルと最先端の商用ソリューションを超え、明確で重要な性能の上位を示します。詳細性：Wanは2つの能力のあるモデルを提供します、それは、1.3Bと14Bパラメータ、それぞれ効率的さや有効性を示します。さらに、画像からビデオ、指示ガイドされたビデオ編集、ビジネス用ビデオ生成などの複数の下流アプリケーションを扱います。消費者レベルの効率性：1.3Bモデルは、例外的なリソース効率を示し、8.19GB VRAMしか必要となりません。これは、広範囲の消費者レベルのGPUに対応します。開放性：Wanの全系列を開放ソース化します、それは、ソースコードとすべてのモデルを含み、ビデオ生成コミュニティの成長を促進するための目標を持ちます。この開放性は、業界でのビデオ制作の創造的可能性を大幅に拡大し、高品質のビデオベースモデルを学術界に提供することを目指しています。すべてのコードとモデルは、https://github.com/Wan-Video/Wan2.1 から利用可能です。",
      "upvotes": 16,
      "discussionId": "67e4b663080a33e3955b371a",
      "ai_keywords": [
        "diffusion transformer",
        "VAE",
        "large-scale data curation",
        "automated evaluation metrics",
        "scaling laws",
        "image-to-video",
        "instruction-guided video editing",
        "personal video generation"
      ]
    },
    "publishedAt": "2025-03-26T04:25:43.000Z",
    "title": "Wan: Open and Advanced Large-Scale Video Generative Models",
    "summary": "This report presents Wan, a comprehensive and open suite of video foundation\nmodels designed to push the boundaries of video generation. Built upon the\nmainstream diffusion transformer paradigm, Wan achieves significant\nadvancements in generative capabilities through a series of innovations,\nincluding our novel VAE, scalable pre-training strategies, large-scale data\ncuration, and automated evaluation metrics. These contributions collectively\nenhance the model's performance and versatility. Specifically, Wan is\ncharacterized by four key features: Leading Performance: The 14B model of Wan,\ntrained on a vast dataset comprising billions of images and videos,\ndemonstrates the scaling laws of video generation with respect to both data and\nmodel size. It consistently outperforms the existing open-source models as well\nas state-of-the-art commercial solutions across multiple internal and external\nbenchmarks, demonstrating a clear and significant performance superiority.\nComprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B\nparameters, for efficiency and effectiveness respectively. It also covers\nmultiple downstream applications, including image-to-video, instruction-guided\nvideo editing, and personal video generation, encompassing up to eight tasks.\nConsumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource\nefficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range\nof consumer-grade GPUs. Openness: We open-source the entire series of Wan,\nincluding source code and all models, with the goal of fostering the growth of\nthe video generation community. This openness seeks to significantly expand the\ncreative possibilities of video production in the industry and provide academia\nwith high-quality video foundation models. All the code and models are\navailable at https://github.com/Wan-Video/Wan2.1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20314.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20201",
      "authors": [
        {
          "_id": "67e4b04c8c0347025bd0fe84",
          "user": {
            "_id": "6109bc89e84ad84682a69754",
            "avatarUrl": "/avatars/067aac8784320d4e8e875379dc4cc209.svg",
            "isPro": false,
            "fullname": "Salaheddin Alzubi",
            "user": "salzubi401",
            "type": "user"
          },
          "name": "Salaheddin Alzubi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:03.915Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe85",
          "user": {
            "_id": "673f945e6cd62dbd4b02790d",
            "avatarUrl": "/avatars/3742e4e6b88d4f8b78d5c5308f55773e.svg",
            "isPro": false,
            "fullname": "Creston Brooks",
            "user": "cabxyz",
            "type": "user"
          },
          "name": "Creston Brooks",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-27T01:56:28.853Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe86",
          "user": {
            "_id": "666619508a270cedd594e55e",
            "avatarUrl": "/avatars/79bb2b09a663cae555140ec9379f05d9.svg",
            "isPro": false,
            "fullname": "Purva Chiniya",
            "user": "pchiniya",
            "type": "user"
          },
          "name": "Purva Chiniya",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:10.184Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe87",
          "name": "Edoardo Contente",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe88",
          "name": "Chiara von Gerlach",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe89",
          "user": {
            "_id": "62296d3f2df798b7e951e475",
            "avatarUrl": "/avatars/661c23416c3d418e2996f9b9a024db82.svg",
            "isPro": false,
            "fullname": "Lucas Irwin",
            "user": "ljirwin",
            "type": "user"
          },
          "name": "Lucas Irwin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:26.927Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8a",
          "name": "Yihan Jiang",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8b",
          "user": {
            "_id": "67759bf644ceb61f96739324",
            "avatarUrl": "/avatars/44cb431a9cbc73e060aff7d90435c42d.svg",
            "isPro": false,
            "fullname": "Arda Kaz",
            "user": "speedyarda",
            "type": "user"
          },
          "name": "Arda Kaz",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:57.570Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8c",
          "user": {
            "_id": "64b98bcf842aa47891bc0f63",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/L-smrOCQ3MXtvnISJqmxJ.png",
            "isPro": false,
            "fullname": "Windsor Nguyen",
            "user": "windsornguyen",
            "type": "user"
          },
          "name": "Windsor Nguyen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:54:50.840Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8d",
          "user": {
            "_id": "6756dffd88428044e2ddbdd9",
            "avatarUrl": "/avatars/41dbb83c68b56546cdf8e34379faf6b3.svg",
            "isPro": false,
            "fullname": "Sewoong Oh",
            "user": "sewoong79",
            "type": "user"
          },
          "name": "Sewoong Oh",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:55:20.604Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8e",
          "user": {
            "_id": "65f86cc77b704590d4a5439f",
            "avatarUrl": "/avatars/1a828cf755839f058241fb19ca83341f.svg",
            "isPro": false,
            "fullname": "Himanshu Tyagi",
            "user": "HimanshuTyagi",
            "type": "user"
          },
          "name": "Himanshu Tyagi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:55:27.139Z",
          "hidden": false
        },
        {
          "_id": "67e4b04c8c0347025bd0fe8f",
          "name": "Pramod Viswanath",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T03:51:32.000Z",
      "submittedOnDailyAt": "2025-03-27T00:26:59.804Z",
      "title": "Open Deep Search: オープンソースの理由効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効果効",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "オープンディープサーチ（ODS）を紹介します。ODSは、PerplexityのSonar Reasoning ProとOpenAIのGPT-4o Search Previewといった専有のサーチAIソリューションと、そのオープンソース版の間の差を狭めることを目的としています。ODSでは、最新のオープンソースLLMの推論能力を強化するために、推論アガントを用いてWebサーチツールを慎重に使用して質問に答えることが主な革新です。具體には、ODSは、ユーザが選択したベースLLMと協力して動作する2つのコンポーネントからなります：Open Search ToolとOpen Reasoning Agent。Open Reasoning Agentは、与えられたタスクを理解し、アクションの列を楽譜することでタスクを完了します。このアクションの中には、Open Search Toolを含むタービンがあります。Open Search Toolは、専有版のソフトウェアよりも優れている新しいWebサーチツールです。ODSは、DeepSeek-R1といった強力なオープンソース推論LLMと組み合わせて、SimpleQAとFRAMESの2ベンチマークで現在の最先端のベースラインと近くなり、時には超える性能を収めます。例えば、FRAMESの評価ベンチマークでは、ODSは最近発表されたGPT-4o Search Previewの最良ベースラインを9.7%増え、精度を向上させます。ODSは、より高い性能を達成するために、どのようなLLMにもセミフレームワークを提供し、サーチと推論能力を追加します。例えば、DeepSeek-R1は、SimpleQAでは82.4%、FRAMESでは30.1%の性能を達成し、ODSを利用しては、SimpleQAでは88.3%、FRAMESでは75.3%の最先端の性能を収めます。",
      "upvotes": 12,
      "discussionId": "67e4b04c8c0347025bd0fed2",
      "ai_keywords": [
        "LLMs",
        "reasoning agents",
        "web search tools",
        "Open Search Tool",
        "Open Reasoning Agent",
        "DeepSeek-R1",
        "SimpleQA",
        "FRAMES",
        "GPT-4o Search Preview"
      ]
    },
    "publishedAt": "2025-03-25T23:51:32.000Z",
    "title": "Open Deep Search: Democratizing Search with Open-source Reasoning Agents",
    "summary": "We introduce Open Deep Search (ODS) to close the increasing gap between the\nproprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and\nOpenAI's GPT-4o Search Preview, and their open-source counterparts. The main\ninnovation introduced in ODS is to augment the reasoning capabilities of the\nlatest open-source LLMs with reasoning agents that can judiciously use web\nsearch tools to answer queries. Concretely, ODS consists of two components that\nwork with a base LLM chosen by the user: Open Search Tool and Open Reasoning\nAgent. Open Reasoning Agent interprets the given task and completes it by\norchestrating a sequence of actions that includes calling tools, one of which\nis the Open Search Tool. Open Search Tool is a novel web search tool that\noutperforms proprietary counterparts. Together with powerful open-source\nreasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses\nthe existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES.\nFor example, on the FRAMES evaluation benchmark, ODS improves the best existing\nbaseline of the recently released GPT-4o Search Preview by 9.7% in accuracy.\nODS is a general framework for seamlessly augmenting any LLMs -- for example,\nDeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search\nand reasoning capabilities to achieve state-of-the-art performance: 88.3% on\nSimpleQA and 75.3% on FRAMES.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20201.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19480",
      "authors": [
        {
          "_id": "67e3693eebafaa1efbed08d2",
          "user": {
            "_id": "67d30d9ae45dc43004b31425",
            "avatarUrl": "/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg",
            "isPro": false,
            "fullname": "Shijie Ma",
            "user": "msj9817",
            "type": "user"
          },
          "name": "Shijie Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-26T20:44:37.274Z",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d3",
          "user": {
            "_id": "6455cc8f654d8bccae50e4d4",
            "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
            "isPro": false,
            "fullname": "Yuying Ge",
            "user": "tttoaster",
            "type": "user"
          },
          "name": "Yuying Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:55:42.975Z",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d4",
          "name": "Teng Wang",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d5",
          "user": {
            "_id": "67bc21106a3e748d80d11dc7",
            "avatarUrl": "/avatars/fbc7e76a3266a3c06d03e85db96a51cf.svg",
            "isPro": false,
            "fullname": "yuxin guo",
            "user": "aether25",
            "type": "user"
          },
          "name": "Yuxin Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:56:02.897Z",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d6",
          "user": {
            "_id": "640e9762b03f4cd29f58d982",
            "avatarUrl": "/avatars/81da37d628163fe3e094b247c7c3a3b5.svg",
            "isPro": false,
            "fullname": "Yixiao Ge",
            "user": "yxgeee",
            "type": "user"
          },
          "name": "Yixiao Ge",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:56:08.653Z",
          "hidden": false
        },
        {
          "_id": "67e3693eebafaa1efbed08d7",
          "user": {
            "_id": "63ca3ddc04c979828310bfcb",
            "avatarUrl": "/avatars/615e0d8622950b4408b40d550f02a894.svg",
            "isPro": false,
            "fullname": "Ying Shan",
            "user": "yshan2u",
            "type": "user"
          },
          "name": "Ying Shan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:56:14.275Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T09:15:34.000Z",
      "submittedOnDailyAt": "2025-03-27T01:26:37.430Z",
      "title": "GenHancer: 不完全生成モデルは秘密的に強力です\n  ビジョンシーンキャンペーンエンハンサー",
      "submittedOnDailyBy": {
        "_id": "67d30d9ae45dc43004b31425",
        "avatarUrl": "/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg",
        "isPro": false,
        "fullname": "Shijie Ma",
        "user": "msj9817",
        "type": "user"
      },
      "summary": "生成モデルと判別モデルの融合が増えて注目を集めています。判別的なContrastive Language-Image Pre-Training (CLIP)は高レベルの語義で優れていますが、微妙な視覚詳細を理解するのに難しいことがあります。一般的に、表現を向上させるためには、生成モデルはCLIPの視覚特徴を再構築の条件として使用します。しかし、潜在的な原理は調査が不足しています。本稿では、視覚的に完全な生成は表現向上には常に最適ではありませんと実験的に発見しました。本質は、生成モデルからの微妙な知識を有効に抽出しながら、関係ない情報を抑制することであると考えます。重要な因子を調査するために、3点の面で検討します：1. 条件付け機構：小数の局所トークンの使用が再構築の難易度を大幅に減らし、学習が崩れることを見出しました。そこで、グローバルな視覚トークンのみを条件とした方が最も効果的であると結論しました。2. デノイザーの設定：端末からの学習は外価情報を含むことを見出しました。これに対して、2段階の学習戦略を提案し、有用な視覚知識の学習を優先します。また、軽量デノイザーが驚異的な改善を示すことを示しました。3. 生成パラダイム：連続的および離散的なデノイザーを試し、渾身の適用性を認識しました。深い検討を通じて、最適な方法としてGenHancerを提案し、MMVP-VLMベンチマークで先行技術を超えることができました（例えば、OpenAICLIPで6.0%の改善）。強化されたCLIPは、多タイプ大語言モデルにプラグインでビジョンシーンの性能を向上させることができます。すべてのモデルとコードは公開にされています。",
      "upvotes": 11,
      "discussionId": "67e36940ebafaa1efbed0951",
      "projectPage": "https://mashijie1028.github.io/GenHancer/",
      "githubRepo": "https://github.com/mashijie1028/GenHancer",
      "ai_keywords": [
        "generative",
        "discriminative",
        "Contrastive Language-Image Pre-Training (CLIP)",
        "visual features",
        "reconstruction",
        "local tokens",
        "global visual tokens",
        "Conditioning mechanisms",
        "end-to-end training",
        "denoising configurations",
        "two-stage training",
        "lightweight denoisers",
        "continuous denoisers",
        "discrete denoisers",
        "Generation paradigms",
        "GenHancer",
        "MMVP-VLM benchmark",
        "OpenAICLIP",
        "multimodal large language models",
        "vision-centric performance"
      ]
    },
    "publishedAt": "2025-03-25T05:15:34.000Z",
    "title": "GenHancer: Imperfect Generative Models are Secretly Strong\n  Vision-Centric Enhancers",
    "summary": "The synergy between generative and discriminative models receives growing\nattention. While discriminative Contrastive Language-Image Pre-Training (CLIP)\nexcels in high-level semantics, it struggles with perceiving fine-grained\nvisual details. Generally, to enhance representations, generative models take\nCLIP's visual features as conditions for reconstruction. However, the\nunderlying principle remains underexplored. In this work, we empirically found\nthat visually perfect generations are not always optimal for representation\nenhancement. The essence lies in effectively extracting fine-grained knowledge\nfrom generative models while mitigating irrelevant information. To explore\ncritical factors, we delve into three aspects: (1) Conditioning mechanisms: We\nfound that even a small number of local tokens can drastically reduce the\ndifficulty of reconstruction, leading to collapsed training. We thus conclude\nthat utilizing only global visual tokens as conditions is the most effective\nstrategy. (2) Denoising configurations: We observed that end-to-end training\nintroduces extraneous information. To address this, we propose a two-stage\ntraining strategy to prioritize learning useful visual knowledge. Additionally,\nwe demonstrate that lightweight denoisers can yield remarkable improvements.\n(3) Generation paradigms: We explore both continuous and discrete denoisers\nwith desirable outcomes, validating the versatility of our method. Through our\nin-depth explorations, we have finally arrived at an effective method, namely\nGenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark,\ne.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into\nmultimodal large language models for better vision-centric performance. All the\nmodels and codes are made publicly available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19480.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67d30d9ae45dc43004b31425",
      "avatarUrl": "/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg",
      "fullname": "Shijie Ma",
      "name": "msj9817",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20672",
      "authors": [
        {
          "_id": "67e4b82c672b3d9c9cb42c70",
          "name": "Yuyang Peng",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c71",
          "name": "Shishi Xiao",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c72",
          "user": {
            "_id": "66bf00ca5b4e241fe266059d",
            "avatarUrl": "/avatars/f3eedfecf5baa8e2ac80d37abe42c63f.svg",
            "isPro": false,
            "fullname": "Keming Wu",
            "user": "wukeming11",
            "type": "user"
          },
          "name": "Keming Wu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:20.576Z",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c73",
          "user": {
            "_id": "672894ff1905afcdc9132fc6",
            "avatarUrl": "/avatars/78297eadad816c45d680aa70cea3b973.svg",
            "isPro": false,
            "fullname": "QISHENG LIAO",
            "user": "Marseclipse",
            "type": "user"
          },
          "name": "Qisheng Liao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:14.624Z",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c74",
          "user": {
            "_id": "64ba249e5c4deebf69aa17fd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/h7YdQe9wEr1TTJfP1ALhb.jpeg",
            "isPro": false,
            "fullname": "chen",
            "user": "bohanChen",
            "type": "user"
          },
          "name": "Bohan Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:07.459Z",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c75",
          "user": {
            "_id": "6298fd95b58e71e2ac9f3ad8",
            "avatarUrl": "/avatars/7d34644d537bc5c17cf1e4ce4095355c.svg",
            "isPro": false,
            "fullname": "Kevin Lin",
            "user": "kevinlin311tw",
            "type": "user"
          },
          "name": "Kevin Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:01.691Z",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c76",
          "name": "Danqing Huang",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c77",
          "name": "Ji Li",
          "hidden": false
        },
        {
          "_id": "67e4b82c672b3d9c9cb42c78",
          "user": {
            "_id": "631f108bb45367a05fe74260",
            "avatarUrl": "/avatars/c20da6800b643728062712d9a2771648.svg",
            "isPro": false,
            "fullname": "Researcher",
            "user": "YuanYuhui",
            "type": "user"
          },
          "name": "Yuhui Yuan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:56:39.210Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T16:04:57.000Z",
      "submittedOnDailyAt": "2025-03-27T01:00:23.038Z",
      "title": "ビジネスジェネレーション: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レベルの可視化テキスト表示の進歩\n\nBizGen: 情報グラフィックの記事レ",
      "submittedOnDailyBy": {
        "_id": "62333a88fd7bb4a39b92d387",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
        "isPro": false,
        "fullname": "Alex Jinpeng Wang",
        "user": "Awiny",
        "type": "user"
      },
      "summary": "最近、最先進なテキストから画像生成モデルのように、FluxやIdeogram 2.0は文書レベルの可視的なテキストレンディングに関して重要な進歩を達成しています。本論文では、より難しいシナリオであるビジネスレベルの可視的なテキストレンディングに焦点を当て、ユーザーが提供したビジネスレベルの説明プロンプトと超高密度レイアウトに基づいて、高品質のビジネスコンテンツ（情報グラフィックやスライドなど）を生成する新しいタスクを解決します。基本的な課題は二つです：長いコンテキスト長と高品質のビジネスコンテンツデータの不足です。\n\nこれらのプログラムと比較して、ビジネスコンテンツにおいて、多数のサブライエンスと文書レベルのプロンプトを焦点にしたほど、超高密度レイアウトを準確に遵守させることはより難しいです。私たちは、二つの鍵の技術的な貢献を行います：それは、スケーラブルな高品質のビジネスコンテンツデータセットの構築であるInfographics-650Kで、超高密度レイアウトとプロンプトを準備するためにレイヤーごとの検索アウジュメントアウジュメント情報グラフィック生成シェームを実装します。そして、ライエンスごとのプロンプトを超高密度レイアウトに従って、切り取りされたサブライエンス潜在空間に挿入し、推論中にサブライエンスを柔軟に精確化するライエンスガイドされた交差注意シェームです。\n\n私たちのシステムの強い結果を、ビジネスレベルのプロンプトセットでのソートダークシステム（FluxやSD3）と比較して示します。また、各コンポーネントの効果を確認するために、詳細な消去実験を実施します。私たちは、構築したInfographics-650KやBizEvalをビジネスコンテンツ生成の進歩に促進することを望むと思います。",
      "upvotes": 7,
      "discussionId": "67e4b831672b3d9c9cb42ebb",
      "ai_keywords": [
        "scalable, high-quality business content dataset",
        "Infographics-650K",
        "layer-wise retrieval-augmented infographic generation scheme",
        "layout-guided cross attention scheme",
        "cropped region latent space",
        "layout conditional CFG",
        "BizEval prompt set",
        "ablation experiments"
      ]
    },
    "publishedAt": "2025-03-26T12:04:57.000Z",
    "title": "BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation",
    "summary": "Recently, state-of-the-art text-to-image generation models, such as Flux and\nIdeogram 2.0, have made significant progress in sentence-level visual text\nrendering. In this paper, we focus on the more challenging scenarios of\narticle-level visual text rendering and address a novel task of generating\nhigh-quality business content, including infographics and slides, based on user\nprovided article-level descriptive prompts and ultra-dense layouts. The\nfundamental challenges are twofold: significantly longer context lengths and\nthe scarcity of high-quality business content data.\n  In contrast to most previous works that focus on a limited number of\nsub-regions and sentence-level prompts, ensuring precise adherence to\nultra-dense layouts with tens or even hundreds of sub-regions in business\ncontent is far more challenging. We make two key technical contributions: (i)\nthe construction of scalable, high-quality business content dataset, i.e.,\nInfographics-650K, equipped with ultra-dense layouts and prompts by\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\nand (ii) a layout-guided cross attention scheme, which injects tens of\nregion-wise prompts into a set of cropped region latent space according to the\nultra-dense layouts, and refine each sub-regions flexibly during inference\nusing a layout conditional CFG.\n  We demonstrate the strong results of our system compared to previous SOTA\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\nconduct thorough ablation experiments to verify the effectiveness of each\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\nthe broader community to advance the progress of business content generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20672.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20757",
      "authors": [
        {
          "_id": "67e4c08fd9b7021d4a600fa4",
          "user": {
            "_id": "662b4e3bc709a61df840fda1",
            "avatarUrl": "/avatars/fc73c63a4e1f8fbb084ec43ec9af0af0.svg",
            "isPro": false,
            "fullname": "Hu Yunhai",
            "user": "AlexCCtop",
            "type": "user"
          },
          "name": "Yunhai Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:51.052Z",
          "hidden": false
        },
        {
          "_id": "67e4c08fd9b7021d4a600fa5",
          "user": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "isPro": true,
            "fullname": "Yilun",
            "user": "yilunzhao",
            "type": "user"
          },
          "name": "Yilun Zhao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-27T03:05:54.275Z",
          "hidden": false
        },
        {
          "_id": "67e4c08fd9b7021d4a600fa6",
          "user": {
            "_id": "660103ec4ae78d4ded4633fc",
            "avatarUrl": "/avatars/efce106d70f5d092bf44d0638aa49984.svg",
            "isPro": false,
            "fullname": "CHEN Zhao",
            "user": "chenzhao",
            "type": "user"
          },
          "name": "Chen Zhao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:04.608Z",
          "hidden": false
        },
        {
          "_id": "67e4c08fd9b7021d4a600fa7",
          "user": {
            "_id": "5f5ba21188f57f65f951f255",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1599840760465-noauth.png",
            "isPro": false,
            "fullname": "Arman Cohan",
            "user": "armanc",
            "type": "user"
          },
          "name": "Arman Cohan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:57:57.092Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:46:08.000Z",
      "submittedOnDailyAt": "2025-03-27T01:36:43.674Z",
      "title": "MCTS-RAG: モンテカルロ木の検索を用いた検索増強生成",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "MCTS-RAGは、小規模の言語モデルの推理能力を強化する新しいアプローチです。これは、レビューアグメント生成（RAG）を使用して関連するコンテキストを提供し、モンテカルロ木検索（MCTS）を使用して推理パスを精確化することで、知識密集型タスクに対応します。MCTS-RAGは、イテレーション的な決策プロセスを通じてレビューと推理を動的に統合します。標準的なRAG手法と違い、MCTS-RAGはレビューと推理を独立して取り出すことで、知識を最適的に統合しません。また、伝統的なMCTS推理は、内部モデルの知識だけを依存し、外部事実を含まないことで、MCTS-RAGは構造化された推理とアダプティブなレビューを組み合わせます。この統合アプローチは、決策を強化し、ハロカノミーを減らし、事実的な精度とレスポンスの一貫性を保証します。ComplexWebQA、GPQA、FoolMeTwiceなどの複数の推理と知識密集型データセットの実験結果から、我々の方法は、小規模のLMsを使用して、GPT-4oと同じレベルの性能を達成することを可能にし、小規模モデルの推理で新しい標準を設定します。",
      "upvotes": 6,
      "discussionId": "67e4c092d9b7021d4a60108b",
      "ai_keywords": [
        "MCTS-RAG",
        "retrieval-augmented generation (RAG)",
        "Monte Carlo Tree Search (MCTS)",
        "reasoning paths",
        "iterative decision-making",
        "knowledge suboptimally",
        "structured reasoning",
        "adaptive retrieval",
        "decision-making",
        "hallucinations",
        "factual accuracy",
        "response consistency",
        "ComplexWebQA",
        "GPQA",
        "FoolMeTwice",
        "small-scale LMs",
        "frontier LLMs (GPT-4o)",
        "scaling inference-time compute"
      ]
    },
    "publishedAt": "2025-03-26T13:46:08.000Z",
    "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search",
    "summary": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20757.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20020",
      "authors": [
        {
          "_id": "67e4b288fa81c69f446da710",
          "name": "Gemini Robotics Team",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da711",
          "user": {
            "_id": "61fd7ac3fbafe89f48101d83",
            "avatarUrl": "/avatars/cb2c86a04574498e71d6c447c2b289c1.svg",
            "isPro": false,
            "fullname": "Saminda Abeyruwan",
            "user": "saminda",
            "type": "user"
          },
          "name": "Saminda Abeyruwan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:16.630Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da712",
          "name": "Joshua Ainslie",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da713",
          "user": {
            "_id": "6253ee39e4e98393660b5c35",
            "avatarUrl": "/avatars/9c032f6a0729bfe5c16b3affe190834d.svg",
            "isPro": false,
            "fullname": "Jean-Baptiste Alayrac",
            "user": "jalayrac",
            "type": "user"
          },
          "name": "Jean-Baptiste Alayrac",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:27.846Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da714",
          "user": {
            "_id": "63f47a88121f9894707465ed",
            "avatarUrl": "/avatars/d85d409d19068aea02a2532b587dd1ef.svg",
            "isPro": false,
            "fullname": "Montserrat Gonzalez Arenas",
            "user": "montse90",
            "type": "user"
          },
          "name": "Montserrat Gonzalez Arenas",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:33.087Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da715",
          "user": {
            "_id": "66140283cf3fef4fa812e92f",
            "avatarUrl": "/avatars/033586adc3931d6c85bf9e84220992b4.svg",
            "isPro": false,
            "fullname": "Travis Armstrong",
            "user": "TravisAStrong",
            "type": "user"
          },
          "name": "Travis Armstrong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:39.411Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da716",
          "user": {
            "_id": "653ad36e5f1703225b266b7b",
            "avatarUrl": "/avatars/170e6b54a9859c7fca0289a09654c47f.svg",
            "isPro": false,
            "fullname": "Ashwin Balakrishna",
            "user": "abalakrishna123",
            "type": "user"
          },
          "name": "Ashwin Balakrishna",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T09:58:46.130Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da717",
          "name": "Robert Baruch",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da718",
          "name": "Maria Bauza",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da719",
          "name": "Michiel Blokzijl",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71a",
          "name": "Steven Bohez",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71b",
          "name": "Konstantinos Bousmalis",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71c",
          "name": "Anthony Brohan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71d",
          "name": "Thomas Buschmann",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71e",
          "name": "Arunkumar Byravan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da71f",
          "name": "Serkan Cabi",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da720",
          "name": "Ken Caluwaerts",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da721",
          "name": "Federico Casarini",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da722",
          "name": "Oscar Chang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da723",
          "name": "Jose Enrique Chen",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da724",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da725",
          "name": "Hao-Tien Lewis Chiang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da726",
          "name": "Krzysztof Choromanski",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da727",
          "name": "David D'Ambrosio",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da728",
          "name": "Sudeep Dasari",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da729",
          "name": "Todor Davchev",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72a",
          "name": "Coline Devin",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72b",
          "user": {
            "_id": "62dac377f014388f908974f4",
            "avatarUrl": "/avatars/39bf0ca206441575a45f577060cdd8bc.svg",
            "isPro": false,
            "fullname": "Norman Di Palo",
            "user": "normandipalo",
            "type": "user"
          },
          "name": "Norman Di Palo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:09.232Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72c",
          "user": {
            "_id": "64d89da3bab152b24713108e",
            "avatarUrl": "/avatars/22e10f9a13b0d18b3a3b1f5281c7124d.svg",
            "isPro": false,
            "fullname": "Tianli Ding",
            "user": "Tding",
            "type": "user"
          },
          "name": "Tianli Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:18.146Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72d",
          "user": {
            "_id": "63b7d36469b7bd7324f9f438",
            "avatarUrl": "/avatars/67b1864c378102b1cf2de571cce7bf9a.svg",
            "isPro": false,
            "fullname": "Adil Dostmohamed",
            "user": "adild",
            "type": "user"
          },
          "name": "Adil Dostmohamed",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:29.517Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72e",
          "user": {
            "_id": "67225875b46c703941fa7967",
            "avatarUrl": "/avatars/7c89fbdd9a135210209bcd0cbfe7988a.svg",
            "isPro": false,
            "fullname": "Danny Driess",
            "user": "dannydriess",
            "type": "user"
          },
          "name": "Danny Driess",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:37.044Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da72f",
          "user": {
            "_id": "63c9bd445fdc575773c732fe",
            "avatarUrl": "/avatars/def472d1ab3fbf751225357c0932ae7e.svg",
            "isPro": false,
            "fullname": "Yilun Du",
            "user": "yilundu",
            "type": "user"
          },
          "name": "Yilun Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:43.648Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da730",
          "name": "Debidatta Dwibedi",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da731",
          "user": {
            "_id": "66f6e9a737473469e871cae8",
            "avatarUrl": "/avatars/8c247380cee5d879aac204299963d3a7.svg",
            "isPro": false,
            "fullname": "Michael Elabd",
            "user": "michaelelabd",
            "type": "user"
          },
          "name": "Michael Elabd",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:00:55.368Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da732",
          "name": "Claudio Fantacci",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da733",
          "name": "Cody Fong",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da734",
          "name": "Erik Frey",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da735",
          "name": "Chuyuan Fu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da736",
          "name": "Marissa Giustina",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da737",
          "name": "Keerthana Gopalakrishnan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da738",
          "name": "Laura Graesser",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da739",
          "name": "Leonard Hasenclever",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73a",
          "name": "Nicolas Heess",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73b",
          "name": "Brandon Hernaez",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73c",
          "name": "Alexander Herzog",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73d",
          "name": "R. Alex Hofer",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73e",
          "name": "Jan Humplik",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da73f",
          "name": "Atil Iscen",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da740",
          "name": "Mithun George Jacob",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da741",
          "name": "Deepali Jain",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da742",
          "name": "Ryan Julian",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da743",
          "user": {
            "_id": "64b7bf04a5018e3c7ca2ecda",
            "avatarUrl": "/avatars/4a434c344f68f2915c6e823262e62946.svg",
            "isPro": false,
            "fullname": "Dmitry Kalashnikov",
            "user": "dmitry-kalashnikov",
            "type": "user"
          },
          "name": "Dmitry Kalashnikov",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:01:27.098Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da744",
          "name": "M. Emre Karagozler",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da745",
          "name": "Stefani Karp",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da746",
          "name": "Chase Kew",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da747",
          "name": "Jerad Kirkland",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da748",
          "name": "Sean Kirmani",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da749",
          "name": "Yuheng Kuang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74a",
          "user": {
            "_id": "631fa7e9124782a19efd20f2",
            "avatarUrl": "/avatars/456ef70cdb2a6a1f79a078746e96034a.svg",
            "isPro": false,
            "fullname": "Thomas Lampe",
            "user": "tlampe",
            "type": "user"
          },
          "name": "Thomas Lampe",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:01:36.252Z",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74b",
          "name": "Antoine Laurens",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74c",
          "name": "Isabel Leal",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74d",
          "name": "Alex X. Lee",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74e",
          "name": "Tsang-Wei Edward Lee",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da74f",
          "name": "Jacky Liang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da750",
          "name": "Yixin Lin",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da751",
          "name": "Sharath Maddineni",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da752",
          "name": "Anirudha Majumdar",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da753",
          "name": "Assaf Hurwitz Michaely",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da754",
          "name": "Robert Moreno",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da755",
          "name": "Michael Neunert",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da756",
          "name": "Francesco Nori",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da757",
          "name": "Carolina Parada",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da758",
          "name": "Emilio Parisotto",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da759",
          "name": "Peter Pastor",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75a",
          "name": "Acorn Pooley",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75b",
          "name": "Kanishka Rao",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75c",
          "name": "Krista Reymann",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75d",
          "name": "Dorsa Sadigh",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75e",
          "name": "Stefano Saliceti",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da75f",
          "name": "Pannag Sanketi",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da760",
          "name": "Pierre Sermanet",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da761",
          "name": "Dhruv Shah",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da762",
          "name": "Mohit Sharma",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da763",
          "name": "Kathryn Shea",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da764",
          "name": "Charles Shu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da765",
          "name": "Vikas Sindhwani",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da766",
          "name": "Sumeet Singh",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da767",
          "name": "Radu Soricut",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da768",
          "name": "Jost Tobias Springenberg",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da769",
          "name": "Rachel Sterneck",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76a",
          "name": "Razvan Surdulescu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76b",
          "name": "Jie Tan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76c",
          "name": "Jonathan Tompson",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76d",
          "name": "Vincent Vanhoucke",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76e",
          "name": "Jake Varley",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da76f",
          "name": "Grace Vesom",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da770",
          "name": "Giulia Vezzani",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da771",
          "name": "Oriol Vinyals",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da772",
          "name": "Ayzaan Wahid",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da773",
          "name": "Stefan Welker",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da774",
          "name": "Paul Wohlhart",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da775",
          "name": "Fei Xia",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da776",
          "name": "Ted Xiao",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da777",
          "name": "Annie Xie",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da778",
          "name": "Jinyu Xie",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da779",
          "name": "Peng Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77a",
          "name": "Sichun Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77b",
          "name": "Ying Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77c",
          "name": "Zhuo Xu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77d",
          "name": "Yuxiang Yang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77e",
          "name": "Rui Yao",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da77f",
          "name": "Sergey Yaroshenko",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da780",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da781",
          "name": "Wentao Yuan",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da782",
          "name": "Jingwei Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da783",
          "name": "Tingnan Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da784",
          "name": "Allan Zhou",
          "hidden": false
        },
        {
          "_id": "67e4b288fa81c69f446da785",
          "name": "Yuxiang Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T19:02:56.000Z",
      "submittedOnDailyAt": "2025-03-27T00:36:27.703Z",
      "title": "ジェミニ・ロボティックス：物理世界にAIを導入する",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "最近の大規模多モデルの進展は、デジタル領域での卓越した一般的な能力の出現を促しましたが、ロボットやその他の物理的なアガントにその転移は重大な課題となっています。本報告書は、新しい家族のAIモデルをロボティクスに特に設計し、Gemini 2.0の基盤により構築されたものを紹介します。我々は、直接ロボットを制御可能な高度な視覚・言語・行動（VLA）一般的なモデルとしてのGemini Roboticsを紹介します。Gemini Roboticsは、広範囲の複雑な操作タスクを解決するためにスムーズな反応的な動作を行い、物体の種類や位置の変化に対して強健であり、見たことのない環境を扱うことができ、多様な開放ボキャブラリーの指示を従うことができます。また、追加の微調節を通じて、Gemini Roboticsは新しい能力への特殊化を可能にし、長期ホライゾン、高度なディジェンドタスクの解決、100例の示唆から新しい短期ホライゾンタスクの学習、完全に新しいロボットの構造を慣れることができます。これが可能なのは、Gemini Roboticsは、これまで紹介したGemini Robotics-ERモデルの基盤に構築されているからです。Gemini Robotics-ER（Embodied Reasoning）は、Geminiの多モデル論理能力を物理的な世界に拡張し、空間的および時間的理解を強化しています。これにより、ロボティクスに関連した能力を持つことができます。物体検出、指向、軌道とグラス予測、多ビューの対応、3Dバウンディングボックス予測など。この新しい組み合わせがロボティクスの多様なアプリケーションをサポートすることを示します。また、この新しいロボティクスベースモデルの関連する重要な安全考慮についても議論し、取り組みます。Gemini Robotics家族は、AIの実力を物理的な世界に実現する一般的なロボットの開発に向けて重要なステップを踏み出しています。",
      "upvotes": 6,
      "discussionId": "67e4b28cfa81c69f446da8c7",
      "ai_keywords": [
        "Vision-Language-Action (VLA) generalist model",
        "multimodal model",
        "multimodal reasoning capabilities",
        "fine-tuning",
        "long-horizon, highly dexterous tasks",
        "short-horizon tasks",
        "robot embodiments",
        "embodied reasoning",
        "object detection",
        "pointing",
        "trajectory prediction",
        "grasp prediction",
        "multi-view correspondence",
        "3D bounding box predictions",
        "robotics applications",
        "safety considerations",
        "robotics foundation models",
        "general-purpose robots"
      ]
    },
    "publishedAt": "2025-03-25T15:02:56.000Z",
    "title": "Gemini Robotics: Bringing AI into the Physical World",
    "summary": "Recent advancements in large multimodal models have led to the emergence of\nremarkable generalist capabilities in digital domains, yet their translation to\nphysical agents such as robots remains a significant challenge. This report\nintroduces a new family of AI models purposefully designed for robotics and\nbuilt upon the foundation of Gemini 2.0. We present Gemini Robotics, an\nadvanced Vision-Language-Action (VLA) generalist model capable of directly\ncontrolling robots. Gemini Robotics executes smooth and reactive movements to\ntackle a wide range of complex manipulation tasks while also being robust to\nvariations in object types and positions, handling unseen environments as well\nas following diverse, open vocabulary instructions. We show that with\nadditional fine-tuning, Gemini Robotics can be specialized to new capabilities\nincluding solving long-horizon, highly dexterous tasks, learning new\nshort-horizon tasks from as few as 100 demonstrations and adapting to\ncompletely novel robot embodiments. This is made possible because Gemini\nRobotics builds on top of the Gemini Robotics-ER model, the second model we\nintroduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends\nGemini's multimodal reasoning capabilities into the physical world, with\nenhanced spatial and temporal understanding. This enables capabilities relevant\nto robotics including object detection, pointing, trajectory and grasp\nprediction, as well as multi-view correspondence and 3D bounding box\npredictions. We show how this novel combination can support a variety of\nrobotics applications. We also discuss and address important safety\nconsiderations related to this new class of robotics foundation models. The\nGemini Robotics family marks a substantial step towards developing\ngeneral-purpose robots that realizes AI's potential in the physical world.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20020.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19950",
      "authors": [
        {
          "_id": "67e4b27cfe1f5acc68028de9",
          "user": {
            "_id": "6399c67bf78f75ae73146760",
            "avatarUrl": "/avatars/d1c3b0eff67b598a1ad58e297382957f.svg",
            "isPro": false,
            "fullname": "CHEN Han",
            "user": "Concyclics",
            "type": "user"
          },
          "name": "Han Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:29.747Z",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028dea",
          "user": {
            "_id": "650ccf6a36ac7eba06ea1cfa",
            "avatarUrl": "/avatars/50374fca4f6cc2cf7a6601cd8d3f725b.svg",
            "isPro": false,
            "fullname": "Zicong Jiang",
            "user": "Zicong99",
            "type": "user"
          },
          "name": "Zicong Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:02:00.596Z",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028deb",
          "user": {
            "_id": "64b781c5da8017900e7b8b25",
            "avatarUrl": "/avatars/0db9a83b0908cc6b9417360ed77fcc1a.svg",
            "isPro": false,
            "fullname": "zining zhang",
            "user": "deciding",
            "type": "user"
          },
          "name": "Zining Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:02:07.675Z",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028dec",
          "name": "Bingsheng He",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028ded",
          "name": "Pingyi Luo",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028dee",
          "name": "Mian Lu",
          "hidden": false
        },
        {
          "_id": "67e4b27cfe1f5acc68028def",
          "name": "Yuqiang Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T16:24:45.000Z",
      "submittedOnDailyAt": "2025-03-27T02:17:05.033Z",
      "title": "LogQuant: KVキャッシュのログ分布2ビット量化での精度保持の上位レベル",
      "submittedOnDailyBy": {
        "_id": "6399c67bf78f75ae73146760",
        "avatarUrl": "/avatars/d1c3b0eff67b598a1ad58e297382957f.svg",
        "isPro": false,
        "fullname": "CHEN Han",
        "user": "Concyclics",
        "type": "user"
      },
      "summary": "LogQuantは、大規模言語モデル（LLM）推論のKV Cacheに対するショートプリント化技術です。これは、メモリサポートを大幅に削減しながら、優れた性能を維持するための革新的な技術です。先行の方法は、後ろのトークンが重要であることを前提としていたり、より早いアテンションパターンに基づいて重要なトークンを予測していました。しかし、両方のアプローチは、性能バックロックまたは頻繁な予測誤差による問題を生じることがありました。\n\nLogQuantは、異なるアプローチを採用しています。ログベースのフィルタリング機構を適用し、コンテキスト全体で選択的にKV Cacheをショートプリント化し、既存の方法と比較して同じまたは減少したメモリフットプリントでより良い性能を実現します。ベンチマークテストでは、メモリ消費を増やさない状態でトランソーフローを25%増加させ、バッチサイズを60%増加させます。数学やコード完成などの難しいタスクでは、同じコンプレッション比で精度を40%〜200%増加させ、比較的な技術を上回ります。\n\nLogQuantは、Pythonのtransformersライブラリなどの人気の推論フレームワークと統合できます。実装は、https://github.com/Concyclics/LogQuantKVにおけるように提供できます。",
      "upvotes": 4,
      "discussionId": "67e4b27efe1f5acc68028e72",
      "githubRepo": "https://github.com/Concyclics/LogQuantKV",
      "ai_keywords": [
        "KV Cache",
        "large language model (LLM)",
        "token",
        "attention pattern",
        "log-based filtering mechanism",
        "throughput",
        "batch size",
        "accuracy",
        "Math Completion",
        "Code Completion",
        "compression ratio",
        "transformers library"
      ]
    },
    "publishedAt": "2025-03-25T12:24:45.000Z",
    "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
    "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19950.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6399c67bf78f75ae73146760",
      "avatarUrl": "/avatars/d1c3b0eff67b598a1ad58e297382957f.svg",
      "fullname": "CHEN Han",
      "name": "Concyclics",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.19462",
      "authors": [
        {
          "_id": "67e3641cd8da46951f860d84",
          "user": {
            "_id": "645b8bf6438d6cfbe1ae47ae",
            "avatarUrl": "/avatars/3e4dc0523ba8e314b298567cf8f0c0ae.svg",
            "isPro": false,
            "fullname": "Haiyu Zhang",
            "user": "aejion",
            "type": "user"
          },
          "name": "Haiyu Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:02:45.776Z",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d85",
          "user": {
            "_id": "643e943a70c6a27621eb1c89",
            "avatarUrl": "/avatars/73ec521ab5ba84cc7908c52c0acef6ef.svg",
            "isPro": false,
            "fullname": "Xinyuan Chen",
            "user": "AriaChen",
            "type": "user"
          },
          "name": "Xinyuan Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:02:58.635Z",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d86",
          "user": {
            "_id": "63201256c6b20f03c829c4b8",
            "avatarUrl": "/avatars/a42092119777d65e60b12eb5ba0e45f1.svg",
            "isPro": false,
            "fullname": "Yaohui Wang",
            "user": "YaohuiW",
            "type": "user"
          },
          "name": "Yaohui Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:03:18.769Z",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d87",
          "user": {
            "_id": "65d5ec74cd05bc1eaa125040",
            "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
            "isPro": false,
            "fullname": "Xihui Liu",
            "user": "XihuiLiu",
            "type": "user"
          },
          "name": "Xihui Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:03:26.468Z",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d88",
          "name": "Yunhong Wang",
          "hidden": false
        },
        {
          "_id": "67e3641cd8da46951f860d89",
          "name": "Yu Qiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T08:52:07.000Z",
      "submittedOnDailyAt": "2025-03-27T00:39:48.103Z",
      "title": "AccVideo: 合成データセットを用いた動画拡散モデルの加速",
      "submittedOnDailyBy": {
        "_id": "645b8bf6438d6cfbe1ae47ae",
        "avatarUrl": "/avatars/3e4dc0523ba8e314b298567cf8f0c0ae.svg",
        "isPro": false,
        "fullname": "Haiyu Zhang",
        "user": "aejion",
        "type": "user"
      },
      "summary": "Diffusionモデルは映画生成分野で驚異的な進歩を達成しました。しかし、その複数のイテレーションのデノイズ化の性質により、映画を生成するためには多くの推論ステップが必要で、これは遅くて計算量が高いです。本論文では、現在の存在するDiffusion Distillationメソッドにおける課題について詳細な分析を行い、合成データセットを用いて映画デノイズ化モデルを加速するための新しい効率的な方法、AccVideoを提案します。訓練された映画デノイズ化モデルを利用して、複数の有効なデノイズ化データポイントを生成し、それらを合成データセットとして使用し、デジスタイル化の際に無駄なデータポイントを使用しないようにします。この合成データセットに基づいて、デノイズ化データポイントからのキーデータポイントを利用して、ノイズから映画へのマッピングを学習するための軌道ベースの少ないステップのガイドを設計します。また、合成データセットは各デノイズ化時間ステップのデータ分布を捉えているため、学生モデルの出力分布を我々の合成データセットの分布と一致させるための相諜学習の戦略を導入し、映画の質を向上させます。拡張された実験により、我々のモデルは教師モデルに比べて8.5倍の生成速度向上を達成し、比較的な性能を維持します。以前の加速メソッドと比較して、我々のアプローチは5秒、720x1280、24fpsの高い質と解像度の映画を生成することができます。",
      "upvotes": 4,
      "discussionId": "67e3641ed8da46951f860e12",
      "ai_keywords": [
        "diffusion models",
        "video generation",
        "iterative denoising",
        "inference steps",
        "diffusion distillation",
        "AccVideo",
        "synthetic dataset",
        "pretrained video diffusion model",
        "denoising trajectories",
        "trajectory-based few-step guidance",
        "noise-to-video mapping",
        "adversarial training strategy"
      ]
    },
    "publishedAt": "2025-03-25T04:52:07.000Z",
    "title": "AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset",
    "summary": "Diffusion models have achieved remarkable progress in the field of video\ngeneration. However, their iterative denoising nature requires a large number\nof inference steps to generate a video, which is slow and computationally\nexpensive. In this paper, we begin with a detailed analysis of the challenges\npresent in existing diffusion distillation methods and propose a novel\nefficient method, namely AccVideo, to reduce the inference steps for\naccelerating video diffusion models with synthetic dataset. We leverage the\npretrained video diffusion model to generate multiple valid denoising\ntrajectories as our synthetic dataset, which eliminates the use of useless data\npoints during distillation. Based on the synthetic dataset, we design a\ntrajectory-based few-step guidance that utilizes key data points from the\ndenoising trajectories to learn the noise-to-video mapping, enabling video\ngeneration in fewer steps. Furthermore, since the synthetic dataset captures\nthe data distribution at each diffusion timestep, we introduce an adversarial\ntraining strategy to align the output distribution of the student model with\nthat of our synthetic dataset, thereby enhancing the video quality. Extensive\nexperiments demonstrate that our model achieves 8.5x improvements in generation\nspeed compared to the teacher model while maintaining comparable performance.\nCompared to previous accelerating methods, our approach is capable of\ngenerating videos with higher quality and resolution, i.e., 5-seconds,\n720x1280, 24fps.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19462.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b8bf6438d6cfbe1ae47ae",
      "avatarUrl": "/avatars/3e4dc0523ba8e314b298567cf8f0c0ae.svg",
      "fullname": "Haiyu Zhang",
      "name": "aejion",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.20756",
      "authors": [
        {
          "_id": "67e4b0b850ca38886d7e78d0",
          "name": "Chenxi Wang",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d1",
          "name": "Jizhan Fang",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d2",
          "name": "Xiang Chen",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d3",
          "name": "Bozhong Tian",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d4",
          "name": "Ziwen Xu",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d5",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67e4b0b850ca38886d7e78d6",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T17:45:29.000Z",
      "submittedOnDailyAt": "2025-03-27T00:28:51.612Z",
      "title": "ADS-Edit: 自動運転システム向けの多様化カンフェースキャンパスエディティングデータセット",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "最近の大規模多モデル（LMMs）の進展は、自動運転システム（ADS）に望ましい効果を示しています。しかし、それらの直接的なADSへの応用は、交通知識の理解の誤り、複雑な道路状況、車両の多様な状態などの課題により妨われています。これらの課題に対処するために、モデルの行動を特定の改修を可能にすることで、完全な再訓練の必要性をなくすことを提案しています。また、ADS向けの知識編集データセット「ADS-Edit」を介して、実世界的なシナリオ、多数のデータタイプ、詳細な評価指標を含むものを紹介しています。詳細な実験を行い、数多くの興味深い結論を得ました。私たちの研究は、自動運転システム分野での知識編集アプリケーションの進展に貢献していきたいと思います。コードとデータは、https://github.com/zjunlp/EasyEdit に公開されています。",
      "upvotes": 3,
      "discussionId": "67e4b0bb50ca38886d7e79d0",
      "ai_keywords": [
        "Knowledge Editing",
        "ADS-Edit",
        "multimodal knowledge editing dataset",
        "autonomous driving systems"
      ]
    },
    "publishedAt": "2025-03-26T13:45:29.000Z",
    "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\n  Systems",
    "summary": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20756.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20271",
      "authors": [
        {
          "_id": "67e4dc6a38e4d1444c71ce70",
          "name": "Haoqin Tu",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce71",
          "name": "Weitao Feng",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce72",
          "name": "Hardy Chen",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce73",
          "name": "Hui Liu",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce74",
          "name": "Xianfeng Tang",
          "hidden": false
        },
        {
          "_id": "67e4dc6a38e4d1444c71ce75",
          "name": "Cihang Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T06:38:31.000Z",
      "submittedOnDailyAt": "2025-03-27T06:03:39.751Z",
      "title": "ViLBench: 視覚言語処理報酬モデリングのシステム",
      "submittedOnDailyBy": {
        "_id": "604ae011caabafacfa48e3de",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1615519738679-noauth.jpeg",
        "isPro": false,
        "fullname": "Haoqin Tu",
        "user": "PahaII",
        "type": "user"
      },
      "summary": "プロセス観測報酬モデルは、複雑なタスクの理由軌道の適切な選択を促すための細かい機能であり、モデルの回答に詳細なステップごとのフィードバックを提供する。その優めさに加えて、PRMsの評価は特に多めのモデルでは少なく、この空白を填ぐために、この論文は最初に現在の視覚大語言モデル（VLLMs）を2種類の報酬モデルとして、複数の視覚言語ベンチマークで比較し、それぞれのタスクで一貫して優れているものはありません、そして優れたVLLMsは必ずしも報酬の性能が良くなるわけではないことを示しています。ベンチマークの進歩に向けて、ViLBenchという視覚言語ベンチマークを介して、厳密なプロセス報酬シグナルを必要とするように設計し、これにより現在のVLLMsの評価を進めることを試みます。特に、OpenAIのGPT-4oはChain-of-Thought（CoT）を使用しても27.3%の精度を達成することを示し、これは現在のVLLMsの評価の難易度を示しています。最後に、一般的なVLLMsと報酬モデルの間の隙を埋めるための可能性を示すために、拡張された木検索アルゴリズムを使用して73.6Kの視覚言語プロセス報酬データを収集し、これにより3Bモデルは平均的にCoTより3.3%の改善、そしてOpenAI o1の生成を選択した場合、その未学習コンペナントより2.5%の改善を達成します。実装はhttps://ucsc-vlaa.github.io/ViLBenchで公開しています。",
      "upvotes": 2,
      "discussionId": "67e4dc6b38e4d1444c71cee2",
      "ai_keywords": [
        "vision large language models (VLLMs)",
        "output reward models (ORMs)",
        "process reward models (PRMs)",
        "vision-language benchmarks",
        "Chain-of-Thought (CoT)",
        "vision-language process reward data",
        "tree-search algorithm",
        "ViLBench"
      ]
    },
    "publishedAt": "2025-03-26T02:38:31.000Z",
    "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling",
    "summary": "Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\nand data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20271.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "604ae011caabafacfa48e3de",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1615519738679-noauth.jpeg",
      "fullname": "Haoqin Tu",
      "name": "PahaII",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20198",
      "authors": [
        {
          "_id": "67e4b98039509b0149142daa",
          "name": "Alex Jinpeng Wang",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dab",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dac",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dad",
          "name": "Lijuan Wang",
          "hidden": false
        },
        {
          "_id": "67e4b98039509b0149142dae",
          "name": "Min Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T03:44:25.000Z",
      "submittedOnDailyAt": "2025-03-27T01:06:03.239Z",
      "title": "ワードよりも遠く：多モデル自動復元モデルによる長文画像生成の進歩",
      "submittedOnDailyBy": {
        "_id": "62333a88fd7bb4a39b92d387",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
        "isPro": false,
        "fullname": "Alex Jinpeng Wang",
        "user": "Awiny",
        "type": "user"
      },
      "summary": "最近の自動復元モデルとディフュージョンモデルの進展は、短文書きの画像生成に強い性能を達成しました。しかし、画像にコラーな、長文の生成（例えばスライドや文書のパグラフ）は現在の生成モデルでは主な課題として残されています。私たちは、現在の文図画像システムで通常は短いフレーズや単語だけを処理するという重要な欠点を解決するために、長文画像生成に特化した最初の研究を紹介します。最先端の自動復元生成モデルの詳細な分析を通じて、画像トーキナナラーを文生成の質の重要なボトルネックとして識別しました。これを解決するために、文ドライブニュートライナラーを導入し、詳細な場面文字の特徴を捉えることを最適化しました。このトーキナナラーを拡張して、高品質の長文画像を生成するための多タイプ自動復元モデルを開発しました。このモデルは、フォントスタイル、サイズ、カラー、アライメントなどの文の属性をカスタマイズするための強力的な制御性を提供します。拡張なエクスペリメントは、DALL-E 3、SD3.5 Large、GPT4oと比較して、長文の生成に優れた精度、一致性、柔軟性を示しました。このモデルは、ドキュメントとスライドの交差生成などの創造的なアプリケーションの機会を開拓し、長文画像生成の新しい境界をめざしています。",
      "upvotes": 2,
      "discussionId": "67e4b98439509b0149142f3c",
      "ai_keywords": [
        "autoregressive models",
        "diffusion models",
        "image tokenizer",
        "binary tokenizer",
        "multimodal autoregressive model",
        "font style",
        "text properties",
        "size",
        "color",
        "alignment",
        "SD3.5 Large",
        "GPT4o",
        "DALL-E 3",
        "long-text image generation"
      ]
    },
    "publishedAt": "2025-03-25T23:44:25.000Z",
    "title": "Beyond Words: Advancing Long-Text Image Generation via Multimodal\n  Autoregressive Models",
    "summary": "Recent advancements in autoregressive and diffusion models have led to strong\nperformance in image generation with short scene text words. However,\ngenerating coherent, long-form text in images, such as paragraphs in slides or\ndocuments, remains a major challenge for current generative models. We present\nthe first work specifically focused on long text image generation, addressing a\ncritical gap in existing text-to-image systems that typically handle only brief\nphrases or single sentences. Through comprehensive analysis of state-of-the-art\nautoregressive generation models, we identify the image tokenizer as a critical\nbottleneck in text generating quality. To address this, we introduce a novel\ntext-focused, binary tokenizer optimized for capturing detailed scene text\nfeatures. Leveraging our tokenizer, we develop \\ModelName, a multimodal\nautoregressive model that excels in generating high-quality long-text images\nwith unprecedented fidelity. Our model offers robust controllability, enabling\ncustomization of text properties such as font style, size, color, and\nalignment. Extensive experiments demonstrate that \\ModelName~significantly\noutperforms SD3.5 Large~sd3 and GPT4o~gpt4o with DALL-E\n3~dalle3 in generating long text accurately, consistently, and flexibly.\nBeyond its technical achievements, \\ModelName~opens up exciting opportunities\nfor innovative applications like interleaved document and PowerPoint\ngeneration, establishing a new frontier in long-text image generating.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20198.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62333a88fd7bb4a39b92d387",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png",
      "fullname": "Alex Jinpeng Wang",
      "name": "Awiny",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19846",
      "authors": [
        {
          "_id": "67e4c449672b3d9c9cb8aa4b",
          "user": {
            "_id": "67e4c444692ba54248a6b337",
            "avatarUrl": "/avatars/7695a30d185ccb2354129c72538e9180.svg",
            "isPro": false,
            "fullname": "Aaron Serianni",
            "user": "serianni",
            "type": "user"
          },
          "name": "Aaron Serianni",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-27T09:03:27.573Z",
          "hidden": false
        },
        {
          "_id": "67e4c449672b3d9c9cb8aa4c",
          "name": "Tyler Zhu",
          "hidden": false
        },
        {
          "_id": "67e4c449672b3d9c9cb8aa4d",
          "name": "Olga Russakovsky",
          "hidden": false
        },
        {
          "_id": "67e4c449672b3d9c9cb8aa4e",
          "name": "Vikram V. Ramaswamy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:11:39.000Z",
      "submittedOnDailyAt": "2025-03-27T01:57:35.164Z",
      "title": "Attention IoU: ショートランキング IoU: ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキング ショートランキ",
      "submittedOnDailyBy": {
        "_id": "67e4c444692ba54248a6b337",
        "avatarUrl": "/avatars/7695a30d185ccb2354129c72538e9180.svg",
        "isPro": false,
        "fullname": "Aaron Serianni",
        "user": "serianni",
        "type": "user"
      },
      "summary": "コンピュータビジョンモデルは、広範囲のデータセットとタスクで偏りを表現し、拡大することが見られています。クラス分類モデルにおける偏りの定量化の既存の方法は、データセットの分布とモデルのサブグループの性能に焦点を当てていますが、モデルの内部機能を見落としています。私たちは、Attention-IoU（Attention Intersection over Union）メトリックと関連するスコアを紹介し、これらはアテンションマップを使用してモデルの内部表現での偏りを明らかにし、バイアスを引き起こす可能性のある画像特徴量を特定します。まず、Attention-IoUを合成データセット「Waterbirds」で検証し、このメトリックがモデルの偏りを正確に測定することを示します。次に、CelebAデータセットを分析し、Attention-IoUが精度の差異よりも関連性を明らかにします。保護属性「Male」を通じて個々の属性を調査することで、CelebAでの偏りの表現の特別な方法を調査します。最後に、トレーニングセットを部分サンプリングして属性関連性を変更することで、Attention-IoUがデータセットラベルに含まれない潜在的な混雑変数を明らかにします。",
      "upvotes": 2,
      "discussionId": "67e4c44a672b3d9c9cb8aaae",
      "ai_keywords": [
        "Attention-IoU",
        "attention maps",
        "internal representations",
        "image features",
        "Waterbirds dataset",
        "CelebA dataset",
        "accuracy disparities",
        "protected attribute",
        "attribute correlations",
        "confounding variables"
      ]
    },
    "publishedAt": "2025-03-25T13:11:39.000Z",
    "title": "Attention IoU: Examining Biases in CelebA using Attention Maps",
    "summary": "Computer vision models have been shown to exhibit and amplify biases across a\nwide array of datasets and tasks. Existing methods for quantifying bias in\nclassification models primarily focus on dataset distribution and model\nperformance on subgroups, overlooking the internal workings of a model. We\nintroduce the Attention-IoU (Attention Intersection over Union) metric and\nrelated scores, which use attention maps to reveal biases within a model's\ninternal representations and identify image features potentially causing the\nbiases. First, we validate Attention-IoU on the synthetic Waterbirds dataset,\nshowing that the metric accurately measures model bias. We then analyze the\nCelebA dataset, finding that Attention-IoU uncovers correlations beyond\naccuracy disparities. Through an investigation of individual attributes through\nthe protected attribute of Male, we examine the distinct ways biases are\nrepresented in CelebA. Lastly, by subsampling the training set to change\nattribute correlations, we demonstrate that Attention-IoU reveals potential\nconfounding variables not present in dataset labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19846.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "67e4c444692ba54248a6b337",
      "avatarUrl": "/avatars/7695a30d185ccb2354129c72538e9180.svg",
      "fullname": "Aaron Serianni",
      "name": "serianni",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20220",
      "authors": [
        {
          "_id": "67e4b035af7d0551dc377e13",
          "name": "Weijie Guo",
          "hidden": false
        },
        {
          "_id": "67e4b035af7d0551dc377e14",
          "name": "Guofeng Zhang",
          "hidden": false
        },
        {
          "_id": "67e4b035af7d0551dc377e15",
          "name": "Wufei Ma",
          "hidden": false
        },
        {
          "_id": "67e4b035af7d0551dc377e16",
          "name": "Alan Yuille",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T04:23:53.000Z",
      "submittedOnDailyAt": "2025-03-27T00:26:48.304Z",
      "title": "DINeMo: 3Dアノテーションなしでニューラルメッシュモデルを学習する",
      "submittedOnDailyBy": {
        "_id": "625f81afe1994410eef1c36a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650426282769-noauth.jpeg",
        "isPro": false,
        "fullname": "Wufei Ma",
        "user": "wufeim",
        "type": "user"
      },
      "summary": "カテゴリレベル3D/6D姿勢推定は、3Dスケーン理解のための重要なステップで、ロボティクスおよび具象AIにおいて広く応用可能です。最近の研究は、分析ベースの合成からの視点から2Dと3Dの様々なタスクを扱うニューラルメッシュモデルについて調査しました。部分遮蔽とドメインシフトに対する強固性が大幅に向上したのに反して、これらの方法は、部品比較的学習による3D注釈の依存関係が強いため、狭いカテゴリーへの制限があり、効率的なスケーリングを妨げていました。本研究では、大規模な可視基礎モデルから得られるファクトローバーコンピロンダテージを利用して3D注釈を必要としない新しいニューラルメッシュモデル、DINeMoを紹介します。双方向的なファクトローバーコンピロンダテージ生成法を採用し、局所的な外観特徴と全球的なコンテキスト情報を両方とも使用してファクトローバーコンピロンダテージを生成します。車用データセット上での実験結果は、我々のDINeMoが先行の0-shotとfew-shot 3D姿勢推定を大幅に超え、完全サブジューバー方法との間の間違いを67.3%に縮小しました。DINeMoは、学習中に追加する無ラベル画像を含めても効率的にスケーリングでき、3D注釈に依存するサブジューバー学習モデルよりも優れた性能を示します。プロジェクトページは、https://analysis-by-synthesis.github.io/DINeMo/ にアクセス可能です。",
      "upvotes": 1,
      "discussionId": "67e4b038af7d0551dc377f07",
      "projectPage": "https://analysis-by-synthesis.github.io/DINeMo/",
      "ai_keywords": [
        "neural mesh models",
        "analysis-by-synthesis",
        "robustness",
        "partial occlusion",
        "domain shifts",
        "part-contrastive learning",
        "pseudo-correspondence",
        "visual foundation models",
        "bidirectional pseudo-correspondence generation",
        "local appearance features",
        "global context information",
        "zero-shot",
        "few-shot",
        "fully-supervised methods"
      ]
    },
    "publishedAt": "2025-03-26T00:23:53.000Z",
    "title": "DINeMo: Learning Neural Mesh Models with no 3D Annotations",
    "summary": "Category-level 3D/6D pose estimation is a crucial step towards comprehensive\n3D scene understanding, which would enable a broad range of applications in\nrobotics and embodied AI. Recent works explored neural mesh models that\napproach a range of 2D and 3D tasks from an analysis-by-synthesis perspective.\nDespite the largely enhanced robustness to partial occlusion and domain shifts,\nthese methods depended heavily on 3D annotations for part-contrastive learning,\nwhich confines them to a narrow set of categories and hinders efficient\nscaling. In this work, we present DINeMo, a novel neural mesh model that is\ntrained with no 3D annotations by leveraging pseudo-correspondence obtained\nfrom large visual foundation models. We adopt a bidirectional\npseudo-correspondence generation method, which produce pseudo correspondence\nutilize both local appearance features and global context information.\nExperimental results on car datasets demonstrate that our DINeMo outperforms\nprevious zero- and few-shot 3D pose estimation by a wide margin, narrowing the\ngap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively\nand efficiently when incorporating more unlabeled images during training, which\ndemonstrate the advantages over supervised learning methods that rely on 3D\nannotations. Our project page is available at\nhttps://analysis-by-synthesis.github.io/DINeMo/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20220.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625f81afe1994410eef1c36a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650426282769-noauth.jpeg",
      "fullname": "Wufei Ma",
      "name": "wufeim",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.19953",
      "authors": [
        {
          "_id": "67e4b46cc90e5edf25f581f8",
          "name": "Stefan Stojanov",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581f9",
          "name": "David Wendt",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fa",
          "name": "Seungwoo Kim",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fb",
          "name": "Rahul Venkatesh",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fc",
          "name": "Kevin Feigelis",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fd",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "67e4b46cc90e5edf25f581fe",
          "name": "Daniel LK Yamins",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-25T17:58:52.000Z",
      "submittedOnDailyAt": "2025-03-27T00:44:41.359Z",
      "title": "動作概念の自動認識を最適化するカウンタフェクタル学習",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "ビデオにおける動きの推定は、複数の下流アプリケーションを含む重要なコンピュータビジョン問題です。これには、制御可能なビデオ生成やロボット工学などが含まれます。現在の解決策は主に合成データを用いて訓練されていますか、または状況に適合するヒューリスティクスの調整が必要です。これは、モデルの能力を実世界のコンテキストで制限します。最近、ビデオからの大規模な自動認識学習についての進展がありますが、これらの表現を動きの推定に活用するのは相対的に調査が不足しています。本研究では、既に学習された次のフレーム予測モデルを用いて、流れと遮蔽の推定を行う自動認識手法を開発しました。Opt-CWMは、基盤ビデオモデルから動き情報を抽出するために、実際のビデオ入力を用いて学習することで、固定ヒューリスティクスの必要性を回避します。実世界のビデオにおける動きの推定に最先端の性能を達成し、標準化データが必要とならないようにしました。",
      "upvotes": 1,
      "discussionId": "67e4b46ec90e5edf25f582db",
      "ai_keywords": [
        "self-supervised learning",
        "flow estimation",
        "occlusion estimation",
        "next-frame prediction model",
        "counterfactual probes",
        "motion estimation"
      ]
    },
    "publishedAt": "2025-03-25T13:58:52.000Z",
    "title": "Self-Supervised Learning of Motion Concepts by Optimizing\n  Counterfactuals",
    "summary": "Estimating motion in videos is an essential computer vision problem with many\ndownstream applications, including controllable video generation and robotics.\nCurrent solutions are primarily trained using synthetic data or require tuning\nof situation-specific heuristics, which inherently limits these models'\ncapabilities in real-world contexts. Despite recent developments in large-scale\nself-supervised learning from videos, leveraging such representations for\nmotion estimation remains relatively underexplored. In this work, we develop\nOpt-CWM, a self-supervised technique for flow and occlusion estimation from a\npre-trained next-frame prediction model. Opt-CWM works by learning to optimize\ncounterfactual probes that extract motion information from a base video model,\navoiding the need for fixed heuristics while training on unrestricted video\ninputs. We achieve state-of-the-art performance for motion estimation on\nreal-world videos while requiring no labeled data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19953.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6479
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.17358",
      "authors": [
        {
          "_id": "67e3b41d0fa8f886db6b323a",
          "name": "Jerred Chen",
          "hidden": false
        },
        {
          "_id": "67e3b41d0fa8f886db6b323b",
          "name": "Ronald Clark",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/jIZXKaHvYvl_B_Tg_SAeY.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/cn0aTY48FezRlyiSMberj.mp4"
      ],
      "publishedAt": "2025-03-21T17:58:56.000Z",
      "submittedOnDailyAt": "2025-03-27T07:45:14.887Z",
      "title": "画像をIMUとして：動きブラーの画像からカメラの動きを推定する",
      "submittedOnDailyBy": {
        "_id": "6305ee63d70693fdf1c7dbb8",
        "avatarUrl": "/avatars/0e81ed3757b4e65be82063b538c3fe49.svg",
        "isPro": false,
        "fullname": "Ronald Clark",
        "user": "r0nn13",
        "type": "user"
      },
      "summary": "ロボティクスおよびVR/ARアプリケーションでは、高速なカメラ動きにより高レベルの移動ブラーが発生し、既存のカメラ姿勢計測方法が失敗することがあります。本稿では、移動ブラーを移動計測の豊富なカットとして機能させることで失敗している方法を改良する新しいフレームワークを提案します。我々のアプローチは、単一の移動ブラー画像から直接に豊富な移動フローフィールドと単目の深さマップを予測することで働きます。次に、小さな移動を仮定して線形最小二乗問題を解くことで瞬時のカメラ速度を復元します。本質的に、我々の方法は、強靭に高速と攻撃的なカメラ動きを捉えるIMUマイナスエフェクトのような計測を提供します。モデルの訓練には、ScanNet++v2からの写実的な合成移動ブラーを用いた大規模なデータセットを構築し、完全に微分可能なパイプラインを使用して実データで終端から訓練することでモデルを進化させます。実世界のベンチマークでの拡大評価により、我々の方法は現在の方法よりも最先端の角速度と移動速度の計測を実現し、MASt3RやCOLMAPなどの方法を超えることが証明されました。",
      "upvotes": 1,
      "discussionId": "67e3b4200fa8f886db6b3328",
      "projectPage": "https://jerredchen.github.io/image-as-imu/",
      "ai_keywords": [
        "motion blur",
        "camera pose estimation",
        "motion flow field",
        "monocular depth map",
        "linear least squares problem",
        "small motion assumption",
        "IMU-like measurement",
        "ScanNet++v2",
        "fully differentiable pipeline",
        "real-world benchmarks",
        "angular velocity estimates",
        "translational velocity estimates"
      ]
    },
    "publishedAt": "2025-03-21T13:58:56.000Z",
    "title": "Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred\n  Image",
    "summary": "In many robotics and VR/AR applications, fast camera motions cause a high\nlevel of motion blur, causing existing camera pose estimation methods to fail.\nIn this work, we propose a novel framework that leverages motion blur as a rich\ncue for motion estimation rather than treating it as an unwanted artifact. Our\napproach works by predicting a dense motion flow field and a monocular depth\nmap directly from a single motion-blurred image. We then recover the\ninstantaneous camera velocity by solving a linear least squares problem under\nthe small motion assumption. In essence, our method produces an IMU-like\nmeasurement that robustly captures fast and aggressive camera movements. To\ntrain our model, we construct a large-scale dataset with realistic synthetic\nmotion blur derived from ScanNet++v2 and further refine our model by training\nend-to-end on real data using our fully differentiable pipeline. Extensive\nevaluations on real-world benchmarks demonstrate that our method achieves\nstate-of-the-art angular and translational velocity estimates, outperforming\ncurrent methods like MASt3R and COLMAP.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/jIZXKaHvYvl_B_Tg_SAeY.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/6305ee63d70693fdf1c7dbb8/cn0aTY48FezRlyiSMberj.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.17358.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6305ee63d70693fdf1c7dbb8",
      "avatarUrl": "/avatars/0e81ed3757b4e65be82063b538c3fe49.svg",
      "fullname": "Ronald Clark",
      "name": "r0nn13",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16870",
      "authors": [
        {
          "_id": "67e4c860136c8a867191e52e",
          "user": {
            "_id": "6627ff2a3b4fbc8420a416c3",
            "avatarUrl": "/avatars/de3aafdaf5563fe25edcdb92b394474f.svg",
            "isPro": false,
            "fullname": "AR",
            "user": "Anshumann",
            "type": "user"
          },
          "name": "Anshumann",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:03:45.930Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e52f",
          "user": {
            "_id": "61765fe0b0715831eab6d465",
            "avatarUrl": "/avatars/830406cebdd7ddf66799bf71f6bbff4b.svg",
            "isPro": false,
            "fullname": "Mohd Abbas Zaidi",
            "user": "ya-mehdi",
            "type": "user"
          },
          "name": "Mohd Abbas Zaidi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:03:57.163Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e530",
          "name": "Akhil Kedia",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e531",
          "user": {
            "_id": "65d61eebee922bc7a777d5a6",
            "avatarUrl": "/avatars/180efd503ef412b4dc728e6aa477c16e.svg",
            "isPro": false,
            "fullname": "Jinwoo Ahn",
            "user": "AndrewAhn",
            "type": "user"
          },
          "name": "Jinwoo Ahn",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:07.687Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e532",
          "user": {
            "_id": "629059cdb90dde28ef5cbb30",
            "avatarUrl": "/avatars/451b0e8999447b3a2f03378fe98c0661.svg",
            "isPro": false,
            "fullname": "Taehwak Kwon",
            "user": "Rock222",
            "type": "user"
          },
          "name": "Taehwak Kwon",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:23.731Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e533",
          "user": {
            "_id": "6354137306d707b332451421",
            "avatarUrl": "/avatars/46770f32702e3ad08f91faeef9e4ea6e.svg",
            "isPro": false,
            "fullname": "Kangwook Lee",
            "user": "kw1jjang",
            "type": "user"
          },
          "name": "Kangwook Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:33.400Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e534",
          "user": {
            "_id": "653b6e7232bd4db35d981615",
            "avatarUrl": "/avatars/0fbf0c5d502b840bf26baf8c420c7593.svg",
            "isPro": false,
            "fullname": "Haejun Lee",
            "user": "haejunlee",
            "type": "user"
          },
          "name": "Haejun Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:39.155Z",
          "hidden": false
        },
        {
          "_id": "67e4c860136c8a867191e535",
          "user": {
            "_id": "64b4be0665a7e15eac085878",
            "avatarUrl": "/avatars/dd19ec2987fb0735457c6492b53aacfe.svg",
            "isPro": false,
            "fullname": "Joo-hyung Lee",
            "user": "snrbs17",
            "type": "user"
          },
          "name": "Joohyung Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-03-27T10:04:46.702Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-21T05:58:18.000Z",
      "submittedOnDailyAt": "2025-03-27T02:09:27.795Z",
      "title": "Sparse Logit Sampling: LLMの知識転移を加速する",
      "submittedOnDailyBy": {
        "_id": "61765fe0b0715831eab6d465",
        "avatarUrl": "/avatars/830406cebdd7ddf66799bf71f6bbff4b.svg",
        "isPro": false,
        "fullname": "Mohd Abbas Zaidi",
        "user": "ya-mehdi",
        "type": "user"
      },
      "summary": "知識転移は、教師の出力ロジットが事前計算されてキャッシュされている場合、大規模言語モデルの知識を効率的に転移する手がかりです。しかし、これを予め学習に成功して適用することはまだ大きく探索されていません。本研究では、稀疏知識転移の直感的なアプローチ（例：Top-K確率のキャッシュ）が教師の確率分布を学生に偏った評価によって、最適な性能と補正につながりませんでした。私たちは、無偏的な評価を提供し、期待値で勾配を保持し、エネルギー的に非常に稀疏なロジットを保存するための重要度サンプリングベースの方法「ランダムサンプリング知識転移」を提案します。この方法は、交差エントロピーに基づく訓練に比べて微調整（<10%）で学生モデルの学習を高速化でき、300Mから3Bの様々なモデルサイズでの性能に対して、完全な知識転移と競争的な性能を維持します。",
      "upvotes": 1,
      "discussionId": "67e4c861136c8a867191e58c",
      "ai_keywords": [
        "Knowledge distillation",
        "Large Language Models",
        "teacher output logits",
        "pre-computed",
        "cached",
        "sparse knowledge distillation",
        "Top-K probabilities",
        "biased estimates",
        "teacher probability distribution",
        "student",
        "suboptimal performance",
        "calibration",
        "importance-sampling-based method",
        "Random Sampling Knowledge Distillation",
        "unbiased estimates",
        "gradient in expectation",
        "storing significantly sparser logits",
        "cross-entropy based training",
        "competitive performance",
        "model sizes"
      ]
    },
    "publishedAt": "2025-03-21T01:58:18.000Z",
    "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
    "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16870.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61765fe0b0715831eab6d465",
      "avatarUrl": "/avatars/830406cebdd7ddf66799bf71f6bbff4b.svg",
      "fullname": "Mohd Abbas Zaidi",
      "name": "ya-mehdi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]