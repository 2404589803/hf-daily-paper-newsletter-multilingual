[
  {
    "paper": {
      "id": "2506.19851",
      "authors": [
        {
          "_id": "685b5a46d2ee4fac76521dce",
          "user": {
            "_id": "6375d136dee28348a9c63cbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
            "isPro": false,
            "fullname": "zehuan-huang",
            "user": "huanngzh",
            "type": "user"
          },
          "name": "Zehuan Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-25T08:19:21.031Z",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dcf",
          "user": {
            "_id": "65240d0ca801972b6eb12ed8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65240d0ca801972b6eb12ed8/hl2RAssBperb5JlgOIDvw.jpeg",
            "isPro": false,
            "fullname": "Haoran Feng",
            "user": "fenghora",
            "type": "user"
          },
          "name": "Haoran Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T09:19:31.409Z",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd0",
          "user": {
            "_id": "63a41cb584a6a25c65bd8316",
            "avatarUrl": "/avatars/1d474831c320c7f9ca9e6d88f68acc06.svg",
            "isPro": false,
            "fullname": "Yangtian Sun",
            "user": "Yang-Tian",
            "type": "user"
          },
          "name": "Yangtian Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T09:19:29.490Z",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd1",
          "name": "Yuanchen Guo",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd2",
          "user": {
            "_id": "638066faf022c8a5803f7eb8",
            "avatarUrl": "/avatars/4cfd699c3f6c5461b12b7dc5e3fe183d.svg",
            "isPro": false,
            "fullname": "Yanpei Cao",
            "user": "pookiefoof",
            "type": "user"
          },
          "name": "Yanpei Cao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-25T08:21:27.467Z",
          "hidden": false
        },
        {
          "_id": "685b5a46d2ee4fac76521dd3",
          "user": {
            "_id": "65b722dbe02a17f0f8d1cc6b",
            "avatarUrl": "/avatars/65f20601ef9b8ebfdddadd737f9153d6.svg",
            "isPro": false,
            "fullname": "Lu Sheng",
            "user": "lsheng2024",
            "type": "user"
          },
          "name": "Lu Sheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T09:19:33.186Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a96a375a69e2ca889abdff/ttfXuQOJQmOqnO-_-F1op.mp4"
      ],
      "publishedAt": "2025-06-24T17:59:58.000Z",
      "submittedOnDailyAt": "2025-06-25T01:12:40.364Z",
      "title": "アニマX: 3Dで無生物を動かすビデオ-ポーズディフュージョンモデル",
      "submittedOnDailyBy": {
        "_id": "64a96a375a69e2ca889abdff",
        "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
        "isPro": false,
        "fullname": "fanhongxing",
        "user": "fanhongxing",
        "type": "user"
      },
      "summary": "AnimaXは、ビデオディフュージョンモデルの動き先驭とスケルトンベースアニメーションの制御可能な構造を結びつける前向き3Dアニメーションフレームワークです。従来の動き合成手法は、固定のスケルトントピオロジーに制限されていたり、高次元の変形空間で費用高い最適化が必要となっていました。反対に、AnimaXは効果的にビデオベースの動き知識を3D領域に伝え、任意のスケルトンを持つ多様なアーティカルメッシュをサポートします。我々の方法では、3D動きを多角度、多フレーム2D姿勢マップとして表現し、テンプレートレンディングと文字列ベースの動きプロンプトに基づく連続ビデオと姿勢のディフュージョンを可能にします。また、ビデオと姿勢シーケンスの空間時間のアライメントを確保するために共有位置付けエンコーディングとモデル認識エンブディングを導入します。これにより、ビデオの先驭を動き生成タスクに効果的に伝えます。そして、これらの多角度姿勢シーケンスは三角化され3D関節位置に変換され、逆関節キネマティクスによりメッシュアニメーションに変換されます。新規整頓されたデータセット（160,000プロップシーケンス）で訓練されたAnimaXは、VBenchでの一般化、動きの忠実性、および効率性において最先端の結果を収め、カテゴリ無視的な3Dアニメーションのスケーラブルな解決策を提供します。プロジェクトページは、https://anima-x.github.io/ です。",
      "upvotes": 30,
      "discussionId": "685b5a47d2ee4fac76521dd4",
      "projectPage": "https://anima-x.github.io/",
      "githubRepo": "https://github.com/anima-x/anima-x",
      "ai_summary": "AnimaX creates multi-skeleton 3D animations by blending video diffusion model priors with skeleton-based control, using joint video-pose diffusion and shared positional encodings.",
      "ai_keywords": [
        "feed-forward 3D animation framework",
        "video diffusion models",
        "skeleton-based animation",
        "motion synthesis",
        "high-dimensional deformation spaces",
        "2D pose maps",
        "joint video-pose diffusion",
        "template renderings",
        "textual motion prompt",
        "shared positional encodings",
        "modality-aware embeddings",
        "spatial-temporal alignment",
        "inverse kinematics",
        "VBench",
        "category-agnostic 3D animation"
      ],
      "githubStars": 34
    },
    "publishedAt": "2025-06-24T13:59:58.000Z",
    "title": "AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion\n  Models",
    "summary": "We present AnimaX, a feed-forward 3D animation framework that bridges the\nmotion priors of video diffusion models with the controllable structure of\nskeleton-based animation. Traditional motion synthesis methods are either\nrestricted to fixed skeletal topologies or require costly optimization in\nhigh-dimensional deformation spaces. In contrast, AnimaX effectively transfers\nvideo-based motion knowledge to the 3D domain, supporting diverse articulated\nmeshes with arbitrary skeletons. Our method represents 3D motion as multi-view,\nmulti-frame 2D pose maps, and enables joint video-pose diffusion conditioned on\ntemplate renderings and a textual motion prompt. We introduce shared positional\nencodings and modality-aware embeddings to ensure spatial-temporal alignment\nbetween video and pose sequences, effectively transferring video priors to\nmotion generation task. The resulting multi-view pose sequences are\ntriangulated into 3D joint positions and converted into mesh animation via\ninverse kinematics. Trained on a newly curated dataset of 160,000 rigged\nsequences, AnimaX achieves state-of-the-art results on VBench in\ngeneralization, motion fidelity, and efficiency, offering a scalable solution\nfor category-agnostic 3D animation. Project page:\nhttps://anima-x.github.io/{https://anima-x.github.io/}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a96a375a69e2ca889abdff/ttfXuQOJQmOqnO-_-F1op.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19851.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a96a375a69e2ca889abdff",
      "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
      "fullname": "fanhongxing",
      "name": "fanhongxing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.16141",
      "authors": [
        {
          "_id": "6858b1fac0c8e29df8ea3c18",
          "name": "Yi Chen",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c19",
          "name": "Yuying Ge",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1a",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1b",
          "name": "Yixiao Ge",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1c",
          "name": "Junhao Cheng",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1d",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "6858b1fac0c8e29df8ea3c1e",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-19T08:49:13.000Z",
      "submittedOnDailyAt": "2025-06-25T01:50:33.428Z",
      "title": "GRPO-CARE: マルチモーダル推論向けの一致性意識のある強化学習",
      "submittedOnDailyBy": {
        "_id": "60d045c4778bafd0fbcfa3f5",
        "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
        "isPro": false,
        "fullname": "Yi Chen",
        "user": "ChenYi99",
        "type": "user"
      },
      "summary": "最近の強化学習アプローチの例である結果観測グローバルポリシー（GRPO）は、大規模言語モデル（LLMs）の思考進行（Chain-of-Thought reasoning）に進展していますが、これらのアプローチは多タイプ言語モデル（MLLMs）に適用されていません。MLLMのトレーニング後の方法についての厳密な評価の欠如を解決するために、SEED-Bench-R1というベンチマークを導入しました。これは複雑な実世界的な映像での均衡的な認識と理由論を必要とするもので、大規模なトレーニングセットを提供し、分布内、環境間、タスク間の3つの進段的な挑戦における一般化を評価します。SEED-Bench-R1を使用して、標準GRPOは答えの正確性を向上させるにもかかわらず、理由の連鎖と答えの間の論理的な一致性を減少させ、一致率が57.9%でした。これは、報酬信号が最終的な答えだけに焦点を当て、短絡道を促成し、厳格なKLペナルティが探索を制限しているからです。これに対して、GRPO-CAREという理由の一致性を意識したRLフレームワークを提案します。これは答えの正確性と理由の一致性を最適化し、明示的なスーパービジョンを不要としています。GRPO-CAREは2段階の報酬を導入します：（1）答えの正確性に対する基礎的な報酬と（2）理由の一致性に対する適応的なベストワークボーナス、これはモデルの理由の一致性を計算するために、徐々に進化する参照モデルを使用して、グループのペアとの比較を行います。この双重機構は、正確で論理的に一致した理由のパスに対して報酬を増幅させます。KLペナルティをこの適応的ボーナスに置き換えると、GRPO-CAREはSEED-Bench-R1で標準GRPOを上回り、最難易度の評価レベルで6.7%の性能向上と、24.5%の一致性向上を収めます。また、強いトランスファレンスを示し、多様な映像理解ベンチマークでのモデルの性能を向上させます。我々の研究は、システム的に設計されたベンチマークと一般化可能なトレーニング後のフレームワークを提供し、解釈的で強固なMLLMの開発に貢献します。",
      "upvotes": 22,
      "discussionId": "6858b1fac0c8e29df8ea3c1f",
      "githubRepo": "https://github.com/TencentARC/GRPO-CARE",
      "ai_summary": "GRPO-CARE, a reinforcement learning framework optimizing for consistency and correctness, outperforms standard GRPO on a new video understanding benchmark, SEED-Bench-R1, improving both performance and logical coherence in multimodal large language models.",
      "ai_keywords": [
        "reinforcement learning",
        "outcome-supervised GRPO",
        "Chain-of-Thought reasoning",
        "large language models",
        "multimodal large language models",
        "SEED-Bench-R1",
        "in-distribution",
        "cross-environment",
        "cross-environment-task",
        "logical coherence",
        "reasoning steps",
        "answer accuracy",
        "reward signals",
        "shortcuts",
        "KL penalties",
        "exploration",
        "consistency-aware RL framework",
        "two-tiered reward",
        "reasoning-to-answer likelihood",
        "adaptive consistency bonus",
        "video understanding benchmarks",
        "transferability",
        "interpretable models",
        "robust models"
      ],
      "githubStars": 19
    },
    "publishedAt": "2025-06-19T04:49:13.000Z",
    "title": "GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal\n  Reasoning",
    "summary": "Recent reinforcement learning approaches, such as outcome-supervised GRPO,\nhave advanced Chain-of-Thought reasoning in large language models (LLMs), yet\ntheir adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack\nof rigorous evaluation for MLLM post-training methods, we introduce\nSEED-Bench-R1, a benchmark with complex real-world videos requiring balanced\nperception and reasoning. It offers a large training set and evaluates\ngeneralization across three escalating challenges: in-distribution,\ncross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,\nwe find that standard GRPO, while improving answer accuracy, often reduces\nlogical coherence between reasoning steps and answers, with only a 57.9%\nconsistency rate. This stems from reward signals focusing solely on final\nanswers, encouraging shortcuts, and strict KL penalties limiting exploration.To\naddress this, we propose GRPO-CARE, a consistency-aware RL framework optimizing\nboth answer correctness and reasoning coherence without explicit supervision.\nGRPO-CARE introduces a two-tiered reward: (1) a base reward for answer\ncorrectness, and (2) an adaptive consistency bonus, computed by comparing the\nmodel's reasoning-to-answer likelihood (via a slowly-evolving reference model)\nagainst group peers.This dual mechanism amplifies rewards for reasoning paths\nthat are both correct and logically consistent. Replacing KL penalties with\nthis adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,\nachieving a 6.7% performance gain on the hardest evaluation level and a 24.5%\nimprovement in consistency. It also shows strong transferability, improving\nmodel performance across diverse video understanding benchmarks. Our work\ncontributes a systematically designed benchmark and a generalizable\npost-training framework, advancing the development of more interpretable and\nrobust MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16141.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d045c4778bafd0fbcfa3f5",
      "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
      "fullname": "Yi Chen",
      "name": "ChenYi99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19848",
      "authors": [
        {
          "_id": "685b7cc2d2ee4fac76521e83",
          "name": "Long Xing",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e84",
          "user": {
            "_id": "656f1b21b075b63c90ba02ee",
            "avatarUrl": "/avatars/d6856815ef06261394178161e4d511b4.svg",
            "isPro": false,
            "fullname": "Huang Qidong",
            "user": "shikiw",
            "type": "user"
          },
          "name": "Qidong Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:23.757Z",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e85",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e86",
          "name": "Pan Zhang",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e87",
          "user": {
            "_id": "63859cf3b2906edaf83af9f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
            "isPro": false,
            "fullname": "Yuhang Zang",
            "user": "yuhangzang",
            "type": "user"
          },
          "name": "Yuhang Zang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:21.449Z",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e88",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e89",
          "name": "Jinsong Li",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8a",
          "name": "Shuangrui Ding",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8b",
          "name": "Weiming Zhang",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8c",
          "name": "Nenghai Yu",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8d",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8e",
          "name": "Feng Wu",
          "hidden": false
        },
        {
          "_id": "685b7cc2d2ee4fac76521e8f",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:59:55.000Z",
      "submittedOnDailyAt": "2025-06-25T03:07:04.508Z",
      "title": "スケールキャプチング：ダブルモデュールでのデビアス補正による推論時スケーラブルな画像キャプチング",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "この論文では、スケールカプ（ScaleCap）という推論時スケーラブルな画像キャプションステージを紹介し、詳細な画像キャプションを生成する。高品質な画像キャプションの主な課題は、LVLM（Large Vision Language Model）の固有バイアスである：多タイプバイアスが説明のグラニュリティの不均衡につながり、一部の要素に詳細な説明を提供しながら、他の要素にはそれほど説明を与えない；言語バイアスが存在しない物体についてのハウシャイン化された説明を生成する。これらの問題に対処するために、スケールカプは推論バッジグコストを増やしながらキャプションを継続的に豊富にし、調整するスケーラブルなデビアスフリーキャプションステージを提案している。具体的には、2つの新しい構成要素を提案している：ヒューリスティッククエスチョン回答と対比的な文脈評価。前者は画像に基づいて内容関連のクエストを生成し、それらを答えることで、キャプションに関連する情報を進段的に注入する。後者は文脈レベルのオフライン対比的な解確定を使用し、言語バイアスによるハウシャイン化を有効に識別し、排除する。推論コストが増加することで、スケールカプは進段的に追加の視覚詳細を捉え、より正確な、バランスのある、情報豊富なキャプションを生成する。幅広いモディバイディング実験では、スケールカプの効果が示されている。450K画像をスケールカプで注釈し、LVLMの事前学習に使用したことで、11つの広く使用されているベンチマークでの性能の向上が継続的に見られる。また、スケールカプは、VQAタスクで画像をキャプションに置き換え、キャプションから画像を再構築して語意的なカバレージを評価する2つの追加タスクで、生成されるキャプションの豊富さと忠実度を示している。コードは、https://github.com/Cooperx521/ScaleCap から利用できる。",
      "upvotes": 19,
      "discussionId": "685b7cc2d2ee4fac76521e90",
      "githubRepo": "https://github.com/Cooperx521/ScaleCap",
      "ai_summary": "ScaleCap enhances image captioning by iteratively enriching and calibrating captions using heuristic question answering and contrastive sentence rating, addressing multimodal and linguistic biases to improve accuracy, balance, and informativeness.",
      "ai_keywords": [
        "LVLMs",
        "multimodal bias",
        "linguistic bias",
        "heuristic question answering",
        "contrastive sentence rating",
        "VQA task"
      ],
      "githubStars": 22
    },
    "publishedAt": "2025-06-24T13:59:55.000Z",
    "title": "ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality\n  Debiasing",
    "summary": "This paper presents ScaleCap, an inference-time scalable image captioning\nstrategy that generates comprehensive and detailed image captions. The key\nchallenges of high-quality image captioning lie in the inherent biases of\nLVLMs: multimodal bias resulting in imbalanced descriptive granularity,\noffering detailed accounts of some elements while merely skimming over others;\nlinguistic bias leading to hallucinated descriptions of non-existent objects.\nTo address these issues, we propose a scalable debiased captioning strategy,\nwhich continuously enriches and calibrates the caption with increased inference\nbudget. Specifically, we propose two novel components: heuristic question\nanswering and contrastive sentence rating. The former generates\ncontent-specific questions based on the image and answers them to progressively\ninject relevant information into the caption. The latter employs sentence-level\noffline contrastive decoding to effectively identify and eliminate\nhallucinations caused by linguistic biases. With increased inference cost, more\nheuristic questions are raised by ScaleCap to progressively capture additional\nvisual details, generating captions that are more accurate, balanced, and\ninformative. Extensive modality alignment experiments demonstrate the\neffectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them\nfor LVLM pretraining leads to consistent performance gains across 11 widely\nused benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity\nof generated captions with two additional tasks: replacing images with captions\nin VQA task, and reconstructing images from captions to assess semantic\ncoverage. Code is available at https://github.com/Cooperx521/ScaleCap.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19848.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19290",
      "authors": [
        {
          "_id": "685b6640d2ee4fac76521e42",
          "user": {
            "_id": "6621efe1a6eec3ad03e38759",
            "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
            "isPro": false,
            "fullname": "Liang Zeng",
            "user": "zengliangcs",
            "type": "user"
          },
          "name": "Liang Zeng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:30.529Z",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e43",
          "user": {
            "_id": "612cfc6e1f69b222aacf831b",
            "avatarUrl": "/avatars/b6c7d15ebc7b5dd4b56620bfab324c77.svg",
            "isPro": false,
            "fullname": "lycfight",
            "user": "lycfight",
            "type": "user"
          },
          "name": "Yongcong Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:28.275Z",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e44",
          "name": "Yuzhen Xiao",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e45",
          "name": "Changshi Li",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e46",
          "user": {
            "_id": "658229ef5f6d83438257fce5",
            "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
            "isPro": false,
            "fullname": "Chris (Yuhao) Liu",
            "user": "chrisliu298",
            "type": "user"
          },
          "name": "Chris Yuhao Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:22.815Z",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e47",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e48",
          "name": "Tianwen Wei",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e49",
          "name": "Jujie He",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e4a",
          "name": "Xuchen Song",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e4b",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "685b6640d2ee4fac76521e4c",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T03:53:36.000Z",
      "submittedOnDailyAt": "2025-06-25T01:35:02.603Z",
      "title": "Skywork-SWE: LLMsでのソフトウェア開発のデータスケーリング法を明らかにする",
      "submittedOnDailyBy": {
        "_id": "6621efe1a6eec3ad03e38759",
        "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
        "isPro": false,
        "fullname": "Liang Zeng",
        "user": "zengliangcs",
        "type": "user"
      },
      "summary": "ソフトウェア工学（SWE）は最近、次世代のLLMアガントの重要なテストボックスとして立ち上がり、2つの重要な次元の固有能力を求めています：継続的な複雑な問題解決（例：50回以上の相互作用ロード）と長期コンテキスト依存関係の解決（例：32,000トークン以上）。しかし、SWEでのデータ整備プロセスは、コードファイルのフィルタリングと専用の実行環境の設定によって手動注釈を重視しているため、よく知られているように時間がかかります。その結果、現在のデータセットは、ほとんどGitHubからの数千インスタンスのみに限られています。\n\nこのような状況に対して、我々は、SWEデータセットのサイズと多様性をシステマチクラスタムに拡大するためのインクリメンタル的な自動化データ整備プラインフィルを提案します。我々のデータセットは、2,531個の異なるGitHubリポジトリからの10,169件の実世界的なPythonタスクインスタンスを含み、それぞれのタスクは自然言語で指定され、自動化ユニットテストの検証に用いる専用の実行環境イメージを付けています。我々は、提案したSWEデータセットから、8,000件以上の成功した実行検証データセットをより慎重に整備しました。このデータセットでのSkywork-SWEモデルの微調節を行うと、LLMのソフトウェア工学能力の性能がデータサイズの増大に伴い続けて向上し、サチュレーションの跡が見られません。特に、我々のSkywork-SWEモデルは、SWE-bench Verifiedベンチマークでのpass@1アカカディティは38.0%を達成し、OpenHandsアガントフレームワークを用いたQwen2.5-Coder-32BベースのLLMの中で新たな最先端（SOTA）を立てました。また、テストタイムスケーリングテクニックの採用により、性能は更に向上し、32Bパラメータモデルの前のSOTA結果を超え、47.0%のアカカディティを達成しました。我々は、Skywork-SWE-32Bモデルチェックポイントをリリースし、将来の研究を加速しようとします。",
      "upvotes": 18,
      "discussionId": "685b6641d2ee4fac76521e4d",
      "projectPage": "https://quixotic-sting-239.notion.site/eb17f379610040ceb54da5d5d24065bd",
      "ai_summary": "An automated data-curation pipeline for software engineering improves large language model performance on SWE tasks, achieving state-of-the-art results with and without test-time scaling techniques.",
      "ai_keywords": [
        "LLM agents",
        "iterative problem-solving",
        "long-context dependency resolution",
        "code file filtering",
        "unit tests",
        "runtime environments",
        "data-curation pipeline",
        "software engineering capabilities",
        "Skywork-SWE model",
        "SWE-bench Verified",
        "pass@1 accuracy",
        "OpenHands agent framework",
        "test-time scaling techniques",
        "parameter-efficient fine-tuning"
      ]
    },
    "publishedAt": "2025-06-23T23:53:36.000Z",
    "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in\n  LLMs",
    "summary": "Software engineering (SWE) has recently emerged as a crucial testbed for\nnext-generation LLM agents, demanding inherent capabilities in two critical\ndimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)\nand long-context dependency resolution (e.g., >32k tokens). However, the data\ncuration process in SWE remains notoriously time-consuming, as it heavily\nrelies on manual annotation for code file filtering and the setup of dedicated\nruntime environments to execute and validate unit tests. Consequently, most\nexisting datasets are limited to only a few thousand GitHub-sourced instances.\nTo this end, we propose an incremental, automated data-curation pipeline that\nsystematically scales both the volume and diversity of SWE datasets. Our\ndataset comprises 10,169 real-world Python task instances from 2,531 distinct\nGitHub repositories, each accompanied by a task specified in natural language\nand a dedicated runtime-environment image for automated unit-test validation.\nWe have carefully curated over 8,000 successfully runtime-validated training\ntrajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE\nmodel on these trajectories, we uncover a striking data scaling phenomenon: the\ntrained model's performance for software engineering capabilities in LLMs\ncontinues to improve as the data size increases, showing no signs of\nsaturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on\nthe SWE-bench Verified benchmark without using verifiers or multiple rollouts,\nestablishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based\nLLMs built on the OpenHands agent framework. Furthermore, with the\nincorporation of test-time scaling techniques, the performance further improves\nto 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter\nmodels. We release the Skywork-SWE-32B model checkpoint to accelerate future\nresearch.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19290.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6621efe1a6eec3ad03e38759",
      "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
      "fullname": "Liang Zeng",
      "name": "zengliangcs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18701",
      "authors": [
        {
          "_id": "685a14da0e4ad7e21975854d",
          "user": {
            "_id": "63aed0e7f873109b112dbb1b",
            "avatarUrl": "/avatars/0c07beb1ef7287128528b165d18371f6.svg",
            "isPro": false,
            "fullname": "Yifan Zhang",
            "user": "Vanint",
            "type": "user"
          },
          "name": "Yifan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:09:21.431Z",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e21975854e",
          "name": "Chunli Peng",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e21975854f",
          "name": "Boyang Wang",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758550",
          "name": "Puyi Wang",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758551",
          "name": "Qingcheng Zhu",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758552",
          "name": "Fei Kang",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758553",
          "name": "Biao Jiang",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758554",
          "name": "Zedong Gao",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758555",
          "name": "Eric Li",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758556",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "685a14da0e4ad7e219758557",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63aed0e7f873109b112dbb1b/2AbnZSPpuvFOad20XamiG.mp4"
      ],
      "publishedAt": "2025-06-23T14:40:49.000Z",
      "submittedOnDailyAt": "2025-06-25T07:50:07.299Z",
      "title": "Matrix-Game: インタージャクトワールドファンダメンタルモデル",
      "submittedOnDailyBy": {
        "_id": "63aed0e7f873109b112dbb1b",
        "avatarUrl": "/avatars/0c07beb1ef7287128528b165d18371f6.svg",
        "isPro": false,
        "fullname": "Yifan Zhang",
        "user": "Vanint",
        "type": "user"
      },
      "summary": "Matrix-Gameは、制御可能なゲームワールド生成のためのインタラクティブなワールドベースモデルです。Matrix-Gameは、環境理解のための大規模な無ラベルプレトレーニングを実行し、次にインタラクティブなビデオ生成のためのアクションラベル付きトレーニングを行います。これにサポートするために、Matrix-Game-MCという詳細なデータセットを選択しました。このデータセットは、2,700時間以上の無ラベルゲームプレイビデオクリップと1,000時間以上の高品質ラベル付きクリップを含み、細かいキーボードとマウスアクションアノテーションを含みます。我々のモデルは、参照画像、動作コンテキスト、ユーザーアクションに基づいて、制御可能な画像からワールド生成パラダイムを採用しています。170億以上のパラメータを持つMatrix-Gameは、キャラクターアクションとカメラ移動の精密な制御を可能にし、高い視覚品質と時系列的な一貫性を維持します。性能評価のために、GameWorld Scoreという統一的なベンチマークを開発し、ビデオ生成の視覚品質、時系列品質、アクション制御可能度、物理法則理解を評価します。拡散的な実験により、Matrix-Gameはすべてのメトリックで先週の開放ソースミンクワールドモデル（OasisとMineWorldを含む）を一致して優れています。特に、制御可能度と物理的な一致性においては特に強い効果が見られます。双盲人間評価は、Matrix-Gameの上位性を確認し、多様なゲームシナリオで視覚的に現実的なそして精密に制御可能なビデオの生成能力を強調します。将来の研究のためのインタラクティブな画像からワールド生成について、Matrix-Gameモデル重みとGameWorld Scoreベンチマークを公開します。",
      "upvotes": 18,
      "discussionId": "685a14da0e4ad7e219758558",
      "projectPage": "https://matrix-game-homepage.github.io",
      "githubRepo": "https://github.com/SkyworkAI/Matrix-Game",
      "ai_summary": "Matrix-Game, a controllable game world generation model trained in a two-stage process, outperforms existing models by producing high-quality, action-controllable, and physically consistent Minecraft world videos.",
      "ai_keywords": [
        "Matrix-Game",
        "interactive world foundation model",
        "large-scale unlabeled pretraining",
        "action-labeled training",
        "contrrollable image-to-world generation",
        "Matrix-Game-MC",
        "motion context",
        "character actions",
        "camera movements",
        "visual quality",
        "temporal coherence",
        "GameWorld Score",
        "double-blind human evaluations",
        "interactive image-to-world generation",
        "Oasis",
        "MineWorld",
        "perceptually realistic"
      ],
      "githubStars": 744
    },
    "publishedAt": "2025-06-23T10:40:49.000Z",
    "title": "Matrix-Game: Interactive World Foundation Model",
    "summary": "We introduce Matrix-Game, an interactive world foundation model for\ncontrollable game world generation. Matrix-Game is trained using a two-stage\npipeline that first performs large-scale unlabeled pretraining for environment\nunderstanding, followed by action-labeled training for interactive video\ngeneration. To support this, we curate Matrix-Game-MC, a comprehensive\nMinecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips\nand over 1,000 hours of high-quality labeled clips with fine-grained keyboard\nand mouse action annotations. Our model adopts a controllable image-to-world\ngeneration paradigm, conditioned on a reference image, motion context, and user\nactions. With over 17 billion parameters, Matrix-Game enables precise control\nover character actions and camera movements, while maintaining high visual\nquality and temporal coherence. To evaluate performance, we develop GameWorld\nScore, a unified benchmark measuring visual quality, temporal quality, action\ncontrollability, and physical rule understanding for Minecraft world\ngeneration. Extensive experiments show that Matrix-Game consistently\noutperforms prior open-source Minecraft world models (including Oasis and\nMineWorld) across all metrics, with particularly strong gains in\ncontrollability and physical consistency. Double-blind human evaluations\nfurther confirm the superiority of Matrix-Game, highlighting its ability to\ngenerate perceptually realistic and precisely controllable videos across\ndiverse game scenarios. To facilitate future research on interactive\nimage-to-world generation, we will open-source the Matrix-Game model weights\nand the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63aed0e7f873109b112dbb1b/2AbnZSPpuvFOad20XamiG.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18701.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63aed0e7f873109b112dbb1b",
      "avatarUrl": "/avatars/0c07beb1ef7287128528b165d18371f6.svg",
      "fullname": "Yifan Zhang",
      "name": "Vanint",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18951",
      "authors": [
        {
          "_id": "685ba757d2ee4fac76521f47",
          "name": "Jinyang Li",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f48",
          "user": {
            "_id": "653693cb8ee17cfd44eed8ce",
            "avatarUrl": "/avatars/82be2428bec4e06c0a15a27647b9b8aa.svg",
            "isPro": false,
            "fullname": "Xiaolong Li",
            "user": "xia01ongLi",
            "type": "user"
          },
          "name": "Xiaolong Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:05.354Z",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f49",
          "name": "Ge Qu",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4a",
          "name": "Per Jacobsson",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4b",
          "name": "Bowen Qin",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4c",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4d",
          "name": "Shuzheng Si",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4e",
          "name": "Nan Huo",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f4f",
          "user": {
            "_id": "63a3eb8af460e4379b5991e7",
            "avatarUrl": "/avatars/7564a048d8496cac38d689178d90a8f9.svg",
            "isPro": false,
            "fullname": "Xiaohan Xu",
            "user": "Tebmer",
            "type": "user"
          },
          "name": "Xiaohan Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T09:19:04.596Z",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f50",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f51",
          "name": "Ziwei Tang",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f52",
          "name": "Yuanshuai Li",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f53",
          "name": "Florensia Widjaja",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f54",
          "name": "Xintong Zhu",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f55",
          "name": "Feige Zhou",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f56",
          "name": "Yongfeng Huang",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f57",
          "name": "Yannis Papakonstantinou",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f58",
          "name": "Fatma Ozcan",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f59",
          "name": "Chenhao Ma",
          "hidden": false
        },
        {
          "_id": "685ba757d2ee4fac76521f5a",
          "name": "Reynold Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-23T09:41:37.000Z",
      "submittedOnDailyAt": "2025-06-25T06:09:19.766Z",
      "title": "SWE-SQL: リアルウェアプロジェクトでのユーザーのSQL問題を解決するためのLLMのパスウェーザーを明らかにする",
      "submittedOnDailyBy": {
        "_id": "6419435385030eca6ac94701",
        "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
        "isPro": false,
        "fullname": "Ge Qu",
        "user": "gq2138",
        "type": "user"
      },
      "summary": "複雑なSQL問題の解決は、実世界的なデータベースアプリケーションでは重大なボトルネックとして残っています。現在の大規模な言語モデル（LLMs）は、テキストからSQLの翻訳には優れていますが、SQL問題のダブグタスクには厳密な評価が行われていません。この空間を填ぐために、我々は、実際のユーザー問題から抜粋し、新しい環境で再現された530ポストグレスSQLタスク（BIRD-CRITIC-PG）と570マルチディアレクトルタスク（BIRD-CRITIC-Multi）を構成した新しいSQL問題ダブグベンチマーク、BIRD-CRITICを紹介します。ベースライン評価はこのタスクの複雑さを強調し、リーディングの理由モデルO3-MiniはBIRD-CRITIC-PGでは38.87%の成功率、BIRD-CRITIC-Multiでは33.33%の成功率を達成しました。また、データプライバシーを守わせながら地域的開発を支援するために、データベースタスク向けのオープンソースモデルの進化は重要です。そこで、我々は、SQL問題ダブグのオープンソースモデルの能力を向上させるための訓練環境、Six-Gym（Sql-fIX-Gym）を紹介します。この環境は、SQL-Rewind戦略を使用し、正確なSQLから逆引きした問題データセットを自動的に生成します。しかし、プロパーティーモデルのファイナルチューニングメソッドは規模的な視聴者信号を調べていません。さらに、f-Plan Boostingを提案し、SQL解決策から高レベルのダブグプランを抽出し、教師LLMsが学習用の成功の多角性を73.7%増やします。これらのコンポーネントを統合し、オープンソースアグェント、Bird-Fixerを構築しました。Qwen-2.5-Coder-14Bに基づくBird-Fixerは、BIRD-CRITIC-PGでは38.11%の成功率、BIRD-CRITIC-Multiでは29.65%の成功率を達成し、Claude-3.7-SonnetとGPT-4.1を超え、複雑なSQLダブグ能力の民主化に重大なステップを踏み出します。リーダブoardとソースコードは、https://bird-critic.github.io/に提供されています。",
      "upvotes": 10,
      "discussionId": "685ba758d2ee4fac76521f5b",
      "ai_summary": "A new benchmark and training environment for debugging SQL issues using advanced open-source models significantly improves their performance compared to proprietary solutions.",
      "ai_keywords": [
        "BIRD-CRITIC",
        "BIRD-CRITIC-PG",
        "BIRD-CRITIC-Multi",
        "PostgreSQL",
        "Six-Gym (Sql-fIX-Gym)",
        "SQL-Rewind",
        "f-Plan Boosting",
        "Bird-Fixer",
        "Qwen-2.5-Coder-14B",
        "Claude-3.7-Sonnet",
        "GPT-4.1"
      ]
    },
    "publishedAt": "2025-06-23T05:41:37.000Z",
    "title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in\n  Real-World Applications",
    "summary": "Resolution of complex SQL issues persists as a significant bottleneck in\nreal-world database applications. Current Large Language Models (LLMs), while\nadept at text-to-SQL translation, have not been rigorously evaluated on the\nmore challenging task of debugging SQL issues. To address this gap, we\nintroduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530\nPostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks\n(BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within\nnew environments to facilitate rigorous evaluation. Baseline evaluations\nunderscore the task's complexity, with the leading reasoning model O3-Mini\nachieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on\nBIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks\nis crucial for empowering local development while safeguarding data privacy.\nTherefore, we present Six-Gym (Sql-fIX-Gym), a training environment for\nelevating open-source model capabilities for SQL issue debugging. This\nenvironment leverages SQL-Rewind strategy, which automatically generates\nexecutable issue-solution datasets by reverse-engineering issues from verified\nSQLs. However, popular trajectory-based fine-tuning methods do not explore\nsubstantial supervisory signals. We further propose f-Plan Boosting, which\nextracts high-level debugging plans from SQL solutions, enabling teacher LLMs\nto produce 73.7% more successful trajectories for training. We integrate these\ncomponents into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B,\nBird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on\nBIRD-CRITIC-Multi, surpassing leading proprietary models such as\nClaude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing\nsophisticated SQL-debugging capabilities. The leaderboard and source code are\navailable: https://bird-critic.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18951.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6419435385030eca6ac94701",
      "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
      "fullname": "Ge Qu",
      "name": "gq2138",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19767",
      "authors": [
        {
          "_id": "685b5791d2ee4fac76521dc2",
          "user": {
            "_id": "670aa09d35918e99fe7ff6b1",
            "avatarUrl": "/avatars/5cbea2284165191e96544bacf2bfb50f.svg",
            "isPro": false,
            "fullname": "Yuqian Fu",
            "user": "Yuqian-Fu",
            "type": "user"
          },
          "name": "Yuqian Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:54.115Z",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc3",
          "name": "Tinghong Chen",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc4",
          "name": "Jiajun Chai",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc5",
          "name": "Xihuai Wang",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc6",
          "user": {
            "_id": "66e14f4142ceed655c731966",
            "avatarUrl": "/avatars/d708562349913b89c8c4cf384628f82a.svg",
            "isPro": false,
            "fullname": "SONGJUN TU",
            "user": "SONGJUNTU",
            "type": "user"
          },
          "name": "Songjun Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:08:52.066Z",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc7",
          "name": "Guojun Yin",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc8",
          "name": "Wei Lin",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dc9",
          "name": "Qichao Zhang",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dca",
          "name": "Yuanheng Zhu",
          "hidden": false
        },
        {
          "_id": "685b5791d2ee4fac76521dcb",
          "name": "Dongbin Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T16:31:37.000Z",
      "submittedOnDailyAt": "2025-06-25T01:33:02.160Z",
      "title": "SRFT: 理由のためのサブジェクトとリフォーマリゼーションを含むシングルステージメソッド",
      "submittedOnDailyBy": {
        "_id": "66e14f4142ceed655c731966",
        "avatarUrl": "/avatars/d708562349913b89c8c4cf384628f82a.svg",
        "isPro": false,
        "fullname": "SONGJUN TU",
        "user": "SONGJUNTU",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、理由任務において驚異的な進歩を達成していますが、観督付き微調節（SFT）と強化学習（RL）の最適な統合は、基本的な課題です。トークン分布、学習ダイナミクス、統合機構の詳細な分析から、ヒストリーベースの観点から、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの間の關連性を明らかにし、これらのパラダイムの",
      "upvotes": 7,
      "discussionId": "685b5792d2ee4fac76521dcc",
      "projectPage": "https://anonymous.4open.science/w/SRFT2025",
      "ai_summary": " Supervised Reinforcement Fine-Tuning (SRFT) integrates Supervised Fine-Tuning and Reinforcement Learning through entropy-aware weighting to achieve high accuracy in language model optimization.",
      "ai_keywords": [
        "Large language models",
        "Supervised Fine-Tuning",
        "Reinforcement Learning",
        "token distributions",
        "learning dynamics",
        "entropy",
        "Supervised Reinforcement Fine-Tuning"
      ]
    },
    "publishedAt": "2025-06-24T12:31:37.000Z",
    "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning",
    "summary": "Large language models (LLMs) have achieved remarkable progress in reasoning\ntasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) remains a fundamental challenge. Through\ncomprehensive analysis of token distributions, learning dynamics, and\nintegration mechanisms from entropy-based perspectives, we reveal key\ndifferences between these paradigms: SFT induces coarse-grained global changes\nto LLM policy distributions, while RL performs fine-grained selective\noptimizations, with entropy serving as a critical indicator of training\neffectiveness. Building on these observations, we propose Supervised\nReinforcement Fine-Tuning (SRFT), a single-stage method that unifies both\nfine-tuning paradigms through entropy-aware weighting mechanisms. Our approach\nsimultaneously applies SFT and RL to directly optimize the LLM using\ndemonstrations and self-exploration rollouts rather than through two-stage\nsequential methods. Extensive experiments show that SRFT achieves 59.1% average\naccuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning\nbenchmarks and 10.9% on three out-of-distribution benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19767.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e14f4142ceed655c731966",
      "avatarUrl": "/avatars/d708562349913b89c8c4cf384628f82a.svg",
      "fullname": "SONGJUN TU",
      "name": "SONGJUNTU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.19838",
      "authors": [
        {
          "_id": "685b5e05d2ee4fac76521ddd",
          "name": "Liangbin Xie",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521dde",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521ddf",
          "name": "Shian Du",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de0",
          "name": "Menghan Xia",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de1",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de2",
          "name": "Fanghua Yu",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de3",
          "name": "Ziyan Chen",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de4",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de5",
          "name": "Jiantao Zhou",
          "hidden": false
        },
        {
          "_id": "685b5e05d2ee4fac76521de6",
          "name": "Chao Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:57:26.000Z",
      "submittedOnDailyAt": "2025-06-25T00:55:41.694Z",
      "title": "SimpleGVR: 潜在シーケンス連鎖化の簡単なベースラインのビデオ超解像化",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "潜在扩散モデルは、効率的なビデオ生成の先進的なパラダイムとして現れてきました。しかし、ユーザーの期待が高解像度の出力に向けて変化しているため、潜在計算をそのみに依存することは不十分になります。有望なアプローチとして、プロセスを二つのステップに分けることが考えられます：セマンティック内容の生成と詳細の合成。前のステップは、計算量が豊かな基礎モデルを低解像度で使用しますが、後者は軽量な連続ビデオ超解像（VSR）モデルを使用して高解像度の出力を実現します。本研究では、現在調査されていない後者の連続ビデオ超解像モデルのキーの設計原則を研究します。まず、基礎モデルの出力特徴をより良く模倣するための訓練ペアを生成するために、二つの悪化戦略を提案します。これにより、VSRモデルと上流のジェネレーターのアライメントを確保します。次に、時間ステップのサンプリング戦略と、低解像度（LR）入力に対するノイズ増強の影響についてのシステマティックな分析を行い、VSRモデルの行為について重要なエイリアスを提供します。これらの発見は、アーキテクチャと訓練の革新に直接的に影響を及ぼします。最後に、間引き時間ユニットと稀疏な局所的アテンションを使用して、効率的な訓練と推論を実現し、計算オーバーヘッドを大きく減少します。拡散的な実験は、我々のフレームワークが既存の方法よりも上位にあることを示し、消滅研究は各デザイン選択の効果を確認します。我々の研究は、簡単で効果的な基礎を提供し、将来の効率的な連続合成システムの進歩についての実用的なエイリアスを提供します。",
      "upvotes": 6,
      "discussionId": "685b5e05d2ee4fac76521de7",
      "ai_summary": "Researchers propose design principles for cascaded video super-resolution models to improve high-resolution video generation by introduces degradation strategies, timestep sampling, noise augmentation, and interleaving temporal units with sparse local attention.",
      "ai_keywords": [
        "latent diffusion models",
        "video generation",
        "cascaded video super-resolution",
        "VSR",
        "degradation strategies",
        "timestep sampling",
        "noise augmentation",
        "interleaving temporal unit",
        "sparse local attention"
      ]
    },
    "publishedAt": "2025-06-24T13:57:26.000Z",
    "title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution",
    "summary": "Latent diffusion models have emerged as a leading paradigm for efficient\nvideo generation. However, as user expectations shift toward higher-resolution\noutputs, relying solely on latent computation becomes inadequate. A promising\napproach involves decoupling the process into two stages: semantic content\ngeneration and detail synthesis. The former employs a computationally intensive\nbase model at lower resolutions, while the latter leverages a lightweight\ncascaded video super-resolution (VSR) model to achieve high-resolution output.\nIn this work, we focus on studying key design principles for latter cascaded\nVSR models, which are underexplored currently. First, we propose two\ndegradation strategies to generate training pairs that better mimic the output\ncharacteristics of the base model, ensuring alignment between the VSR model and\nits upstream generator. Second, we provide critical insights into VSR model\nbehavior through systematic analysis of (1) timestep sampling strategies, (2)\nnoise augmentation effects on low-resolution (LR) inputs. These findings\ndirectly inform our architectural and training innovations. Finally, we\nintroduce interleaving temporal unit and sparse local attention to achieve\nefficient training and inference, drastically reducing computational overhead.\nExtensive experiments demonstrate the superiority of our framework over\nexisting methods, with ablation studies confirming the efficacy of each design\nchoice. Our work establishes a simple yet effective baseline for cascaded video\nsuper-resolution generation, offering practical insights to guide future\nadvancements in efficient cascaded synthesis systems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19838.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7183
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19794",
      "authors": [
        {
          "_id": "685b75d0d2ee4fac76521e70",
          "name": "Yuqi Zhu",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e71",
          "name": "Yi Zhong",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e72",
          "name": "Jintian Zhang",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e73",
          "name": "Ziheng Zhang",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e74",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e75",
          "name": "Yujie Luo",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e76",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e77",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e78",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "685b75d0d2ee4fac76521e79",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:04:23.000Z",
      "submittedOnDailyAt": "2025-06-25T02:37:00.536Z",
      "title": "オープンソースLLMがデータ分析に悩みる理由は何か？システム的な実験的な研究",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、データ分析タスクの自動化に望ましい可能性を持ちますが、これらの理由的なスキャンデンスでは、開放ソースモデルは重大な制限を見せています。本研究では、開放ソースLLMsのデータ分析能力を向上させる戦略を調査しています。多様な実用的なスキャンデンスのシードデータセットをカレードし、モデルの3つの次元で評価します：データ理解、コード生成、戦略計画。分析では、3つの主な発見が明らかになりました：（1）戦略計画の品質がモデルの性能の主な決定因素です；（2）インタラクションデザインとタスクの複雑性が理由的な能力を大きく影響します；（3）データの品質は最適な性能を達成するためには多様性よりも大きな影響を示します。これらのインサイトを活用して、データ合成メソッドオロジーを開発し、開放ソースLLMsの分析的な理由的な能力に顕著な改善を示します。",
      "upvotes": 6,
      "discussionId": "685b75d1d2ee4fac76521e7a",
      "ai_summary": "Enhancements to open-source large language models' data analysis capabilities through strategic planning, interaction design, and data quality improvements were identified and applied.",
      "ai_keywords": [
        "Large Language Models",
        "data analysis",
        "data understanding",
        "code generation",
        "strategic planning",
        "interaction design",
        "task complexity",
        "data quality",
        "data synthesis methodology"
      ]
    },
    "publishedAt": "2025-06-24T13:04:23.000Z",
    "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study",
    "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19794.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.19713",
      "authors": [
        {
          "_id": "685b9a5dd2ee4fac76521ecc",
          "user": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "isPro": false,
            "fullname": "Seyedmorteza Sadat",
            "user": "msadat97",
            "type": "user"
          },
          "name": "Seyedmorteza Sadat",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:14.606Z",
          "hidden": false
        },
        {
          "_id": "685b9a5dd2ee4fac76521ecd",
          "name": "Tobias Vontobel",
          "hidden": false
        },
        {
          "_id": "685b9a5dd2ee4fac76521ece",
          "name": "Farnood Salehi",
          "hidden": false
        },
        {
          "_id": "685b9a5dd2ee4fac76521ecf",
          "name": "Romann M. Weber",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T15:19:42.000Z",
      "submittedOnDailyAt": "2025-06-25T05:16:23.771Z",
      "title": "周波数領域でのガイドラインが、低スケールでの高品質なサンプリングを可能にします。",
      "submittedOnDailyBy": {
        "_id": "63b4b02a103617b0a5b0ee2e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
        "isPro": false,
        "fullname": "Seyedmorteza Sadat",
        "user": "msadat97",
        "type": "user"
      },
      "summary": "クラスフレードガイダンス（CFG）は、現代の条件付きディフフォーションモデルの重要な構成要素として機能しています。実践的には非常に効果的であることは知られていますが、CFGが品質、詳細、プロンプトのアラインメントを向上させる機構は完全に理解されていません。我々は、CFGの影響を周波数領域で分析し、低周波数と高周波数が異なる影響を持つことを示します。特に、低周波数ガイダンスはグローバルな構造と条件のアラインメントを支配し、高周波数ガイダンスは主に可視的なフィデティーを向上させます。しかし、標準的なCFGではすべての周波数に同じスケールを適用し、高スケールでの過剰化と多様性の低下、低スケールでの可視的な品質の低下が発生します。これらの見解に基づき、我々は周波数離れガイダンス（FDG）を提案します。FDGはCFGを低周波数と高周波数の成分に分解し、各成分に別々のガイダンスストレングスを適用します。FDGは低ガイダンススケールで画像の品質を向上させ、高ガイダンススケールの欠点を避けることを目的としています。複数のデータセットとモデルの幅広い実験を通じて、我々はFDGがサンプルのフィデティーを向上させ、多様性を保ち、CFGよりもFIDとrecallを改善することを示し、標準的なクラスフレードガイダンスのプラグインとパラットワークとして我々の方法を確立しました。",
      "upvotes": 6,
      "discussionId": "685b9a5ed2ee4fac76521ed0",
      "ai_summary": "Frequency-decoupled guidance (FDG) enhances image quality and diversity by separately controlling low- and high-frequency guidance components in diffusion models, outperforming standard classifier-free guidance.",
      "ai_keywords": [
        "classifier-free guidance",
        "conditional diffusion models",
        "frequency domain",
        "low-frequency guidance",
        "high-frequency guidance",
        "frequency-decoupled guidance",
        "FID",
        "recall"
      ]
    },
    "publishedAt": "2025-06-24T11:19:42.000Z",
    "title": "Guidance in the Frequency Domain Enables High-Fidelity Sampling at Low\n  CFG Scales",
    "summary": "Classifier-free guidance (CFG) has become an essential component of modern\nconditional diffusion models. Although highly effective in practice, the\nunderlying mechanisms by which CFG enhances quality, detail, and prompt\nalignment are not fully understood. We present a novel perspective on CFG by\nanalyzing its effects in the frequency domain, showing that low and high\nfrequencies have distinct impacts on generation quality. Specifically,\nlow-frequency guidance governs global structure and condition alignment, while\nhigh-frequency guidance mainly enhances visual fidelity. However, applying a\nuniform scale across all frequencies -- as is done in standard CFG -- leads to\noversaturation and reduced diversity at high scales and degraded visual quality\nat low scales. Based on these insights, we propose frequency-decoupled guidance\n(FDG), an effective approach that decomposes CFG into low- and high-frequency\ncomponents and applies separate guidance strengths to each component. FDG\nimproves image quality at low guidance scales and avoids the drawbacks of high\nCFG scales by design. Through extensive experiments across multiple datasets\nand models, we demonstrate that FDG consistently enhances sample fidelity while\npreserving diversity, leading to improved FID and recall compared to CFG,\nestablishing our method as a plug-and-play alternative to standard\nclassifier-free guidance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19713.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b4b02a103617b0a5b0ee2e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
      "fullname": "Seyedmorteza Sadat",
      "name": "msadat97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.18843",
      "authors": [
        {
          "_id": "685a06460e4ad7e2197584c0",
          "user": {
            "_id": "6179f36a2a4e9edab3a95798",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
            "isPro": false,
            "fullname": "Heng-Jui Chang",
            "user": "vectominist",
            "type": "user"
          },
          "name": "Heng-Jui Chang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-24T08:08:49.104Z",
          "hidden": false
        },
        {
          "_id": "685a06460e4ad7e2197584c1",
          "user": {
            "_id": "67d301cbba86f5d66eb73d7c",
            "avatarUrl": "/avatars/8546bbd2145c16d4be5675624516b649.svg",
            "isPro": false,
            "fullname": "Saurabhchand Bhati",
            "user": "saurabhati",
            "type": "user"
          },
          "name": "Saurabhchand Bhati",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:09:27.586Z",
          "hidden": false
        },
        {
          "_id": "685a06460e4ad7e2197584c2",
          "name": "James Glass",
          "hidden": false
        },
        {
          "_id": "685a06460e4ad7e2197584c3",
          "name": "Alexander H. Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6179f36a2a4e9edab3a95798/QPPxw2hyPglQF_q3uWSyk.png"
      ],
      "publishedAt": "2025-06-23T17:02:00.000Z",
      "submittedOnDailyAt": "2025-06-25T02:21:09.630Z",
      "title": "USAD: 普遍的言語と音声の表現による経験収穫",
      "submittedOnDailyBy": {
        "_id": "6179f36a2a4e9edab3a95798",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
        "isPro": false,
        "fullname": "Heng-Jui Chang",
        "user": "vectominist",
        "type": "user"
      },
      "summary": "自监督学习（SSL）は音声表現に革命的な影響を及ぼしていますが、モデルは通常言語や非言語のタスクに対してドメイン専門的で、それぞれのタスクに焦点を当てています。本論文では、言語と音響、音楽の多様な音声タイプを統一的に統合した音声表現学習の一つの手法であるUniversal Speech and Audio Distillation (USAD)を紹介します。USADは、ドメイン専門的なSSLモデルから効率的な層ごとの煉成を用いて、実用的な音声データセットで学生モデルを訓練します。USADは、様々なベンチマークとデータセットで強力な性能を示し、フレームレベルとインスタンスレベルの言語処理タスク、音声タグジング、音響分類などを含む様々なタスクに対して優れた結果を収め、SUPERBとHEARベンチマークで近況の上位を達成します。",
      "upvotes": 6,
      "discussionId": "685a06470e4ad7e2197584c4",
      "ai_summary": "USAD integrates diverse audio types using efficient layer-to-layer distillation from domain-specific models, achieving competitive performance across various benchmarks with a single encoder.",
      "ai_keywords": [
        "self-supervised learning",
        "universal speech and audio distillation",
        "domain-specific models",
        "layer-to-layer distillation",
        "frame and instance-level speech processing",
        "audio tagging",
        "sound classification",
        "encoder",
        "SUPERB benchmarks",
        "HEAR benchmarks"
      ]
    },
    "publishedAt": "2025-06-23T13:02:00.000Z",
    "title": "USAD: Universal Speech and Audio Representation via Distillation",
    "summary": "Self-supervised learning (SSL) has revolutionized audio representations, yet\nmodels often remain domain-specific, focusing on either speech or non-speech\ntasks. In this work, we present Universal Speech and Audio Distillation (USAD),\na unified approach to audio representation learning that integrates diverse\naudio types - speech, sound, and music - into a single model. USAD employs\nefficient layer-to-layer distillation from domain-specific SSL models to train\na student on a comprehensive audio dataset. USAD offers competitive performance\nacross various benchmarks and datasets, including frame and instance-level\nspeech processing tasks, audio tagging, and sound classification, achieving\nnear state-of-the-art results with a single encoder on SUPERB and HEAR\nbenchmarks.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6179f36a2a4e9edab3a95798/QPPxw2hyPglQF_q3uWSyk.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18843.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6179f36a2a4e9edab3a95798",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
      "fullname": "Heng-Jui Chang",
      "name": "vectominist",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.19807",
      "authors": [
        {
          "_id": "685b75edd2ee4fac76521e7c",
          "name": "Baochang Ren",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e7d",
          "name": "Shuofei Qiao",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e7e",
          "name": "Wenhao Yu",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e7f",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "685b75edd2ee4fac76521e80",
          "name": "Ningyu Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:17:17.000Z",
      "submittedOnDailyAt": "2025-06-25T02:37:40.331Z",
      "title": "KnowRL: 知識を持つ強化学習における事実性の探索",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）、特に遅れて考えるモデルは、理由の際に知識の境界を正確に認識できないため、厳しいハウシャルションが発生し、不正确な内容を出力することが多い。強化学習（RL）は複雑な理由能力を向上させることができますが、結果に向けた報酬機構は、考えの過程における事実的なサバイジェンスが不足し、ハウシャルションの問題を進めています。遅れて考えるモデルの高いハウシャルション問題を解決するために、我々は知識強化（KnowRL）を提案します。KnowRLは、知識検証に基づく事実性報酬をRLトレーニングプロセスに統合し、モデルを事実に基づく遅れて考えることを促し、知識の境界を認識することを助けます。RLトレーニングプロセス中のターゲット的な事実的な入力により、モデルは事実に基づく理由戦略を学習し、内部化します。理由ステップ内で事実に従うことを直接報酬することにより、KnowRLは信頼性の高い思考プロセスを促進します。3つのハウシャルション評価データセットと2つの理由評価データセットの実験結果から、KnowRLは遅れて考えるモデルのハウシャルションを効果的に軽減し、その元の強い理由能力を維持することを示しています。コードは、https://github.com/zjunlp/KnowRL にアクセスできます。",
      "upvotes": 4,
      "discussionId": "685b75edd2ee4fac76521e81",
      "ai_summary": "KnowRL, a knowledge-enhanced reinforcement learning approach, reduces hallucinations in slow-thinking large language models by incorporating factuality rewards based on knowledge verification during training.",
      "ai_keywords": [
        "Large Language Models",
        "slow-thinking models",
        "hallucination",
        "Reinforcement Learning",
        "KnowRL",
        "factuality reward",
        "knowledge verification",
        "reasoning",
        "reasoning capabilities"
      ]
    },
    "publishedAt": "2025-06-24T13:17:17.000Z",
    "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
    "summary": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 24
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.17612",
      "authors": [
        {
          "_id": "685b7538d2ee4fac76521e63",
          "user": {
            "_id": "64ecb174f22081b4ac7ca397",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ecb174f22081b4ac7ca397/PiAPtD_rbuhGOqfE6ZSIu.jpeg",
            "isPro": true,
            "fullname": "Yunlong Lin",
            "user": "LYL1015",
            "type": "user"
          },
          "name": "Yunlong Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:25.557Z",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e64",
          "name": "Zixu Lin",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e65",
          "name": "Kunjie Lin",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e66",
          "name": "Jinbin Bai",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e67",
          "name": "Panwang Pan",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e68",
          "name": "Chenxin Li",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e69",
          "name": "Haoyu Chen",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6a",
          "name": "Zhongdao Wang",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6b",
          "name": "Xinghao Ding",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6c",
          "name": "Wenbo Li",
          "hidden": false
        },
        {
          "_id": "685b7538d2ee4fac76521e6d",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ecb174f22081b4ac7ca397/PCTU9EbA4XUz8eKeLbxVC.gif"
      ],
      "publishedAt": "2025-06-21T06:36:00.000Z",
      "submittedOnDailyAt": "2025-06-25T02:43:05.885Z",
      "title": "ジャビズアート：知能写真編集アガントによる人間の芸術的創造性の解放",
      "submittedOnDailyBy": {
        "_id": "64ecb174f22081b4ac7ca397",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ecb174f22081b4ac7ca397/PiAPtD_rbuhGOqfE6ZSIu.jpeg",
        "isPro": true,
        "fullname": "Yunlong Lin",
        "user": "LYL1015",
        "type": "user"
      },
      "summary": "写真の編集は、現代の視覚的な物語を構成する重要な部分になりました。ユーザーは美術性を捉え、創造性を表現することができます。専門的なツールとしてAdobe Lightroomなどが提供されており、強力な機能を持ちますが、それらは高度な専門知識と手動的な努力が必要となります。対照的に、現在のAIベースの解決策は自動化を提供しますが、調整可能性が限られ、汎用性が低いことがあり、多様なユーザーの編集の需要に適応しません。この隙間を埋めるために、ユーザーの意図を理解し、専門家のアーティストの理由論を模倣し、Lightroom内の200点以上の編集ツールを計画的に協調するための、多モーダル大語言モデル(MLLM)を機能させるアガントJarvisArtを紹介します。JarvisArtは2段階の訓練プロセスを通じて成長します。最初の段階は、基本的な理由論とツールの使用スキルを確立するためのChain-of-Thoughtのチャイナフィードバック調整を通じ、その後は、編集の決策論とツールの熟練度を向上させるためのGroup Relative Policy Optimization for Retouching(GRPO-R)を通じます。また、Agent-to-Lightroomプロトコルを提案し、Lightroomと無間違った統合を促進します。性能の評価において、MMArt-Benchという新しいベンチマークを開発しました。JarvisArtは、ユーザーフレンドリーなインターフェース、上位の汎用性、グローバルと局所的な調整の細かい制御を示し、計画的な写真編集の新しい道を開拓します。特に、MMArt-Benchの内容の忠実性においてGPT-4oを60%以上改善し、平均ピクセルレベルのメトリックを超えますが、同様の指示従い能力を維持しています。プロジェクトページは、https://jarvisart.vercel.app/です。",
      "upvotes": 4,
      "discussionId": "685b7539d2ee4fac76521e6e",
      "projectPage": "https://jarvisart.vercel.app/",
      "githubRepo": "https://github.com/LYL1015/JarvisArt",
      "ai_summary": "JarvisArt, an MLLM-driven agent, achieves superior photo retouching by understanding user intent and coordinating multiple retouching tools in Lightroom, outperforming GPT-4o on a novel benchmark.",
      "ai_keywords": [
        "multi-modal large language model",
        "Chain-of-Thought supervised fine-tuning",
        "Group Relative Policy Optimization",
        "Agent-to-Lightroom Protocol",
        "MMArt-Bench",
        "global adjustments",
        "local adjustments",
        "content fidelity",
        "instruction-following capabilities"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-06-21T02:36:00.000Z",
    "title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo\n  Retouching Agent",
    "summary": "Photo retouching has become integral to contemporary visual storytelling,\nenabling users to capture aesthetics and express creativity. While professional\ntools such as Adobe Lightroom offer powerful capabilities, they demand\nsubstantial expertise and manual effort. In contrast, existing AI-based\nsolutions provide automation but often suffer from limited adjustability and\npoor generalization, failing to meet diverse and personalized editing needs. To\nbridge this gap, we introduce JarvisArt, a multi-modal large language model\n(MLLM)-driven agent that understands user intent, mimics the reasoning process\nof professional artists, and intelligently coordinates over 200 retouching\ntools within Lightroom. JarvisArt undergoes a two-stage training process: an\ninitial Chain-of-Thought supervised fine-tuning to establish basic reasoning\nand tool-use skills, followed by Group Relative Policy Optimization for\nRetouching (GRPO-R) to further enhance its decision-making and tool\nproficiency. We also propose the Agent-to-Lightroom Protocol to facilitate\nseamless integration with Lightroom. To evaluate performance, we develop\nMMArt-Bench, a novel benchmark constructed from real-world user edits.\nJarvisArt demonstrates user-friendly interaction, superior generalization, and\nfine-grained control over both global and local adjustments, paving a new\navenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a\n60% improvement in average pixel-level metrics on MMArt-Bench for content\nfidelity, while maintaining comparable instruction-following capabilities.\nProject Page: https://jarvisart.vercel.app/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ecb174f22081b4ac7ca397/PCTU9EbA4XUz8eKeLbxVC.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17612.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64ecb174f22081b4ac7ca397",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ecb174f22081b4ac7ca397/PiAPtD_rbuhGOqfE6ZSIu.jpeg",
      "fullname": "Yunlong Lin",
      "name": "LYL1015",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.19850",
      "authors": [
        {
          "_id": "685b63c2d2ee4fac76521dee",
          "name": "Yuqi Wang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521def",
          "name": "Xinghang Li",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df0",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df1",
          "name": "Junbo Zhang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df2",
          "name": "Yingyan Li",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df3",
          "name": "Yuntao Chen",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df4",
          "name": "Xinlong Wang",
          "hidden": false
        },
        {
          "_id": "685b63c2d2ee4fac76521df5",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-24T17:59:57.000Z",
      "submittedOnDailyAt": "2025-06-25T06:01:30.093Z",
      "title": "統合ビジョン・言語・アクションモデル",
      "submittedOnDailyBy": {
        "_id": "649fe21d59c1ae90dbfacf91",
        "avatarUrl": "/avatars/7f77fa77113e80cb45406927e2386387.svg",
        "isPro": false,
        "fullname": "Wang Yuqi",
        "user": "Yuqi1997",
        "type": "user"
      },
      "summary": "Vision-language-actionモデル（VLAs）は、機械手の操作を進めることの可能性について注目を集めています。しかし、以前のアプローチは主に、視覚言語モデル（VLMs）の一般的な理解能力を利用してアクション信号を生成し、視覚観測に含まれる豊富な時系列的および因果的構造を遺していました。本論文では、UniVLAという一連続的な、原生の多タイプのVLAモデルを提出します。このモデルは、視覚、言語、アクション信号を分散トークン列として自動回帰的にモデル化します。この構成は、特に大規模なビデオデータからの柔軟な多タイプタスク学習を可能にします。後ほどトレーニングの際に世界モデリングを採用し、UniVLAはビデオからの因果的な動力学を捉え、下流のポリシー学習に効果的なタンスファレーションを促進します。私たちのアプローチは、CALVIN、LIBERO、Simplenv-Bridgeなどの広く使用されているシミュレーションベンチマークで新しい最先端の結果を収め、以前の方法を大幅に超えました。例えば、UniVLAはLIBEROベンチマークで95.5%の平均成功率を達成し、pi0-FASTの85.5%を超えました。また、私たちは、実世界的なALOHA操作と自動運転においてその広範囲の応用を示しました。",
      "upvotes": 3,
      "discussionId": "685b63c3d2ee4fac76521df6",
      "ai_summary": "UniVLA is a multimodal VLA model that autoregressively processes vision, language, and action as token sequences, incorporating world modeling for effective long-horizon policy learning and achieving state-of-the-art results across simulation and real-world benchmarks.",
      "ai_keywords": [
        "vision-language-action models",
        "VLAs",
        "vision-language models",
        "VLMs",
        "autoregressive models",
        "discrete token sequences",
        "multimodal tasks learning",
        "world modeling",
        "causal dynamics",
        "policy learning",
        "simulation benchmarks",
        "CALVIN",
        "LIBERO",
        "Simplenv-Bridge",
        "ALOHA manipulation",
        "autonomous driving"
      ]
    },
    "publishedAt": "2025-06-24T13:59:57.000Z",
    "title": "Unified Vision-Language-Action Model",
    "summary": "Vision-language-action models (VLAs) have garnered significant attention for\ntheir potential in advancing robotic manipulation. However, previous approaches\npredominantly rely on the general comprehension capabilities of vision-language\nmodels (VLMs) to generate action signals, often overlooking the rich temporal\nand causal structure embedded in visual observations. In this paper, we present\nUniVLA, a unified and native multimodal VLA model that autoregressively models\nvision, language, and action signals as discrete token sequences. This\nformulation enables flexible multimodal tasks learning, particularly from\nlarge-scale video data. By incorporating world modeling during post-training,\nUniVLA captures causal dynamics from videos, facilitating effective transfer to\ndownstream policy learning--especially for long-horizon tasks. Our approach\nsets new state-of-the-art results across several widely used simulation\nbenchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly\nsurpassing previous methods. For example, UniVLA achieves 95.5% average success\nrate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate\nits broad applicability on real-world ALOHA manipulation and autonomous\ndriving.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19850.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649fe21d59c1ae90dbfacf91",
      "avatarUrl": "/avatars/7f77fa77113e80cb45406927e2386387.svg",
      "fullname": "Wang Yuqi",
      "name": "Yuqi1997",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14012",
      "authors": [
        {
          "_id": "685b863bd2ee4fac76521e92",
          "user": {
            "_id": "655efd24afee0e00788bb589",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
            "isPro": false,
            "fullname": "Amr Mohamed",
            "user": "amr-mohamed",
            "type": "user"
          },
          "name": "Amr Mohamed",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:19.266Z",
          "hidden": false
        },
        {
          "_id": "685b863bd2ee4fac76521e93",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "685b863bd2ee4fac76521e94",
          "name": "Michalis Vazirgiannis",
          "hidden": false
        },
        {
          "_id": "685b863bd2ee4fac76521e95",
          "user": {
            "_id": "6087e598e2b7cc3a117b0dc5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png",
            "isPro": false,
            "fullname": "Guokan Shang",
            "user": "guokan-shang",
            "type": "user"
          },
          "name": "Guokan Shang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-25T08:07:16.772Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-16T21:19:27.000Z",
      "submittedOnDailyAt": "2025-06-25T03:51:26.828Z",
      "title": "ミックスに迷い込み：コードスイッチテキストの理解を評価する",
      "submittedOnDailyBy": {
        "_id": "655efd24afee0e00788bb589",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
        "isPro": false,
        "fullname": "Amr Mohamed",
        "user": "amr-mohamed",
        "type": "user"
      },
      "summary": "コードスイッチ（CSW）は、一辺のデコース内で2語以上の言語を交換する行為です。この現象は多言語社会で広く見られ、オンラインコンテンツでは自然に日常的なコミュニケーションで言語を混ぜ合わせるようになりました。その結果、大規模言語モデル（LLMs）は、コンテンツの処理と生成の中心となっていますが、コードスイッチされた入力を頻繁に受け付けます。その普及に伴い、LLMsがどのようにこの言語混合された文章を処理し、理由を理解するかが重要です。本論文では、既存の理由と理解ベンチマークのコードスイッチされたバージョンを生成し、LLMsのコードスイッチ理解をシステマティックに評価します。外国のトークンが英語テキストを干渉すると、言語制約の下でも損傷が見られますが、英語を他の言語に埋め込むことで理解が向上します。提示は戻り結果が混ぜ合わせられますが、微調校はより安定した損傷抑制の道があることを示します。",
      "upvotes": 3,
      "discussionId": "685b863bd2ee4fac76521e96",
      "ai_summary": "LLMs' comprehension and reasoning skills are evaluated under code-switching conditions, revealing that embedding English into other languages can improve understanding, while prompts and fine-tuning affect degradation mitigation differently.",
      "ai_keywords": [
        "Large Language Models",
        "code-switching",
        "CSW",
        "reasoning benchmarks",
        "comprehension benchmarks",
        "foreign tokens",
        "embedding",
        "fine-tuning"
      ]
    },
    "publishedAt": "2025-06-16T17:19:27.000Z",
    "title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text",
    "summary": "Code-switching (CSW) is the act of alternating between two or more languages\nwithin a single discourse. This phenomenon is widespread in multilingual\ncommunities, and increasingly prevalent in online content, where users\nnaturally mix languages in everyday communication. As a result, Large Language\nModels (LLMs), now central to content processing and generation, are frequently\nexposed to code-switched inputs. Given their widespread use, it is crucial to\nunderstand how LLMs process and reason about such mixed-language text. This\npaper presents a systematic evaluation of LLM comprehension under\ncode-switching by generating CSW variants of established reasoning and\ncomprehension benchmarks. While degradation is evident when foreign tokens\ndisrupt English textx2013even under linguistic\nconstraintsx2013embedding English into other languages often\nimproves comprehension. Though prompting yields mixed results, fine-tuning\noffers a more stable path to degradation mitigation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655efd24afee0e00788bb589",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
      "fullname": "Amr Mohamed",
      "name": "amr-mohamed",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  }
]