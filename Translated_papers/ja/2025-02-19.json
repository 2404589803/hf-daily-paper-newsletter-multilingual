[
  {
    "paper": {
      "id": "2502.12900",
      "authors": [
        {
          "_id": "67b54851b986e35c41e063da",
          "user": {
            "_id": "66975b9f8031bf92b428e138",
            "avatarUrl": "/avatars/3254281a7bac1c8ddde1d6bc7e518b2f.svg",
            "isPro": false,
            "fullname": "Yuhao Zhang",
            "user": "Yoohao",
            "type": "user"
          },
          "name": "Yuhao Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-19T02:56:18.848Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063db",
          "user": {
            "_id": "66597f2cf769c3c443b7cf41",
            "avatarUrl": "/avatars/735cc8aa430748d20ca7312c72b1eaf1.svg",
            "isPro": false,
            "fullname": "Chihang Lau",
            "user": "puccho",
            "type": "user"
          },
          "name": "Zhiheng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:05.678Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063dc",
          "user": {
            "_id": "668e7f46c243a12604035758",
            "avatarUrl": "/avatars/35bd20032fafb7d7603266cf9a72d1e0.svg",
            "isPro": false,
            "fullname": "Fan Bu",
            "user": "FanBuCUHK",
            "type": "user"
          },
          "name": "Fan Bu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:42:08.544Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063dd",
          "user": {
            "_id": "67b587c8882e49771f610b51",
            "avatarUrl": "/avatars/aecfb38b44141b8284416fc261692909.svg",
            "isPro": false,
            "fullname": "Ruiyu Zhang",
            "user": "PhoenixAxis",
            "type": "user"
          },
          "name": "Ruiyu Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:42:14.866Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063de",
          "user": {
            "_id": "637c6703ca8542a0ba900ccb",
            "avatarUrl": "/avatars/288ed63a1efa566c3f01e850c6ba5dd5.svg",
            "isPro": false,
            "fullname": "Wang",
            "user": "Benyou",
            "type": "user"
          },
          "name": "Benyou Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:42:23.845Z",
          "hidden": false
        },
        {
          "_id": "67b54851b986e35c41e063df",
          "name": "Haizhou Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T14:36:39.000Z",
      "title": "Soundwave: フレーバーワーブの少ないほうが多いです。",
      "summary": "現在の端末から端末までの言語大語言モデル（LLMs）は通常、大規模なラベリングデータを用いて訓練されており、データ効率的な訓練は深く議論されていません。私たちは、音声と文字間の2つの基本的な問題に焦点を当てています：表現空間の隙間と順序長さの不調和。私たちは、Soundwaveという新しいアーキテクチャと効率的な訓練戦略を用いてこれらの問題を解決する方法を提案します。結果として、Soundwaveは音声翻訳およびAIR-Bench音声タスクで最先端のQwen2-Audioよりも優れていることがわかり、訓練データの1/50を使用しています。進ける分析により、Soundwaveはコンバーション中にも知能を保持していることがわかります。このプロジェクトは、https://github.com/FreedomIntelligence/Soundwave にアクセスできます。",
      "upvotes": 47,
      "discussionId": "67b54852b986e35c41e06426"
    },
    "publishedAt": "2025-02-19T00:22:36.628Z",
    "title": "Soundwave: Less is More for Speech-Text Alignment in LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66975b9f8031bf92b428e138",
      "avatarUrl": "/avatars/3254281a7bac1c8ddde1d6bc7e518b2f.svg",
      "fullname": "Yuhao Zhang",
      "name": "Yoohao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11564",
      "authors": [
        {
          "_id": "67b40f93aba9e111862052ab",
          "user": {
            "_id": "65e5bd4568234ef5d6decadc",
            "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
            "isPro": false,
            "fullname": "Jaehyeong Jo",
            "user": "harryjo97",
            "type": "user"
          },
          "name": "Jaehyeong Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:27.544Z",
          "hidden": false
        },
        {
          "_id": "67b40f93aba9e111862052ac",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T08:54:29.000Z",
      "title": "連続拡散モデルの言語モデリング",
      "summary": "ディフュージョンモデルは、離散分類データのモデリングに対して自動回帰モデルの有望な代替として登場しました。しかし、直接離散データスペースで機能するディフュージョンモデルは、離散状態間の遷移中に信号が失われるため、連続的な改善の力を全く活用していません。現在の離散データ向けの連続ディフュージョンモデルは、離散アプローチに比べて性能が限られ、その間の不明確な連絡が離散データ向けのディフュージョンモデルの開発に制限をかけています。本研究では、離散分類分布の構造を統合する連続ディフュージョンモデルを提案します。離散ディフュージョンと統計多様体上の連続フローの間の連絡を確立し、その類似性に基づいて、以前の離散ディフュージョンモデルを一般化する簡単なディフュージョンプロセスの設計を提案します。また、径向対称性に基づくシミュレーション無制限の訓練フレームワークと高次元多様体の問題を解決する簡単な手法を提案します。言語モデリングベンチマークと他のモデールに対する詳細な実験は、我々の方法が現在の離散ディフュージョンモデルを超え、自動回帰モデルの性能に近づいていることを示します。コードは、https://github.com/harryjo97/RDLM{https://github.com/harryjo97/RDLM} に公開されています。",
      "upvotes": 30,
      "discussionId": "67b40f94aba9e111862052d5"
    },
    "publishedAt": "2025-02-18T22:43:02.567Z",
    "title": "Continuous Diffusion Model for Language Modeling",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11564.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "65e5bd4568234ef5d6decadc",
      "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg",
      "fullname": "Jaehyeong Jo",
      "name": "harryjo97",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11079",
      "authors": [
        {
          "_id": "67b40141ad717fe02e188c1a",
          "user": {
            "_id": "63a950ac3453852ef5394178",
            "avatarUrl": "/avatars/48a5e537b10e2247a17e63502e3201a6.svg",
            "isPro": false,
            "fullname": "Lijie Liu",
            "user": "liulj13",
            "type": "user"
          },
          "name": "Lijie Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-18T09:31:42.570Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1b",
          "user": {
            "_id": "657ab4705e1c941f4c2f7877",
            "avatarUrl": "/avatars/c450f81f83dd0436ae120ab15616c4f7.svg",
            "isPro": false,
            "fullname": "Tianxiang Ma",
            "user": "Grayson111",
            "type": "user"
          },
          "name": "Tianxiang Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:45:00.117Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1c",
          "user": {
            "_id": "63b415037af2e415f2599c18",
            "avatarUrl": "/avatars/4afbe7d6d05a702f1beeed9c53e78153.svg",
            "isPro": false,
            "fullname": "Bingchuan Li",
            "user": "lbc402",
            "type": "user"
          },
          "name": "Bingchuan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:47:57.441Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1d",
          "user": {
            "_id": "6304e2dabad6ce7fc0287d57",
            "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
            "isPro": false,
            "fullname": "Zhuowei_Chen",
            "user": "ZhuoweiChen",
            "type": "user"
          },
          "name": "Zhuowei Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:47:50.995Z",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1e",
          "name": "Jiawei Liu",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c1f",
          "name": "Qian He",
          "hidden": false
        },
        {
          "_id": "67b40141ad717fe02e188c20",
          "name": "Xinglong Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T11:02:50.000Z",
      "title": "ファントム: 主題一致のビデオ生成によるクロスモーダルアライメント",
      "summary": "連続的なビデオ生成の基盤モデルの開発は、多様なアプリケーションに向けて進化していますが、主題一致のビデオ生成はまだ探索階段に残っています。これを「Subject-to-Video」と呼び、参照画像から主題要素を抽出し、文脈指示によって主題一致のビデオを生成することを指します。私たちは、Subject-to-Videoの本質は、文脈と画像のダブルモードプライムのバランスを調整し、両方の文脈と可視内容を深く同時に対応させることであると信じています。そこで、私たちは、Phantomという統一的なビデオ生成フレームワークを提案します。既存の文脈からビデオへのアーキテクチャを基に、文脈画像注入モデルを再設計し、文脈画像ビデオトライプルトデータをより深く学習させることで、文脈と画像のクロスモード対応を達成します。特に、人間生成での主題一致を強調し、既存のID保持のビデオ生成を含む同時に、機能を向上させます。このプロジェクトのホームページは、https://phantom-video.github.io/Phantom/にあります。",
      "upvotes": 27,
      "discussionId": "67b40144ad717fe02e188cb2"
    },
    "publishedAt": "2025-02-18T21:56:39.407Z",
    "title": "Phantom: Subject-consistent video generation via cross-modal alignment",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63a950ac3453852ef5394178/HuVZ5d9xTlI4R1onRv_F5.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a950ac3453852ef5394178",
      "avatarUrl": "/avatars/48a5e537b10e2247a17e63502e3201a6.svg",
      "fullname": "Lijie Liu",
      "name": "liulj13",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12464",
      "authors": [
        {
          "_id": "67b55b2cc92c4aa82c13562d",
          "user": {
            "_id": "64ad5f59b7e4b2c1ce47eb43",
            "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg",
            "isPro": false,
            "fullname": "Seanie Lee",
            "user": "Seanie-lee",
            "type": "user"
          },
          "name": "Seanie Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:53.341Z",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c13562e",
          "name": "Dong Bok Lee",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c13562f",
          "name": "Dominik Wagner",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135630",
          "name": "Minki Kang",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135631",
          "user": {
            "_id": "63a9379e2e05ca32e352d93b",
            "avatarUrl": "/avatars/6cda37befc873a92ed6d5dcba507954a.svg",
            "isPro": false,
            "fullname": "Haebin Seong",
            "user": "hbseong",
            "type": "user"
          },
          "name": "Haebin Seong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:51:37.783Z",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135632",
          "name": "Tobias Bocklet",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135633",
          "name": "Juho Lee",
          "hidden": false
        },
        {
          "_id": "67b55b2cc92c4aa82c135634",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T02:51:17.000Z",
      "title": "SafeRoute: 効率よく正確な安全性のためのアダプティブなモデル選択\n大規模言語モデルの安全性ガードライン",
      "summary": "リアルウォールディスプレイズでの大規模言語モデル（LLMs）の採用には、強力な安全ゲードモデルが必要です。これらは有害なユーザープロンプターを検出し、ブロックするために使用されます。しかし、大規模な安全ゲードモデルは強い性能を達成するが、計算コストが大きくなります。これを軽減するために、小さなジュースモデルが使用されますが、「難しい」エキゾティック例では、大きなモデルが正確な予測を提供するようなモデルと比較して悪い性能を示します。私たちは、多くの入力が小さなモデルで信頼的に処理でき、その小さな部分のみが大きなモデルの計算能力を必要とすることを見出しました。このことをモットーに、SafeRouteという二値ローターを提案します。この方法は、ローターが難しいデータを考える限りに大きな安全ゲードモデルを選択的に適用し、そのようなデータに対しての効率化を図りながら、そのようなデータに対しての精度を維持します。複数のベンチマークデータセット上での実験結果は、我々の適応的なモデル選択が計算コストと安全性の性能のトレードオフを大幅に向上させ、関連するベースラインを超えることを示します。",
      "upvotes": 24,
      "discussionId": "67b55b2dc92c4aa82c13568b"
    },
    "publishedAt": "2025-02-18T23:23:34.214Z",
    "title": "SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ad5f59b7e4b2c1ce47eb43/ZEq_vSLjsXuPX3O-TWIpE.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12464.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ad5f59b7e4b2c1ce47eb43",
      "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg",
      "fullname": "Seanie Lee",
      "name": "Seanie-lee",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13131",
      "authors": [
        {
          "_id": "67b5461d29cc269e5a4eb823",
          "name": "Feng Luo",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb824",
          "user": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "isPro": false,
            "fullname": "Rui Yang",
            "user": "Ray2333",
            "type": "user"
          },
          "name": "Rui Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:23.095Z",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb825",
          "name": "Hao Sun",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb826",
          "user": {
            "_id": "634b9914dcf125e4da02498b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634b9914dcf125e4da02498b/crRgFroWq5U6XWtvlTXSZ.jpeg",
            "isPro": false,
            "fullname": "Chunyuan Deng",
            "user": "CharlesDDDD",
            "type": "user"
          },
          "name": "Chunyuan Deng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:56:33.053Z",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb827",
          "name": "Jiarui Yao",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb828",
          "name": "Jingyan Shen",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb829",
          "user": {
            "_id": "6719d581a6cad13741b8bc7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719d581a6cad13741b8bc7f/w4EttqfXRgWZJc6HpYOS9.jpeg",
            "isPro": false,
            "fullname": "Huan Zhang",
            "user": "huanzhang12",
            "type": "user"
          },
          "name": "Huan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:52:47.329Z",
          "hidden": false
        },
        {
          "_id": "67b5461d29cc269e5a4eb82a",
          "name": "Hanjie Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:55:26.000Z",
      "title": "主成分分析による多様な人間の好み学習の再考",
      "summary": "人間の好みを理解することは、基盤モデルの向上とパーソナリティーのAIシステムの構築に重要です。しかし、好みは本質的に多様で複雑であり、傳統的な報酬モデルがその全範囲を捉えることが難しいことになります。また、細かい好みデータは役立ちますが、その収集は高価でスケーリングが難しいことになります。本論文では、二値比較から多様な人間の好みを抽出するための新しいアプローチである分解された報酬モデル（DRMs）を介しています。私たちの主な見通りは、人間の好みをベクトルとして表現し、主成分分析（PCA）を用いて分析することです。好みと拒否されたレスポンスの隠れベクトルの差のデータセットを構築することで、DRMsは好みの異なる面を捉える正交基底ベクトルを特定します。これらの分解された報酬は、適切なユーザーの需要に合わせて柔軟に組み合わせることができ、説明可能でスケーリング可能な傳統的な報酬モデルの代わりとして提供されます。DRMsが意味のある好みの次元（例：役立ち、安全、ハモー）を有効に抽出し、新しいユーザーに対して訓練を追加しないように変更することができることを示します。これらの結果は、DRMsがパーソナリティーおよび説明可能なLLMのアラインメントの有力なフレームワークとしての役割を明らかにしています。",
      "upvotes": 23,
      "discussionId": "67b5461f29cc269e5a4eb8bc"
    },
    "publishedAt": "2025-02-18T21:59:45.466Z",
    "title": "Rethinking Diverse Human Preference Learning through Principal Component Analysis",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13131.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d45451c34a346181b130dd",
      "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
      "fullname": "Rui Yang",
      "name": "Ray2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13143",
      "authors": [
        {
          "_id": "67b546c0d8a1eac02c605f6a",
          "user": {
            "_id": "63c3e8abc7d7f4c63a515a02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
            "isPro": false,
            "fullname": "Zekun Qi",
            "user": "qizekun",
            "type": "user"
          },
          "name": "Zekun Qi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:21.001Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6b",
          "user": {
            "_id": "65f9533b136fb8ddbd14e1fa",
            "avatarUrl": "/avatars/d88f75da0448093ccd1babba2a37d73f.svg",
            "isPro": false,
            "fullname": "Zhang",
            "user": "WenyaoZhang",
            "type": "user"
          },
          "name": "Wenyao Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T10:08:31.789Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6c",
          "user": {
            "_id": "66bde456198f9d79f2be2d17",
            "avatarUrl": "/avatars/8c349aecb8a3a7cd7ef9d69e94eca8bd.svg",
            "isPro": false,
            "fullname": "Yufei Ding",
            "user": "YufeiD",
            "type": "user"
          },
          "name": "Yufei Ding",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T10:08:57.294Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6d",
          "user": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "isPro": false,
            "fullname": "Runpei Dong",
            "user": "RunpeiDong",
            "type": "user"
          },
          "name": "Runpei Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:18.622Z",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6e",
          "name": "Xinqiang Yu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f6f",
          "name": "Jingwen Li",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f70",
          "name": "Lingyun Xu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f71",
          "name": "Baoyu Li",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f72",
          "name": "Xialin He",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f73",
          "name": "Guofan Fan",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f74",
          "name": "Jiazhao Zhang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f75",
          "name": "Jiawei He",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f76",
          "name": "Jiayuan Gu",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f77",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f78",
          "name": "Kaisheng Ma",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f79",
          "name": "Zhizheng Zhang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f7a",
          "name": "He Wang",
          "hidden": false
        },
        {
          "_id": "67b546c0d8a1eac02c605f7b",
          "name": "Li Yi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:02.000Z",
      "title": "これまで：言語に基づく方向付け橋が空間的な理由論と対象物の操作を結びつけています。",
      "summary": "空間知能は具象的AIの重要な構成要素で、ロボットが環境と理解し、相互作用することを促進しています。最近の進歩は、VLMの物体の位置を認識し、位置関係を理解する能力を向上させていますが、物体の向きを決める能力はまだありません。この欠点を解決するには、幾何学的推理が必要であり、直感的で表現力のある向きの表現方法も必要です。このコンテキストでは、自然言語が標準的なフレームよりもフレックスフルな表現空間を提供し、指示を聞いたロボットシステムに特に適していることを提案します。この論文では、セマンティックな向きの概念を介し、自然言語で参照フレームを無くして物体の向きを定義する方法を紹介します（例えば、USBの''プラグイン''方向やナイフの''ハンドル''方向）。これをサポートするために、OrienText300Kという3Dモデルの大規模なデータセットを構築します。このデータセットは、機能的なセマンティクスと幾何学的理解を結びつけることを目的としています。VLMシステムにセマンティックな向きを組み込み、ロボットが位置と向きの両方の制約を持つ操作行動を生成することを可能にします。シミュレーションと実世界での拡大の実験は、例えばOpen6DORでは48.7%の精度、SIMPLERでは74.9%の精度を示し、ロボットの操作能力を大幅に向上させたことを示しています。",
      "upvotes": 22,
      "discussionId": "67b546c5d8a1eac02c606090"
    },
    "publishedAt": "2025-02-18T21:51:33.957Z",
    "title": "SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13143.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c3e8abc7d7f4c63a515a02",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
      "fullname": "Zekun Qi",
      "name": "qizekun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13145",
      "authors": [
        {
          "_id": "67b54b04bd51b4e46e39d287",
          "user": {
            "_id": "6577073fc2bf55b1f6bafb49",
            "avatarUrl": "/avatars/58803398b1a918b7570db17893e65122.svg",
            "isPro": false,
            "fullname": "liao",
            "user": "LegendBC",
            "type": "user"
          },
          "name": "Bencheng Liao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:00.934Z",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d288",
          "name": "Hongyuan Tao",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d289",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28a",
          "user": {
            "_id": "646b3db131968a60a01e4cf5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646b3db131968a60a01e4cf5/DhfdqUYQaD1Qa8Svw996J.jpeg",
            "isPro": false,
            "fullname": "Tianheng Cheng",
            "user": "wondervictor",
            "type": "user"
          },
          "name": "Tianheng Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:58.351Z",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28b",
          "name": "Yingyue Li",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28c",
          "name": "Haoran Yin",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28d",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67b54b04bd51b4e46e39d28e",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:57.000Z",
      "title": "マルチモーダルマンバー：解码器だけのマルチモーダル状態スペクトルモデルによる二次元から線形の煉熱",
      "summary": "最近の多モダル大語言モデル（MLLM）は、驚異的な性能を達成していますが、オーダー的な計算複雑性、増加するKey-Valueキャッシュ要求、そして別々の視覚エンコーダーの依存関係により、実装において課題があります。私たちは、現在のMLLMから進歩的な煙突きによる、中間計算複雑性の多モダル状態空間モデルの開発を行うフレームワークmmMambaを提案します。このアプローチは、訓練済みの解碼器だけのMLLMを直接的に、RNNベースのLLMや視覚エンコーダーを必要としない線形計算複雑性のアーキテクチャに変換することを可能にします。また、現在のTransformerからMambaを作成するためのエンドポイント策略を提案し、TransformerからMambaへの知識の効果的な伝達を行う三段階の煙突きレシピも提案します。この方法は、TransformerとMambaのレイヤーを組み合わせた柔軟なハイブリッドアーキテクチャをサポートし、コンティニューエフィシェンスと性能の調整を可能にします。Transformerベースの解碼器だけのHoVLEから煙突きされたmmMamba-linearは、現在の線形とオーダー計算複雑性のVLMsと比較して、競争的な性能を達成し、mmMamba-hybridはこれに加えて、より高い性能を収め、HoVLEの能力に近づきます。103Kトークンでは、mmMamba-linearはHoVLEより20.6倍のスピードアップと75.8%のGPUメモリ削減を示し、mmMamba-hybridは13.5倍のスピードアップと60.2%のメモリサービスを収めます。コードとモデルは、https://github.com/hustvl/mmMambaに公開されています。",
      "upvotes": 18,
      "discussionId": "67b54b05bd51b4e46e39d2bb"
    },
    "publishedAt": "2025-02-18T22:08:27.750Z",
    "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13145.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6577073fc2bf55b1f6bafb49",
      "avatarUrl": "/avatars/58803398b1a918b7570db17893e65122.svg",
      "fullname": "liao",
      "name": "LegendBC",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11433",
      "authors": [
        {
          "_id": "67b54a644508bd0617598c21",
          "name": "Guojun Xiong",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c22",
          "name": "Zhiyang Deng",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c23",
          "name": "Keyi Wang",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c24",
          "name": "Yupeng Cao",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c25",
          "name": "Haohang Li",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c26",
          "name": "Yangyang Yu",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c27",
          "name": "Xueqing Peng",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c28",
          "name": "Mingquan Lin",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c29",
          "name": "Kaleb E Smith",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2a",
          "name": "Xiao-Yang Liu",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2b",
          "user": {
            "_id": "63b58ed5889aa6707f0bb0f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
            "isPro": true,
            "fullname": "Jimin Huang",
            "user": "jiminHuang",
            "type": "user"
          },
          "name": "Jimin Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:03.181Z",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2c",
          "name": "Sophia Ananiadou",
          "hidden": false
        },
        {
          "_id": "67b54a644508bd0617598c2d",
          "name": "Qianqian Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T04:45:53.000Z",
      "title": "FLAG-Trader: 勾配に基づく強化学習を用いたFusion LLM-Agent ファイナンストレーディング向け",
      "summary": "大語言モデル（LLMs）を多タイプの金融データに微調節したものは、多様な金融タスクで驚異的な理由能力を示しています。しかし、交互する金融市場での多段階的、目的対象指向的なシナリオに対しては、複雑なアウトプットアプローチが必要となり、決済策を改善するためには難しいことが多いです。これに対処するために、FLAG-Traderを提案します。FLAG-Traderは、言語処理（LLMsを通じて）と勾配を基にした強化学習（RL）の政策最適化を統合した一連のアーキテクチャです。部分に微調節されたLLMは、既知の知識を活用しながら、金融データに対してパラメータ効率的な微調節を通じて、政策ネットワークとして機能します。トレーディング報酬に基づく政策勾配最適化を通じて、我々のフレームワークは、トレーディングでのLLMの性能を向上させ、そして他の金融データベースタスクの結果を改善することもできます。これらの向上を証明するために、拡張された実験データを提供します。",
      "upvotes": 18,
      "discussionId": "67b54a654508bd0617598c7e"
    },
    "publishedAt": "2025-02-18T22:06:19.200Z",
    "title": "FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63b58ed5889aa6707f0bb0f4/2C9mhT-1Qz14hik7sxjf2.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11433.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63b58ed5889aa6707f0bb0f4",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/9-6SJBOLdqUoc2LrKsI6y.jpeg",
      "fullname": "Jimin Huang",
      "name": "jiminHuang",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.09245",
      "authors": [
        {
          "_id": "67b57a993d4f319f1fa9424b",
          "name": "Gleb Gerasimov",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424c",
          "user": {
            "_id": "63ed5676684767daecac6f8a",
            "avatarUrl": "/avatars/d0e4a715f9c3fb6d74c183bab751ec35.svg",
            "isPro": false,
            "fullname": "Yaroslav Aksenov",
            "user": "yaraksen",
            "type": "user"
          },
          "name": "Yaroslav Aksenov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:41.123Z",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424d",
          "user": {
            "_id": "60b364e7f88532cd79eaff7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg",
            "isPro": false,
            "fullname": "Nikita Balagansky",
            "user": "elephantmipt",
            "type": "user"
          },
          "name": "Nikita Balagansky",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:33:26.858Z",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424e",
          "name": "Viacheslav Sinii",
          "hidden": false
        },
        {
          "_id": "67b57a993d4f319f1fa9424f",
          "user": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "isPro": false,
            "fullname": "Daniil Gavrilov",
            "user": "kefirski",
            "type": "user"
          },
          "name": "Daniil Gavrilov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:43.143Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-13T12:00:50.000Z",
      "title": "あなたはTransformerの表現能力を完全に活用していません。",
      "summary": "RNNと違い、Transformersはすべての前のトークンを直接にアチューラーすることができます。しかし、標準的なTransformersは、直前のレイヤーからの表現をだけ使用しています。本論文では、このデザイン選択が表現崩壊を引き起こし、最適な性能を得ることができないことを示します。この問題を解決するために、Layer-Integrated Memory (LIMe) を紹介します。LIMeは、モデルの全体的なメモリーフィットを保ちながら、前のレイヤーからの隠れ状態をアクセスできるようにして、表現力を拡大します。様々なアーキテクチャと異なる検索機構を組み合わせた拡張的な実験を通じて、LIMeは幅広いタスクにおいて統一的な性能向上を示します。また、学習された表現ダイナミクスの分析と、深さ方向き回路の探索から、LIMeがレイヤー間で情報を統合することを示し、将来の研究の誘いの方への指導的な方向を示します。",
      "upvotes": 12,
      "discussionId": "67b57a9a3d4f319f1fa94274"
    },
    "publishedAt": "2025-02-19T03:03:51.930Z",
    "title": "You Do Not Fully Utilize Transformer's Representation Capacity",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63ed5676684767daecac6f8a/tZDsnW0gjHoYCpbZ-wwJi.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09245.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ed5676684767daecac6f8a",
      "avatarUrl": "/avatars/d0e4a715f9c3fb6d74c183bab751ec35.svg",
      "fullname": "Yaroslav Aksenov",
      "name": "yaraksen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13130",
      "authors": [
        {
          "_id": "67b5625fb27eb6046b2ceec5",
          "name": "Jianwei Yang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec6",
          "name": "Reuben Tan",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec7",
          "name": "Qianhui Wu",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec8",
          "name": "Ruijie Zheng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceec9",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceeca",
          "name": "Yongyuan Liang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecb",
          "name": "Yu Gu",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecc",
          "name": "Mu Cai",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecd",
          "name": "Seonghyeon Ye",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceece",
          "name": "Joel Jang",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceecf",
          "name": "Yuquan Deng",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceed0",
          "name": "Lars Liden",
          "hidden": false
        },
        {
          "_id": "67b5625fb27eb6046b2ceed1",
          "name": "Jianfeng Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:55:21.000Z",
      "title": "Magma: 多モデルAIアガントの基礎モデル",
      "summary": "マグマ、デジタルおよび物理的な世界での多様的なAIアウトリジンシャルタスクを担当する基盤モデルを紹介します。マグマは、視覚言語（VL）モデルの重要な拡張であり、それらの視覚言語理解能力（語調知能）を維持するのにともなうが、また視覚スペース（スペースタイム知能）での計画と行動の能力を持っています。また、UIナビゲーションからロボット操作までのアウトリジンシャルタスクを扱うことができます。これらのアウトリジンシャル能力を獲得するために、マグマは画像、ビデオからロボティックデータまでの多様なデータセットを大規模に予ちらします。画像中の行動可能な視覚的オブジェクト（例：GUIのクリック可能なボタン）をSet-of-Mark（SoM）で標識し、ビデオ中の物体の移動（例：人手またはロボットアームの跡）をTrace-of-Mark（ToM）で標識します。拡張された実験は、SoMとToMがよい調和を奏で、マグマモデルのスペースタイム知能の獲得に役立ちます。これは、図1に示されるように、幅広い範囲のタスクに基盤となるものです。特に、マグマは、これらのタスクに特化された先行モデルを超え、UIナビゲーションおよびロボット操作タスクに新たな最先端の結果を収めます。画像およびビデオ関連の多様的なタスクにおいても、マグマは、それらのデータセットによって訓練された人気の大規模な多様的なモデルに対しても優れています。我々のモデルとコードを公開し、重複可能な実験を行うために、以下のURLを参照してください。https://microsoft.github.io/Magma",
      "upvotes": 11,
      "discussionId": "67b56265b27eb6046b2cf08f"
    },
    "publishedAt": "2025-02-18T23:51:36.910Z",
    "title": "Magma: A Foundation Model for Multimodal AI Agents",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13130.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6142
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12513",
      "authors": [
        {
          "_id": "67b545fd88527668fa8bcc14",
          "name": "Tiancheng Gu",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc15",
          "name": "Kaicheng Yang",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc16",
          "name": "Chaoyi Zhang",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc17",
          "name": "Yin Xie",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc18",
          "name": "Xiang An",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc19",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1a",
          "name": "Dongnan Liu",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1b",
          "name": "Weidong Cai",
          "hidden": false
        },
        {
          "_id": "67b545fd88527668fa8bcc1c",
          "name": "Jiankang Deng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T03:58:38.000Z",
      "title": "RealSyn: 有効かつスケーラブルな多モーダルインターライトドキュメント変換パラダイム",
      "summary": "広範な画像テキストペアにプレトレーンされた後、Contrastive Language-Image Pre-training (CLIP) は多様なベンチマークで有望的な性能を示します。しかし、多模態の間接ドキュメントなどの非ペアデータの大規模な量は、視覚言語表現学習において利用不足しています。これらの非ペアドキュメントを最大限に活用するために、最初に、Real-World Data Extractionパイプラインを構築し、高品質の画像とテキストを抽出します。次に、各画像と語意的に関連付けられた実写的なテキストを効率的に関連付けるための階層的な検索方法を設計します。また、細かい視覚情報を進歩させるために、合成テキストの生成に向けた画像語意的拡張生成モジュールを提案します。さらに、データセットの多様性を向上させ、長尾概念の学習を改善するために、語意的バランスサンプリング戦略を導入します。これらの革新的な機能に基づいて、RealSynという、実写的と合成的なテキストを組み合わせたデータセットを構築し、15M、30M、100Mの3つのスケールで提供します。拡張検証は、RealSynが視覚言語表現学習を進め、強いスケーラビリティを示していることを明らかにします。RealSynでプレトレーンされたモデルは、複数の次世代タスクで最先端の性能を達成します。将来の研究を促進するために、RealSynデータセットとプレトレーンモデルの重みは、https://github.com/deepglint/RealSynに公開されています。",
      "upvotes": 10,
      "discussionId": "67b545fe88527668fa8bcc65"
    },
    "publishedAt": "2025-02-18T21:52:22.326Z",
    "title": "RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12513.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63e202f352b7578dba448ab5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg",
      "fullname": "Yang",
      "name": "Kaichengalex",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12859",
      "authors": [
        {
          "_id": "67b576aa489d68b981e086ad",
          "name": "Chenxing Wei",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086ae",
          "name": "Yao Shu",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086af",
          "name": "Mingwen Ou",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086b0",
          "name": "Ying Tiffany He",
          "hidden": false
        },
        {
          "_id": "67b576aa489d68b981e086b1",
          "name": "Fei Richard Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T13:46:47.000Z",
      "title": "PAFT: プロンプト無関係の微調校",
      "summary": "LLMsは微調節後に下流タスクに適応していますが、この適応性は、プロンプトの微小な変化も性能を大幅に低下させることがあり、プロンプトの強固性を補損します。これに対処するために、Prompt-Agnostic Fine-Tuning(PAFT)を提案します。PAFTは簡単で効果的なアプローチで、微調節中にプロンプトを動的に調整します。これにより、モデルは課題の基礎的な原理を学習し、特定のプロンプトの表現に過学習しないようにします。PAFTは2ステップで動作します。最初に、意味のある合成的な候補プロンプトの多様なセットを構築します。次に、このセットからプロンプトをランダムにサンプリングし、動的な学習入力を作成します。多様なデータセットとLLMsを構成した広範囲の実験で、PAFTで学習されたモデルは、広範囲のプロンプト、特に見ぬプロンプトにも強い強固性と一般化能力を示します。この強固性の向上は、モデルの性能と推論速度を向上させ、同時に訓練の効率を維持します。消滅調査は、PAFTの効果性を進一に確認します。",
      "upvotes": 8,
      "discussionId": "67b576aa489d68b981e08708"
    },
    "publishedAt": "2025-02-19T01:21:54.836Z",
    "title": "PAFT: Prompt-Agnostic Fine-Tuning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12859.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ed3051492a7f35db21fea2",
      "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
      "fullname": "Chenxing Wei",
      "name": "kittttttt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12170",
      "authors": [
        {
          "_id": "67b5434f2b2ec6908fffe75e",
          "name": "Da Xiao",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe75f",
          "name": "Qingye Meng",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe760",
          "name": "Shengping Li",
          "hidden": false
        },
        {
          "_id": "67b5434f2b2ec6908fffe761",
          "name": "Xingyuan Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-13T10:26:27.000Z",
      "title": "MUDDFormer: Transformerの残差ボトルネックを破壊するための多方向的な動的な密接結合",
      "summary": "We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value, or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer.",
      "upvotes": 6,
      "discussionId": "67b543502b2ec6908fffe788"
    },
    "publishedAt": "2025-02-18T22:59:16.530Z",
    "title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12170.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d77440bad37ef354028365",
      "avatarUrl": "/avatars/df0dea879e06fa814867e9aad03d1e68.svg",
      "fullname": "Da Xiao",
      "name": "xiaoda99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12215",
      "authors": [
        {
          "_id": "67b56007fa141a55e51d9d78",
          "name": "Zhiyuan Zeng",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d79",
          "name": "Qinyuan Cheng",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7a",
          "name": "Zhangyue Yin",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7b",
          "name": "Yunhua Zhou",
          "hidden": false
        },
        {
          "_id": "67b56007fa141a55e51d9d7c",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-17T07:21:11.000Z",
      "title": "再調査o1らしいモデルのテストタイムスケーリング：それらは実際にテストタイムスケーリング能力を持っているか？",
      "summary": "大語言モデル（LLMs）でのテスト時スケーリングの到来、OpenAIのo1シリーズを例にして、推論時に計算コンピューティングリソースの割り当てをスケーリングして理由論の能力を進化させることができる。QwQ、Deepseek-R1（R1）、LIMOなどの後続モデルはこれらの進歩を再現しているが、これらのモデルが本質的にテスト時スケーリング能力を持っているかどうかはまだ調査が不足している。本研究では、これらのo1ようなモデルの長いCoTs（理由論）が精度を一貫して向上させることはありません。実際、同じ問題に対して正しい解答は間違った解答よりも短いことが多い。進みに、この現象はモデルの自動訂正能力と密接に関連していることが明らかになりました。長いCoTsにはより多くの自動訂正が含まれ、これらは通常性能の低下につながる。そして、QwQ、R1、LIMOに対して、順列や並列スケーリングスニペットを比較し、並列スケーリングがより良いカバー率とスケーラビリティを達成していることが明らかになりました。これらの洞察に基づいて、並列スケーリングスニペットとCoTsの長さの特徴を組み合わせた方法「最短多数投票」を提案し、傳統的な多数投票アプローチに比べてテスト時スケーリング能力を大幅に向上させることができることを示しました。",
      "upvotes": 5,
      "discussionId": "67b56007fa141a55e51d9da7"
    },
    "publishedAt": "2025-02-18T23:37:46.756Z",
    "title": "Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12215.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6142
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12501",
      "authors": [
        {
          "_id": "67b547ffc9071a3e97139532",
          "user": {
            "_id": "62a42f22c683d02f5b63320c",
            "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
            "isPro": false,
            "fullname": "Qiyuan Zhang",
            "user": "DonJoey",
            "type": "user"
          },
          "name": "Qiyuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:10.215Z",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139533",
          "name": "Yufei Wang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139534",
          "user": {
            "_id": "63c20105726f62e411fbe882",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c20105726f62e411fbe882/2UsU9O2psbDjJzz-sAmGH.jpeg",
            "isPro": false,
            "fullname": "Yuxin Jiang",
            "user": "YuxinJiang",
            "type": "user"
          },
          "name": "Yuxin Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:01:08.101Z",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139535",
          "name": "Liangyou Li",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139536",
          "name": "Chuhan Wu",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139537",
          "name": "Yasheng Wang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139538",
          "name": "Xin Jiang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e97139539",
          "name": "Lifeng Shang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953a",
          "name": "Ruiming Tang",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953b",
          "name": "Fuyuan Lyu",
          "hidden": false
        },
        {
          "_id": "67b547ffc9071a3e9713953c",
          "name": "Chen Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T03:31:06.000Z",
      "title": "クラスター比較的理由：LLM-as-a-Judgeの全面的評価を解く",
      "summary": "LLM-as-a-Judgeは、chain-of-thought (CoT) 判断を生成することで、広泛に採用された自動評価方法として機能しています。しかし、この方法の信頼性は、CoT 推理が詳細なものを完全に捉えることができないため、詳細なものを捉えられないことにより、不完全な結果を生み出すことによって損なわれています。現在の方法は主に多数決まりや評価基準の拡張に依存しており、CoT の限界を解決することはできません。私たちは、Crowd-based Comparative Evaluation を提案しています。これは、候補回答と比較するために追加したコードのレスポンスを介し、候補回答の中の詳細なものを明らかにします。このプロセスは、LLM-as-a-Judge が詳細な CoT 判断を提供することを促進します。広範囲の実験により、私たちのアプローチは評価の信頼性を向上させ、5つのベンチマークで平均 6.7% の精度向上を実現しました。また、私たちの方法は、判定の経験を利用することで、高品質の CoT を生成し、訓練フィードバック (SFT) の収束サンプリングで上位の性能を示し、これを「コードの収束サンプリング」と呼ばれるようになり、SFT のより効率的な実行を可能にします。分析により、私たちが生成した CoT は詳細で高品質であり、評価精度は推論のスケールアップによって向上します。",
      "upvotes": 5,
      "discussionId": "67b54800c9071a3e9713956c"
    },
    "publishedAt": "2025-02-18T21:55:26.822Z",
    "title": "Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12501.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a42f22c683d02f5b63320c",
      "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
      "fullname": "Qiyuan Zhang",
      "name": "DonJoey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.11271",
      "authors": [
        {
          "_id": "67b4322c217ec18a40587bec",
          "user": {
            "_id": "60f5f68fa7fd83d025749234",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
            "isPro": false,
            "fullname": "Pan Lu",
            "user": "lupantech",
            "type": "user"
          },
          "name": "Pan Lu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:04:43.677Z",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bed",
          "name": "Bowen Chen",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bee",
          "name": "Sheng Liu",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bef",
          "name": "Rahul Thapa",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bf0",
          "name": "Joseph Boen",
          "hidden": false
        },
        {
          "_id": "67b4322c217ec18a40587bf1",
          "name": "James Zou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T21:18:47.000Z",
      "title": "OctoTools: 拡張可能なツールを備えたアガンティックフレームワークで複雑な理由論理",
      "summary": "複雑な理由論理タスクの解決には、視覚理解、領域知識の検索、数値計算、多段階理由論理などが含まれます。現在の方法は、外部ツールを使用して大規模言語モデル（LLMs）を強化していますが、特殊化された領域、ツールの種類の制限や追加の訓練データの必要性があります。この論文では、OctoToolsという、訓練不要、ユーザーフレンドリーさと容易に拡張可能な開放ソースアガントフレームワークを紹介します。このフレームワークは、多様な領域での複雑な理由論理を解決するために設計されています。OctoToolsは、標準化されたツールカードを使用してツール機能を収め、高レベルと低レベルの計画の両方の計画機、ツールの使用を実行する実行機能を備えています。OctoToolsは、MathVista、MMLU-Pro、MedQA、GAIA-Textなど16種類の多様なタスクでの一般性を証明し、GPT-4oを基準にして平均正確率の大幅な増加（9.3%）を実現しました。また、同じツールのセットを与えると、AutoGen、GPT-Functions、LangChainを上回り、10.6%の上限値で優位を取ります。詳細な分析と消去試験を通じて、OctoToolsはタスク計画、有効なツール使用、多段階問題解決において優れた性能を示しています。",
      "upvotes": 4,
      "discussionId": "67b4322d217ec18a40587c27"
    },
    "publishedAt": "2025-02-19T02:27:36.940Z",
    "title": "OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.11271.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f5f68fa7fd83d025749234",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
      "fullname": "Pan Lu",
      "name": "lupantech",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.09838",
      "authors": [
        {
          "_id": "67b55078a64445f58c771d84",
          "name": "Tianwei Lin",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d85",
          "name": "Wenqiao Zhang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d86",
          "name": "Sijing Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d87",
          "name": "Yuqian Yuan",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d88",
          "name": "Binhe Yu",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d89",
          "name": "Haoyuan Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8a",
          "name": "Wanggui He",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8b",
          "name": "Hao Jiang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8c",
          "name": "Mengze Li",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8d",
          "name": "Xiaohui Song",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8e",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d8f",
          "name": "Jun Xiao",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d90",
          "name": "Hui Lin",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d91",
          "name": "Yueting Zhuang",
          "hidden": false
        },
        {
          "_id": "67b55078a64445f58c771d92",
          "name": "Beng Chin Ooi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-14T00:42:36.000Z",
      "title": "健康GPT: 医学用大視覚言語モデルの統合\n  理解と生成を通じて異なる知識の適応による統合",
      "summary": "ヘルスGPT、強力な医学用大規模な視覚言語モデル（Med-LVLM）を紹介します。これは、統一的な自動復元パラダイム内で医学的な視覚的理解と生成能力を統合したものです。私たちのスタートアップ哲学は、違うコンピューティングの理解と生成の知識を進段的に適用し、事前学習された大規模な言語モデル（LLMs）に適用することです。これは、新しい違うランクアダプター（H-LoRA）手法により実現され、これは、製品化された階層的な視覚認識アプローチと三段階学習戦略に補われています。ヘルスGPTの効果的な学習を行うために、VL-Healthという医学領域に特化した詳細な理解と生成データセットを設計しました。実験結果により、ヘルスGPTは医学的な視覚的統一的タスクでの卓越した性能とスケーラビリティを示します。このプロジェクトは、https://github.com/DCDmllm/HealthGPT からアクセスできます。",
      "upvotes": 4,
      "discussionId": "67b5507aa64445f58c771df9"
    },
    "publishedAt": "2025-02-18T22:35:23.066Z",
    "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09838.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fc18edfb66882aba4d548e",
      "avatarUrl": "/avatars/f70d47fe4aba98b5a5cd64f7e002dfd2.svg",
      "fullname": "wenqiao",
      "name": "wannature",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.12574",
      "authors": [
        {
          "_id": "67b547f555d0424a31b9c384",
          "user": {
            "_id": "64cb48f7667f4f808535107e",
            "avatarUrl": "/avatars/8f77f378ad665b246e1ea3aaba2153ae.svg",
            "isPro": false,
            "fullname": "chengluo",
            "user": "wdlctc",
            "type": "user"
          },
          "name": "Cheng Luo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:40:25.130Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c385",
          "user": {
            "_id": "64b15284372d4340772a3dca",
            "avatarUrl": "/avatars/417d5f1bc1bcb5e4d5de6169673c2cf7.svg",
            "isPro": false,
            "fullname": "Zefan Cai",
            "user": "ZefanCai",
            "type": "user"
          },
          "name": "Zefan Cai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:40:47.077Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c386",
          "name": "Hanshi Sun",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c387",
          "user": {
            "_id": "64c15c5bea792b1950e302e4",
            "avatarUrl": "/avatars/51f84365cc08a1dcd5da70968389aed2.svg",
            "isPro": false,
            "fullname": "Jinqi Xiao",
            "user": "jinqixiao",
            "type": "user"
          },
          "name": "Jinqi Xiao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:41:01.931Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c388",
          "name": "Bo Yuan",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c389",
          "name": "Wen Xiao",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38a",
          "user": {
            "_id": "675f8271a63fff7b5bcbc478",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/9tJn7NyzLMreCJVH4wRho.png",
            "isPro": false,
            "fullname": "Junjie Hu",
            "user": "junjiehu",
            "type": "user"
          },
          "name": "Junjie Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:41:18.304Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38b",
          "name": "Jiawei Zhao",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38c",
          "user": {
            "_id": "64b732f832403871593e082c",
            "avatarUrl": "/avatars/dd21932b0c167131ee7545a622c46c3c.svg",
            "isPro": false,
            "fullname": "Beidi Chen",
            "user": "beidic",
            "type": "user"
          },
          "name": "Beidi Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:39:20.563Z",
          "hidden": false
        },
        {
          "_id": "67b547f555d0424a31b9c38d",
          "user": {
            "_id": "6532920b3e385cfc6002938d",
            "avatarUrl": "/avatars/cb9cc6d2733031582c83f56dc6cd1dd5.svg",
            "isPro": false,
            "fullname": "Anima Anandkumar",
            "user": "animakumar",
            "type": "user"
          },
          "name": "Anima Anandkumar",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:39:15.091Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T06:26:05.000Z",
      "title": "HeadInfer: メモリ効率的LLM推論を頭ごとにオフロードで実行する方法",
      "summary": "Transformerベースの大規模言語モデル（LLMs）は、長文脈生成に優れた性能を示す。文脈長の拡大は、推論時のLLMsのメモリフットプリントを鍵値キャッシュ（KV cache）に不均衡的に移し込みました。本論文では、HEADINFERを提案します。HEADINFERは、GPU上での全てのtransformer層のKV cacheの完全な保存を避けるようにCPU RAMにKV cacheをオフロードします。HEADINFERは、選択的な注意ヘッドのKV cacheをGPU上に残しながら、注意出力を動的に計算します。ロフィンライン分析を通じて、HEADINFERは計算効率を維持しながら、メモリフットプリントを大幅に減少させることを示します。Llama-3-8Bモデルに対して1,000,000トークンのシーケンスを用いてHEADINFERを評価します。KV cacheのGPUメモリフットプリントは128GBから1GBに減少し、総合的なGPUメモリ使用量は207GBから17GBに減少し、BF16ベースライン推論に対する92%減少を達成します。特に、HEADINFERは、24GBメモリのコンシューマーGPU（例：NVIDIA RTX 4090）で8Bモデルで4,000,000トークンの推論を行うことができます。また、近似手法を使用しないようにしています。",
      "upvotes": 3,
      "discussionId": "67b547f755d0424a31b9c3e5"
    },
    "publishedAt": "2025-02-18T21:57:00.289Z",
    "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12574.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64cb48f7667f4f808535107e",
      "avatarUrl": "/avatars/8f77f378ad665b246e1ea3aaba2153ae.svg",
      "fullname": "chengluo",
      "name": "wdlctc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13063",
      "authors": [
        {
          "_id": "67b5a7896f72266cb765e744",
          "user": {
            "_id": "618b9540682ec1c38327e586",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618b9540682ec1c38327e586/v_ZBkfh8O9Zh6C2YQpuBX.jpeg",
            "isPro": false,
            "fullname": "Yury Kuratov",
            "user": "yurakuratov",
            "type": "user"
          },
          "name": "Yuri Kuratov",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-02-19T09:42:34.422Z",
          "hidden": false
        },
        {
          "_id": "67b5a7896f72266cb765e745",
          "name": "Mikhail Arkhipov",
          "hidden": false
        },
        {
          "_id": "67b5a7896f72266cb765e746",
          "name": "Aydar Bulatov",
          "hidden": false
        },
        {
          "_id": "67b5a7896f72266cb765e747",
          "user": {
            "_id": "639c6e978a34ed9a404c6a7b",
            "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
            "isPro": false,
            "fullname": "MIKHAIL BURTSEV",
            "user": "mbur",
            "type": "user"
          },
          "name": "Mikhail Burtsev",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:56:59.080Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T17:08:45.000Z",
      "title": "クラミング1568トークンを1つのベクトルに入れ込み戻す：埋め込み空間の容量の限界を調べる",
      "summary": "最近の様々な研究は、トークンの順列を実数値のベクトルの短い順列に圧縮し、トークン埋めコードまたはキー値キャッシュを代わりに使用することを目的としています。これらのアプローチは、現在の言語モデルの計算量を減らすことができます。しかし、強力なモデルをエンコーダーとして使用することに依存しているのに対して、無失真圧縮比の最大値は通常 x10 以下となります。この事実は非常に興味深いです。理論的には、大きな実数値ベクトルの最大情報容量は、16ビット精度と軽いベクトルサイズでも、現在のレートよりも遠く遠くあります。本稿では、エンコーダーをサンプル毎の最適化手順に置き換えることで圧縮の限界を調べています。圧縮比が x1500 までのベクトルが存在することを示し、既存の解法と実用的な解法の二乗の間のギャップを明らかにします。さらに、実験的には、圧縮の限界は入力の長さではなく、減らす不確実さの量である、つまり、この順列のクロスエントロピー損失で決まります。得られた限界は、入力埋めコードの理論的な容量と実用的な利用の間の極めて大きな間違いを明らかにし、モデルの設計における大幅な最適化の余地を示します。",
      "upvotes": 1,
      "discussionId": "67b5a78a6f72266cb765e779"
    },
    "publishedAt": "2025-02-19T04:43:42.973Z",
    "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13063.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "639c6e978a34ed9a404c6a7b",
      "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
      "fullname": "MIKHAIL BURTSEV",
      "name": "mbur",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.10708",
      "authors": [
        {
          "_id": "67b58e32e972a2806a9a0451",
          "user": {
            "_id": "65407ba7a38390065750233f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
            "isPro": false,
            "fullname": "Zirui Song",
            "user": "Ziruibest",
            "type": "user"
          },
          "name": "Zirui Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:00:38.943Z",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0452",
          "name": "Bin Yan",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0453",
          "name": "Yuhan Liu",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0454",
          "name": "Miao Fang",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0455",
          "name": "Mingzhe Li",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0456",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "67b58e32e972a2806a9a0457",
          "name": "Xiuying Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-15T07:43:43.000Z",
      "title": "特定領域の知識を大規模言語モデルに注入する：一貫する調査",
      "summary": "大語言モデル（LLMs）は、自然言語理解、テキスト要約、機械翻訳などの多様なタスクで驚異的な成功を示しています。しかし、一般的な用途の性質は、健康ケア、化学、法規分析などの特殊化された知識が必要な領域への効果性を制限しています。これに対して、研究者はLLMsに特殊化された知識を統合する方法を見つけてそれらを強化しています。本調査では、これらの方法を動的な知識注入、静的な知識埋め込み、モジュール化ディレイター、プロンプト最適化の4つのキーアプローチに分類して、それぞれのアプローチがLLMsに領域専門的知識を持つ機構を提供し、柔軟性、スケーラビリティ、効率性のトレードオフを調和していることを説明します。これらの方法がLLMsが特殊化されたタスクを取り掛けることを可能にし、その利点と欠点を比較し、特殊化されたLLMsと一般的なLLMsを比較し、この新興分野の課題と機会を明らかにします。それに興味がある方におすすめしますが、この領域にさらに深く興味を持ってくださるために、通常使用されるデータセットとベンチマークをまとめています。最新の研究について研究者にアップデートしています。https://github.com/abilliyb/Knowledge_Injection_Survey_Papersにおいて、専門的なLLMsの研究を記録しています。",
      "upvotes": 1,
      "discussionId": "67b58e33e972a2806a9a04b8"
    },
    "publishedAt": "2025-02-19T02:56:09.510Z",
    "title": "Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10708.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65407ba7a38390065750233f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
      "fullname": "Zirui Song",
      "name": "Ziruibest",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.12669",
      "authors": [
        {
          "_id": "67b58c806e53744c2a373351",
          "user": {
            "_id": "63024676056ec3a2a8714b24",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
            "isPro": false,
            "fullname": "Xiang Liu",
            "user": "Dominic789654",
            "type": "user"
          },
          "name": "Xiang Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:34:03.429Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373352",
          "user": {
            "_id": "64eded5fdfe0a679d840bc98",
            "avatarUrl": "/avatars/4d4c67c13e547a4d296a301e8694e79e.svg",
            "isPro": false,
            "fullname": "sunpenglei",
            "user": "sunpenglei",
            "type": "user"
          },
          "name": "Penglei Sun",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:34:15.889Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373353",
          "name": "Shuyan Chen",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373354",
          "name": "Longhan Zhang",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373355",
          "name": "Peijie Dong",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373356",
          "name": "Huajie You",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373357",
          "user": {
            "_id": "64473221dcbe1333b64b2db2",
            "avatarUrl": "/avatars/5e4495d3581ad3e6ea3c47650f20b993.svg",
            "isPro": false,
            "fullname": "yongqi zhang",
            "user": "yongqi2023",
            "type": "user"
          },
          "name": "Yongqi Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:35:12.059Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373358",
          "name": "Chang Yan",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a373359",
          "user": {
            "_id": "6676935fcd0b89a0115174b0",
            "avatarUrl": "/avatars/4caca1b672d29e787814f9a30bf20bcc.svg",
            "isPro": false,
            "fullname": "Xiaowen Chu",
            "user": "wenxinsiju",
            "type": "user"
          },
          "name": "Xiaowen Chu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:35:20.611Z",
          "hidden": false
        },
        {
          "_id": "67b58c806e53744c2a37335a",
          "name": "Tong-yi Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T09:19:24.000Z",
      "title": "Perovskite-LLM: ペルビティー特化の知識増強型大語言モデル",
      "summary": "ポロビティ太陽光電池（PSCs）の急速な進歩は、研究論文の指数的な増加により、この領域での効率的な知識管理および理由論理システムの急な要請を生み出しました。我々は、ポロビティ太陽光電池の専門的な知識グラフを構築した知識増強システムを紹介します。このシステムは、3つの主な構成要素を統合しています。まず、1,517の研究論文から構築された専門的な知識グラフを開発します。このグラフは、23,789のエンティティと22,272の関係を含みます。次に、2つの補間的なデータセットを作成します。ポロビティシャットは、新しい多エージェントフレームワークによって生成された高品質の質問回答ペア（55,101件）を含み、ポロビティラボリングは、2,217件の謹重に選択された材料科学の問題を含みます。最後に、2つの専門的な大規模言語モデルを介します。ポロビティシャットLLMは、専門的な知識支援を提供し、ポロビティラボリングLLMは、科学の理由論理タスクを対応します。実験結果は、このシステムが現在のモデルに比べて、専門的な知識の検索と科学の理由論理タスクにおいて显著に優れていることを示し、研究者に文献検索、実験設計、および複雑な問題解決に効果的なツールを提供していることを示しています。",
      "upvotes": 1,
      "discussionId": "67b58c826e53744c2a3733c2"
    },
    "publishedAt": "2025-02-19T02:47:33.654Z",
    "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.12669.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63024676056ec3a2a8714b24",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
      "fullname": "Xiang Liu",
      "name": "Dominic789654",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.10990",
      "authors": [
        {
          "_id": "67b3ee6c1e80a69e79c3155a",
          "user": {
            "_id": "647d834618274bce03013cc2",
            "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
            "isPro": true,
            "fullname": "yixuan",
            "user": "yixuantt",
            "type": "user"
          },
          "name": "Yixuan Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-19T09:04:50.969Z",
          "hidden": false
        },
        {
          "_id": "67b3ee6c1e80a69e79c3155b",
          "name": "Yi Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-16T04:23:52.000Z",
      "title": "FinMTEB: 財務マスタイクテキストエンドフィーディングベンチマーク",
      "summary": "Embeddingモデルは、多様なNLPアプリケーションで情報の表現と検索に重要な役割を果たしています。大規模な言語モデル（LLMs）の最近の進展は、Embeddingモデルの性能を進めました。これらのモデルは一般的なデータセットで通常ベンチマークされますが、実世界的なアプリケーションはドミンスプチーフの評価を求めています。本稿では、FinMTEB（Finance Massive Text Embedding Benchmark）を紹介します。FinMTEBは、MTEBとの特化されたコンタラットで、金融領域に特化しています。FinMTEBは、7つのタスクで構成されており、中国語と英語の多様な文章型をカバーしています。例えば、金融新聞記事、会社年度報告書、ESG報告書、規制申込書、経理会議の講演要旨などです。また、FinPersona-E5という金融適応モデルを開発しました。FinPersona-E5は、ポートライトベースのデータ合成手法を用いて、多様な金融エンベディングタスクをカバーしています。15つのエンベディングモデルの拡大評価を通じて、3つのキーファインドを示しました。それは、(1)一般的なベンチマークでの性能は金融ドミンスプチーフのタスクに限られた関連性が限られています、(2)ドミンスプチーフへの適応モデルは一般的なコンタラットモデルを確実に上回ります、(3)簡単なBag-of-Words（BoW）アプローチは、金融のSemantic Textual Similarity（STS）タスクで複雑なデンスエンベディングよりも上回り、現在のデンスエンベディングテクニックの制限を明らかにします。本稿は、金融のNLPアプリケーションに対する強固な評価フレームワークを構築し、ドミンスプチーフ特化されたエンベディングモデルの開発に重要なインサイトを提供します。",
      "upvotes": 0,
      "discussionId": "67b3ee6d1e80a69e79c3158f"
    },
    "publishedAt": "2025-02-19T04:54:27.788Z",
    "title": "FinMTEB: Finance Massive Text Embedding Benchmark",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647d834618274bce03013cc2",
      "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
      "fullname": "yixuan",
      "name": "yixuantt",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.13142",
      "authors": [
        {
          "_id": "67b5790132be608036ee94e5",
          "user": {
            "_id": "65c3fdf79d062be813813e45",
            "avatarUrl": "/avatars/52528a61abe5bbbef4a4a431944973cd.svg",
            "isPro": false,
            "fullname": "Dantong Niu",
            "user": "NdtSoCool",
            "type": "user"
          },
          "name": "Dantong Niu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:12:28.457Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e6",
          "user": {
            "_id": "65406e82deee4716f1c29271",
            "avatarUrl": "/avatars/25331a773f8125f9ad1c3d6ac3375586.svg",
            "isPro": false,
            "fullname": "Yuvan Sharma",
            "user": "yuvansharma",
            "type": "user"
          },
          "name": "Yuvan Sharma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:12:35.531Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e7",
          "name": "Haoru Xue",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e8",
          "user": {
            "_id": "650bd36a7c99ca283e58e973",
            "avatarUrl": "/avatars/606d24b2dac190ebcbb4b2a2e4671380.svg",
            "isPro": false,
            "fullname": "Giscard Biamby",
            "user": "gbiamby",
            "type": "user"
          },
          "name": "Giscard Biamby",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:12:49.219Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94e9",
          "name": "Junyi Zhang",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94ea",
          "user": {
            "_id": "66a09aec369dd38cf2113070",
            "avatarUrl": "/avatars/cc13bdd3dc1271d33b083b61e12f1a05.svg",
            "isPro": false,
            "fullname": "Ziteng Ji",
            "user": "zitengj0618",
            "type": "user"
          },
          "name": "Ziteng Ji",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:13:13.907Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94eb",
          "user": {
            "_id": "64cbdf02f103036e23d1c7f3",
            "avatarUrl": "/avatars/496069463900dea20929b57381182d39.svg",
            "isPro": false,
            "fullname": "Trevor Darrell",
            "user": "trevordarrell",
            "type": "user"
          },
          "name": "Trevor Darrell",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:13:20.379Z",
          "hidden": false
        },
        {
          "_id": "67b5790132be608036ee94ec",
          "user": {
            "_id": "667c5764186b27ef806636d3",
            "avatarUrl": "/avatars/5c08f0109bc0e350624112c0aff544f6.svg",
            "isPro": false,
            "fullname": "Roei Herzig",
            "user": "roeiherz",
            "type": "user"
          },
          "name": "Roei Herzig",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-02-19T09:13:26.134Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-18T18:59:01.000Z",
      "title": "4D表現を用いた予習自動帰納型ロボットモデルの訓練",
      "summary": "ファンダメンタルモデルは、巨大な無ラベルデータセットで事前学習されたもので、自然言語とコンピュータビジョンに革命的な影響を与え、驚異的な一般化能力を示し、事前学習の重要性を強調しています。しかし、ロボティクスにおいてこのような成功を達成することは困難で、要望の高いロボットデータの記録や物理的な世界を有効に表現する表現の欠落によって制限されています。本論文では、人間のビデオデータから学習された低レベル4D表現を活用し、ロボットのモデルを改善するための自動回帰的ロボットモデルを介して、ARM4Rというものを紹介します。特に、時間にわたってモノカラムデプステーション推定を用いて2D表現を3D空間に引き上げた3D点追跡表現を利用します。これらの4D表現は、点とロボットの状態表現の間に共有する幾何的構造を保持し、線形変換によって可能なように設計されているため、人間のビデオデータから低レベルロボット制御に効率的にテンプレート学習を行うことができます。私たちの実験は、ARM4Rが人間のビデオデータからロボットに効率的にテンプレート学習を行え、様々なロボット環境と構成においてタスクの性能を一貫的に向上させることを示しています。",
      "upvotes": 0,
      "discussionId": "67b5790832be608036ee9638"
    },
    "publishedAt": "2025-02-19T01:24:26.365Z",
    "title": "Pre-training Auto-regressive Robotic Models with 4D Representations",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.13142.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667c5764186b27ef806636d3",
      "avatarUrl": "/avatars/5c08f0109bc0e350624112c0aff544f6.svg",
      "fullname": "Roei Herzig",
      "name": "roeiherz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2502.10852",
      "authors": [
        {
          "_id": "67b55321f703732d151de666",
          "name": "Zeli Su",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de667",
          "name": "Ziyin Zhang",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de668",
          "name": "Guixian Xu",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de669",
          "name": "Jianing Liu",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66a",
          "name": "XU Han",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66b",
          "name": "Ting Zhang",
          "hidden": false
        },
        {
          "_id": "67b55321f703732d151de66c",
          "name": "Yushuang Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-15T16:53:10.000Z",
      "title": "多語言エンコーダーは、実際に考えなかったほど知っている：重みの共有\n低資源言語の非常に少ない資源での事前学習",
      "summary": "多言語モデルの例としてXLM-RがNLPの多言語化を進めているのに対して、もっとも言語資源が少ない言語での性能は悪く、この状況は現代のLLMs（例えばLLaMAとQwen）がXLM-Rに比べて少ない言語をサポートしていることによって悪化している。このチャレンジに対処するために、私たちは多言語エンコーダーを非常に少ない言語の文章生成に適用するための新しいフレームワークを提案します。エンコーダーとデコーダーの重みを再利用することで、モデルは学習したエンコーダーの意味空間を活用でき、低言語資源の言語での効率的な学習と有効な一般化を可能にします。このフレームワークを4つの中国の少数民族語言に適用し、XLM-SWCMを紹介し、その優れた性能を示します。",
      "upvotes": 0,
      "discussionId": "67b55322f703732d151de69d"
    },
    "publishedAt": "2025-02-18T22:46:16.586Z",
    "title": "Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.10852.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6430bdd8cd31d174a9f900fb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
      "fullname": "Ziyin Zhang",
      "name": "Geralt-Targaryen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]