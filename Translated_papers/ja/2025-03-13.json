[
  {
    "paper": {
      "id": "2503.09566",
      "authors": [
        {
          "_id": "67d274c467e782a7eeb4ab70",
          "name": "Lingmin Ran",
          "hidden": false
        },
        {
          "_id": "67d274c467e782a7eeb4ab71",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T17:33:22.000Z",
      "title": "TPDiff: 時系列ピラミッドビデオディフューションモデル",
      "summary": "ビデオディフュージョンモデルの開発において重要な課題が浮かび上がります：計算量の大きな要求。この課題を軽減するために、我々は、ディフュージョンの逆過程が内蔵的にエントロピー減少の性質を持つことを確認しました。ビデオモデールのフレーム間の冗長性を考慮すると、高エントロピーステージで全フレームレートを維持する必要はありません。この見通しに基づき、我々はTPDiffという統一フレームワークを提案します。このフレームワークでは、ディフュージョンを数えられるステージに分割し、最後のステージだけが全フレームレートで動作し、計算効率を最適化します。多ステージディフュージョンモデルの訓練において、我々は特別な訓練フレームワーク「ステージごとディフュージョン」を導入します。この訓練戦略では、対応されたデータとノイズのプールフロー普通微分方程式（ODE）を解くことで、多様なディフュージョン形式に適用可能であり、訓練効率を進化させます。詳細な実験評価により、我々の方法の一般性が証明され、訓練コストが50%減少し、推論効率が1.5倍向上しました。",
      "upvotes": 27,
      "discussionId": "67d274c567e782a7eeb4abb0",
      "ai_keywords": [
        "video diffusion models",
        "entropy-reducing nature",
        "inter-frame redundancy",
        "TPDiff",
        "unified framework",
        "frame rate",
        "diffusion stages",
        "stage-wise diffusion",
        "partitioned probability flow ordinary differential equations (ODE)",
        "diffusion forms"
      ]
    },
    "publishedAt": "2025-03-12T13:33:22.000Z",
    "title": "TPDiff: Temporal Pyramid Video Diffusion Model",
    "summary": "The development of video diffusion models unveils a significant challenge:\nthe substantial computational demands. To mitigate this challenge, we note that\nthe reverse process of diffusion exhibits an inherent entropy-reducing nature.\nGiven the inter-frame redundancy in video modality, maintaining full frame\nrates in high-entropy stages is unnecessary. Based on this insight, we propose\nTPDiff, a unified framework to enhance training and inference efficiency. By\ndividing diffusion into several stages, our framework progressively increases\nframe rate along the diffusion process with only the last stage operating on\nfull frame rate, thereby optimizing computational efficiency. To train the\nmulti-stage diffusion model, we introduce a dedicated training framework:\nstage-wise diffusion. By solving the partitioned probability flow ordinary\ndifferential equations (ODE) of diffusion under aligned data and noise, our\ntraining strategy is applicable to various diffusion forms and further enhances\ntraining efficiency. Comprehensive experimental evaluations validate the\ngenerality of our method, demonstrating 50% reduction in training cost and 1.5x\nimprovement in inference efficiency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09566.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09151",
      "authors": [
        {
          "_id": "67d2784fbe3b4e06086d8eec",
          "user": {
            "_id": "656ee8008bb9f4f8d95bd8f7",
            "avatarUrl": "/avatars/4069d70f1279d928da521211c495d638.svg",
            "isPro": true,
            "fullname": "Hyeonho Jeong",
            "user": "hyeonho-jeong-video",
            "type": "user"
          },
          "name": "Hyeonho Jeong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-13T08:23:54.861Z",
          "hidden": false
        },
        {
          "_id": "67d2784fbe3b4e06086d8eed",
          "name": "Suhyeon Lee",
          "hidden": false
        },
        {
          "_id": "67d2784fbe3b4e06086d8eee",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T08:26:15.000Z",
      "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
      "summary": "Reangle-A-Videoは、単一の入力ビデオから同期された多角度ビデオを生成するための統一的なフレームワークです。主流のアプローチと異なり、大規模な4Dデータセットで多角度ビデオ拡散モデルを訓練するようなアプローチを採用していません。私たちの方法は、公開的に利用可能な画像とビデオ拡散プロジョリを活用し、ビデオからビデオへの翻訳タスクに再構成されています。本質的に、Reangle-A-Videoは2ステップで動作します。1) 多角度動作学習：画像からビデオへの拡散変換器は、自動転教的な方法で同期されて微調節され、揺ぎ出したビデオから視点不変な動作を抽出します。2) 多角度一致性画像から画像への翻訳：入力ビデオの最初のフレームは、DUSt3Rを用いた同期時のクロスビュー一致性ガイドラインの下で揺ぎ出しとインパイントされ、多角度一致性の始まりの画像を生成します。静的な視点運搬と動的なカメラ制御における拡張的な実験は、Reangle-A-Videoが既存の方法を超え、多角度ビデオ生成の新しい解決策を提供しています。私たちのコードとデータを公開します。プロジェクトページ：https://hyeonho99.github.io/reangle-a-video/",
      "upvotes": 21,
      "discussionId": "67d27857be3b4e06086d9160",
      "projectPage": "https://hyeonho99.github.io/reangle-a-video/",
      "githubRepo": "https://github.com/HyeonHo99/Reangle-Video",
      "ai_keywords": [
        "image-to-video diffusion transformer",
        "self-supervised manner",
        "view-invariant motion",
        "warped videos",
        "Multi-View Consistent Image-to-Images Translation",
        "cross-view consistency",
        "DUSt3R",
        "multi-view video generation",
        "static view transport",
        "dynamic camera control"
      ]
    },
    "publishedAt": "2025-03-12T04:26:15.000Z",
    "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
    "summary": "We introduce Reangle-A-Video, a unified framework for generating synchronized\nmulti-view videos from a single input video. Unlike mainstream approaches that\ntrain multi-view video diffusion models on large-scale 4D datasets, our method\nreframes the multi-view video generation task as video-to-videos translation,\nleveraging publicly available image and video diffusion priors. In essence,\nReangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An\nimage-to-video diffusion transformer is synchronously fine-tuned in a\nself-supervised manner to distill view-invariant motion from a set of warped\nvideos. (2) Multi-View Consistent Image-to-Images Translation: The first frame\nof the input video is warped and inpainted into various camera perspectives\nunder an inference-time cross-view consistency guidance using DUSt3R,\ngenerating multi-view consistent starting images. Extensive experiments on\nstatic view transport and dynamic camera control show that Reangle-A-Video\nsurpasses existing methods, establishing a new solution for multi-view video\ngeneration. We will publicly release our code and data. Project page:\nhttps://hyeonho99.github.io/reangle-a-video/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09151.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09573",
      "authors": [
        {
          "_id": "67d2511e7d0fc37e67269f85",
          "name": "Marianne Arriola",
          "hidden": false
        },
        {
          "_id": "67d2511e7d0fc37e67269f86",
          "name": "Aaron Gokaslan",
          "hidden": false
        },
        {
          "_id": "67d2511e7d0fc37e67269f87",
          "name": "Justin T Chiu",
          "hidden": false
        },
        {
          "_id": "67d2511e7d0fc37e67269f88",
          "name": "Zhihan Yang",
          "hidden": false
        },
        {
          "_id": "67d2511e7d0fc37e67269f89",
          "name": "Zhixuan Qi",
          "hidden": false
        },
        {
          "_id": "67d2511e7d0fc37e67269f8a",
          "name": "Jiaqi Han",
          "hidden": false
        },
        {
          "_id": "67d2511e7d0fc37e67269f8b",
          "name": "Subham Sekhar Sahoo",
          "hidden": false
        },
        {
          "_id": "67d2511e7d0fc37e67269f8c",
          "name": "Volodymyr Kuleshov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T17:43:40.000Z",
      "title": "ブロックディフュージョン：自動回帰とディフュージョンの間でのインタープレート\n  言語モデル",
      "summary": "ディフュージョン言語モデルは、自動回帰モデルに対して異なる特徴を持っていることで、並列化生成の可能性と制御性により特徴があるが、尤度モデリングに遅れ、固定長の生成に限定されている。本研究では、離散デノイズディフュージョンと自動回帰モデルの間を補間するブロックディフュージョン言語モデルのクラスを介して、両方のアプローチの主要な制限を克服し、柔軟な長さの生成を支え、KVキャッチと並行トークンサンプリングを用いて推論効率を向上させる。効果的なブロックディフュージョンモデルの構築に向けたアルゴリズム、勾配分散の推定器、データ駆動ノイズスケジュールを含むアルゴリズムを提案し、ボリューム分散を最小化する。ブロックディフュージョンは言語モデリングベンチマーク上で最先端の性能を設定し、任意長のシーケンスの生成を可能にします。コード、モデル重みとプロジェクトページのブログ記事を提供します：https://m-arriola.com/bd3lms/",
      "upvotes": 15,
      "discussionId": "67d2511e7d0fc37e67269fbf",
      "projectPage": "https://m-arriola.com/bd3lms/",
      "ai_keywords": [
        "diffusion language models",
        "autoregressive models",
        "parallelized generation",
        "controllability",
        "likelihood modeling",
        "fixed-length generation",
        "block diffusion language models",
        "discrete denoising diffusion",
        "flexible-length generation",
        "inference efficiency",
        "KV caching",
        "parallel token sampling",
        "efficient training algorithm",
        "gradient variance estimators",
        "data-driven noise schedules",
        "arbitrary-length sequences"
      ]
    },
    "publishedAt": "2025-03-12T13:43:40.000Z",
    "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
    "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09573.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08525",
      "authors": [
        {
          "_id": "67d280f20a6a6dd4a0ffe9e8",
          "name": "Tong Wei",
          "hidden": false
        },
        {
          "_id": "67d280f20a6a6dd4a0ffe9e9",
          "name": "Yijun Yang",
          "hidden": false
        },
        {
          "_id": "67d280f20a6a6dd4a0ffe9ea",
          "name": "Junliang Xing",
          "hidden": false
        },
        {
          "_id": "67d280f20a6a6dd4a0ffe9eb",
          "name": "Yuanchun Shi",
          "hidden": false
        },
        {
          "_id": "67d280f20a6a6dd4a0ffe9ec",
          "name": "Zongqing Lu",
          "hidden": false
        },
        {
          "_id": "67d280f20a6a6dd4a0ffe9ed",
          "name": "Deheng Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T15:17:02.000Z",
      "title": "GTR: ガイドド・スモートフォース強化防止思いの崩壊を実現するRLベースのVLMアグエントトレーニング",
      "summary": "RLVR（確認可能の報酬を持つ強化学習）は、大規模な言語モデル（LLMs）での連鎖的思考（CoT）のスケーリングに効果的に対応しています。しかし、その効果性は、視覚環境での目標導向的行動論理に向けた視覚言語モデル（VLM）アガントの訓練においては、少しも確立されていません。本研究は、複雑なカードゲーム（例えば24点）やALFWorldからの具象化タスクにおいて、広範囲な実験を通じてこの問題を調査しています。私たちは、報酬が行動の結果だけに基づく場合、RLはVLMでのCoT論理を奨励することができません、その代わりに、我々が「思い込み崩壊」と命名した現象を見出しました。この現象は、アガントの思い込みの多様性の急速な損失、状態無関係的な不完全な論理、そして続いて無効な行動、そして負の報酬を引き起こすものです。「思い込み崩壊」を対策するために、我々は手順ごとにアガントの論理を評価し、改良するための自動化されたコラーを主張し、このシンプルでスケーラブルなGTR（Guided Thought Reinforcement）フレームワークを提案します。このフレームワークは、密な、ステップ毎の人間のラベルによる訓練が必要なく、理由論理と行動を同時に学習させます。私たちの実験は、GTRはLLaVA-7bモデルの性能と一般化能力を大幅に向上させ、SoTAモデルと比較して3～5倍のタスク成功率を達成することを示しました。",
      "upvotes": 8,
      "discussionId": "67d280f30a6a6dd4a0ffea45",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable outcome rewards",
        "chain-of-thought (CoT) reasoning",
        "large language models (LLMs)",
        "vision-language model (VLM)",
        "goal-directed action reasoning",
        "visual environments",
        "complex card games",
        "24 points",
        "embodied tasks",
        "ALFWorld",
        "action outcomes",
        "thought collapse",
        "diversity",
        "state-irrelevant",
        "incomplete reasoning",
        "invalid actions",
        "negative rewards",
        "process guidance",
        "automated corrector",
        "GTR (Guided Thought Reinforcement)",
        "simultaneous training",
        "human labeling",
        "performance",
        "generalization",
        "task success rates",
        "state-of-the-art (SoTA) models",
        "model sizes"
      ]
    },
    "publishedAt": "2025-03-11T11:17:02.000Z",
    "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training",
    "summary": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08525.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04388",
      "authors": [
        {
          "_id": "67d05aa2348bae81a8ae572e",
          "name": "Shahar Levy",
          "hidden": false
        },
        {
          "_id": "67d05aa2348bae81a8ae572f",
          "name": "Nir Mazor",
          "hidden": false
        },
        {
          "_id": "67d05aa2348bae81a8ae5730",
          "user": {
            "_id": "63b433ee7af2e415f25b1a7b",
            "avatarUrl": "/avatars/0b03f66d263bffd22ed864d1241fe28b.svg",
            "isPro": false,
            "fullname": "Lihi Shalmon",
            "user": "LihiShalmon",
            "type": "user"
          },
          "name": "Lihi Shalmon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T16:09:23.471Z",
          "hidden": false
        },
        {
          "_id": "67d05aa2348bae81a8ae5731",
          "name": "Michael Hassid",
          "hidden": false
        },
        {
          "_id": "67d05aa2348bae81a8ae5732",
          "name": "Gabriel Stanovsky",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T12:38:17.000Z",
      "title": "もっと記録、同じ長さ：複数の記録を分離してRAGでの課題を特定する",
      "summary": "レタイブアップデータ生成（RAG）は、LLMsに関連するドキュメントを提供します。過去の研究では、複数のドキュメントの検索が性能を低下させることを指摘しましたが、それらはドキュメントの数量がコンテキストの長さを制御しながら性能にどのように影響を与えるかを特定していませんでした。私たちは、多段階QAタスクから得られたカスタムデータセット上で様々な言語モデルを評価します。コンテキストの長さと関連情報の位置を固定しながら、ドキュメントの数量を変化させ、RAGセットでドキュメントの数量を増やすことはLLMsに大きな課題となることを見出しました。また、私たちの結果からは、複数のドキュメントの処理は長いコンテキストの処理と異なる独立な課題となることがわかります。また、データセットとコードを公開しています：https://github.com/shaharl6000/MoreDocsSameLen",
      "upvotes": 8,
      "discussionId": "67d05aa3348bae81a8ae5780",
      "githubRepo": "https://github.com/shaharl6000/MoreDocsSameLen",
      "ai_keywords": [
        "Retrieval-augmented generation (RAG)",
        "LLMs",
        "relevant documents",
        "multi-hop QA task",
        "document count",
        "long contexts"
      ]
    },
    "publishedAt": "2025-03-06T07:38:17.000Z",
    "title": "More Documents, Same Length: Isolating the Challenge of Multiple\n  Documents in RAG",
    "summary": "Retrieval-augmented generation (RAG) provides LLMs with relevant documents.\nAlthough previous studies noted that retrieving many documents can degrade\nperformance, they did not isolate how the quantity of documents affects\nperformance while controlling for context length. We evaluate various language\nmodels on custom datasets derived from a multi-hop QA task. We keep the context\nlength and position of relevant information constant while varying the number\nof documents, and find that increasing the document count in RAG settings poses\nsignificant challenges for LLMs. Additionally, our results indicate that\nprocessing multiple documents is a separate challenge from handling long\ncontexts. We also make the datasets and code available:\nhttps://github.com/shaharl6000/MoreDocsSameLen .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04388.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09601",
      "authors": [
        {
          "_id": "67d29b617d0fc37e673c7e65",
          "name": "Itay Chachy",
          "hidden": false
        },
        {
          "_id": "67d29b617d0fc37e673c7e66",
          "name": "Guy Yariv",
          "hidden": false
        },
        {
          "_id": "67d29b617d0fc37e673c7e67",
          "name": "Sagie Benaim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T17:59:47.000Z",
      "title": "RewardSDS: 報酬重み付きサンプリングによるスコアの統合",
      "summary": "スコアディスタイルサンプリング（SDS）は、2D ディフュージョン先頭を活用することで、テキストから3D生成などのタスクに効果的な手法として登場しました。しかし、強力であることに反して、SDSはユーザインテントに対する細かいアラインメントを達成することに苦戦します。これを克服するために、私たちは、報酬モデルからのアラインメントスコアに基づいたノイズサンプルの重み付けを行う新しいアプローチ、RewardSDSを紹介します。この損失関数は、高報酬の出力を得るノイズサンプルからの勾配を優先します。我々のアプローチは広範囲に適用可能で、SDSベースの方法を拡張することができます。特に、Variational Score Distillation（VSD）に対して、RewardVSDを紹介し、テキストから画像生成、2D編集、テキストから3D生成のタスクに対して、RewardSDSとRewardVSDを評価し、生成質と報酬モデルに対するアラインメントを測定する多様なメトリックにおいて、SDSとVSDより显著な向上を示し、最先端の性能を実現することができます。プロジェクトページは、https://itaychachy.github.io/reward-sds/ にあります。",
      "upvotes": 7,
      "discussionId": "67d29b637d0fc37e673c7efc",
      "projectPage": "https://itaychachy.github.io/reward-sds/",
      "githubRepo": "https://github.com/itaychachy/RewardSDS",
      "ai_keywords": [
        "score distillation sampling (SDS)",
        "2D diffusion priors",
        "text-to-3D generation",
        "reward model",
        "weighted SDS loss",
        "gradients",
        "high-reward output",
        "variational score distillation (VSD)",
        "RewardVSD",
        "text-to-image",
        "2D editing",
        "generation quality",
        "alignment to desired reward models"
      ]
    },
    "publishedAt": "2025-03-12T13:59:47.000Z",
    "title": "RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling",
    "summary": "Score Distillation Sampling (SDS) has emerged as an effective technique for\nleveraging 2D diffusion priors for tasks such as text-to-3D generation. While\npowerful, SDS struggles with achieving fine-grained alignment to user intent.\nTo overcome this, we introduce RewardSDS, a novel approach that weights noise\nsamples based on alignment scores from a reward model, producing a weighted SDS\nloss. This loss prioritizes gradients from noise samples that yield aligned\nhigh-reward output. Our approach is broadly applicable and can extend SDS-based\nmethods. In particular, we demonstrate its applicability to Variational Score\nDistillation (VSD) by introducing RewardVSD. We evaluate RewardSDS and\nRewardVSD on text-to-image, 2D editing, and text-to-3D generation tasks,\nshowing significant improvements over SDS and VSD on a diverse set of metrics\nmeasuring generation quality and alignment to desired reward models, enabling\nstate-of-the-art performance. Project page is available at https://itaychachy.\ngithub.io/reward-sds/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09601.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06955",
      "authors": [
        {
          "_id": "67d2748117d5cbff7c23621e",
          "user": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "isPro": false,
            "fullname": "Zeyu Zhang",
            "user": "SteveZeyuZhang",
            "type": "user"
          },
          "name": "Zeyu Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-13T08:23:56.553Z",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c23621f",
          "name": "Yiran Wang",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236220",
          "name": "Wei Mao",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236221",
          "name": "Danning Li",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236222",
          "name": "Rui Zhao",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236223",
          "name": "Biao Wu",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236224",
          "name": "Zirui Song",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236225",
          "name": "Bohan Zhuang",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236226",
          "name": "Ian Reid",
          "hidden": false
        },
        {
          "_id": "67d2748117d5cbff7c236227",
          "name": "Richard Hartley",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T06:04:31.000Z",
      "title": "モーションぜんぶ：そのものをモーション生成",
      "summary": "条件運動生成は、コンピュータビジョンで様々に研究されており、2つの重要な課題が残っています。1つ目は、最近、マスクされた自動増加器手法がディフュージョンベースの手法を上回ったが、現在のマスクモデルは、与えられた条件に基づいて動的なフレームや身体部を優先順位する機構を持たないことです。2つ目は、異なる条件付けモデルの場合、現在の手法は複数のモデルを有効に統合することができないことで、生成される運動の制御と一貫性を制限しています。これらの課題に対処するために、私たちは、Attentionベースのマスクモデリングアプローチを導入し、鍵フレームとアクションに対する空間的および時間的な詳細な制御を可能にするMotion Anything、多モデル運動生成フレームワークを提案します。私たちのモデルは、テキストおよび音楽などの多モデル条件を適応的にエンコードし、制御可能さを向上させます。また、私たちは、2,153ペアのテキスト、音楽、ダンスを含む新しい運動データセットText-Music-Dance（TMD）を導入しました。このデータセットはAIST++の2倍のサイズになり、コミュニティにおける重要な欠点を埋めます。拡張された実験は、Motion Anythingが複数のベンチマークで最先端の手法を超え、HumanML3DでのFIDに15%の改善を達成し、AIST++とTMDでの一致した性能向上を示しました。プロジェクトウェブサイトを参照してください：https://steve-zeyu-zhang.github.io/MotionAnything",
      "upvotes": 5,
      "discussionId": "67d2748317d5cbff7c2362f2",
      "projectPage": "https://steve-zeyu-zhang.github.io/MotionAnything",
      "githubRepo": "https://github.com/steve-zeyu-zhang/MotionAnything",
      "ai_keywords": [
        "masked autoregressive methods",
        "diffusion-based approaches",
        "masking models",
        "dynamic frames",
        "body parts",
        "conditional motion generation",
        "multimodal motion generation framework",
        "Attention-based Mask Modeling",
        "key frames",
        "actions",
        "multimodal conditions",
        "Text-Music-Dance (TMD)",
        "FID",
        "HumanML3D",
        "AIST++"
      ]
    },
    "publishedAt": "2025-03-10T02:04:31.000Z",
    "title": "Motion Anything: Any to Motion Generation",
    "summary": "Conditional motion generation has been extensively studied in computer\nvision, yet two critical challenges remain. First, while masked autoregressive\nmethods have recently outperformed diffusion-based approaches, existing masking\nmodels lack a mechanism to prioritize dynamic frames and body parts based on\ngiven conditions. Second, existing methods for different conditioning\nmodalities often fail to integrate multiple modalities effectively, limiting\ncontrol and coherence in generated motion. To address these challenges, we\npropose Motion Anything, a multimodal motion generation framework that\nintroduces an Attention-based Mask Modeling approach, enabling fine-grained\nspatial and temporal control over key frames and actions. Our model adaptively\nencodes multimodal conditions, including text and music, improving\ncontrollability. Additionally, we introduce Text-Music-Dance (TMD), a new\nmotion dataset consisting of 2,153 pairs of text, music, and dance, making it\ntwice the size of AIST++, thereby filling a critical gap in the community.\nExtensive experiments demonstrate that Motion Anything surpasses\nstate-of-the-art methods across multiple benchmarks, achieving a 15%\nimprovement in FID on HumanML3D and showing consistent performance gains on\nAIST++ and TMD. See our project website\nhttps://steve-zeyu-zhang.github.io/MotionAnything",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06955.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.07103",
      "authors": [
        {
          "_id": "67d27d4c6fbfb8ee21886b60",
          "user": {
            "_id": "663486a1f64712540644cb68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663486a1f64712540644cb68/YZFR41ERY6UrC6rCC6Nan.jpeg",
            "isPro": true,
            "fullname": "Alessandro",
            "user": "Devy1",
            "type": "user"
          },
          "name": "Alessandro Giagnorio",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-13T08:23:50.840Z",
          "hidden": false
        },
        {
          "_id": "67d27d4c6fbfb8ee21886b61",
          "name": "Antonio Mastropaolo",
          "hidden": false
        },
        {
          "_id": "67d27d4c6fbfb8ee21886b62",
          "name": "Saima Afrin",
          "hidden": false
        },
        {
          "_id": "67d27d4c6fbfb8ee21886b63",
          "user": {
            "_id": "63f378004745321de351a554",
            "avatarUrl": "/avatars/3be79f0f6eb0bcfdc9f31b46d2bafc14.svg",
            "isPro": false,
            "fullname": "Max Di Penta",
            "user": "mdiipenta",
            "type": "user"
          },
          "name": "Massimiliano Di Penta",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-13T06:38:05.744Z",
          "hidden": false
        },
        {
          "_id": "67d27d4c6fbfb8ee21886b64",
          "name": "Gabriele Bavota",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T09:26:08.000Z",
      "title": "ラージーラングジャングルモデルのコード生成に適用されるデジタル化：別途の再現",
      "summary": "大語言モデル（LLMs）在コード生成には驚異的な能力を示し、特に、自然言語で記述された要求を自動的に実装することができる。LLMの効果性は一般的にモデルのサイズが大きくなるほど上昇する：LLMの学習可能なパラメータ数が高いほど、コードの実装能力が上がる。しかし、LLMベースのコードジェネレーターを採用する場合、そのメモリ（そして、そのもとにコーン）のフットプリントに関する大きな課題がある。前の研究では、Wei et al.は、メモリフットプリントを減少させるために、LLMベースのコードジェネレーターを利用することを提案し、その効果性の大幅な低下を避けることを目指していた。簡単に言えば、16BパラメータのLLMを挙げ、浮動小数点32ビットからint8ビットまでの精度を減らし、その効果が限られていることを示した。LLMの能力とその減量化技術が急速に進化する中で、本稿では、Wei et al.の研究を異なる再現し、最近のコード関連のLLM（34Bパラメータ）、最新のモデル減量化技術（モデルパラメータの極端な減量化レベル2ビット）、データセットを用いた減量化プロセスの指導について調査した。実験的評価により、LLMの減量化の新たな境界は4ビット精度であり、元のモデルと比較して平均70%のメモリフットプリント減少が見られ、性能の大幅な低下は見られなかった。また、減量化がもっと極端になる（3ビットと2ビット）場合、コード関連のデータセットは性能の損失を制限することができる。",
      "upvotes": 4,
      "discussionId": "67d27d4d6fbfb8ee21886bab",
      "githubRepo": "https://github.com/Devy99/lowbit-quantization",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "code generation",
        "trainable parameters",
        "memory footprint",
        "quantization techniques",
        "precision",
        "floating point 32 bits",
        "int 8 bits",
        "code generation performance",
        "calibration datasets",
        "code-specific calibration datasets"
      ]
    },
    "publishedAt": "2025-03-10T05:26:08.000Z",
    "title": "Quantizing Large Language Models for Code Generation: A Differentiated\n  Replication",
    "summary": "Large Language Models (LLMs) have shown an impressive capability in code\ngeneration and, specifically, to automatically implement requirements described\nin natural language. The LLM effectiveness generally increases with its size:\nThe higher the number of LLM's trainable parameters the better its ability to\nimplement code. However, when it comes to deploying LLM-based code generators,\nlarger LLMs pose significant challenges related to their memory (and,\nconsequently, carbon) footprint. A previous work by Wei et al. proposed to\nleverage quantization techniques to reduce the memory footprint of LLM-based\ncode generators without substantially degrading their effectiveness. In short,\nthey studied LLMs featuring up to 16B parameters, quantizing their precision\nfrom floating point 32 bits down to int 8 bits and showing their limited impact\non code generation performance. Given the fast pace at which LLM capabilities\nand quantization techniques are evolving, in this work we present a\ndifferentiated replication of the work by Wei et al. in which we consider (i)\non the one side, more recent and larger code-related LLMs, of up to 34B\nparameters; (ii) the latest advancements in model quantization techniques,\nwhich allow pushing the compression to the extreme quantization level of 2 bits\nper model parameter and; (iii) different types of calibration datasets to guide\nthe quantization process, including code-specific ones. Our empirical\nevaluation reveals that the new frontier for LLM quantization is 4-bit\nprecision, resulting in an average memory footprint reduction of 70% compared\nto the original model without observing any significant decrease in\nperformance. Additionally, when the quantization becomes even more extreme (3\nand 2 bits), a code-specific calibration dataset helps to limit the loss of\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07103.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09402",
      "authors": [
        {
          "_id": "67d27a9117d5cbff7c2519f6",
          "user": {
            "_id": "64440be5af034cdfd69ca3a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
            "isPro": true,
            "fullname": "Qinghong (Kevin) Lin",
            "user": "KevinQHLin",
            "type": "user"
          },
          "name": "Kevin Qinghong Lin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-13T08:23:53.060Z",
          "hidden": false
        },
        {
          "_id": "67d27a9117d5cbff7c2519f7",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T13:53:30.000Z",
      "title": "VLog: ビデオ-言語モデルへの生成的な説明の検索\n  語彙",
      "summary": "人間の日常活動を簡潔に説明するには、映像シード内の例えばアラームをオフにするようなルーチュール的なイベントの順番で表現でき、イベントのバーケュリーを形成する。これに基づき、私たちは新しい映像理解フレームワークVLogを介し、既存の生成的な映像言語モデルの典型的なサブウォードバーケュリーを超えるような映像の説明をバーケュリーとして定義することを導入します。VLogは、軽量ラジオンモデルGPT-2を基礎に構築されており、3つのキーの革新的な機能を持ちます。 (i) 生成的な検索モデル、言語モデルの複雑な論理能力と比較的検索の効率的な類似検索を結びつける。 (ii) ラージオンモデルの大規模な映像の説明から得られるヒューリスティックなバーケュリー、我々の説明ペアエンコーディングアルゴリズムを用いて、特定のイベント（例えばトマトを切る）の効率的なインデックス化を可能にします。 (iii) 新しいイベントを検出した際にバーケュリーを拡張するためのバーケュリーアップデート戦略、生成的モデルを活用します。私たちのアプローチを検証するために、VidCap-Evalを紹介します。VidCap-Evalは、理由関係を持つ簡潔な説明を必要とする開発セットです。EgoSchema、COIN、HiRESTの実験は、VLogの効果性を進一に示し、簡潔でコンテキスト的に正確な、効率的な説明を生成する能力を特徴的にし、映像理解の新しい視点を提供します。コードは、https://github.com/showlab/VLogでリリースされています。",
      "upvotes": 3,
      "discussionId": "67d27a9517d5cbff7c251b41",
      "githubRepo": "https://github.com/showlab/VLog",
      "ai_keywords": [
        "generative retrieval model",
        "contrastive retrieval",
        "hierarchical vocabulary",
        "narration pair encoding algorithm",
        "expressive postfixes",
        "vocabulary update strategy",
        "generative models",
        "VidCap-Eval",
        "EgoSchema",
        "COIN",
        "HiREST",
        "concise narrations",
        "reasoning relationships"
      ]
    },
    "publishedAt": "2025-03-12T09:53:30.000Z",
    "title": "VLog: Video-Language Models by Generative Retrieval of Narration\n  Vocabulary",
    "summary": "Human daily activities can be concisely narrated as sequences of routine\nevents (e.g., turning off an alarm) in video streams, forming an event\nvocabulary. Motivated by this, we introduce VLog, a novel video understanding\nframework that define video narrations as vocabulary, going beyond the typical\nsubword vocabularies in existing generative video-language models. Built on the\nlightweight language model GPT-2, VLog feature three key innovations: (i) A\ngenerative retrieval model, marrying language model's complex reasoning\ncapabilities with contrastive retrieval's efficient similarity search. (ii) A\nhierarchical vocabulary derived from large-scale video narrations using our\nnarration pair encoding algorithm, enabling efficient indexing of specific\nevents (e.g., cutting a tomato) by identifying broader scenarios (e.g.,\nkitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary\nupdate strategy leveraging generative models to extend the vocabulary for novel\nevents encountered during inference. To validate our approach, we introduce\nVidCap-Eval, a development set requiring concise narrations with reasoning\nrelationships (e.g., before and after). Experiments on EgoSchema, COIN, and\nHiREST further demonstrate the effectiveness of VLog, highlighting its ability\nto generate concise, contextually accurate, and efficient narrations, offering\na novel perspective on video understanding. Codes are released at\nhttps://github.com/showlab/VLog.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09402.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.06573",
      "authors": [
        {
          "_id": "67d02a147d82f613a31ed396",
          "name": "Gili Lior",
          "hidden": false
        },
        {
          "_id": "67d02a147d82f613a31ed397",
          "name": "Asaf Yehudai",
          "hidden": false
        },
        {
          "_id": "67d02a147d82f613a31ed398",
          "name": "Ariel Gera",
          "hidden": false
        },
        {
          "_id": "67d02a147d82f613a31ed399",
          "name": "Liat Ein-Dor",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T12:06:29.000Z",
      "title": "WildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval: 野生ワイルドでの指示従い\n\nWildIFEval:",
      "summary": "最近のLLMsは、ユーザーの指示を従うことに関して驚異的な成功を示していますが、複数の制約を含む指示を処理することは大きな課題です。本稿では、12,000件の実際のユーザー指示を含む大規模なデータセット「WildIFEval」を紹介します。先行のデータセットと異なり、本データセットは自然なユーザーのプロンプトで広く構文とトピックの範囲を収録しています。これらの制約を8つの高レベルのクラスに分類し、実世界的なスケーニングでの分布と動態を捉えます。「WildIFEval」を活用して、先鋭のLLMsの指示従い能力を評価するために拡張的な実験を実施しました。評価されたすべてのモデルは、制約の数が増加することに伴い性能が低下することがわかりました。これらの結果から、このようなタスクですべてのモデルが大幅に改善の余地があることを示しました。また、制約の特定の種類がモデルの性能に重大な影響を与えることを見出しました。このデータセットを公開し、複雑な実用的な条件の下での指示従いの研究につながることを促進することを目的としています。",
      "upvotes": 3,
      "discussionId": "67d02a167d82f613a31ed44b"
    },
    "publishedAt": "2025-03-09T08:06:29.000Z",
    "title": "WildIFEval: Instruction Following in the Wild",
    "summary": "Recent LLMs have shown remarkable success in following user instructions, yet\nhandling instructions with multiple constraints remains a significant\nchallenge. In this work, we introduce WildIFEval - a large-scale dataset of 12K\nreal user instructions with diverse, multi-constraint conditions. Unlike prior\ndatasets, our collection spans a broad lexical and topical spectrum of\nconstraints, in natural user prompts. We categorize these constraints into\neight high-level classes to capture their distribution and dynamics in\nreal-world scenarios. Leveraging WildIFEval, we conduct extensive experiments\nto benchmark the instruction-following capabilities of leading LLMs. Our\nfindings reveal that all evaluated models experience performance degradation\nwith an increasing number of constraints. Thus, we show that all models have a\nlarge room for improvement on such tasks. Moreover, we observe that the\nspecific type of constraint plays a critical role in model performance. We\nrelease our dataset to promote further research on instruction-following under\ncomplex, realistic conditions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06573.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.09579",
      "authors": [
        {
          "_id": "67d26ae40a955727b9687d8f",
          "user": {
            "_id": "6144e4667f2544bb450787b2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6144e4667f2544bb450787b2/5wSDDqJbI4TGtBLP9IvjY.png",
            "isPro": false,
            "fullname": "Yingfa Chen",
            "user": "chen-yingfa",
            "type": "user"
          },
          "name": "Yingfa Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-13T08:24:03.831Z",
          "hidden": false
        },
        {
          "_id": "67d26ae40a955727b9687d90",
          "name": "Yutong Wu",
          "hidden": false
        },
        {
          "_id": "67d26ae40a955727b9687d91",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "67d26ae40a955727b9687d92",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67d26ae40a955727b9687d93",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T17:50:42.000Z",
      "title": "コスト最適化グループ化クエリアテンション（Cost-Optimal Grouped-Query Attention）を長コンテキストLLMsに対して使用する",
      "summary": "Transformerベースの効果的で効率的な大規模言語モデル（LLMs）の構築が最近の研究焦点となり、モデルの言語能力を最大限に高め、学習と部署コストを最小化する必要があります。既存の試みは主にモデルの性能、パラメータサイズ、データサイズの複雑な関係を説明し、LLMsの最適な計算配分を探しています。しかし、これらはコンテキスト長とアテンションヘッドの設計（グループされたクエリアタックのクエリとキー-バリューヘッドの数）が学習と推論に及ぼす影響を考慮していません。この論文では、モデルの性能、計算コスト、メモリコストについて、異なるパラメータサイズ、コンテキスト長、アテンションヘッドの設計を挙げたモデルをシステマティックに比較します。そして、現在のスケーリング方法（パラメータサイズと学習計算に基づくみたいなもの）を拡張し、トレーニングと推論の両方でコスト最適なLLMsの構築をガイドすることを目指します。我々の定量的なスケーリング研究により、十分に長いシーケンスを処理する場合、少ないアテンションヘッドを持つ大きなモデルは損失を低く抑え、計算コストとメモリコストを低く抑えることができることが明らかになりました。我々の発見は、実用的なLLMsの開発において特に長コンテキスト処理のスケーラーで有價しいエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエンドプロフェッショナルなコンテキストについてのエ",
      "upvotes": 2,
      "discussionId": "67d26ae90a955727b9687eff",
      "githubRepo": "https://github.com/thunlp/cost-optimal-gqa",
      "ai_keywords": [
        "Transformer-based",
        "large language models (LLMs)",
        "parameter size",
        "context length",
        "attention head configuration",
        "grouped-query attention",
        "query and key-value heads",
        "model performance",
        "computational cost",
        "memory cost",
        "cost-optimal LLMs"
      ]
    },
    "publishedAt": "2025-03-12T13:50:42.000Z",
    "title": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs",
    "summary": "Building effective and efficient Transformer-based large language models\n(LLMs) has recently become a research focus, requiring maximizing model\nlanguage capabilities and minimizing training and deployment costs. Existing\nefforts have primarily described complex relationships among model performance,\nparameter size, and data size, as well as searched for the optimal compute\nallocation to train LLMs. However, they overlook the impacts of context length\nand attention head configuration (the number of query and key-value heads in\ngrouped-query attention) on training and inference. In this paper, we\nsystematically compare models with different parameter sizes, context lengths,\nand attention head configurations in terms of model performance, computational\ncost, and memory cost. Then, we extend the existing scaling methods, which are\nbased solely on parameter size and training compute, to guide the construction\nof cost-optimal LLMs during both training and inference. Our quantitative\nscaling studies show that, when processing sufficiently long sequences, a\nlarger model with fewer attention heads can achieve a lower loss while\nincurring lower computational and memory costs. Our findings provide valuable\ninsights for developing practical LLMs, especially in long-context processing\nscenarios. We will publicly release our code and data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09579.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09419",
      "authors": [
        {
          "_id": "67d27032825fbe674d2e4109",
          "user": {
            "_id": "64a63f9449b08110f761cd73",
            "avatarUrl": "/avatars/61860202fc818b105ef24e74dd4f7d3c.svg",
            "isPro": false,
            "fullname": "Yifan Zhou",
            "user": "SingleZombie",
            "type": "user"
          },
          "name": "Yifan Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-13T08:24:00.485Z",
          "hidden": false
        },
        {
          "_id": "67d27032825fbe674d2e410a",
          "name": "Zeqi Xiao",
          "hidden": false
        },
        {
          "_id": "67d27032825fbe674d2e410b",
          "name": "Shuai Yang",
          "hidden": false
        },
        {
          "_id": "67d27032825fbe674d2e410c",
          "name": "Xingang Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T14:16:30.000Z",
      "title": "アイアラスフリーラテンディフューションモデル：ディフューションラテンスペースの分数シフト等対称性向上",
      "summary": "潜在ディフュージョンモデル（LDMs）は、生成プロセスが不穩定で、入力ノイズの微小な摂動や変化がグラフィックにより大きく異なる出力を生み出すことがあることが知られています。これは、結果の一致性を求める応用においての適用性を妨げています。本稿では、LDMsをシフト等対称性を強化して、結果の一致性を向上させることを目指し、シフト等対称性を持たせるようにリデザインします。アンチアライアンシング操作の導入は、シフト等対称性を一部改善することができますが、LDMsにおける特有の課題であるアライアンシングの拡大や、VAEの訓練および複数のU-Netの推論による不穩定な結果の発生により、顕著なアライアンシングと不穩定な結果が残っています。これらの問題を解決するために、アフションモジュールをシフト等対称性を持たせるようにリデザインし、頻度バンディットの特徴量の周波数帯域を効果的に抑える等対称性損失を提案します。このようにして得られたアライアンシングなしのLDM（AF-LDM）は、強いシフト等対称性を持ち、不規則な縮小や拡大に対しても強靭です。拡張された実験は、AF-LDMはビデオ編集や画像から画像への転訳などの様々な応用で、ベースモデルのLDMに比べてもっともより統一的な結果を生成します。コードは、https://github.com/SingleZombie/AFLDM から利用可能です。",
      "upvotes": 2,
      "discussionId": "67d27034825fbe674d2e4185",
      "projectPage": "https://zhouyifan.net/AF-LDM-Page/",
      "githubRepo": "https://github.com/SingleZombie/AFLDM",
      "ai_keywords": [
        "Latent Diffusion Models (LDMs)",
        "shift-equivariant",
        "anti-aliasing operations",
        "aliasing amplification",
        "VAE training",
        "U-Net",
        "self-attention modules",
        "attention modules",
        "equivariance loss",
        "alias-free LDM (AF-LDM)",
        "video editing",
        "image-to-image translation"
      ]
    },
    "publishedAt": "2025-03-12T10:16:30.000Z",
    "title": "Alias-Free Latent Diffusion Models:Improving Fractional Shift\n  Equivariance of Diffusion Latent Space",
    "summary": "Latent Diffusion Models (LDMs) are known to have an unstable generation\nprocess, where even small perturbations or shifts in the input noise can lead\nto significantly different outputs. This hinders their applicability in\napplications requiring consistent results. In this work, we redesign LDMs to\nenhance consistency by making them shift-equivariant. While introducing\nanti-aliasing operations can partially improve shift-equivariance, significant\naliasing and inconsistency persist due to the unique challenges in LDMs,\nincluding 1) aliasing amplification during VAE training and multiple U-Net\ninferences, and 2) self-attention modules that inherently lack\nshift-equivariance. To address these issues, we redesign the attention modules\nto be shift-equivariant and propose an equivariance loss that effectively\nsuppresses the frequency bandwidth of the features in the continuous domain.\nThe resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is\nalso robust to irregular warping. Extensive experiments demonstrate that AF-LDM\nproduces significantly more consistent results than vanilla LDM across various\napplications, including video editing and image-to-image translation. Code is\navailable at: https://github.com/SingleZombie/AFLDM",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09419.png",
    "numComments": 1,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09600",
      "authors": [
        {
          "_id": "67d270d88f3def6bcbb87b6b",
          "user": {
            "_id": "658e85bb5b7553ca5c29ba89",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658e85bb5b7553ca5c29ba89/KK6UpS9agtrxevvBoup5N.jpeg",
            "isPro": false,
            "fullname": "Jihao Zhao",
            "user": "Robot2050",
            "type": "user"
          },
          "name": "Jihao Zhao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-13T08:23:58.864Z",
          "hidden": false
        },
        {
          "_id": "67d270d88f3def6bcbb87b6c",
          "name": "Zhiyuan Ji",
          "hidden": false
        },
        {
          "_id": "67d270d88f3def6bcbb87b6d",
          "name": "Zhaoxin Fan",
          "hidden": false
        },
        {
          "_id": "67d270d88f3def6bcbb87b6e",
          "name": "Hanyu Wang",
          "hidden": false
        },
        {
          "_id": "67d270d88f3def6bcbb87b6f",
          "name": "Simin Niu",
          "hidden": false
        },
        {
          "_id": "67d270d88f3def6bcbb87b70",
          "name": "Bo Tang",
          "hidden": false
        },
        {
          "_id": "67d270d88f3def6bcbb87b71",
          "name": "Feiyu Xiong",
          "hidden": false
        },
        {
          "_id": "67d270d88f3def6bcbb87b72",
          "name": "Zhiyu Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T17:59:42.000Z",
      "title": "モック: 文字チャンクリーナーの混在を用いた検索アウゲージ生成システム",
      "summary": "レビュアル・アウゲーション（RAG）は、大規模言語モデル（LLMs）の実用的な補助として役立つが、そのパイプライン内でのテキストチャンキング（chunking）の重要な面を間違えています。本論文は、バランス明確性とチャンク粘着性を構成する双観評価方法を初めに紹介し、チャンクキャリングの品質を直接数値化できるようにします。この評価方法を活用して、我々は、単一的なチャンクキャリングとセマンティックなチャンクキャリングが複雑なコンテキストの微妙な難点を処理する限界性を明らかにし、RAGシステムのパーフェクションを達成するためにLLMsをチャンクキャリングプロセスに統合する必要性を証明します。LLMベースのアプローチでの計算効率とチャンク精度の固有のトレードオフを解決するために、我々は、粒度に関するMixture-of-Chunkers（MoC）フレームワークを提案します。これは3ステップの処理機構を構成し、特に、MoCフレームワークの目標は、チャンクキャリングの構造化されたリストを生成するようにチャンクキャリングプログラムにガイドし、それらのチャンクキャリング正規表現を元のテキストから抽出することです。拡大的な実験は、我々の提案メトリックとMoCフレームワークがチャンクキャリングタスクの課題を解決し、RAGシステムの性能を向上させることを示します。",
      "upvotes": 1,
      "discussionId": "67d270d98f3def6bcbb87bfb",
      "ai_keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "Large language models (LLMs)",
        "text chunking",
        "dual-metric evaluation method",
        "Boundary Clarity",
        "Chunk Stickiness",
        "chunking quality",
        "traditional chunking",
        "semantic chunking",
        "contextual nuances",
        "granularity-aware Mixture-of-Chunkers (MoC)",
        "three-stage processing mechanism",
        "chunking regular expressions",
        "chunk extraction",
        "chunking task",
        "chunking kernel"
      ]
    },
    "publishedAt": "2025-03-12T13:59:42.000Z",
    "title": "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System",
    "summary": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09600.png",
    "numComments": 2,
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.09427",
      "authors": [
        {
          "_id": "67d247c2a48964532e40e78e",
          "name": "Yaorui Shi",
          "hidden": false
        },
        {
          "_id": "67d247c2a48964532e40e78f",
          "name": "Jiaqi Yang",
          "hidden": false
        },
        {
          "_id": "67d247c2a48964532e40e790",
          "name": "Sihang Li",
          "hidden": false
        },
        {
          "_id": "67d247c2a48964532e40e791",
          "name": "Junfeng Fang",
          "hidden": false
        },
        {
          "_id": "67d247c2a48964532e40e792",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "67d247c2a48964532e40e793",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67d247c2a48964532e40e794",
          "name": "Yang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-12T14:26:16.000Z",
      "title": "多モダル言語モデリングによる高精度シングルセルトランスクリプトムーデリング解析と生成",
      "summary": "予習された言語モデル（PLMs）は科学研究に革命的な影響を及ぼしていますが、単細胞解析への応用は限られています。文脈PLMsは単細胞RNA配列データを処理できませんが、細胞PLMsは自由文を処理する能力がないため、多タイプタスクでの使用が制限されています。現在のモーダル間のブライドラインエフェクトは、情報の損失または不十分な単一モーダル予習により、最適な性能を達成できないことがあります。これらの課題に対処するために、私たちは統一的なPLMであるSingle-Cell MultiModal Generative Pre-trained Transformer (scMMGPT)を提案します。scMMGPTは最先端の細胞および文脈PLMsを有効に統合し、クロスモーダル知識共有を促進して性能を向上させます。文脈-細胞モーダル間の隙間を埋めるために、scMMGPTは専用のクロスモーダルプロジョナーを利用し、現在まで最大のデータセットである2700万カラーの細胞による拡大予習を通じて、統一的なPLMとしての実力を発揮します。この大規模な予習により、scMMGPTは統一的な細胞-文脈タスクで優れた性能を達成し、細胞説明生成の文脈的違和感の相対的改善率は84%、細胞タイプ注釈の精度は20.5%上昇、文脈条件付きのファクトシール生成のk-NN精度は4%上昇し、ベースラインを超えます。",
      "upvotes": 1,
      "discussionId": "67d247c4a48964532e40e802",
      "ai_keywords": [
        "single-cell RNA sequencing data",
        "pre-trained language models (PLMs)",
        "cell PLMs",
        "cross-modal tasks",
        "information loss",
        "single-modal pre-training",
        "Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT)",
        "cross-modal projectors",
        "multimodal cell-text PLMs",
        "cell description generation",
        "cell type annotation",
        "$k$-NN accuracy",
        "text-conditioned pseudo-cell generation"
      ]
    },
    "publishedAt": "2025-03-12T10:26:16.000Z",
    "title": "Multimodal Language Modeling for High-Accuracy Single Cell\n  Transcriptomics Analysis and Generation",
    "summary": "Pre-trained language models (PLMs) have revolutionized scientific research,\nyet their application to single-cell analysis remains limited. Text PLMs cannot\nprocess single-cell RNA sequencing data, while cell PLMs lack the ability to\nhandle free text, restricting their use in multimodal tasks. Existing efforts\nto bridge these modalities often suffer from information loss or inadequate\nsingle-modal pre-training, leading to suboptimal performances. To address these\nchallenges, we propose Single-Cell MultiModal Generative Pre-trained\nTransformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT\neffectively integrates the state-of-the-art cell and text PLMs, facilitating\ncross-modal knowledge sharing for improved performance. To bridge the text-cell\nmodality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes\nextensive pre-training on 27 million cells -- the largest dataset for\nmultimodal cell-text PLMs to date. This large-scale pre-training enables\nscMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative\nimprovement of textual discrepancy for cell description generation, 20.5\\%\nhigher accuracy for cell type annotation, and 4\\% improvement in k-NN\naccuracy for text-conditioned pseudo-cell generation, outperforming baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.09427.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07588",
      "authors": [
        {
          "_id": "67d1808e13d7b3f8c6ea9147",
          "name": "Junwei Luo",
          "hidden": false
        },
        {
          "_id": "67d1808e13d7b3f8c6ea9148",
          "name": "Yingying Zhang",
          "hidden": false
        },
        {
          "_id": "67d1808e13d7b3f8c6ea9149",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "67d1808e13d7b3f8c6ea914a",
          "name": "Kang Wu",
          "hidden": false
        },
        {
          "_id": "67d1808e13d7b3f8c6ea914b",
          "name": "Qi Zhu",
          "hidden": false
        },
        {
          "_id": "67d1808e13d7b3f8c6ea914c",
          "name": "Lei Liang",
          "hidden": false
        },
        {
          "_id": "67d1808e13d7b3f8c6ea914d",
          "name": "Jingdong Chen",
          "hidden": false
        },
        {
          "_id": "67d1808e13d7b3f8c6ea914e",
          "name": "Yansheng Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:51:16.000Z",
      "title": "大視野ビジョン言語モデルと大規模な遠隔観測画像の邂逅：\n  コアスからフィニッShテキストガイドされたトークンプリンシング",
      "summary": "大規模のロバートセンシング画像（RSIs）の効率的な視覚言語理解は意味がありますが、難しいです。現在の大規模ビジョン言語モデル（LVLMs）は通常限定的なグリッドを使用して画像を処理し、ギガピクセルのRSIsを処理する際に情報の損失を招きます。逆に、無制限のグリッドの使用は計算コストを大幅に増加させます。画像の詳細を保存しながら計算複雑性を減らすために、グラフィックシーンピラミッド（DIP）を統合したテキストガイドドモンストラのトークン削減法を提案します。私たちの方法には、(i) 領域フォーカスモジュール（RFM）がテキストに関連する領域検出能力を利用して重要な視覚トークンを特定し、(ii) RFMの出力によってガイドされたDIPに基づく粗略から細かい画像ティリング選択と視覚トークン削減戦略が導入されます。また、LVLMsの観察能力を評価する既存のベンチマークは問題の多様性の限界と画像サイズの制限を伴います。新しいベンチマークを構築し、LRS-VQAとして、8カテゴリーにわたる7,333ペアのQAペアを含み、画像の長さが27,328ピクセルまで伸びます。私たちの方法は同じデータを使用して4つのデータセットで既存の高解像度戦略を上回り、また、既存のトークン削減方法に比べて高解像度設定でもより高い効率を示します。データセットとコードはhttps://github.com/VisionXLab/LRS-VQAにあります。",
      "upvotes": 1,
      "discussionId": "67d1809013d7b3f8c6ea9271",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "gigapixel RSIs",
        "Dynamic Image Pyramid (DIP)",
        "text-guided token pruning",
        "Region Focus Module (RFM)",
        "text-aware region localization",
        "vision tokens",
        "coarse-to-fine image tile selection",
        "vision token pruning strategy",
        "large imagery",
        "LRS-VQA",
        "QA pairs",
        "high-resolution strategies",
        "token reduction methods"
      ]
    },
    "publishedAt": "2025-03-10T13:51:16.000Z",
    "title": "When Large Vision-Language Model Meets Large Remote Sensing Imagery:\n  Coarse-to-Fine Text-Guided Token Pruning",
    "summary": "Efficient vision-language understanding of large Remote Sensing Images (RSIs)\nis meaningful but challenging. Current Large Vision-Language Models (LVLMs)\ntypically employ limited pre-defined grids to process images, leading to\ninformation loss when handling gigapixel RSIs. Conversely, using unlimited\ngrids significantly increases computational costs. To preserve image details\nwhile reducing computational complexity, we propose a text-guided token pruning\nmethod with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i)\na Region Focus Module (RFM) that leverages text-aware region localization\ncapability to identify critical vision tokens, and (ii) a coarse-to-fine image\ntile selection and vision token pruning strategy based on DIP, which is guided\nby RFM outputs and avoids directly processing the entire large imagery.\nAdditionally, existing benchmarks for evaluating LVLMs' perception ability on\nlarge RSI suffer from limited question diversity and constrained image sizes.\nWe construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs\nacross 8 categories, with image length up to 27,328 pixels. Our method\noutperforms existing high-resolution strategies on four datasets using the same\ndata. Moreover, compared to existing token reduction methods, our approach\ndemonstrates higher efficiency under high-resolution settings. Dataset and code\nare in https://github.com/VisionXLab/LRS-VQA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07588.png",
    "numComments": 1,
    "isAuthorParticipating": false
  }
]