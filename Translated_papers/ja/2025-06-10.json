[
  {
    "paper": {
      "id": "2506.08007",
      "authors": [
        {
          "_id": "684794553ec10bdd8ab4de1a",
          "name": "Qingxiu Dong",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1b",
          "user": {
            "_id": "5df85abada6d0311fd3d5408",
            "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
            "isPro": false,
            "fullname": "Li Dong",
            "user": "unilm",
            "type": "user"
          },
          "name": "Li Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:23.723Z",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1c",
          "user": {
            "_id": "667119d6578448466d9531a6",
            "avatarUrl": "/avatars/72c31909a5584b1306b6404b94a22b2a.svg",
            "isPro": false,
            "fullname": "Yao Tang",
            "user": "YaoTang23",
            "type": "user"
          },
          "name": "Yao Tang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:20.414Z",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1d",
          "name": "Tianzhu Ye",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1e",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de1f",
          "name": "Zhifang Sui",
          "hidden": false
        },
        {
          "_id": "684794553ec10bdd8ab4de20",
          "user": {
            "_id": "67ecd6178647cfa1775f75ed",
            "avatarUrl": "/avatars/98882cc58dc0a5de94df765d523d92c9.svg",
            "isPro": false,
            "fullname": "FW",
            "user": "frontierai",
            "type": "user"
          },
          "name": "Furu Wei",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T02:11:34.050Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/OeDc7c4QFJOxdkJWvdbWB.png"
      ],
      "publishedAt": "2025-06-09T17:59:53.000Z",
      "submittedOnDailyAt": "2025-06-10T00:43:01.816Z",
      "title": "Reinforcement Pre-Training",
      "submittedOnDailyBy": {
        "_id": "5df85abada6d0311fd3d5408",
        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
        "isPro": false,
        "fullname": "Li Dong",
        "user": "unilm",
        "type": "user"
      },
      "summary": "この研究では、大規模な言語モデルと強化学習（RL）に対して新しいスケーリングパラダイムとしてReinforcement Pre-Training（RPT）を紹介します。特に、次のトークン予測をRLを使用して学習した論理タスクとして再構成し、既定のコンテキストで正確に次のトークンを予測することによって確認可能な報酬を受け取るようにします。RPTは、一般的な場合に対して、ドメイン専門的な注釈された答えに依存しないで、巨大な文章データを活用するスケーリング方法を提供します。次のトークンの予測精度を大幅に向上させることを可能にします。また、RPTは進むことのできる追加の強化学習微調節の強い事前学習基盤を提供します。スケーリング曲線は、増加する学習計算量が次のトークンの予測精度を一貫的に向上させることを示します。この結果から、RPTは言語モデルの事前学習において効果的で望ましいスケーリングパラダイムとして位置付けされます。",
      "upvotes": 108,
      "discussionId": "684794553ec10bdd8ab4de21",
      "ai_summary": "Reinforcement Pre-Training (RPT) improves language model accuracy through reinforcement learning and offers a scalable method for leveraging text data for general-purpose RL.",
      "ai_keywords": [
        "Reinforcement Pre-Training (RPT)",
        "next-token prediction",
        "reasoning task",
        "reinforcement learning (RL)",
        "verifiable rewards",
        "language modeling accuracy",
        "reinforcement fine-tuning",
        "scaling curves"
      ]
    },
    "publishedAt": "2025-06-09T13:59:53.000Z",
    "title": "Reinforcement Pre-Training",
    "summary": "In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling\nparadigm for large language models and reinforcement learning (RL).\nSpecifically, we reframe next-token prediction as a reasoning task trained\nusing RL, where it receives verifiable rewards for correctly predicting the\nnext token for a given context. RPT offers a scalable method to leverage vast\namounts of text data for general-purpose RL, rather than relying on\ndomain-specific annotated answers. By incentivizing the capability of\nnext-token reasoning, RPT significantly improves the language modeling accuracy\nof predicting the next tokens. Moreover, RPT provides a strong pre-trained\nfoundation for further reinforcement fine-tuning. The scaling curves show that\nincreased training compute consistently improves the next-token prediction\naccuracy. The results position RPT as an effective and promising scaling\nparadigm to advance language model pre-training.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/OeDc7c4QFJOxdkJWvdbWB.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08007.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "5df85abada6d0311fd3d5408",
      "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
      "fullname": "Li Dong",
      "name": "unilm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 28
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07044",
      "authors": [
        {
          "_id": "684795093ec10bdd8ab4de43",
          "name": "LASA Team",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de44",
          "user": {
            "_id": "64118689756b9e455c7eac62",
            "avatarUrl": "/avatars/cdb3da22593facf545a0bafbf548b07e.svg",
            "isPro": false,
            "fullname": "Xu Weiwen",
            "user": "xww033",
            "type": "user"
          },
          "name": "Weiwen Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:07.459Z",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de45",
          "user": {
            "_id": "604f67ef0fe8ff3ec13d71ef",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
            "isPro": false,
            "fullname": "Hou Pong (Ken) Chan",
            "user": "kenchan0226",
            "type": "user"
          },
          "name": "Hou Pong Chan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:05.163Z",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de46",
          "name": "Long Li",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de47",
          "name": "Mahani Aljunied",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de48",
          "name": "Ruifeng Yuan",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de49",
          "user": {
            "_id": "61e09ec13a1781f66b4e9ae2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1642110635503-noauth.jpeg",
            "isPro": false,
            "fullname": "Jianyu Wang",
            "user": "Jianyu",
            "type": "user"
          },
          "name": "Jianyu Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:03.340Z",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4a",
          "name": "Chenghao Xiao",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4b",
          "name": "Guizhen Chen",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4c",
          "name": "Chaoqun Liu",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4d",
          "name": "Zhaodonghui Li",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4e",
          "name": "Yu Sun",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de4f",
          "name": "Junao Shen",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de50",
          "name": "Chaojun Wang",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de51",
          "name": "Jie Tan",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de52",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de53",
          "name": "Tingyang Xu",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de54",
          "name": "Hao Zhang",
          "hidden": false
        },
        {
          "_id": "684795093ec10bdd8ab4de55",
          "user": {
            "_id": "642eecbf9b2484d7d8526781",
            "avatarUrl": "/avatars/773606f4a37d48861ec4f0f2df8a956f.svg",
            "isPro": false,
            "fullname": "Yu Rong",
            "user": "Swrooy",
            "type": "user"
          },
          "name": "Yu Rong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:01.224Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/604f67ef0fe8ff3ec13d71ef/R3ajyza5JHjd8tOwwV2ht.png"
      ],
      "publishedAt": "2025-06-08T08:47:30.000Z",
      "submittedOnDailyAt": "2025-06-10T00:48:48.080Z",
      "title": "リングス：一般用モデルとしての統合的な多タイプ医学理解と理由論",
      "submittedOnDailyBy": {
        "_id": "604f67ef0fe8ff3ec13d71ef",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
        "isPro": false,
        "fullname": "Hou Pong (Ken) Chan",
        "user": "kenchan0226",
        "type": "user"
      },
      "summary": "多タイプ大語言モデル（MLLMs）は、一般的な視覚的要素を理解するために驚異的な能力を示しています。これは、大規模なデータセットと先進的な学習戦略によって生み出されました。しかし、医療領域の応用における効果性は、医療場合のデータとタスクと一般領域のものとの固有の差異によって限定されています。具體に、現在の医療用MLLMは以下の重要な制限を見守っています：医療画像を除く医療知識の幅が狭い、不適切なデータ整備プロセスによるホライゼーションの増加、複雑な医療場合に適した論理能力の欠如。これらの課題に対処するために、まず、以下のような詳細なデータ整備手順を提案します：医療画像からはじめて、幅広い医療文書および一般領域データから豊富な医療知識データを効率的に取得する；正確な医療キャプション、可視的質問回答（VQA）、論理サンプルを合成する。その結果、豊富な医療知識を満ちた多タイプデータセットを構築します。この整備されたデータに基づいて、私たちは医療専門のMLLMを紹介します：Lingshu。Lingshuは、医療の専門知識を埋め込み、進歩的にタスク解決能力を向上させるために多段階学習を行います。また、私たちは、可証明可能な報酬パラダイムを用いた強化学習の可能性を初めて試みました。また、MedEvalKitを開発しました。これは、標準的、公平で、効率的なモデル評価を可能にするために、先進的な多タイプデータとテキストデータの医療ベンチマークを統合した一連の評価フレームワークです。Lingshuは、3つの基本的な医療タスク、多タイプQA、テキストベースQA、医療レポート生成において、現在の開放ソースの多タイプモデルを上回る結果を示しました...",
      "upvotes": 50,
      "discussionId": "684795093ec10bdd8ab4de56",
      "ai_summary": "A medical-specialized multimodal large language model, Lingshu, is introduced with enhanced data curation and reinforcement learning to address limitations in medical applications.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "medical knowledge",
        "hallucinations",
        "data curation",
        "medical texts",
        "general-domain data",
        "accurate medical captions",
        "visual question answering",
        "VQA",
        "reasoning capabilities",
        "multi-stage training",
        "medical expertise",
        "reinforcement learning",
        "verifiable rewards paradigm",
        "MedEvalKit",
        "multimodal QA",
        "text-based QA",
        "medical report generation"
      ]
    },
    "publishedAt": "2025-06-08T04:47:30.000Z",
    "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and Reasoning",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ...",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/604f67ef0fe8ff3ec13d71ef/R3ajyza5JHjd8tOwwV2ht.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07044.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "604f67ef0fe8ff3ec13d71ef",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604f67ef0fe8ff3ec13d71ef/KhUwWvZ3OJ9nEee3B-SXO.png",
      "fullname": "Hou Pong (Ken) Chan",
      "name": "kenchan0226",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07900",
      "authors": [
        {
          "_id": "6847924d3ec10bdd8ab4ddb9",
          "name": "MiniCPM Team",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddba",
          "name": "Chaojun Xiao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbb",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbc",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbd",
          "name": "Yuzhuo Bai",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbe",
          "name": "Jie Cai",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddbf",
          "name": "Haotian Chen",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc0",
          "name": "Wentong Chen",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc1",
          "name": "Xin Cong",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc2",
          "name": "Ganqu Cui",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc3",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc4",
          "name": "Shengdan Fan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc5",
          "name": "Yewei Fang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc6",
          "name": "Zixuan Fu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc7",
          "name": "Wenyu Guan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc8",
          "name": "Yitong Guan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddc9",
          "name": "Junshao Guo",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddca",
          "name": "Yufeng Han",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddcb",
          "name": "Bingxiang He",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddcc",
          "name": "Yuxiang Huang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddcd",
          "name": "Cunliang Kong",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddce",
          "name": "Qiuzuo Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddcf",
          "name": "Siyuan Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd0",
          "name": "Wenhao Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd1",
          "name": "Yanghao Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd2",
          "name": "Yishan Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd3",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd4",
          "name": "Dan Liu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd5",
          "name": "Biyuan Lin",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd6",
          "name": "Yankai Lin",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd7",
          "name": "Xiang Long",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd8",
          "name": "Quanyu Lu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddd9",
          "name": "Yaxi Lu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddda",
          "name": "Peiyan Luo",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dddb",
          "name": "Hongya Lyu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dddc",
          "name": "Litu Ou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dddd",
          "name": "Yinxu Pan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddde",
          "name": "Zekai Qu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dddf",
          "name": "Qundong Shi",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde0",
          "name": "Zijun Song",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde1",
          "name": "Jiayuan Su",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde2",
          "name": "Zhou Su",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde3",
          "name": "Ao Sun",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde4",
          "name": "Xianghui Sun",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde5",
          "name": "Peijun Tang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde6",
          "name": "Fangzheng Wang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde7",
          "name": "Feng Wang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde8",
          "name": "Shuo Wang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dde9",
          "user": {
            "_id": "63be286fb3b8c44f8cecc16f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63be286fb3b8c44f8cecc16f/1CIkfEKoTnBYdYDSuQ8AT.jpeg",
            "isPro": false,
            "fullname": "Yudong Wang",
            "user": "BigDong",
            "type": "user"
          },
          "name": "Yudong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:45:33.890Z",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddea",
          "name": "Yesai Wu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddeb",
          "name": "Zhenyu Xiao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddec",
          "name": "Jie Xie",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4dded",
          "name": "Zihao Xie",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddee",
          "name": "Yukun Yan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddef",
          "name": "Jiarui Yuan",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf0",
          "name": "Kaihuo Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf1",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf2",
          "name": "Linyue Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf3",
          "name": "Xueren Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf4",
          "name": "Yudi Zhang",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf5",
          "name": "Hengyu Zhao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf6",
          "name": "Weilin Zhao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf7",
          "name": "Weilun Zhao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf8",
          "name": "Yuanqian Zhao",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddf9",
          "name": "Zhi Zheng",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfa",
          "name": "Ge Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfb",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfc",
          "name": "Wei Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfd",
          "name": "Zihan Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddfe",
          "name": "Zixuan Zhou",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4ddff",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4de00",
          "name": "Guoyang Zeng",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4de01",
          "name": "Chao Jia",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4de02",
          "name": "Dahai Li",
          "hidden": false
        },
        {
          "_id": "6847924d3ec10bdd8ab4de03",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/608f6d72283d0a8d7be9d1f9/NF1aHsqQbJ_Dl18__cn2H.qt"
      ],
      "publishedAt": "2025-06-09T16:16:50.000Z",
      "submittedOnDailyAt": "2025-06-10T00:50:56.021Z",
      "title": "MiniCPM4: 端デバイス上での超効率的LLMs",
      "submittedOnDailyBy": {
        "_id": "608f6d72283d0a8d7be9d1f9",
        "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
        "isPro": false,
        "fullname": "Chaojun XIAO",
        "user": "xcjthu",
        "type": "user"
      },
      "summary": "この論文では、端末側デバイス向けの高い効率的な大規模言語モデル（LLM）であるMiniCPM4を介紹します。この効率性は、モデルアーキテクチャ、学習データ、学習アルゴリズム、推論システムの4つのキーディメンションにおいて体系的な革新を通じて実現されました。特に、モデルアーキテクチャにおいては、InfLLM v2を提案し、長コンテキスト処理の予準書きと解確書きの両方を加速するための学習可能なスパースアテンション機構を提案します。学習データにおいては、UltraCleanとUltraChat v2を提案し、これらのデータセットを用いて8万億トークンの学習でも満足のあるモデル性能を達成できるようにします。学習アルゴリズムにおいては、ModelTunnel v2を提案し、BitCPMというデータ効率的な三分LLMを組み込み、負荷バランスされた強化学習とデータ効率的な学習アルゴリズムを実装します。推論システムにおいては、稀疏アテンション、モデル量子化、予測的取り扱いを組み合わせたCPM.cuを提案し、予準書きと解確書きの両方を効率的に実現します。MiniCPM4は、2つのバージョンで提供されており、それぞれ0.5Bと8Bパラメータです。評価結果は、MiniCPM4は類似サイズのオープンソースモデルを超えるような複数のベンチマークで優れていることを示し、その効率性と有効性を明らかにしています。特に、MiniCPM4-8Bは長シーケンス処理時にQwen3-8Bより显著なスピードアップを示します。さらに、MiniCPM4は多様なアプリケーションに成功して、信頼性のある調査生成やモデルコンテキストプロトコルを用いたツール使用を含む、広範囲の利用可能性を明らかにしています。",
      "upvotes": 45,
      "discussionId": "6847924e3ec10bdd8ab4de04",
      "projectPage": "https://huggingface.co/collections/openbmb/minicpm4-6841ab29d180257e940baa9b",
      "githubRepo": "https://github.com/openbmb/minicpm",
      "ai_summary": "MiniCPM4, a highly efficient large language model for end-side devices, achieves superior performance using innovations in sparse attention, pre-training datasets, training algorithms, and inference systems.",
      "ai_keywords": [
        "InfLLM v2",
        "sparse attention mechanism",
        "UltraClean",
        "UltraChat v2",
        "prefilling",
        "decoding",
        "long-context processing",
        "ModelTunnel v2",
        "chunk-wise rollout",
        "data-efficient tenary LLM",
        "BitCPM",
        "CPM.cu",
        "model quantization",
        "speculative sampling"
      ]
    },
    "publishedAt": "2025-06-09T12:16:50.000Z",
    "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
    "summary": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Sufficient evaluation results show that MiniCPM4\noutperforms open-source models of similar size across multiple benchmarks,\nhighlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B\ndemonstrates significant speed improvements over Qwen3-8B when processing long\nsequences. Through further adaptation, MiniCPM4 successfully powers diverse\napplications, including trustworthy survey generation and tool use with model\ncontext protocol, clearly showcasing its broad usability.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/608f6d72283d0a8d7be9d1f9/NF1aHsqQbJ_Dl18__cn2H.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07900.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "608f6d72283d0a8d7be9d1f9",
      "avatarUrl": "/avatars/7f499a37019359a3c488ba6cc11751fc.svg",
      "fullname": "Chaojun XIAO",
      "name": "xcjthu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06444",
      "authors": [
        {
          "_id": "68479c1e3ec10bdd8ab4de9d",
          "name": "Ruizhong Qiu",
          "hidden": false
        },
        {
          "_id": "68479c1e3ec10bdd8ab4de9e",
          "name": "Gaotang Li",
          "hidden": false
        },
        {
          "_id": "68479c1e3ec10bdd8ab4de9f",
          "name": "Tianxin Wei",
          "hidden": false
        },
        {
          "_id": "68479c1e3ec10bdd8ab4dea0",
          "name": "Jingrui He",
          "hidden": false
        },
        {
          "_id": "68479c1e3ec10bdd8ab4dea1",
          "name": "Hanghang Tong",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65370d95019de94263ad34a7/Uhd5i-LpHkPBjdpr4KJLi.jpeg"
      ],
      "publishedAt": "2025-06-06T18:05:45.000Z",
      "submittedOnDailyAt": "2025-06-10T01:29:36.727Z",
      "title": "サフラン-1: LLM セキュリティの推論スケーリングパラダイムへの向け方",
      "submittedOnDailyBy": {
        "_id": "65370d95019de94263ad34a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65370d95019de94263ad34a7/fH9SoJfX7yifXup0DvXkm.jpeg",
        "isPro": false,
        "fullname": "Ruizhong Qiu",
        "user": "q-rz",
        "type": "user"
      },
      "summary": "現在の安全保証研究は主に学習ステップでのアライメントに焦点を当てて、LLMに安全な行動を習得させることを目的としています。しかし、最近の研究では、これらの方法が多様なジャイルブレイク攻撃に脆弱であることが明らかになりました。同時に、推論スケーリングはLLMの論理能力を大幅に向上させたが、安全保証のテキストではまだ詳細に調査されていません。この欠陥を解決するために、我々の研究は、新しいリスクに対する強固で有効なLLMの安全保証における推論スケーリングを先駆的に開拓しています。我々は、従来の推論スケーリング手法は理由論タスクでの成功により、安全なコンテキストではより低い性能を示し、基本的なアプローチとしてのBest-of-N Samplingよりも劣りますことを明らかにしました。この不適切さは、頻繁なプロセス報酬モデル（PRM）評価に伴う高い計算オーバーヘッドによる探索--効率の二難課題によるものであると説明しています。この二難課題を克服するために、我々はSAFFRONという新しい推論スケーリングパラダイムを提案しています。このパラダイムの中心的な要素として、多分枝報酬モデル（MRM）の採用が重要です。これは、必要な報酬モデル評価の数を大幅に減少させることができます。このパラダイムを実現するために、我々は以下の3つの提案を行います：（i）MRMの部分サブジェクショントレーニングオブジェクト、(ii) 異分布探索を防ぐ保守的な探索制限、(iii) 木検索時にシーケンス間でのキー--値キャッシュ共有を促進するTrieベースのキー--値キャッシュ戦略。拡張検証は、我々の方法の効果を証明しました。また、我々は、学習された多分枝報酬モデル（Saffron-1）とその隣接するトークンレベルの安全報酬データセット（Safety4M）を公開し、LLMの安全保証における将来の研究を加速することを目的としています。我々のコード、モデル、データは、https://github.com/q-rz/saffron で公開しており、我々のプロジェクトホームページは、https://q-rz.github.io/p/saffron です。",
      "upvotes": 40,
      "discussionId": "68479c1e3ec10bdd8ab4dea2",
      "projectPage": "https://q-rz.github.io/p/saffron",
      "githubRepo": "https://github.com/q-rz/saffron",
      "ai_summary": "SAFFRON, a novel inference scaling paradigm, enhances LLM safety by reducing reward model evaluations through a multifurcation reward model and other optimizations.",
      "ai_keywords": [
        "LLMs",
        "inference scaling",
        "safety assurance",
        "jailbreak attacks",
        "Best-of-N Sampling",
        "process reward model",
        "exploration--efficiency dilemma",
        "multifurcation reward model",
        "partial supervision training",
        "conservative exploration constraint",
        "Trie-based key--value caching",
        "Safety4M dataset"
      ]
    },
    "publishedAt": "2025-06-06T14:05:45.000Z",
    "title": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance",
    "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65370d95019de94263ad34a7/Uhd5i-LpHkPBjdpr4KJLi.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06444.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65370d95019de94263ad34a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65370d95019de94263ad34a7/fH9SoJfX7yifXup0DvXkm.jpeg",
      "fullname": "Ruizhong Qiu",
      "name": "q-rz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07977",
      "authors": [
        {
          "_id": "684792f03ec10bdd8ab4de06",
          "name": "Jingjing Chang",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de07",
          "user": {
            "_id": "647469b9a51711a3b58bda2b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647469b9a51711a3b58bda2b/yeDf8Sa8IDEQyney1dGC9.jpeg",
            "isPro": false,
            "fullname": "Yixiao Fang",
            "user": "fangyixiao",
            "type": "user"
          },
          "name": "Yixiao Fang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:54.679Z",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de08",
          "name": "Peng Xing",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de09",
          "name": "Shuhan Wu",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0a",
          "user": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
            "isPro": false,
            "fullname": "Wei Cheng",
            "user": "wchengad",
            "type": "user"
          },
          "name": "Wei Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:45:23.322Z",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0b",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0c",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0d",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "684792f03ec10bdd8ab4de0e",
          "name": "Hai-Bao Chen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/Qq0Ue6mPEkoDJkIMGjRJ0.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ywhobM8HPQHIRy4CRBkLI.jpeg"
      ],
      "publishedAt": "2025-06-09T17:50:21.000Z",
      "submittedOnDailyAt": "2025-06-10T00:52:27.518Z",
      "title": "OneIG-Bench: 画像生成の全方位複雑評価",
      "submittedOnDailyBy": {
        "_id": "64b914c8ace99c0723ad83a9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
        "isPro": false,
        "fullname": "Wei Cheng",
        "user": "wchengad",
        "type": "user"
      },
      "summary": "文脈テキストを日本語に翻訳します。\n\n文脈テキスト：\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻訳します。\n\n文脈テキストを日本語に翻",
      "upvotes": 35,
      "discussionId": "684792f03ec10bdd8ab4de0f",
      "projectPage": "https://oneig-bench.github.io/",
      "githubRepo": "https://github.com/OneIG-Bench/OneIG-Benchmark",
      "ai_summary": "OneIG-Bench is a comprehensive benchmark framework for evaluating text-to-image models across multiple dimensions including reasoning, text rendering, and diversity.",
      "ai_keywords": [
        "text-to-image (T2I) models",
        "prompt-image alignment",
        "text rendering precision",
        "reasoning-generated content",
        "stylization",
        "diversity"
      ]
    },
    "publishedAt": "2025-06-09T13:50:21.000Z",
    "title": "OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation",
    "summary": "Text-to-image (T2I) models have garnered significant attention for generating\nhigh-quality images aligned with text prompts. However, rapid T2I model\nadvancements reveal limitations in early benchmarks, lacking comprehensive\nevaluations, for example, the evaluation on reasoning, text rendering and\nstyle. Notably, recent state-of-the-art models, with their rich knowledge\nmodeling capabilities, show promising results on the image generation problems\nrequiring strong reasoning ability, yet existing evaluation systems have not\nadequately addressed this frontier. To systematically address these gaps, we\nintroduce OneIG-Bench, a meticulously designed comprehensive benchmark\nframework for fine-grained evaluation of T2I models across multiple dimensions,\nincluding prompt-image alignment, text rendering precision, reasoning-generated\ncontent, stylization, and diversity. By structuring the evaluation, this\nbenchmark enables in-depth analysis of model performance, helping researchers\nand practitioners pinpoint strengths and bottlenecks in the full pipeline of\nimage generation. Specifically, OneIG-Bench enables flexible evaluation by\nallowing users to focus on a particular evaluation subset. Instead of\ngenerating images for the entire set of prompts, users can generate images only\nfor the prompts associated with the selected dimension and complete the\ncorresponding evaluation accordingly. Our codebase and dataset are now publicly\navailable to facilitate reproducible evaluation studies and cross-model\ncomparisons within the T2I research community.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/Qq0Ue6mPEkoDJkIMGjRJ0.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ywhobM8HPQHIRy4CRBkLI.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b914c8ace99c0723ad83a9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
      "fullname": "Wei Cheng",
      "name": "wchengad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07491",
      "authors": [
        {
          "_id": "684799083ec10bdd8ab4de8a",
          "user": {
            "_id": "63efbb1efc92a63ac81126d0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676655314726-noauth.jpeg",
            "isPro": true,
            "fullname": "Yongsen Mao",
            "user": "ysmao",
            "type": "user"
          },
          "name": "Yongsen Mao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:08.018Z",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8b",
          "name": "Junhao Zhong",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8c",
          "name": "Chuan Fang",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8d",
          "user": {
            "_id": "6437c0ead38ce48bdd4b0067",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6437c0ead38ce48bdd4b0067/9HdcjbD0ugiPypLDtQl8E.png",
            "isPro": false,
            "fullname": "Jia Zheng",
            "user": "bertjiazheng",
            "type": "user"
          },
          "name": "Jia Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:11.939Z",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8e",
          "name": "Rui Tang",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de8f",
          "name": "Hao Zhu",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de90",
          "name": "Ping Tan",
          "hidden": false
        },
        {
          "_id": "684799083ec10bdd8ab4de91",
          "name": "Zihan Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6437c0ead38ce48bdd4b0067/0ppuxf3I81w8mWqDgei0W.mp4"
      ],
      "publishedAt": "2025-06-09T07:10:58.000Z",
      "submittedOnDailyAt": "2025-06-10T01:04:13.223Z",
      "title": "スペクトルLM: 構造化室内モデリング向けの大規模言語モデルの訓練",
      "submittedOnDailyBy": {
        "_id": "6437c0ead38ce48bdd4b0067",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6437c0ead38ce48bdd4b0067/9HdcjbD0ugiPypLDtQl8E.png",
        "isPro": false,
        "fullname": "Jia Zheng",
        "user": "bertjiazheng",
        "type": "user"
      },
      "summary": "SpatialLMは、3Dポイントクラスデータを処理し、構造化された3Dスペース理解の出力を生成するための大規模な言語モデルです。これらの出力には、壁、ドア、窓などの建築要素や、その語義的カテゴリーを含みます。先行の方法と違って、タスク特化されたネットワーク設計を利用しないで、標準の多モーダルLLMアーキテクチャに従い、開放ソースLLMから直接微調されています。\n\nSpatialLMの訓練には、12,328の室内スペース（54,778の部屋）のポイントクラスデータの高品質な合成データセットを収集し、様々なモデリングと訓練の決定について詳細な研究を行いました。公開ベンチマークでは、我々のモデルは並みながらのスペース推定と3Dオブジェクト検出の最先端の性能を示し、これにより、拡張写真、具象的ロボットやその他の適用分野の現代LLMのスペース理解能力を向上させる可能性のあるパスを示しています。",
      "upvotes": 22,
      "discussionId": "684799083ec10bdd8ab4de92",
      "projectPage": "https://manycore-research.github.io/SpatialLM",
      "githubRepo": "https://github.com/manycore-research/SpatialLM/",
      "ai_summary": "SpatialLM, a multimodal large language model, processes 3D point cloud data to generate structured scene understanding outputs, achieving state-of-the-art performance in layout estimation and competitive results in 3D object detection.",
      "ai_keywords": [
        "large language model",
        "3D point cloud",
        "structured 3D scene understanding",
        "multimodal LLM",
        "fine-tuning",
        "synthetic dataset",
        "ground-truth 3D annotations",
        "layout estimation",
        "3D object detection",
        "augmented reality",
        "embodied robotics"
      ]
    },
    "publishedAt": "2025-06-09T03:10:58.000Z",
    "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling",
    "summary": "SpatialLM is a large language model designed to process 3D point cloud data\nand generate structured 3D scene understanding outputs. These outputs include\narchitectural elements like walls, doors, windows, and oriented object boxes\nwith their semantic categories. Unlike previous methods which exploit\ntask-specific network designs, our model adheres to the standard multimodal LLM\narchitecture and is fine-tuned directly from open-source LLMs.\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset\nconsisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with\nground-truth 3D annotations, and conduct a careful study on various modeling\nand training decisions. On public benchmarks, our model gives state-of-the-art\nperformance in layout estimation and competitive results in 3D object\ndetection. With that, we show a feasible path for enhancing the spatial\nunderstanding capabilities of modern LLMs for applications in augmented\nreality, embodied robotics, and more.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6437c0ead38ce48bdd4b0067/0ppuxf3I81w8mWqDgei0W.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07491.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6437c0ead38ce48bdd4b0067",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6437c0ead38ce48bdd4b0067/9HdcjbD0ugiPypLDtQl8E.png",
      "fullname": "Jia Zheng",
      "name": "bertjiazheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07986",
      "authors": [
        {
          "_id": "68479b0f3ec10bdd8ab4de94",
          "name": "Zhengyao Lv",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de95",
          "name": "Tianlin Pan",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de96",
          "user": {
            "_id": "635f8ed47c05eb9f59963d3a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg",
            "isPro": false,
            "fullname": "ChenyangSi",
            "user": "ChenyangSi",
            "type": "user"
          },
          "name": "Chenyang Si",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:05.835Z",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de97",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de98",
          "name": "Wangmeng Zuo",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de99",
          "name": "Ziwei Liu",
          "hidden": false
        },
        {
          "_id": "68479b0f3ec10bdd8ab4de9a",
          "name": "Kwan-Yee K. Wong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:54:04.000Z",
      "submittedOnDailyAt": "2025-06-10T01:15:51.787Z",
      "title": "再考える多モード間の相互作用",
      "submittedOnDailyBy": {
        "_id": "645aff5121ab438e732c47c1",
        "avatarUrl": "/avatars/23b2a853139b0f2ae1fa88e2bd4e0056.svg",
        "isPro": false,
        "fullname": "Zhengyao Lv",
        "user": "cszy98",
        "type": "user"
      },
      "summary": "マルチモーダルディフュージョントランスフォーマー（MM-DiTs）は、テキスト駆動の視覚生成に関して驚異的な進歩を達成しました。しかし、状態の最先端のMM-DiTモデルのようにFLUXなどは、テキストプロンプトと生成内容の間の準確なアラインメントを達成するには難しいです。MM-DiTの注意機構において2つの重要な問題を識別しました。1）画像と文字のモーダル間のトークンの不均衡によるクロスモーダル注意の抑制と2）時間ステップに関する注意重み付けの欠如、これらはアラインメントを妨げています。これらの問題に対処するために、構造温度調整付きクロスモーダル注意（TACA）を提案しました。これはパラメーターエファシェントな方法で、温度スケーリングと時間ステップ依存的な調整を通じて多モーダルインターセクションを動的に再平衡します。LoRA微調と組み合わせると、TACAは最小限の計算オーバーヘッドでT2I-CompBenchベンチマーク上のテキスト画像アラインメントを大幅に向上させます。TACAは状態の最先端のモデルのようにFLUXとSD3.5に対して検証され、物体の外見、属性の結合、空間関係における画像文脈アラインメントの向上を示しました。我々の見つけは、テキストから画像のディフュージョンモデルでの語意的忠実性の向上においてクロスモーダル注意のバランスの重要性を強調します。我々のコードは、https://github.com/Vchitect/TACAで公開しています。",
      "upvotes": 11,
      "discussionId": "68479b0f3ec10bdd8ab4de9b",
      "projectPage": "https://vchitect.github.io/TACA/",
      "githubRepo": "https://github.com/Vchitect/TACA",
      "ai_summary": "Temperature-Adjusted Cross-modal Attention (TACA) enhances text-image alignment in diffusion models by dynamically rebalancing multimodal interactions through temperature scaling and timestep-dependent adjustment.",
      "ai_keywords": [
        "Temperature-Adjusted Cross-modal Attention",
        "TACA",
        "multimodal interactions",
        "temperature scaling",
        "timestep-dependent adjustment",
        "FLUX",
        "SD3.5",
        "T2I-CompBench",
        "semantic fidelity",
        "text-to-image diffusion models"
      ]
    },
    "publishedAt": "2025-06-09T13:54:04.000Z",
    "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers",
    "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose Temperature-Adjusted Cross-modal Attention\n(TACA), a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\nhttps://github.com/Vchitect/TACA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07986.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645aff5121ab438e732c47c1",
      "avatarUrl": "/avatars/23b2a853139b0f2ae1fa88e2bd4e0056.svg",
      "fullname": "Zhengyao Lv",
      "name": "cszy98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07553",
      "authors": [
        {
          "_id": "684794a43ec10bdd8ab4de24",
          "user": {
            "_id": "65eaa07cb6c760d77468b4b6",
            "avatarUrl": "/avatars/4a1aae58986b40444351e0a167ca807c.svg",
            "isPro": false,
            "fullname": "Jingchao Wang",
            "user": "jcwang0602",
            "type": "user"
          },
          "name": "Jingchao Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:15.857Z",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de25",
          "user": {
            "_id": "65fd45473ccf43503350d837",
            "avatarUrl": "/avatars/11b9679945b3c89b142f0d62a312f362.svg",
            "isPro": false,
            "fullname": "Haote Yang",
            "user": "Hoter",
            "type": "user"
          },
          "name": "Haote Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:44:18.060Z",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de26",
          "name": "Jiang Wu",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de27",
          "name": "Yifan He",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de28",
          "name": "Xingjian Wei",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de29",
          "name": "Yinfan Wang",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2a",
          "name": "Chengjin Liu",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2b",
          "name": "Lingli Ge",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2c",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2d",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2e",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "684794a43ec10bdd8ab4de2f",
          "name": "Conghui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T08:47:10.000Z",
      "submittedOnDailyAt": "2025-06-10T00:49:58.326Z",
      "title": "GTR-CoT: グラフトラバーショーカインオブスコートフォーモルキュラー\n  構造認識",
      "submittedOnDailyBy": {
        "_id": "65fd45473ccf43503350d837",
        "avatarUrl": "/avatars/11b9679945b3c89b142f0d62a312f362.svg",
        "isPro": false,
        "fullname": "Haote Yang",
        "user": "Hoter",
        "type": "user"
      },
      "summary": "光化学構造認識（OCSR）は、化学知識のデジタル化において重要であり、分子画像を機械読み込みフォーマットに変換することができることを重要な役割を果たしています。最近の視覚言語モデル（VLMs）はこの課題に潜っているが、複雑な分子構造と不均一な注釈に対して画像キャプチングアプローチは常に難しいといわれています。これらの挑戦に対処するために、私たちはGTR-Mol-VLMを紹介します。これは2つの主な革新を特徴としています：1）グラフトラバーサスビジュアルコショットオブスコート機構で、人間の理由論を模倣し、原子ボンドの順番的予測を通じて分子グラフを進歩的に解析することを模倣しています。2）データセンタリック原則で、画像の略構造と拡張注釈の間の不対称を解決しています。モデル開発のために、私たちはGTR-CoT-1.3Mを構築しました。これは、細かく修正された注釈を持つ大規模なインストラクションチューニングデータセットです。また、MolRec-Benchを紹介しました。これはOCSRでグラフ解析の精度を細かく評価するための最初のベンチマークです。詳細な実験は、GTR-Mol-VLMが専門モデル、化学領域のVLMs、および商用の一般用VLMsと比較して上位の結果を収めたことを示しています。特に、関数グループの略構造を含む分子画像の場合、SMILEスコアとグラフベースのメトリックでは、GTR-Mol-VLMは最良ベースラインに対して約14パーセント点を超えます。私たちは、この研究はOCSR技術を実世界的な需要によりより効果的に満たすことを進め、化学情報学とAI for Scienceの分野を進めることを望みます。GTR-CoTを公開します。",
      "upvotes": 11,
      "discussionId": "684794a43ec10bdd8ab4de30",
      "ai_summary": "GTR-Mol-VLM, featuring graph traversal and data-centric principles, outperforms existing models in Optical Chemical Structure Recognition by accurately parsing molecular graphs and handling abbreviated structures.",
      "ai_keywords": [
        "Graph Traversal as Visual Chain of Thought",
        "Faithfully Recognize What You've Seen",
        "GTR-CoT-1.3M",
        "MolRec-Bench",
        "graph-parsing accuracy",
        "Optical Chemical Structure Recognition",
        "VLMs",
        "SMILES-based",
        "graph-based metrics"
      ]
    },
    "publishedAt": "2025-06-09T04:47:10.000Z",
    "title": "GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular\n  Structure Recognition",
    "summary": "Optical Chemical Structure Recognition (OCSR) is crucial for digitizing\nchemical knowledge by converting molecular images into machine-readable\nformats. While recent vision-language models (VLMs) have shown potential in\nthis task, their image-captioning approach often struggles with complex\nmolecular structures and inconsistent annotations. To overcome these\nchallenges, we introduce GTR-Mol-VLM, a novel framework featuring two key\ninnovations: (1) the Graph Traversal as Visual Chain of Thought\nmechanism that emulates human reasoning by incrementally parsing molecular\ngraphs through sequential atom-bond predictions, and (2) the data-centric\nprinciple of Faithfully Recognize What You've Seen, which addresses\nthe mismatch between abbreviated structures in images and their expanded\nannotations. To support model development, we constructed GTR-CoT-1.3M, a\nlarge-scale instruction-tuning dataset with meticulously corrected annotations,\nand introduced MolRec-Bench, the first benchmark designed for a fine-grained\nevaluation of graph-parsing accuracy in OCSR. Comprehensive experiments\ndemonstrate that GTR-Mol-VLM achieves superior results compared to specialist\nmodels, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in\nscenarios involving molecular images with functional group abbreviations,\nGTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage\npoints, both in SMILES-based and graph-based metrics. We hope that this work\nwill drive OCSR technology to more effectively meet real-world needs, thereby\nadvancing the fields of cheminformatics and AI for Science. We will release\nGTR-CoT at https://github.com/opendatalab/GTR-CoT.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07553.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fd45473ccf43503350d837",
      "avatarUrl": "/avatars/11b9679945b3c89b142f0d62a312f362.svg",
      "fullname": "Haote Yang",
      "name": "Hoter",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07712",
      "authors": [
        {
          "_id": "684790cd3ec10bdd8ab4ddaa",
          "user": {
            "_id": "66dfb6bac93721c02f75f37e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zzoX0tpkHCNsFGjtnATSt.png",
            "isPro": false,
            "fullname": "Renjie",
            "user": "RogerLos",
            "type": "user"
          },
          "name": "Renjie Luo",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T01:56:30.026Z",
          "hidden": false
        },
        {
          "_id": "684790cd3ec10bdd8ab4ddab",
          "name": "Jiaxi Li",
          "hidden": false
        },
        {
          "_id": "684790cd3ec10bdd8ab4ddac",
          "user": {
            "_id": "65d7b983baa72790a1151923",
            "avatarUrl": "/avatars/938531e84ca01a0c5a2a174057e3e9c5.svg",
            "isPro": false,
            "fullname": "Chen Huang",
            "user": "Albus-Chen",
            "type": "user"
          },
          "name": "Chen Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T09:19:43.446Z",
          "hidden": false
        },
        {
          "_id": "684790cd3ec10bdd8ab4ddad",
          "name": "Wei Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T12:56:41.000Z",
      "submittedOnDailyAt": "2025-06-10T00:30:07.286Z",
      "title": "バレーを通り歩き：小規模言語モデルの効果的な長期コンテキストトレーニングの道",
      "submittedOnDailyBy": {
        "_id": "66dfb6bac93721c02f75f37e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zzoX0tpkHCNsFGjtnATSt.png",
        "isPro": false,
        "fullname": "Renjie",
        "user": "RogerLos",
        "type": "user"
      },
      "summary": "長コンセプト（CoT）の制御は、言語モデルの理由論を強化するためにはよく使用されています。大きなモデルに対しては効果的ですが、小さな言語モデル（SLMs；3Bパラメータ以下）に対しては、限られた長コンセプトデータで訓練されると理由論の性能が大幅に低下する現象を「長CoT減退」と呼び、これを見出しました。Qwen2.5、LLaMA3、Gemma3のファミリーにおいて極めて幅広く実験を行い、この減退はSLMs全体で広く見られました。その設定では、8kの長CoT例をみなして訓練されたモデルは微調節前の性能の75%程度を失います。また、特に小さなモデルに対しては、220kの長CoT例でも訓練しても微調節前の性能を回復または超えることはできませんでした。この現象は誤りの累積によるものであると分析され、長い回答が多段階の理由論の機能を増強する一方で、誤りを重ね合わせるリスクも増大します。また、長CoT減退は後段の強化学習（RL）にも負面影響を及ぼすことが見出され、これは十分なスケールの制御付きの微調節（SFT）で軽減できることがわかりました。これらの発見はSLMsに対する長CoT訓練の利益に関する一般的な仮定を質疑し、小規模な理由論モデルの構築における実用的なガイドラインを提供します。",
      "upvotes": 10,
      "discussionId": "684790cd3ec10bdd8ab4ddae",
      "ai_summary": "Small language models experience significant performance declines when trained on long chain-of-thought data due to error accumulation, impacting downstream reinforcement learning but potentially mitigated by extensive supervised fine-tuning.",
      "ai_keywords": [
        "Long chain-of-thought",
        "Long CoT Degradation",
        "small language models",
        "SLMs",
        "Qwen2.5",
        "LLaMA3",
        "Gemma3",
        "error accumulation",
        "supervised fine-tuning",
        "SFT",
        "reinforcement learning"
      ]
    },
    "publishedAt": "2025-06-09T08:56:41.000Z",
    "title": "Through the Valley: Path to Effective Long CoT Training for Small\n  Language Models",
    "summary": "Long chain-of-thought (CoT) supervision has become a common strategy to\nenhance reasoning in language models. While effective for large models, we\nidentify a phenomenon we call Long CoT Degradation, in which small language\nmodels (SLMs; <=3B parameters) trained on limited long CoT data experience\nsignificant performance deterioration. Through extensive experiments on the\nQwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is\nwidespread across SLMs. In some settings, models trained on only 8k long CoT\nexamples lose up to 75% of their original performance before fine-tuning.\nStrikingly, we further observe that for some particularly small models, even\ntraining on 220k long CoT examples fails to recover or surpass their original\nperformance prior to fine-tuning. Our analysis attributes this effect to error\naccumulation: while longer responses increase the capacity for multi-step\nreasoning, they also amplify the risk of compounding mistakes. Furthermore, we\nfind that Long CoT Degradation may negatively impacts downstream reinforcement\nlearning (RL), although this can be alleviated by sufficiently scaled\nsupervised fine-tuning (SFT). Our findings challenge common assumptions about\nthe benefits of long CoT training for SLMs and offer practical guidance for\nbuilding more effective small-scale reasoning models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07712.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66dfb6bac93721c02f75f37e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zzoX0tpkHCNsFGjtnATSt.png",
      "fullname": "Renjie",
      "name": "RogerLos",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07530",
      "authors": [
        {
          "_id": "68478dae3ec10bdd8ab4dd9b",
          "name": "Hongyu Wang",
          "hidden": false
        },
        {
          "_id": "68478dae3ec10bdd8ab4dd9c",
          "name": "Chuyan Xiong",
          "hidden": false
        },
        {
          "_id": "68478dae3ec10bdd8ab4dd9d",
          "name": "Ruiping Wang",
          "hidden": false
        },
        {
          "_id": "68478dae3ec10bdd8ab4dd9e",
          "name": "Xilin Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T08:15:11.000Z",
      "submittedOnDailyAt": "2025-06-10T00:14:07.356Z",
      "title": "BitVLA: 1ビットビジョン・ラングアウェイ・アクションモデルズフォロボティックスマニピュレーション",
      "submittedOnDailyBy": {
        "_id": "63f71771d36951307fcb4dcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
        "isPro": false,
        "fullname": "Hongyu Wang",
        "user": "hongyuw",
        "type": "user"
      },
      "summary": "ビット・ビジョン・アクション（VLA）モデルは、機械人操作の様々な複雑なタスクに対して、印象的な能力を示しています。しかし、そのモデルサイズの増大は、資源制限された機械人システムの扱いにとって重大な課題になっています。1-bit 予ティニングは、大規模な言語モデルの推論効率を高めるために、最小限の性能損失を伴わずに効果的であることが証明されていますが、VLAモデルにその適用はまだ調査不足です。本研究では、機械人操作の最初の1-bit VLAモデル「BitVLA」を紹介します。このモデルでは、すべてのパラメータが三値であり、{-1, 0, 1}です。また、視覚エンコーダーのメモリフットプリートを進一步に減少させるために、蒸馏に関する訓練戦略を提案しています。このプロセスでは、全精度エンコーダーはターゲットモデルとして、潜在的な表現をより良く調整することができます。そのため、大規模な機械人予ティニングによるものではないことに見栄えがあるビット・ビジョン・アクション（BitVLA）は、LIBEROベンチマークで4-bit 後処理キャリブレーションで最先端のモデルOpenVLA-OFTと同等の性能を達成し、その際にはメモリの29.8%だけを使用します。これらの結果は、BitVLAがメモリ制限されたエッジデバイス上での扱いに向けての可能性を高めています。コードとモデル重みは、https://github.com/ustcwhy/BitVLA からリリースされています。",
      "upvotes": 9,
      "discussionId": "68478dae3ec10bdd8ab4dd9f",
      "githubRepo": "https://github.com/ustcwhy/BitVLA",
      "ai_summary": "BitVLA, a 1-bit VLA model with ternary parameters, achieves comparable performance to OpenVLA-OFT on LIBERO while using 29.8% less memory through distillation-aware training.",
      "ai_keywords": [
        "VLA models",
        "1-bit pretraining",
        "ternary parameters",
        "distillation-aware training",
        "vision encoder",
        "full-precision encoder",
        "latent representations",
        "memory footprint",
        "robotics manipulation",
        "OpenVLA-OFT",
        "LIBERO benchmark",
        "memory-constrained edge devices"
      ]
    },
    "publishedAt": "2025-06-09T04:15:11.000Z",
    "title": "BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation",
    "summary": "Vision-Language-Action (VLA) models have shown impressive capabilities across\na wide range of robotics manipulation tasks. However, their growing model size\nposes significant challenges for deployment on resource-constrained robotic\nsystems. While 1-bit pretraining has proven effective for enhancing the\ninference efficiency of large language models with minimal performance loss,\nits application to VLA models remains underexplored. In this work, we present\nBitVLA, the first 1-bit VLA model for robotics manipulation, in which every\nparameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint\nof the vision encoder, we propose the distillation-aware training strategy that\ncompresses the full-precision encoder to 1.58-bit weights. During this process,\na full-precision encoder serves as a teacher model to better align latent\nrepresentations. Despite the lack of large-scale robotics pretraining, BitVLA\nachieves performance comparable to the state-of-the-art model OpenVLA-OFT with\n4-bit post-training quantization on the LIBERO benchmark, while consuming only\n29.8% of the memory. These results highlight BitVLA's promise for deployment on\nmemory-constrained edge devices. We release the code and model weights in\nhttps://github.com/ustcwhy/BitVLA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07530.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f71771d36951307fcb4dcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
      "fullname": "Hongyu Wang",
      "name": "hongyuw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07298",
      "authors": [
        {
          "_id": "684795e83ec10bdd8ab4de6a",
          "user": {
            "_id": "62de9e6fdcdc9043efa8b756",
            "avatarUrl": "/avatars/c26974c740633d143f7382f0858ea99a.svg",
            "isPro": false,
            "fullname": "Yijia Dai",
            "user": "DaiYijia",
            "type": "user"
          },
          "name": "Yijia Dai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:53.562Z",
          "hidden": false
        },
        {
          "_id": "684795e83ec10bdd8ab4de6b",
          "name": "Zhaolin Gao",
          "hidden": false
        },
        {
          "_id": "684795e83ec10bdd8ab4de6c",
          "name": "Yahya Satter",
          "hidden": false
        },
        {
          "_id": "684795e83ec10bdd8ab4de6d",
          "user": {
            "_id": "664f92095a60ca2484b90d7a",
            "avatarUrl": "/avatars/3232bb702ed479ac821b7a5dfb457d0b.svg",
            "isPro": false,
            "fullname": "Sarah Dean",
            "user": "sarahdean",
            "type": "user"
          },
          "name": "Sarah Dean",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T02:18:19.596Z",
          "hidden": false
        },
        {
          "_id": "684795e83ec10bdd8ab4de6e",
          "name": "Jennifer J. Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-08T21:49:38.000Z",
      "submittedOnDailyAt": "2025-06-10T01:00:39.516Z",
      "title": "予測学習の大規模言語モデルがコンテキスト内で隠れホップマルコフモデルを学習する",
      "submittedOnDailyBy": {
        "_id": "652eec0aabc673c4204c459e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652eec0aabc673c4204c459e/9otSQFP8G3S8zarR1Y5rE.jpeg",
        "isPro": false,
        "fullname": "Zhaolin Gao",
        "user": "GitBag",
        "type": "user"
      },
      "summary": "隠れマルコフモデル（HMMs）は、隠れマルコフ効果を持つ順序データのモデリングに基盤的なツールですが、実世界データに適合することは計算的に難しいです。本稿では、予った大規模言語モデル（LLMs）が、プロンプト内の例からパターンを推論する能力をもって、HMMsから生成されたデータを効果的にモデリングできることを示します。多様な合成HMMのセットで、LLMsは理論的な最適解に近い予測精度を達成します。HMMの特性による新しいスケーリングトレンドを明らかにし、これらの実験的な観察に関する理論的な予想を提供します。また、科学者におけるICL（In-Context Learning）を診断ツールとして使用する実用的なガイドラインを提供します。実世界的な動物の決定論タスクでは、ICLは人間の専門家が設計したモデルと競争的な性能を達成します。私たちの知識によると、これはICLがHMMから生成されたシーケンスを学習して予測することを示す最初の例で、LLMsのIn-Context Learningの理解を深め、複雑な科学データの隠れた構造を開発する強力なツールとしての潛在的な可能性を確立します。",
      "upvotes": 8,
      "discussionId": "684795e93ec10bdd8ab4de6f",
      "githubRepo": "https://github.com/DaiYijia02/icl-hmm",
      "ai_summary": "In-context learning in large language models can effectively model sequences generated by hidden Markov models, achieving predictive accuracy and uncovering scaling trends, thus demonstrating its potential as a diagnostic tool for complex scientific data.",
      "ai_keywords": [
        "hidden Markov models",
        "HMMs",
        "large language models",
        "LLMs",
        "in-context learning",
        "IC",
        "predictive accuracy",
        "theoretical optimum",
        "synthetic HMMs",
        "scaling trends",
        "empirical observations",
        "animal decision-making tasks",
        "human experts"
      ]
    },
    "publishedAt": "2025-06-08T17:49:38.000Z",
    "title": "Pre-trained Large Language Models Learn Hidden Markov Models In-context",
    "summary": "Hidden Markov Models (HMMs) are foundational tools for modeling sequential\ndata with latent Markovian structure, yet fitting them to real-world data\nremains computationally challenging. In this work, we show that pre-trained\nlarge language models (LLMs) can effectively model data generated by HMMs via\nin-context learning (ICL)x2013their ability to infer patterns from\nexamples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve\npredictive accuracy approaching the theoretical optimum. We uncover novel\nscaling trends influenced by HMM properties, and offer theoretical conjectures\nfor these empirical observations. We also provide practical guidelines for\nscientists on using ICL as a diagnostic tool for complex data. On real-world\nanimal decision-making tasks, ICL achieves competitive performance with models\ndesigned by human experts. To our knowledge, this is the first demonstration\nthat ICL can learn and predict HMM-generated sequencesx2013an\nadvance that deepens our understanding of in-context learning in LLMs and\nestablishes its potential as a powerful tool for uncovering hidden structure in\ncomplex scientific data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07298.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "652eec0aabc673c4204c459e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652eec0aabc673c4204c459e/9otSQFP8G3S8zarR1Y5rE.jpeg",
      "fullname": "Zhaolin Gao",
      "name": "GitBag",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07463",
      "authors": [
        {
          "_id": "68478a493ec10bdd8ab4dd90",
          "user": {
            "_id": "632c234f42c386ebd2710434",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg",
            "isPro": false,
            "fullname": "Guang Liu",
            "user": "ZacLiu",
            "type": "user"
          },
          "name": "Guang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T09:19:47.891Z",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd91",
          "user": {
            "_id": "63a11ce02fabbbb899a01d58",
            "avatarUrl": "/avatars/ee3d4088b6d32b2c18b8be91913e90dd.svg",
            "isPro": false,
            "fullname": "ldwang",
            "user": "ldwang",
            "type": "user"
          },
          "name": "Liangdong Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T09:19:50.398Z",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd92",
          "name": "Jijie Li",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd93",
          "name": "Yang Yu",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd94",
          "name": "Yao Xu",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd95",
          "name": "Jiabei Chen",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd96",
          "name": "Yu Bai",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd97",
          "name": "Feng Liao",
          "hidden": false
        },
        {
          "_id": "68478a493ec10bdd8ab4dd98",
          "user": {
            "_id": "629aa3155ab4232a3fe0893e",
            "avatarUrl": "/avatars/cf2d4a9295b5da9e2e4d2278bbb36040.svg",
            "isPro": false,
            "fullname": "Yonghua Lin",
            "user": "Yonghua",
            "type": "user"
          },
          "name": "Yonghua Lin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-06-10T09:39:44.976Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T06:14:19.000Z",
      "submittedOnDailyAt": "2025-06-10T01:15:00.564Z",
      "title": "CCI4.0: 大語言モデルの理由論を強化するためのバイリンガル予備学習データセット",
      "submittedOnDailyBy": {
        "_id": "632c234f42c386ebd2710434",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg",
        "isPro": false,
        "fullname": "Guang Liu",
        "user": "ZacLiu",
        "type": "user"
      },
      "summary": "CCI4.0を紹介します。これは、上位のデータ質と多様な人間のような理由論の軌跡を実現するために工学された大規模な二言語予ち編集データセットです。CCI4.0は約35TBのディスクスペースを占め、CCI4.0-M2-BaseとCCI4.0-M2-CoTの2つのサブデータセットから構成されています。CCI4.0-M2-Baseは、5.2TBの謹重なカレーテッドカニジョンコーパス、Nemotron-CCからの22.5TBの英語サブセット、数学、ウィキ、arXiv、コードの多様なソースを統合しています。これらのデータは主に処理されたデータセットから源されていますが、各領域の品質スタンダードは動的で、複数の専門家の経験と労働が必要です。そこで、データの品質を主にモデルに基づいて議論する新しいパイプラインを提案しています。これは、二段階の削除、多クラス分類器の品質スコア、領域に関する流れ性フィルタリングを通じて行われます。45億ページのCoT(Chain-of-Thought)テンプレートを抽出し、CCI4.0-M2-CoTとして命名しています。これは、大きなモデルからのCoTの精進と異なり、多様な理由論パターンを示し、ホラリゼーションの可能性を显著に減らします。実験的評価により、CCI4.0で予ち学習されたLLMは、清かなり信頼性のあるトレーニングシグナルを受け、ダウンストリームタスクにおいて、特に数学とコード反省タスクで、一貫した向上を示しています。我々の結果は、厳密なデータ編集と人間の思い込みテンプレートの重要性を強調し、LLMの性能向上において重要な役割を果たしていることを示し、予ち学習コーパスの自動処理についてもさまざまな光をつけています。",
      "upvotes": 7,
      "discussionId": "68478a493ec10bdd8ab4dd99",
      "projectPage": "https://openseek.baai.ac.cn/",
      "githubRepo": "https://github.com/FlagAI-Open/OpenSeek",
      "ai_summary": "A large-scale bilingual pre-training dataset, CCI4.0, enhances data quality and diverse reasoning patterns for language models, leading to improved performance in downstream tasks like math and code reflection.",
      "ai_keywords": [
        "pre-training dataset",
        "bilingual pre-training",
        "data quality",
        "reasoning trajectory",
        "deduplication",
        "multiclassifier quality scoring",
        "domain-aware fluency filtering",
        "Chain-of-Thought",
        "CoT extraction",
        "language models",
        "LLMs",
        "downstream tasks",
        "math tasks",
        "code reflection tasks",
        "data curation",
        "human thinking templates"
      ]
    },
    "publishedAt": "2025-06-09T02:14:19.000Z",
    "title": "CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large\n  Language Models",
    "summary": "We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered\nfor superior data quality and diverse human-like reasoning trajectory. CCI4.0\noccupies roughly 35 TB of disk space and comprises two sub-datasets:\nCCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a 5.2 TB carefully\ncurated Chinese web corpus, a 22.5 TB English subset from Nemotron-CC, and\ndiverse sources from math, wiki, arxiv, and code. Although these data are\nmostly sourced from well-processed datasets, the quality standards of various\ndomains are dynamic and require extensive expert experience and labor to\nprocess. So, we propose a novel pipeline justifying data quality mainly based\non models through two-stage deduplication, multiclassifier quality scoring, and\ndomain-aware fluency filtering. We extract 4.5 billion pieces of\nCoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the\ndistillation of CoT from larger models, our proposed staged CoT extraction\nexemplifies diverse reasoning patterns and significantly decreases the\npossibility of hallucination. Empirical evaluations demonstrate that LLMs\npre-trained in CCI4.0 benefit from cleaner, more reliable training signals,\nyielding consistent improvements in downstream tasks, especially in math and\ncode reflection tasks. Our results underscore the critical role of rigorous\ndata curation and human thinking templates in advancing LLM performance,\nshedding some light on automatically processing pretraining corpora.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07463.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632c234f42c386ebd2710434",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg",
      "fullname": "Guang Liu",
      "name": "ZacLiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07434",
      "authors": [
        {
          "_id": "684797f33ec10bdd8ab4de7a",
          "user": {
            "_id": "6447ca6ca478b20f1755b294",
            "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
            "isPro": false,
            "fullname": "Feifan Song",
            "user": "songff",
            "type": "user"
          },
          "name": "Feifan Song",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:29.497Z",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7b",
          "user": {
            "_id": "67244a81aa8556c561925ab6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/w-vZ0uwYACagrNq-H1oyO.jpeg",
            "isPro": false,
            "fullname": "Shaohang Wei",
            "user": "SylvainWei",
            "type": "user"
          },
          "name": "Shaohang Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:27.498Z",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7c",
          "name": "Wen Luo",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7d",
          "name": "Yuxuan Fan",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7e",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de7f",
          "name": "Guoyin Wang",
          "hidden": false
        },
        {
          "_id": "684797f33ec10bdd8ab4de80",
          "name": "Houfeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T05:21:22.000Z",
      "submittedOnDailyAt": "2025-06-10T00:59:10.518Z",
      "title": "始めやすいものは半分で終わり：弱さから強さへの解碼による低リソースの好み調整",
      "submittedOnDailyBy": {
        "_id": "6447ca6ca478b20f1755b294",
        "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
        "isPro": false,
        "fullname": "Feifan Song",
        "user": "songff",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、人間の好みに合わせなければ、冒頭、不適切な、または意味のない内容を生成することを避けることが必要です。最近、LLMsの対応を低リソースで行う方法が流行していますが、高品質なおよび対応した内容を得ることには難問があります。生成した対応の難易度が解碼の初期に集中することを観察した上で、私たちは、基礎モデルの対応能力を高めるための新しいフレームワーク「Weak-to-Strong Decoding（WSD）」を提案します。小さな対応モデルは、基礎モデルが続きを生成する前に、対応した開始部分をデキュートし、その後、プログラムマシンが構築したような自動スイッチ機構で制御されます。また、私たちは新しいデータセット「GenerAlign」を収集し、これを用いて小さなプイルオット-3Bを微調節し、デキュートモデルとして使用します。これは、WSDフレームワークの下で、基礎モデルを効果的に高めることができ、すべてのベースライン方法を超える性能を示し、ダウンストリームタスクにおける損失を避けることを目指します。さらに、様々な設定と時間効率の影響、およびWSDの内在的機構の詳細な分析を行いました。",
      "upvotes": 7,
      "discussionId": "684797f33ec10bdd8ab4de81",
      "githubRepo": "https://github.com/F2-Song/Weak-to-Strong-Decoding",
      "ai_summary": "A new decoding framework (Weak-to-Strong Decoding, WSD) enhances the alignment of large language models by using a small aligned model to draft responses, followed by the base model, with a design to prevent degradation in performance on downstream tasks.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "LLM alignment",
        "human preferences",
        "low-resource methods",
        "decoding",
        "small aligned model",
        "auto-switch mechanism",
        "GenerAlign",
        "Pilot-3B",
        "draft model",
        "alignment tax",
        "intrinsic mechanisms"
      ]
    },
    "publishedAt": "2025-06-09T01:21:22.000Z",
    "title": "Well Begun is Half Done: Low-resource Preference Alignment by\n  Weak-to-Strong Decoding",
    "summary": "Large Language Models (LLMs) require alignment with human preferences to\navoid generating offensive, false, or meaningless content. Recently,\nlow-resource methods for LLM alignment have been popular, while still facing\nchallenges in obtaining both high-quality and aligned content. Motivated by the\nobservation that the difficulty of generating aligned responses is concentrated\nat the beginning of decoding, we propose a novel framework, Weak-to-Strong\nDecoding (WSD), to enhance the alignment ability of base models by the guidance\nof a small aligned model. The small model first drafts well-aligned beginnings,\nfollowed by the large base model to continue the rest, controlled by a\nwell-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,\nto fine-tune a small-sized Pilot-3B as the draft model, which effectively\nenhances different base models under the WSD framework to outperform all\nbaseline methods, while avoiding degradation on downstream tasks, termed as the\nalignment tax. Extensive experiments are further conducted to examine the\nimpact of different settings and time efficiency, as well as analyses on the\nintrinsic mechanisms of WSD in depth.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07434.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6447ca6ca478b20f1755b294",
      "avatarUrl": "/avatars/5049856b5ed1b74533fff902e14b4c7c.svg",
      "fullname": "Feifan Song",
      "name": "songff",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06941",
      "authors": [
        {
          "_id": "684797863ec10bdd8ab4de72",
          "user": {
            "_id": "6520621836008ecc88699622",
            "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
            "isPro": false,
            "fullname": "Parshin Shojaee",
            "user": "parshinsh",
            "type": "user"
          },
          "name": "Parshin Shojaee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:43:32.697Z",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de73",
          "name": "Iman Mirzadeh",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de74",
          "name": "Keivan Alizadeh",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de75",
          "name": "Maxwell Horton",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de76",
          "name": "Samy Bengio",
          "hidden": false
        },
        {
          "_id": "684797863ec10bdd8ab4de77",
          "name": "Mehrdad Farajtabar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-07T22:42:29.000Z",
      "submittedOnDailyAt": "2025-06-10T01:00:21.500Z",
      "title": "「思い出の脈絡：問題複雑性の視点から理由論モデルの強さと制限を理解する」",
      "submittedOnDailyBy": {
        "_id": "6520621836008ecc88699622",
        "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
        "isPro": false,
        "fullname": "Parshin Shojaee",
        "user": "parshinsh",
        "type": "user"
      },
      "summary": "最近の言語モデルは、答えを提供する前に詳細な思考過程を生成するためのLarge Reasoning Models（LRMs）を導入しました。これらのモデルは、理由のベンチマークでの性能向上を示していますが、基本的な能力、スケーリングの特性および制限は、十分に理解されていません。現在の評価は、主に数学とコーディングベンチマークに焦点を当て、最終的な答えの正確性を優先しています。しかし、この評価パラダイムは、理由の跡を理解することができないことによって、その理由の跡が汚染されていることがあります。本研究では、構造的にこれらの欠陥を調査し、構造的に複雑さを操作できるコントローラブルなパズル環境の助けを受けて、この評価パラダイムの欠陥を調査しました。この設定は、最終的な答えだけでなく、内部の理由の跡を分析することができ、LRMsがどのように思っているかを理解することができます。拡大の実験を通じて、LRMsは特定の複雑さを超えると完全な正確性崩壊を示します。また、それらは反直観的なスケーリング制限を示しています：理由の努力は問題の複雑さによって一定の点まで増加し、残りのトークンバッジがあっても増加しなくなります。同じ推論計算を使用して、LRMsと標準のLLMコンペアソートを比較して、3つの性能ディレクトリーを特定しました：（1）低複雑さのタスクでは標準モデルがLRMsを上回り、（2）中複雑さのタスクではLRMsが優位を示し、（3）高複雑さのタスクでは両方のモデルが完全な崩壊を見落とします。LRMsは、正確な計算に制限があり、明確なアルゴリズムを使用しないことを見出し、スケールの間で不適切な理由を示していることを見出しました。また、理由の跡をより深く調査し、探索された解決策のパターンを研究し、モデルの計算行為を分析し、その強みや制限を明らかにし、その理由の能力についての疑問を持つことを示しました。",
      "upvotes": 7,
      "discussionId": "684797863ec10bdd8ab4de78",
      "ai_summary": "Large Reasoning Models (LRMs) exhibit varying performance across task complexities, with limitations in exact computation and inconsistent reasoning, as assessed using controllable puzzle environments.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRMs",
        "controllable puzzle environments",
        "reasoning traces",
        "standard LLMs",
        "performance regimes",
        "exact computation",
        "reasoning capabilities"
      ]
    },
    "publishedAt": "2025-06-07T18:42:29.000Z",
    "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of\n  Reasoning Models via the Lens of Problem Complexity",
    "summary": "Recent generations of language models have introduced Large Reasoning Models\n(LRMs) that generate detailed thinking processes before providing answers.\nWhile these models demonstrate improved performance on reasoning benchmarks,\ntheir fundamental capabilities, scaling properties, and limitations remain\ninsufficiently understood. Current evaluations primarily focus on established\nmath and coding benchmarks, emphasizing final answer accuracy. However, this\nevaluation paradigm often suffers from contamination and does not provide\ninsights into the reasoning traces. In this work, we systematically investigate\nthese gaps with the help of controllable puzzle environments that allow precise\nmanipulation of complexity while maintaining consistent logical structures.\nThis setup enables the analysis of not only final answers but also the internal\nreasoning traces, offering insights into how LRMs think. Through extensive\nexperiments, we show that LRMs face a complete accuracy collapse beyond certain\ncomplexities. Moreover, they exhibit a counterintuitive scaling limit: their\nreasoning effort increases with problem complexity up to a point, then declines\ndespite having remaining token budget. By comparing LRMs with their standard\nLLM counterparts under same inference compute, we identify three performance\nregimes: (1) low-complexity tasks where standard models outperform LRMs, (2)\nmedium-complexity tasks where LRMs demonstrates advantage, and (3)\nhigh-complexity tasks where both models face complete collapse. We found that\nLRMs have limitations in exact computation: they fail to use explicit\nalgorithms and reason inconsistently across scales. We also investigate the\nreasoning traces in more depth, studying the patterns of explored solutions and\nanalyzing the models' computational behavior, shedding light on their\nstrengths, limitations, and raising questions about their reasoning\ncapabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6520621836008ecc88699622",
      "avatarUrl": "/avatars/b08c00af00f1736a4f4938443e575b0e.svg",
      "fullname": "Parshin Shojaee",
      "name": "parshinsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.06205",
      "authors": [
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf4",
          "user": {
            "_id": "66727b038171db46e7f4f242",
            "avatarUrl": "/avatars/3f8ab656006533fe1a24dd48c9b0a1b6.svg",
            "isPro": false,
            "fullname": "sc",
            "user": "sc-bd",
            "type": "user"
          },
          "name": "Sheng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T09:21:08.452Z",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf5",
          "name": "Peiyu He",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf6",
          "name": "Jiaxin Hu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf7",
          "name": "Ziyang Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf8",
          "name": "Yansheng Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbf9",
          "name": "Tao Xu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfa",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfb",
          "name": "Chongchong Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfc",
          "name": "Chao An",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfd",
          "name": "Shiyu Cai",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbfe",
          "name": "Duo Cao",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dbff",
          "name": "Kangping Chen",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc00",
          "name": "Shuai Chu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc01",
          "name": "Tianwei Chu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc02",
          "name": "Mingdi Dan",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc03",
          "name": "Min Du",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc04",
          "name": "Weiwei Fang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc05",
          "name": "Pengyou Fu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc06",
          "name": "Junkai Hu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc07",
          "name": "Xiaowei Jiang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc08",
          "name": "Zhaodi Jiang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc09",
          "name": "Fuxuan Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0a",
          "name": "Jun Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0b",
          "name": "Minghui Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0c",
          "name": "Mingyao Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0d",
          "name": "Yanchang Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0e",
          "name": "Zhibin Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc0f",
          "name": "Guangming Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc10",
          "name": "Kairui Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc11",
          "name": "Lihao Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc12",
          "name": "Weizhi Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc13",
          "name": "Xiaoshun Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc14",
          "name": "Yufei Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc15",
          "name": "Yunfei Liu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc16",
          "name": "Qiang Lu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc17",
          "name": "Yuanfei Luo",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc18",
          "name": "Xiang Lv",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc19",
          "name": "Hongying Ma",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1a",
          "name": "Sai Ma",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1b",
          "name": "Lingxian Mi",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1c",
          "name": "Sha Sa",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1d",
          "name": "Hongxiang Shu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1e",
          "name": "Lei Tian",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc1f",
          "name": "Chengzhi Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc20",
          "name": "Jiayu Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc21",
          "name": "Kaijie Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc22",
          "name": "Qingyi Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc23",
          "name": "Renwen Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc24",
          "name": "Tao Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc25",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc26",
          "name": "Xirui Wang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc27",
          "name": "Chao Wei",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc28",
          "name": "Xuguang Wei",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc29",
          "name": "Zijun Xia",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2a",
          "name": "Zhaohao Xiao",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2b",
          "name": "Tingshuai Yan",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2c",
          "name": "Liyan Yang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2d",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2e",
          "name": "Zhikai Yang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc2f",
          "name": "Zhong Yin",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc30",
          "name": "Li Yuan",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc31",
          "name": "Liuchun Yuan",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc32",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc33",
          "name": "Jinyang Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc34",
          "name": "Junhui Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc35",
          "name": "Linge Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc36",
          "name": "Zhenyi Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc37",
          "name": "Zheyu Zhang",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc38",
          "name": "Dongjie Zhu",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc39",
          "name": "Hang Li",
          "hidden": false
        },
        {
          "_id": "6846ca9b3ec10bdd8ab4dc3a",
          "name": "Yangang Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66727b038171db46e7f4f242/nrwuIsn9tQaR75nAuym-R.mp4"
      ],
      "publishedAt": "2025-06-06T16:08:47.000Z",
      "submittedOnDailyAt": "2025-06-10T07:53:05.305Z",
      "title": "Astra: ホイローレシピック・マルチモーダル学習をもとに一般的用途のモバイルロボットへの向け方",
      "submittedOnDailyBy": {
        "_id": "66727b038171db46e7f4f242",
        "avatarUrl": "/avatars/3f8ab656006533fe1a24dd48c9b0a1b6.svg",
        "isPro": false,
        "fullname": "sc",
        "user": "sc-bd",
        "type": "user"
      },
      "summary": "現代のロボットナビゲーションシステムは、多様かつ複雑な室内環境で様々な課題を見出します。伝統的なアプローチは、小さなモデルを持つ複数のモジュールやルールベースのシステムを基にしていて、新しい環境に適応することができません。これに対して、私たちは、モバイルロボットナビゲーションに向けた機能的なダブルモデルアーキテクチャ、Astra-GlobalとAstra-Localを開発しました。Astra-Globalは、多モーダルLLMであり、視覚と言語の入力を処理して、混合タピック-セマンティックグラフを用いて自らと目的地の位置情報を決定し、伝統的な視覚的な場所識別法を上回ります。Astra-Localは、複数タスクを扱うネットワークで、局所パスプランニングとオドメトリー計測を処理します。その4次元空間時間エンコーダーは、自己観測学習によって訓練され、下流タスクに向けた強固な4次元特徴量を生成します。計画ヘッドは、流れマッチングと新しいマスク付きESDF損失を利用して、衝突リスクを最小化し、局所プロジェクトを生成します。オドメトリーヘッドは、チャネルエンコーダーを介して多様なセンサー入力を統合して、ロボットの相対的な姿勢を予測します。本物の実際のマインセイプス内のモバイルロボットに実装されたAstraは、多様な室内環境で高い端末から端末までのミッション成功率を達成します。",
      "upvotes": 7,
      "discussionId": "6846ca9b3ec10bdd8ab4dc3b",
      "ai_summary": "Astra, a dual-model architecture for mobile robot navigation, uses a multimodal LLM for global localization and a multitask network for local path planning and odometry estimation, achieving high success rates in diverse indoor environments.",
      "ai_keywords": [
        "LLM",
        "self and goal localization",
        "hybrid topological-semantic graph",
        "multimodal LLM",
        "multitask network",
        "4D spatial-temporal encoder",
        "self-supervised learning",
        "4D features",
        "flow matching",
        "masked ESDF loss",
        "local trajectories",
        "transformer encoder",
        "relative pose prediction"
      ]
    },
    "publishedAt": "2025-06-06T12:08:47.000Z",
    "title": "Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal\n  Learning",
    "summary": "Modern robot navigation systems encounter difficulties in diverse and complex\nindoor environments. Traditional approaches rely on multiple modules with small\nmodels or rule-based systems and thus lack adaptability to new environments. To\naddress this, we developed Astra, a comprehensive dual-model architecture,\nAstra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a\nmultimodal LLM, processes vision and language inputs to perform self and goal\nlocalization using a hybrid topological-semantic graph as the global map, and\noutperforms traditional visual place recognition methods. Astra-Local, a\nmultitask network, handles local path planning and odometry estimation. Its 4D\nspatial-temporal encoder, trained through self-supervised learning, generates\nrobust 4D features for downstream tasks. The planning head utilizes flow\nmatching and a novel masked ESDF loss to minimize collision risks for\ngenerating local trajectories, and the odometry head integrates multi-sensor\ninputs via a transformer encoder to predict the relative pose of the robot.\nDeployed on real in-house mobile robots, Astra achieves high end-to-end mission\nsuccess rate across diverse indoor environments.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66727b038171db46e7f4f242/nrwuIsn9tQaR75nAuym-R.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06205.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66727b038171db46e7f4f242",
      "avatarUrl": "/avatars/3f8ab656006533fe1a24dd48c9b0a1b6.svg",
      "fullname": "sc",
      "name": "sc-bd",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08012",
      "authors": [
        {
          "_id": "684791b63ec10bdd8ab4ddb1",
          "name": "Penghao Wu",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb2",
          "name": "Shengnan Ma",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb3",
          "name": "Bo Wang",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb4",
          "name": "Jiaheng Yu",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb5",
          "name": "Lewei Lu",
          "hidden": false
        },
        {
          "_id": "684791b63ec10bdd8ab4ddb6",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:59:57.000Z",
      "submittedOnDailyAt": "2025-06-10T00:35:58.769Z",
      "title": "GUI-Reflection: グラフィックユーティリティの自覚化\n\nGUI-Reflection: グラフィックユーティリティモデルを自覚化によって強化する\n\nGUI-Reflection: 自覚化をもつ多モーダルGUIモデルを創成する\n\nGUI-Reflection: グラフィックユーティリティモデルを自覚化ベヒノールによって強化する",
      "submittedOnDailyBy": {
        "_id": "64101f81b27543634e377fc1",
        "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
        "isPro": false,
        "fullname": "Penghao Wu",
        "user": "craigwu",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) は、グラフィックユーザインターフェイス（GUI）自動化に革命的な可能性を示しています。しかし、現在のGUIモデルは主に近乎無誤のオフラインツレージェクトから学習しているため、反省と誤り修正の能力が不足しています。この隙を埋めるために、私たちは、GUI-Reflectionという新しいフレームワークを提案します。このフレームワークは、GUI特有の予ティニング、オフラインサブジェクティブ微調節（SFT）、オンライン反省チューニングの設定ごとに、終末からの多モデルGUIモデルに自動化された反省と誤り修正の能力を明記しています。\n\nGUI-Reflectionは、完全に自動化されたデータ生成と学習プロセスを通じて、自動的な反省行動の発生を促成します。特に、1）私たちは、現有の成功ツレージェクトから反省と誤り修正のデータを自動的に構築するスケーラブルなデータパイプラインを提案します。現在のGUIモデルは主にグラフィックの基礎とUI理解能力に焦点を当てているため、GUI-Reflection Task Suiteを提案して、明記的に反省指向的な能力を学習と評価します。2）また、オンライントレーニングとデータ収集のための多様且エフエクティブな環境を構築しました。3）さらに、提案された環境を活用して、迭り返しオンライン反省チューニングアルゴリズムを提案し、モデルが反省と誤り修正の能力を継続的に向上させることを可能にします。我々のフレームワークは、GUIアガントに自動反省と誤り修正の能力を付け加え、より強固、適応性のあるおよび知能的なGUI自動化のための道が開かれ、すべてのデータ、モデル、環境、ツールを公開的にリリースすることを準備しています。",
      "upvotes": 6,
      "discussionId": "684791b63ec10bdd8ab4ddb7",
      "projectPage": "https://penghao-wu.github.io/GUI_Reflection/",
      "githubRepo": "https://github.com/penghao-wu/GUI_Reflection",
      "ai_summary": "GUI-Reflection enhances GUI automation by integrating self-reflection and error correction through scalable data pipelines and an iterative online tuning framework.",
      "ai_keywords": [
        "multimodal large language models",
        "graphical user interface",
        "GUI automation",
        "self-reflection",
        "error correction",
        "GUI-specific pre-training",
        "supervised fine-tuning",
        "online reflection tuning",
        "reflection-oriented abilities",
        "iterative online reflection tuning algorithm"
      ]
    },
    "publishedAt": "2025-06-09T13:59:57.000Z",
    "title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection\n  Behavior",
    "summary": "Multimodal Large Language Models (MLLMs) have shown great potential in\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\nGUI models mostly rely on learning from nearly error-free offline trajectories,\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\npropose GUI-Reflection, a novel framework that explicitly integrates\nself-reflection and error correction capabilities into end-to-end multimodal\nGUI models throughout dedicated training stages: GUI-specific pre-training,\noffline supervised fine-tuning (SFT), and online reflection tuning.\nGUI-reflection enables self-reflection behavior emergence with fully automated\ndata generation and learning processes without requiring any human annotation.\nSpecifically, 1) we first propose scalable data pipelines to automatically\nconstruct reflection and error correction data from existing successful\ntrajectories. While existing GUI models mainly focus on grounding and UI\nunderstanding ability, we propose the GUI-Reflection Task Suite to learn and\nevaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a\ndiverse and efficient environment for online training and data collection of\nGUI models on mobile devices. 3) We also present an iterative online reflection\ntuning algorithm leveraging the proposed environment, enabling the model to\ncontinuously enhance its reflection and error correction abilities. Our\nframework equips GUI agents with self-reflection and correction capabilities,\npaving the way for more robust, adaptable, and intelligent GUI automation, with\nall data, models, environments, and tools to be released publicly.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64101f81b27543634e377fc1",
      "avatarUrl": "/avatars/557dd9d4707e3b38e0805dfb87c08004.svg",
      "fullname": "Penghao Wu",
      "name": "craigwu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07309",
      "authors": [
        {
          "_id": "6847b8793ec10bdd8ab4df4f",
          "user": {
            "_id": "67f42bd98752b56bd349a9db",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_1jLlx7qqMDYb2ylZ2TCa.png",
            "isPro": false,
            "fullname": "Yin Huang",
            "user": "MaggieHuang",
            "type": "user"
          },
          "name": "Yin Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T04:45:46.729Z",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df50",
          "name": "Yifan Ethan Xu",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df51",
          "name": "Kai Sun",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df52",
          "name": "Vera Yan",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df53",
          "name": "Alicia Sun",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df54",
          "name": "Haidar Khan",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df55",
          "name": "Jimmy Nguyen",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df56",
          "name": "Mohammad Kachuee",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df57",
          "name": "Zhaojiang Lin",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df58",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df59",
          "name": "Aaron Colak",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df5a",
          "name": "Anuj Kumar",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df5b",
          "name": "Wen-tau Yih",
          "hidden": false
        },
        {
          "_id": "6847b8793ec10bdd8ab4df5c",
          "name": "Xin Luna Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-08T22:51:46.000Z",
      "submittedOnDailyAt": "2025-06-10T03:17:46.849Z",
      "title": "ConfQA: 確信のあることだけを答える",
      "submittedOnDailyBy": {
        "_id": "67f42bd98752b56bd349a9db",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_1jLlx7qqMDYb2ylZ2TCa.png",
        "isPro": false,
        "fullname": "Yin Huang",
        "user": "MaggieHuang",
        "type": "user"
      },
      "summary": "LLMは事実的な記述をハウリシャニングすることを防ぐことはできますか？本論文では、ConfQAという名前の微調整ステージを提案し、複数の事実性ベンチマークでは、20-40%から5%未満にハウリシャニング率を減少させることができます。核心のアイデアは簡単です：LLMが正確に答えた場合は、その答えを続けるように学習させ、そうでない場合は「私は不安です」と承認するように学習させます。しかし、2つの鍵の要素が高度な学習の効果を生み出すようになっています。最初に、「自信がある限りのみ答える」というダンペンプロンプトを追加し、これを含めない場合は、ハウリシャニングは高く15%-25%に残ることになります。二つ目に、簡単な事実的な記述を利用し、特に知識グラフからの属性値を用いてLLMの自信を調整することで、領域と質問の種類による強固な一般化を実現します。この見解に基づいて、ConfQAの自信に基づいて内部にパラメーター化されたニューラルキャンパスと外部に記録された符号的なキャンパスを選択するDual Neural Knowledgeフレームワークを提案します。このフレームワークは、精度の向上を30%以上減らしながら、95%を超える精度の向上を可能にします。",
      "upvotes": 6,
      "discussionId": "6847b87a3ec10bdd8ab4df5d",
      "ai_summary": "ConfQA fine-tuning strategy reduces factual statement hallucination in LLMs by 80%, using a dampening prompt and factual statements from knowledge graphs to improve confidence calibration and knowledge selection.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "fine-tuning",
        "ConfQA",
        "hallucination",
        "factuality benchmarks",
        "dampening prompt",
        "factual statements",
        "knowledge graphs",
        "confidence calibration",
        "Dual Neural Knowledge framework",
        "neural knowledge",
        "symbolic knowledge",
        "accuracy gains",
        "external retrievals"
      ]
    },
    "publishedAt": "2025-06-08T18:51:46.000Z",
    "title": "ConfQA: Answer Only If You Are Confident",
    "summary": "Can we teach Large Language Models (LLMs) to refrain from hallucinating\nfactual statements? In this paper we present a fine-tuning strategy that we\ncall ConfQA, which can reduce hallucination rate from 20-40% to under 5% across\nmultiple factuality benchmarks. The core idea is simple: when the LLM answers a\nquestion correctly, it is trained to continue with the answer; otherwise, it is\ntrained to admit \"I am unsure\". But there are two key factors that make the\ntraining highly effective. First, we introduce a dampening prompt \"answer only\nif you are confident\" to explicitly guide the behavior, without which\nhallucination remains high as 15%-25%. Second, we leverage simple factual\nstatements, specifically attribute values from knowledge graphs, to help LLMs\ncalibrate the confidence, resulting in robust generalization across domains and\nquestion types. Building on this insight, we propose the Dual Neural Knowledge\nframework, which seamlessly select between internally parameterized neural\nknowledge and externally recorded symbolic knowledge based on ConfQA's\nconfidence. The framework enables potential accuracy gains to beyond 95%, while\nreducing unnecessary external retrievals by over 30%.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07309.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67f42bd98752b56bd349a9db",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_1jLlx7qqMDYb2ylZ2TCa.png",
      "fullname": "Yin Huang",
      "name": "MaggieHuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.08010",
      "authors": [
        {
          "_id": "6847ad3b3ec10bdd8ab4df06",
          "name": "Nick Jiang",
          "hidden": false
        },
        {
          "_id": "6847ad3b3ec10bdd8ab4df07",
          "name": "Amil Dravid",
          "hidden": false
        },
        {
          "_id": "6847ad3b3ec10bdd8ab4df08",
          "name": "Alexei Efros",
          "hidden": false
        },
        {
          "_id": "6847ad3b3ec10bdd8ab4df09",
          "name": "Yossi Gandelsman",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:59:57.000Z",
      "submittedOnDailyAt": "2025-06-10T04:42:01.785Z",
      "title": "Vision Transformers Don't Need Trained Registers",
      "submittedOnDailyBy": {
        "_id": "6398d9d168e3392256aaf952",
        "avatarUrl": "/avatars/062363a9b5ebb26603c543a7fc3ee4ec.svg",
        "isPro": false,
        "fullname": "Nick",
        "user": "nickjiang",
        "type": "user"
      },
      "summary": "私たちは、ビジョン・トランスフォーマーで以前に識別された現象のメカニズムを調査しています。その現象は、高スケールのトークンがノイズづけされたアテンションマップを生み出すことです。私たちは、複数のモデル（例：CLIP、DINOv2）で、一部のスパースなニューロンがオファルチャートークンに高スケールのアクティベーションを集中させ、不連続なアテンションパターンを生み出し、その後のビジョン処理を悪化させることを観察しました。現在の解決策は、これらのオファルチャーを除去するために、追加学習されたレジスタートークンを用いてモデルを再学習することであるが、私たちは、この発見を基に、レジスタートークンの効果を再現するためのトレーニング無制限アプローチを提案します。私たちは、見つけたレジスターニューロンからの高スケールアクティベーションを追加のトレーニングされていないトークンに移し、レジスタートークンの効果をモデルがレジスターを使用しないまま学習されているように製造します。私たちの方法は、クリーンなアテンションマップと特徴マップを生成し、複数のビジョンタスクでベースモデルの性能を向上させ、明示的にレジスタートークンを使用して学習されたモデルと比較的結果を実現します。次に、テストタイムレジスターをオフショールビジョン言語モデルに拡張し、その解釈性を向上させます。私たちの結果は、テストタイムレジスターはテストタイムでレジスタートークンの役割を果たし、レジスタートークンを含まないどのような予エニングモデルにもトレーニング無制限の解決策を提供することができることを示しています。",
      "upvotes": 5,
      "discussionId": "6847ad3c3ec10bdd8ab4df0a",
      "ai_summary": "A training-free method shifts high-norm activations in Vision Transformers to an untrained token, enhancing attention maps and performance across visual tasks, and improving interpretability in vision-language models.",
      "ai_keywords": [
        "Vision Transformers",
        "high-norm tokens",
        "noisy attention maps",
        "activations",
        "neurons",
        "irregular attention patterns",
        "downstream visual processing",
        "register tokens",
        "feature maps",
        "vision-language models",
        "interpretability",
        "test-time registers"
      ]
    },
    "publishedAt": "2025-06-09T13:59:57.000Z",
    "title": "Vision Transformers Don't Need Trained Registers",
    "summary": "We investigate the mechanism underlying a previously identified phenomenon in\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\nsparse set of neurons is responsible for concentrating high-norm activations on\noutlier tokens, leading to irregular attention patterns and degrading\ndownstream visual processing. While the existing solution for removing these\noutliers involves retraining models from scratch with additional learned\nregister tokens, we use our findings to create a training-free approach to\nmitigate these artifacts. By shifting the high-norm activations from our\ndiscovered register neurons into an additional untrained token, we can mimic\nthe effect of register tokens on a model already trained without registers. We\ndemonstrate that our method produces cleaner attention and feature maps,\nenhances performance over base models across multiple downstream visual tasks,\nand achieves results comparable to models explicitly trained with register\ntokens. We then extend test-time registers to off-the-shelf vision-language\nmodels to improve their interpretability. Our results suggest that test-time\nregisters effectively take on the role of register tokens at test-time,\noffering a training-free solution for any pre-trained model released without\nthem.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08010.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6398d9d168e3392256aaf952",
      "avatarUrl": "/avatars/062363a9b5ebb26603c543a7fc3ee4ec.svg",
      "fullname": "Nick",
      "name": "nickjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.08006",
      "authors": [
        {
          "_id": "6847ae533ec10bdd8ab4df0c",
          "name": "Sicheng Mo",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df0d",
          "name": "Ziyang Leng",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df0e",
          "name": "Leon Liu",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df0f",
          "name": "Weizhen Wang",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df10",
          "name": "Honglin He",
          "hidden": false
        },
        {
          "_id": "6847ae533ec10bdd8ab4df11",
          "name": "Bolei Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T17:59:52.000Z",
      "submittedOnDailyAt": "2025-06-10T02:33:18.764Z",
      "title": "ドリームランド：シミュレータと生成モデルを用いた制御可能な世界作成",
      "submittedOnDailyBy": {
        "_id": "637c94d3f219c71f93eda9ad",
        "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
        "isPro": true,
        "fullname": "Sicheng Mo",
        "user": "Sichengmo",
        "type": "user"
      },
      "summary": "大規模のビデオ生成モデルは、動的な世界作成に適した多様な写真的な可視内容を合成できますが、そのエレメント毎の制御可能性が不足し、スケーン編集や具象化AIアガントの学習において使用が難しいことがあります。我々は、物理ベースのシミュレータのグランリューの制御と大規模な事前学習モデルの写真的な内容の出力を統合するハイブリッドワールド生成フレームワーク「Dreamland」を提案します。特に、ピクセルレベルとオブジェクトレベルの語意とジェネリックを含む層々の世界抽象を設計し、シミュレータと生成モデルを結びつけるための中間的表現として使用します。このアプローチは、制御可能性を高め、実世界の分布と早期にアラインして適応コストを最小化し、既存や将来の事前学習モデルのオープンソース利用を支援します。また、D3Simデータセットを構築し、ハイブリッド生成パイプラインの学習と評価を促進します。実験は、Dreamlandが現在のベースラインより画質向上50.8%、制御可能性向上17.9%を示し、具象化アガントの学習に大きな潜力を持っていることを示します。コードとデータは利用可能になります。",
      "upvotes": 4,
      "discussionId": "6847ae533ec10bdd8ab4df12",
      "projectPage": "https://metadriverse.github.io/dreamland/",
      "ai_summary": "Dreamland, a hybrid framework, combines physics-based simulators and generative models to improve controllability and image quality in video generation.",
      "ai_keywords": [
        "video generative models",
        "physics-based simulator",
        "photorealistic content",
        "world abstraction",
        "pixel-level semantics",
        "object-level semantics",
        "geometry",
        "layered world abstraction",
        "early alignment",
        "D3Sim dataset",
        "embodied agent training"
      ]
    },
    "publishedAt": "2025-06-09T13:59:52.000Z",
    "title": "Dreamland: Controllable World Creation with Simulator and Generative\n  Models",
    "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08006.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c94d3f219c71f93eda9ad",
      "avatarUrl": "/avatars/6dae0c30755196ccc0a5a06b3981c47f.svg",
      "fullname": "Sicheng Mo",
      "name": "Sichengmo",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.06266",
      "authors": [
        {
          "_id": "6847b4b43ec10bdd8ab4df33",
          "user": {
            "_id": "6337537b267cee4d068f604d",
            "avatarUrl": "/avatars/15267f0759a6570c98ee6a150558fcc0.svg",
            "isPro": false,
            "fullname": "Sabri Eyuboglu",
            "user": "sabrieyuboglu",
            "type": "user"
          },
          "name": "Sabri Eyuboglu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T04:29:41.818Z",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df34",
          "name": "Ryan Ehrlich",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df35",
          "name": "Simran Arora",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df36",
          "name": "Neel Guha",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df37",
          "name": "Dylan Zinsley",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df38",
          "name": "Emily Liu",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df39",
          "name": "Will Tennien",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df3a",
          "name": "Atri Rudra",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df3b",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df3c",
          "name": "Azalia Mirhoseini",
          "hidden": false
        },
        {
          "_id": "6847b4b43ec10bdd8ab4df3d",
          "name": "Christopher Re",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T17:48:23.000Z",
      "submittedOnDailyAt": "2025-06-10T03:02:08.278Z",
      "title": "カートリッジ：軽量モデルと一般的な長コンテキスト表現を学習して得られる",
      "submittedOnDailyBy": {
        "_id": "6337537b267cee4d068f604d",
        "avatarUrl": "/avatars/15267f0759a6570c98ee6a150558fcc0.svg",
        "isPro": false,
        "fullname": "Sabri Eyuboglu",
        "user": "sabrieyuboglu",
        "type": "user"
      },
      "summary": "大語言モデルは、コードベース、法規文書、やチャット履歴などの大規模なテキストコーパスに基づいたクエリを回答するために、コーパス全体をコンテキストウィンドウに置いて、コンテキスト学習（ICL）を活用しています。現在のモデルは100K-1Mトークンのコンテキストをサポートしていますが、この設定は、KVキャッシュのメモリ消費が入力長に比例して増加するため、サービスにコストがかかります。我々は、コーパスごとにオフラインで小さなKVキャッシュを訓練するような代替設定を検討しています。推論時には、この訓練されたKVキャッシュを「カートリッジ」と呼んで読み込み、応答を解確します。重要な点として、同じコーパスを参照するすべてのクエリに対して、カートリッジの訓練コストを割り当てることができます。しかし、コーパスでの次のトークン予測を用いてカートリッジを訓練することで、ICLと比較して優れていない結果が得られました。代わりに、我々は、コーパスに関する合成コンバーションを生成し、コンテキストのディスティルレーションオブジェクティブを用いてカートリッジを訓練する「自学」という訓練ドリィングを提案しています。カートリッジは、自学を用いて訓練されたものはICLの機能を再現し、サービスコストを大幅に削減できます。難しい長コンテキストベンチマークでは、自学を用いて訓練されたカートリッジはICLの性能と匹敵し、メモリ使用量を38.6倍減らし、トランソーフォープロットを26.4倍増やします。自学も、モデルの効果的なコンテキスト長を拡大し（例えば、MTOBでは128kトークンから484kトークンに）、より驚くことに、推論時にカートリッジを組み立てることができるようになります。",
      "upvotes": 4,
      "discussionId": "6847b4b43ec10bdd8ab4df3e",
      "projectPage": "https://hazyresearch.stanford.edu/blog/2025-06-08-cartridges",
      "githubRepo": "https://github.com/HazyResearch/cartridges",
      "ai_summary": "Training a smaller, offline KV cache (Cartridge) with a context-distillation objective (self-study) for large language models reduces serving costs, matches ICL performance, and extends effective context length.",
      "ai_keywords": [
        "KV cache",
        "Cartridge",
        "in-context learning (ICL)",
        "self-study",
        "context-distillation objective",
        "MTOB"
      ]
    },
    "publishedAt": "2025-06-06T13:48:23.000Z",
    "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
    "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06266.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6337537b267cee4d068f604d",
      "avatarUrl": "/avatars/15267f0759a6570c98ee6a150558fcc0.svg",
      "fullname": "Sabri Eyuboglu",
      "name": "sabrieyuboglu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07848",
      "authors": [
        {
          "_id": "6847de223ec10bdd8ab4e02a",
          "name": "Teng Hu",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02b",
          "name": "Zhentao Yu",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02c",
          "name": "Zhengguang Zhou",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02d",
          "name": "Jiangning Zhang",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02e",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e02f",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "6847de223ec10bdd8ab4e030",
          "name": "Ran Yi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T15:11:09.000Z",
      "submittedOnDailyAt": "2025-06-10T05:57:02.723Z",
      "title": "PolyVivid: クロスモーダルインタラクションとアップデートによるビビッド多視点ビデオ生成",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "最近の映像生成の進歩にもとづいても、現在のモデルは、特に多個体のカスタマイズ化において、一致した識別子と相互作用の微妙な制御性が不足しています。本論文では、識別子の一致性を保ち、柔軟な生成を可能にする多個体の映像カスタマイズフレームワーク「PolyVivid」を提案します。テキストと画像の間の正確な対応関係を確立するために、VLLMベースのテキスト・画像融合モジュールを設計し、視覚的識別子をテキスト空間に埋め込み、精密な基礎を与えます。また、識別子の保存と個体間の相互作用を進めるために、3D-RoPEベースの拡張モジュールを提案し、テキストと画像の埋め込みの構造化された双方向的な融合を可能にします。また、注意を継承した識別子注入モジュールを開発し、融合された識別子特徴を映像生成プロセスに効果的に注入し、識別子漂流を抑制します。最後に、MLLMベースのデータパイプラインを構築し、MLLMベースの基礎、分割、クライックベースの個体統合戦略を組み合わせ、高品質の多個体データを生成し、後続の映像生成での個体の区別と不明確性を減らします。拡張された実験は、PolyVividが識別子の忠実性、映像のリアリティ、個体のアライメントに優れた性能を収め、現在の開放ソースや商業ベースラインを上回ることを示します。",
      "upvotes": 2,
      "discussionId": "6847de223ec10bdd8ab4e031",
      "projectPage": "https://sjtuplayer.github.io/projects/PolyVivid/",
      "ai_summary": "PolyVivid is a multi-subject video customization framework that uses text-image fusion, 3D-RoPE enhancement, attention-inherited identity injection, and MLLM-based data processing to ensure identity consistency and realistic video generation.",
      "ai_keywords": [
        "VLLM-based text-image fusion",
        "3D-RoPE-based enhancement",
        "attention-inherited identity injection",
        "MLLM-based data pipeline",
        "identity fidelity",
        "video realism",
        "subject alignment"
      ]
    },
    "publishedAt": "2025-06-09T11:11:09.000Z",
    "title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal\n  Interaction and Enhancement",
    "summary": "Despite recent advances in video generation, existing models still lack\nfine-grained controllability, especially for multi-subject customization with\nconsistent identity and interaction. In this paper, we propose PolyVivid, a\nmulti-subject video customization framework that enables flexible and\nidentity-consistent generation. To establish accurate correspondences between\nsubject images and textual entities, we design a VLLM-based text-image fusion\nmodule that embeds visual identities into the textual space for precise\ngrounding. To further enhance identity preservation and subject interaction, we\npropose a 3D-RoPE-based enhancement module that enables structured\nbidirectional fusion between text and image embeddings. Moreover, we develop an\nattention-inherited identity injection module to effectively inject fused\nidentity features into the video generation process, mitigating identity drift.\nFinally, we construct an MLLM-based data pipeline that combines MLLM-based\ngrounding, segmentation, and a clique-based subject consolidation strategy to\nproduce high-quality multi-subject data, effectively enhancing subject\ndistinction and reducing ambiguity in downstream video generation. Extensive\nexperiments demonstrate that PolyVivid achieves superior performance in\nidentity fidelity, video realism, and subject alignment, outperforming existing\nopen-source and commercial baselines.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07848.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07527",
      "authors": [
        {
          "_id": "6847dce63ec10bdd8ab4e011",
          "user": {
            "_id": "659e3ea885956d2cccda2b9e",
            "avatarUrl": "/avatars/f26d415f7c1b39550af2075769f38a91.svg",
            "isPro": false,
            "fullname": "马路",
            "user": "RoadQAQ",
            "type": "user"
          },
          "name": "Lu Ma",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T07:21:11.443Z",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e012",
          "name": "Hao Liang",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e013",
          "name": "Meiyi Qiang",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e014",
          "name": "Lexiang Tang",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e015",
          "name": "Xiaochen Ma",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e016",
          "name": "Zhen Hao Wong",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e017",
          "name": "Junbo Niu",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e018",
          "name": "Chengyu Shen",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e019",
          "name": "Runming He",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e01a",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "6847dce63ec10bdd8ab4e01b",
          "name": "Wentao Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/659e3ea885956d2cccda2b9e/Q_orxNfgmdXXT6I2ahrDs.jpeg"
      ],
      "publishedAt": "2025-06-09T08:11:20.000Z",
      "submittedOnDailyAt": "2025-06-10T05:52:14.746Z",
      "title": "学習ディープリミットでは学べないもの：最難問題に対するインターライブオンライン微調節",
      "submittedOnDailyBy": {
        "_id": "659e3ea885956d2cccda2b9e",
        "avatarUrl": "/avatars/f26d415f7c1b39550af2075769f38a91.svg",
        "isPro": false,
        "fullname": "马路",
        "user": "RoadQAQ",
        "type": "user"
      },
      "summary": "最近の大語言モデル（LLM）の推理の進展は、計画と自己反省などの複雑な行動が強化学習（RL）によって現れることを示しています。しかし、これらの成功に対して、現在の形式でのRLは、基盤モデルの制限を超える能力を引き出すことができないことが明らかになっています。それは、主にモデルの現有知識に基づいて最適化されているため、新しい情報の取得を促すことはできません。この制限を解決するために、私たちは、RLが学ぶことができない部分を学習するために、規範的な微調節（SFT）を用います。これにより、高品質な示唆データを活用して新しい知識と理由のパターンを組み込むことができます。RLとSFTの訓練ダイナミクスを分析し、RLはモデルの元の能力に対する質問の性能の保証と向上に特化し、一方でSFTは現在のモデルの範囲を超えた質問において進歩を促すことが効果的であることを見出しました。RLとSFTの補間的な強みに基づき、新しい訓練アプローチ、ReLIFT（Reinforcement Learning Interleaved with Online Fine-Tuning）を紹介します。ReLIFTでは、主にRLを用いてモデルを訓練し、難しい質問を遭遇した際には、高品質な解決策をセレクトして微調節され、RLと微調節の交替訓練を行い、モデルの理由の能力を向上させます。ReLIFTは、他の零RLモデルと比較して5つのコンペティションレベルベンチマークと1つの分布外ベンチマークで平均的に+5.2点以上の向上を収めました。また、ReLIFTは、詳細な示唆データの13%だけを使用してRLとSFTの両方を超える性能を示し、スケーラビリティを強調します。これらの結果は、ReLIFTがRLの基本的な制限を克服し、その重要なポテンシャルを強調しています。",
      "upvotes": 2,
      "discussionId": "6847dce73ec10bdd8ab4e01c",
      "githubRepo": "https://github.com/TheRoadQaQ/ReLIFT",
      "ai_summary": "ReLIFT, a method combining reinforcement learning and supervised fine-tuning, enhances large language model reasoning by addressing limitations of RL through interleaved training, improving performance across benchmarks with minimal data.",
      "ai_keywords": [
        "reinforcement learning",
        "supervised fine-tuning",
        "ReLIFT",
        "large language model",
        "reasoning",
        "training dynamics",
        "zero-RL models",
        "competition-level benchmarks",
        "out-of-distribution benchmark"
      ]
    },
    "publishedAt": "2025-06-09T04:11:20.000Z",
    "title": "Learning What Reinforcement Learning Can't: Interleaved Online\n  Fine-Tuning for Hardest Questions",
    "summary": "Recent advances in large language model (LLM) reasoning have shown that\nsophisticated behaviors such as planning and self-reflection can emerge through\nreinforcement learning (RL). However, despite these successes, RL in its\ncurrent form remains insufficient to induce capabilities that exceed the\nlimitations of the base model, as it is primarily optimized based on existing\nknowledge of the model rather than facilitating the acquisition of new\ninformation. To address this limitation, we employ supervised fine-tuning (SFT)\nto learn what RL cannot, which enables the incorporation of new knowledge and\nreasoning patterns by leveraging high-quality demonstration data. We analyze\nthe training dynamics of RL and SFT for LLM reasoning and find that RL excels\nat maintaining and improving performance on questions within the model's\noriginal capabilities, while SFT is more effective at enabling progress on\nquestions beyond the current scope of the model. Motivated by the complementary\nstrengths of RL and SFT, we introduce a novel training approach,\nReLIFT (Reinforcement Learning Interleaved\nwith Online Fine-Tuning). In ReLIFT, the model is primarily\ntrained using RL, but when it encounters challenging questions, high-quality\nsolutions are collected for fine-tuning, and the training process alternates\nbetween RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT\nachieves an average improvement of over +5.2 points across five\ncompetition-level benchmarks and one out-of-distribution benchmark compared to\nother zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both\nRL and SFT while using only 13\\% of the detailed demonstration data,\nhighlighting its scalability. These results provide compelling evidence that\nReLIFT overcomes the fundamental limitations of RL and underscores the\nsignificant potential.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/659e3ea885956d2cccda2b9e/Q_orxNfgmdXXT6I2ahrDs.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "659e3ea885956d2cccda2b9e",
      "avatarUrl": "/avatars/f26d415f7c1b39550af2075769f38a91.svg",
      "fullname": "马路",
      "name": "RoadQAQ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07240",
      "authors": [
        {
          "_id": "6847b6513ec10bdd8ab4df49",
          "user": {
            "_id": "600bde0c2b417b1d53669bd0",
            "avatarUrl": "/avatars/2d9704713630e96458368b47179c039c.svg",
            "isPro": false,
            "fullname": "Roy Eisenstadt",
            "user": "royeis",
            "type": "user"
          },
          "name": "Roy Eisenstadt",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T04:36:34.797Z",
          "hidden": false
        },
        {
          "_id": "6847b6513ec10bdd8ab4df4a",
          "name": "Itamar Zimerman",
          "hidden": false
        },
        {
          "_id": "6847b6513ec10bdd8ab4df4b",
          "name": "Lior Wolf",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/TYxLNOb6ac6HH-pR04f7W.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/CVHB0FrpM5va85IVFh2bT.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/c-o1T8-RuSJ222Fj79LDN.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/eKIDa2-ZEMdQdYOimgSpC.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/VI5bYWw7ZDAruk6rqNrMh.jpeg"
      ],
      "publishedAt": "2025-06-08T17:54:33.000Z",
      "submittedOnDailyAt": "2025-06-10T03:10:26.281Z",
      "title": "LLMのオーバークロッキング：思考パスの観測と制御\n\nLLMでの思考パスの長さの観測と制御",
      "submittedOnDailyBy": {
        "_id": "65376feed325b3f02fb92c69",
        "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
        "isPro": false,
        "fullname": "Itamar Zimerman",
        "user": "ItamarZ",
        "type": "user"
      },
      "summary": "最近、明示的構造化の理由論法などの技術が、モデルの内部の「思い」プロセスと最終的な回答との区別を強制して、強力なテストタイムスケーリングバヒノールを示しています。この設定での答えの品質を影響する要因の一つは、理由論の長さです。理由論が過短になると、モデルはタスクの複雑性を捉えられないかもしれません。逆に、過長になると、モデルは過度に考えることになり、必要ない計算を行い、性能が低下することもあります。本論文では、LLMが明示的な思いのプロセス中で理由論の長さを理解し、調節するための基盤的な機構を調査し、利用しています。まず、モデルが理由論プロセスを進めることをエンコードし、その後、プロジェクトバーを可視化し、モデルの計画ダイナミクスについての洞察を提供します。次に、推論時の内部の進捗エンコーディングを操作し、必要ないステップを減らし、より簡潔で決断的な思いの連鎖を生成します。我々の実験結果によると、この「オーバークロック」メソッドは過度に考えることを軽減し、答えの正確性を向上させ、推論時間を短縮することができます。我々のコードは公開的に利用可能です。",
      "upvotes": 2,
      "discussionId": "6847b6513ec10bdd8ab4df4c",
      "projectPage": "https://royeisen.github.io/OverclockingLLMReasoning-paper/",
      "githubRepo": "https://github.com/royeisen/reasoning_loading_bar",
      "ai_summary": "LLMs regulate reasoning length through progress encoding, and manipulating this encoding improves accuracy and reduces inference time.",
      "ai_keywords": [
        "explicit structured reasoning",
        "LLMs",
        "reasoning process",
        "progress bar visualization",
        "progress encoding",
        "inference",
        "overclocking",
        "overthinking",
        "answer accuracy",
        "inference latency"
      ]
    },
    "publishedAt": "2025-06-08T13:54:33.000Z",
    "title": "Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path\n  Lengths in LLMs",
    "summary": "Recently, techniques such as explicit structured reasoning have demonstrated\nstrong test-time scaling behavior by enforcing a separation between the model's\ninternal \"thinking\" process and the final response. A key factor influencing\nanswer quality in this setting is the length of the thinking stage. When the\nreasoning is too short, the model may fail to capture the complexity of the\ntask. Conversely, when it is too long, the model may overthink, leading to\nunnecessary computation and degraded performance. This paper explores and\nexploits the underlying mechanisms by which LLMs understand and regulate the\nlength of their reasoning during explicit thought processes. First, we show\nthat LLMs encode their progress through the reasoning process and introduce an\ninteractive progress bar visualization, which is then used to reveal insights\non the model's planning dynamics. Second, we manipulate the internal progress\nencoding during inference to reduce unnecessary steps and generate a more\nconcise and decisive chain of thoughts. Our empirical results demonstrate that\nthis \"overclocking\" method mitigates overthinking, improves answer accuracy,\nand reduces inference latency. Our code is publicly available.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/TYxLNOb6ac6HH-pR04f7W.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/CVHB0FrpM5va85IVFh2bT.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/c-o1T8-RuSJ222Fj79LDN.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/eKIDa2-ZEMdQdYOimgSpC.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/65376feed325b3f02fb92c69/VI5bYWw7ZDAruk6rqNrMh.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65376feed325b3f02fb92c69",
      "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
      "fullname": "Itamar Zimerman",
      "name": "ItamarZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07160",
      "authors": [
        {
          "_id": "6847c0263ec10bdd8ab4df60",
          "user": {
            "_id": "627b73728b6ecd7ece822825",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
            "isPro": false,
            "fullname": "Yikun Wang",
            "user": "LibraTree",
            "type": "user"
          },
          "name": "Yikun Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:42:47.991Z",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df61",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df62",
          "name": "Dianyi Wang",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df63",
          "name": "Zimian Peng",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df64",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df65",
          "name": "Dacheng Tao",
          "hidden": false
        },
        {
          "_id": "6847c0263ec10bdd8ab4df66",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-08T14:18:15.000Z",
      "submittedOnDailyAt": "2025-06-10T04:04:49.808Z",
      "title": "GeometryZero: グループ対比的な策略最適化を用いたLLMの幾何学解法の向上",
      "submittedOnDailyBy": {
        "_id": "627b73728b6ecd7ece822825",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
        "isPro": false,
        "fullname": "Yikun Wang",
        "user": "LibraTree",
        "type": "user"
      },
      "summary": "最近の大規模言語モデル（LLMs）の進展は、数学的推理など多様な領域で驚異的な能力を示しています。特に、幾何問題解決は、助言の構成が重要な役割を果たしている難しい領域です。現在のアプローチは、最適な性能を達成しないか、または巨大なLLMs（例：GPT-4o）を依存し、計算コストが非常に高い場合があります。私たちは、可証明性のある報酬に基づく強化学習（例：GRPO）が小さなモデルを学習させる可能性を示す有望な方向としています。助言の構成と強固な幾何的推理を統合することができます。しかし、GRPOを直接幾何的推理に適用すると、無条件の報酬に依存するため、基本的な制限があり、対応しないさまざまな助言の構成が発生します。これらの課題を解決するために、Group Contrastive Policy Optimization（GCPO）を提案します。GCPOは、2つのキーのイノベーションを特徴としています：（1）Group Contrastive Masking：コンテキストの役割に基づいて助言の構成に正負の報酬信号を適切に提供します。（2）長さ報酬：長い推理チェーンを促進します。GCPOに基づき、GeometryZeroの家族を開発しました。GeometryZeroは、助言の構成を適切に判断することで、計算コストを抑えながら幾何的推理モデルを提供します。Geometry3K、MathVistaなどのベンチマークでの広範囲的な実験結果により、GeometryZeroモデルは基準（例：GRPO）を超え、全ベンチマークで平均4.29%の改善率を達成しました。",
      "upvotes": 2,
      "discussionId": "6847c0273ec10bdd8ab4df67",
      "ai_summary": "A new reinforcement learning framework, Group Contrastive Policy Optimization (GCPO), enhances geometric reasoning in large language models with judicious auxiliary constructions, outperforming existing methods on benchmarks.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "mathematical reasoning",
        "geometry problem solving",
        "reinforcement learning",
        "verifiable reward",
        "GRPO",
        "Group Contrastive Policy Optimization (GCPO)",
        "Group Contrastive Masking",
        "length reward",
        "GeometryZero",
        "Geometry3K",
        "MathVista"
      ]
    },
    "publishedAt": "2025-06-08T10:18:15.000Z",
    "title": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive\n  Policy Optimization",
    "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities across diverse domains, particularly in mathematical reasoning,\namid which geometry problem solving remains a challenging area where auxiliary\nconstruction plays a enssential role. Existing approaches either achieve\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\nmassive computational costs. We posit that reinforcement learning with\nverifiable reward (e.g., GRPO) offers a promising direction for training\nsmaller models that effectively combine auxiliary construction with robust\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\npresents fundamental limitations due to its dependence on unconditional\nrewards, which leads to indiscriminate and counterproductive auxiliary\nconstructions. To address these challenges, we propose Group Contrastive Policy\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\nor negative reward signals for auxiliary construction based on contextual\nutility, and a (2) length reward that promotes longer reasoning chains.\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\ngeometric reasoning models that judiciously determine when to employ auxiliary\nconstruction. Our extensive empirical evaluation across popular geometric\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\nof 4.29% across all benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07160.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "627b73728b6ecd7ece822825",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
      "fullname": "Yikun Wang",
      "name": "LibraTree",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.03690",
      "authors": [
        {
          "_id": "684664f13ec10bdd8ab4dac0",
          "user": {
            "_id": "64e6c617ecce34cb442cb208",
            "avatarUrl": "/avatars/ebc61bf6a043314cb2089b1efd5e6a18.svg",
            "isPro": false,
            "fullname": "JieSun",
            "user": "Sunshine279",
            "type": "user"
          },
          "name": "Jie Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-09T10:11:19.474Z",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac1",
          "name": "Junkang Wu",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac2",
          "name": "Jiancan Wu",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac3",
          "name": "Zhibo Zhu",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac4",
          "name": "Xingyu Lu",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac5",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac6",
          "name": "Lintao Ma",
          "hidden": false
        },
        {
          "_id": "684664f13ec10bdd8ab4dac7",
          "name": "Xiang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-04T08:19:37.000Z",
      "submittedOnDailyAt": "2025-06-10T00:51:18.490Z",
      "title": "ロバストな好み最適化をダイナミックなターゲットマージンを用いて実現する",
      "submittedOnDailyBy": {
        "_id": "64e6c617ecce34cb442cb208",
        "avatarUrl": "/avatars/ebc61bf6a043314cb2089b1efd5e6a18.svg",
        "isPro": false,
        "fullname": "JieSun",
        "user": "Sunshine279",
        "type": "user"
      },
      "summary": "LLMの調整は実用的なアプリケーションでの安全性と信頼性を確保するために重要です。Direct Preference Optimization (DPO)は、好みペアを使用してモデルを直接最適化するという効率的な方法であり、資源の要求を大幅に減少させています。しかし、DPOの効果性はデータの品質により大きく影響され、これは頻繁にノイズによって破壊されることがあります。本稿では、pairwiseレベルで賞与マージンを動的に調整することを通じて、instance-specific margin calibrationを導入したgamma-PO（ガンマ-PO）の動的な目標マージン好み最適化アルゴリズムを提案します。gamma-POは、高信頼性のペア（プレフェレンスペア間の賞与マージンが高いもの）を戦略的に優先し、不明確なペアからのノイズを抑制することで、DPOのバージョンに対応しています。AlpacaEval2やArena-Hardなどのベンチマークでは、gamma-POは平均4.4%の改善を達成し、最先端の性能の新たなベンチマークを設定します。また、gamma-POは最小限のコード変更が必要で、学習エフィシーンに可視な影響を及ぼしません。LLMの調整を強化するための強固な解決策です。コードは、https://github.com/sunjie279/gammaPO{https://github.com/sunjie279/gammaPO}に公開されています。",
      "upvotes": 2,
      "discussionId": "684664f13ec10bdd8ab4dac8",
      "ai_summary": "The paper introduces γ-PO, a dynamic target margin preference optimization algorithm that enhances Large Language Models' alignment by adjusting reward margins at the pairwise level, leading to improved performance with minimal impact on training.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "Direct Preference Optimization",
        "DPO",
        "preference pairs",
        "γ-PO",
        "instance-specific margin calibration",
        "reward margins",
        "AlpacaEval2",
        "Arena-Hard",
        "state-of-the-art performance"
      ]
    },
    "publishedAt": "2025-06-04T04:19:37.000Z",
    "title": "Robust Preference Optimization via Dynamic Target Margins",
    "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose gamma-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, gamma-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, gamma-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, gamma-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, gamma-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\nhttps://github.com/sunjie279/gammaPO{https://github.com/sunjie279/gammaPO}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03690.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6c617ecce34cb442cb208",
      "avatarUrl": "/avatars/ebc61bf6a043314cb2089b1efd5e6a18.svg",
      "fullname": "JieSun",
      "name": "Sunshine279",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.07803",
      "authors": [
        {
          "_id": "6847d4103ec10bdd8ab4dfb1",
          "user": {
            "_id": "6437d0a951c7ebfc813c735b",
            "avatarUrl": "/avatars/6cbac4e4be5029655702c5d8b9046b90.svg",
            "isPro": false,
            "fullname": "Allakhverdov Eduard",
            "user": "combat-helicopter",
            "type": "user"
          },
          "name": "Eduard Allakhverdov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:42:38.549Z",
          "hidden": false
        },
        {
          "_id": "6847d4103ec10bdd8ab4dfb2",
          "name": "Dmitrii Tarasov",
          "hidden": false
        },
        {
          "_id": "6847d4103ec10bdd8ab4dfb3",
          "name": "Elizaveta Goncharova",
          "hidden": false
        },
        {
          "_id": "6847d4103ec10bdd8ab4dfb4",
          "name": "Andrey Kuznetsov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T14:32:18.000Z",
      "submittedOnDailyAt": "2025-06-10T05:17:41.513Z",
      "title": "画像再構成は特徴分析のためのツール",
      "submittedOnDailyBy": {
        "_id": "6310ff34bc152fa3e810c186",
        "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
        "isPro": false,
        "fullname": "Elizaveta Goncharova",
        "user": "Elizaveta",
        "type": "user"
      },
      "summary": "ビジョンエンコーダーは、ビジョンだけのモデルから視覚言語モデルなどの多モーダルシステムに至るまでの現代アプリケーションで、その使用が増加しています。その驚異的な成功にもかかわらず、これらのアーキテクチャが内部的に特徴を表現する方法は明らかではありません。ここで、私たちは画像再構成を通じた視覚特徴の解釈の新しいアプローチを提案します。SigLIPとSigLIP2の2つの関連したモデル家族を比較し、その学習目的のみが異なり、画像バージョンティングタスクでの事前学習されたエンコーダーが、対比的学習などの非画像バージョンティングタスクで学習されたものよりも显著に多くの画像情報を保持していることを示します。また、この方法を複数の視覚エンコーダーに適用し、その特徴表現の情報量に基づいてソートします。最後に、特徴空間の操作が再構成画像に予測できる変化を示し、正交回転（スペース変換よりも）が色エンコーディングを制御していることを明らかにします。我々のアプローチは、どのビジョンエンコーダーにも適用可能で、その特徴空間の内側構造を解明します。実験を再現するためのコードとモデル重みはGitHubにアクセス可能です。",
      "upvotes": 0,
      "discussionId": "6847d4103ec10bdd8ab4dfb5",
      "projectPage": "https://fusionbrainlab.github.io/feature_analysis/",
      "githubRepo": "https://github.com/FusionBrainLab/feature_analysis",
      "ai_summary": "Image reconstruction reveals that vision encoders retain more image information after image-based tasks and that orthogonal rotations in feature space control color encoding.",
      "ai_keywords": [
        "SigLIP",
        "SigLIP2",
        "vision encoders",
        "image reconstruction",
        "contrastive learning",
        "feature representations"
      ]
    },
    "publishedAt": "2025-06-09T10:32:18.000Z",
    "title": "Image Reconstruction as a Tool for Feature Analysis",
    "summary": "Vision encoders are increasingly used in modern applications, from\nvision-only models to multimodal systems such as vision-language models.\nDespite their remarkable success, it remains unclear how these architectures\nrepresent features internally. Here, we propose a novel approach for\ninterpreting vision features via image reconstruction. We compare two related\nmodel families, SigLIP and SigLIP2, which differ only in their training\nobjective, and show that encoders pre-trained on image-based tasks retain\nsignificantly more image information than those trained on non-image tasks such\nas contrastive learning. We further apply our method to a range of vision\nencoders, ranking them by the informativeness of their feature representations.\nFinally, we demonstrate that manipulating the feature space yields predictable\nchanges in reconstructed images, revealing that orthogonal rotations (rather\nthan spatial transformations) control color encoding. Our approach can be\napplied to any vision encoder, shedding light on the inner structure of its\nfeature space. The code and model weights to reproduce the experiments are\navailable in GitHub.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07803.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310ff34bc152fa3e810c186",
      "avatarUrl": "/avatars/bfd63bcd81548283f5e496e3693bf143.svg",
      "fullname": "Elizaveta Goncharova",
      "name": "Elizaveta",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.07645",
      "authors": [
        {
          "_id": "6847ea583ec10bdd8ab4e05a",
          "user": {
            "_id": "635270e36cfb8f14981312e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635270e36cfb8f14981312e7/FpqjdhABEDuxWR8k2zOwA.jpeg",
            "isPro": false,
            "fullname": "Maciej Chrabąszcz",
            "user": "mchraba",
            "type": "user"
          },
          "name": "Maciej Chrabąszcz",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-10T08:42:31.860Z",
          "hidden": false
        },
        {
          "_id": "6847ea583ec10bdd8ab4e05b",
          "user": {
            "_id": "66dab47f8506f9b6cf5f08ed",
            "avatarUrl": "/avatars/e6ba87adbaacdeccf8c4818596c655d0.svg",
            "isPro": false,
            "fullname": "LLM Attack",
            "user": "llmAttack",
            "type": "user"
          },
          "name": "Katarzyna Lorenc",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T08:18:33.059Z",
          "hidden": false
        },
        {
          "_id": "6847ea583ec10bdd8ab4e05c",
          "name": "Karolina Seweryn",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-09T11:09:39.000Z",
      "submittedOnDailyAt": "2025-06-10T06:49:10.646Z",
      "title": "LLMの資源豊かな言語でのロバスト性評価におけるプロクサーモデルの使用",
      "submittedOnDailyBy": {
        "_id": "635270e36cfb8f14981312e7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635270e36cfb8f14981312e7/FpqjdhABEDuxWR8k2zOwA.jpeg",
        "isPro": false,
        "fullname": "Maciej Chrabąszcz",
        "user": "mchraba",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、近年、多様な自然言語処理（NLP）タスクで驚人的能力を示しています。しかし、それらはジャイルブレイク（jailbreaks）と摂動（perturbations）に脆弱であり、追加的な評価が必要です。多くのLLMsは多言語ですが、安全関連のトレーニングデータは主に英語などの高リソース言語に含まれています。これは、ポーランドなどの低リソース言語の摂動に脆弱であることを示します。私たちは、それらの強力な攻撃が、ただの数の文字を変更し、小さなプロクサーモデルを使用して単語の重要度を計算することで、廉幣で作成できることを示します。これらの文字と単語レベルの攻撃は、異なるLLMsの予測を大きく変化させ、内部の安全機能を回避する潜在的な脆弱性を示していることを示します。私たちは、この攻撃構成手法をポーランド語（低リソース言語）に対して検証し、その脆弱性を示しました。また、これを他の言語に拡張する方法も示しました。私たちは、作成したデータセットとコードを進めるための進展研究に提供します。",
      "upvotes": 0,
      "discussionId": "6847ea583ec10bdd8ab4e05d",
      "ai_summary": "Character and word-level attacks using a proxy model reveal vulnerabilities in LLMs across languages, particularly in low-resource languages like Polish.",
      "ai_keywords": [
        "large language models",
        "natural language processing",
        "jailbreaks",
        "perturbations",
        "multilingual",
        "safety-related training data",
        "high-resource languages",
        "low-resource languages",
        "character-level attacks",
        "word-level attacks",
        "word importance calculation",
        "internal safety mechanisms"
      ]
    },
    "publishedAt": "2025-06-09T07:09:39.000Z",
    "title": "Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models",
    "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\nvarious natural language processing (NLP) tasks in recent years. However, their\nsusceptibility to jailbreaks and perturbations necessitates additional\nevaluations. Many LLMs are multilingual, but safety-related training data\ncontains mainly high-resource languages like English. This can leave them\nvulnerable to perturbations in low-resource languages such as Polish. We show\nhow surprisingly strong attacks can be cheaply created by altering just a few\ncharacters and using a small proxy model for word importance calculation. We\nfind that these character and word-level attacks drastically alter the\npredictions of different LLMs, suggesting a potential vulnerability that can be\nused to circumvent their internal safety mechanisms. We validate our attack\nconstruction methodology on Polish, a low-resource language, and find potential\nvulnerabilities of LLMs in this language. Additionally, we show how it can be\nextended to other languages. We release the created datasets and code for\nfurther research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07645.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635270e36cfb8f14981312e7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635270e36cfb8f14981312e7/FpqjdhABEDuxWR8k2zOwA.jpeg",
      "fullname": "Maciej Chrabąszcz",
      "name": "mchraba",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.05904",
      "authors": [
        {
          "_id": "6847e05a3ec10bdd8ab4e03d",
          "user": {
            "_id": "6369b1d456d1f93498130a8a",
            "avatarUrl": "/avatars/8ec228aa6f171715652511f948765db9.svg",
            "isPro": false,
            "fullname": "Yichi Zhang",
            "user": "594zyc",
            "type": "user"
          },
          "name": "Yichi Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T07:35:55.259Z",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e03e",
          "name": "Xin Luna Dong",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e03f",
          "name": "Zhaojiang Lin",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e040",
          "name": "Andrea Madotto",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e041",
          "name": "Anuj Kumar",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e042",
          "name": "Babak Damavandi",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e043",
          "name": "Joyce Chai",
          "hidden": false
        },
        {
          "_id": "6847e05a3ec10bdd8ab4e044",
          "name": "Seungwhan Moon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-06T09:23:29.000Z",
      "submittedOnDailyAt": "2025-06-10T06:07:28.858Z",
      "title": "主観的ビデオからの動作的アシスタントダイアロジー生成",
      "submittedOnDailyBy": {
        "_id": "6369b1d456d1f93498130a8a",
        "avatarUrl": "/avatars/8ec228aa6f171715652511f948765db9.svg",
        "isPro": false,
        "fullname": "Yichi Zhang",
        "user": "594zyc",
        "type": "user"
      },
      "summary": "最近の会話AIの進展は相当大きく、しかし、視覚的な仕事のガイドに対する実時間システムの開発は難しい。これらのシステムは、ストリーミング的な可視入力に基づいて相互作用的、主動的な助言を提供する必要がありますが、その開発はデータの収集とシステムの評価の高額および労働費用の高いプロセスによって制限されています。これらの制限を解決するために、私たちは3つの主な貢献を挙げるコンピューターベースのフレームワークを提出します。1つ目に、私たちは新しいデータカレーティブピープラインを導入し、ディレクトショットからのディアロギーディアロギーを合成し、データセットを生成します。このデータセットは、複数のドメインを横断し、大規模な合成ディアロギーデータセットです。2つ目に、私たちは様々な人間研究を通じて検証された自動評価メトリックシュートを開発します。3つ目に、私たちは流れ動くビデオ入力を処理し、コンテキストに適切な返事を生成する端末から端末までのモデルを提案します。このモデルは、新しい手法を使用してデータの不均衡と長時間のビデオを処理することができます。この研究は、多様なタスクを通じてユーザーをガイドする能力を持つ実時間的、主動的なAIアシスタントの開発に基盤を築くことを示します。プロジェクトページ：https://pro-assist.github.io/",
      "upvotes": 0,
      "discussionId": "6847e05a3ec10bdd8ab4e045",
      "projectPage": "https://pro-assist.github.io/",
      "ai_summary": "A framework provides automated data synthesis, evaluation metrics, and an end-to-end model for real-time, proactive conversational AI task guidance using streaming video inputs.",
      "ai_keywords": [
        "data curation pipeline",
        "synthetic dialogue dataset",
        "automatic evaluation metrics",
        "end-to-end model",
        "data imbalance",
        "long-duration videos"
      ]
    },
    "publishedAt": "2025-06-06T05:23:29.000Z",
    "title": "Proactive Assistant Dialogue Generation from Streaming Egocentric Videos",
    "summary": "Recent advances in conversational AI have been substantial, but developing\nreal-time systems for perceptual task guidance remains challenging. These\nsystems must provide interactive, proactive assistance based on streaming\nvisual inputs, yet their development is constrained by the costly and\nlabor-intensive process of data collection and system evaluation. To address\nthese limitations, we present a comprehensive framework with three key\ncontributions. First, we introduce a novel data curation pipeline that\nsynthesizes dialogues from annotated egocentric videos, resulting in \\dataset,\na large-scale synthetic dialogue dataset spanning multiple domains. Second, we\ndevelop a suite of automatic evaluation metrics, validated through extensive\nhuman studies. Third, we propose an end-to-end model that processes streaming\nvideo inputs to generate contextually appropriate responses, incorporating\nnovel techniques for handling data imbalance and long-duration videos. This\nwork lays the foundation for developing real-time, proactive AI assistants\ncapable of guiding users through diverse tasks. Project page:\nhttps://pro-assist.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05904.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6369b1d456d1f93498130a8a",
      "avatarUrl": "/avatars/8ec228aa6f171715652511f948765db9.svg",
      "fullname": "Yichi Zhang",
      "name": "594zyc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.04807",
      "authors": [
        {
          "_id": "6847c0983ec10bdd8ab4df69",
          "user": {
            "_id": "65fba5700b78c48c9e393a3e",
            "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
            "isPro": false,
            "fullname": "Yuyi Zhang",
            "user": "ZZXF",
            "type": "user"
          },
          "name": "Yuyi Zhang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-10T05:42:47.055Z",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6a",
          "user": {
            "_id": "6616c9e090d2013d26a54b47",
            "avatarUrl": "/avatars/573064303dcdcf778e1fbbfcff3c9a2b.svg",
            "isPro": false,
            "fullname": "Shi",
            "user": "shiyx1",
            "type": "user"
          },
          "name": "Yongxin Shi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T05:20:28.050Z",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6b",
          "name": "Peirong Zhang",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6c",
          "name": "Yixin Zhao",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6d",
          "name": "Zhenhua Yang",
          "hidden": false
        },
        {
          "_id": "6847c0983ec10bdd8ab4df6e",
          "user": {
            "_id": "66a102960072f5db18e860e3",
            "avatarUrl": "/avatars/7679eddb31153c6b868cf496833551d6.svg",
            "isPro": false,
            "fullname": "Lianwen Jin",
            "user": "lianwen",
            "type": "user"
          },
          "name": "Lianwen Jin",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-10T05:20:28.050Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-05T09:33:06.000Z",
      "submittedOnDailyAt": "2025-06-10T04:16:00.729Z",
      "title": "メガハン97K: メガカテゴリー中国文字認識のための大規模データセット（97Kカテゴリ以上）",
      "submittedOnDailyBy": {
        "_id": "65fba5700b78c48c9e393a3e",
        "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
        "isPro": false,
        "fullname": "Yuyi Zhang",
        "user": "ZZXF",
        "type": "user"
      },
      "summary": "中国語と文化の基礎となる中国文字は、非常に広くてずれていく分類を含む、最新の中国GB18030-2022標準は87,887カテゴリを含む。この巨大な文字の正確な認識、ジャンヌカテゴリ認識として呼ばれるものは、文化財の保存とデジタルアプリケーションにとって大きな課題である。光学文字認識（OCR）において進歩があるにも関わらず、ジャンヌカテゴリ認識は、詳細なデータセットのないため、まだ探索されていない。この重要な空間を埋めるために、ジャンヌカテゴリ、大規模なデータセットMegaHan97Kを紹介する。私たちの研究は、3つの主な貢献を提供している：1）MegaHan97Kは、最新のGB18030-2022標準を完全にサポートし、現在のデータセットより少なくとも6倍以上のカテゴリを含むデータセットの初めてのものである；2）長尾分布問題を有効に解決し、手書き、歴史的、合成の3つの異なるサブセットを通じて全カテゴリにおいてバランスのあるサンプルを提供する；3）ジャンヌカテゴリの場合に新しい課題を明らかにし、収蓄負担の増加、形態的に類似な文字の認識、ゼロショット学習の難題を含むものを示し、将来の研究の大きな機会を開発する。私たちの知識の限り、MegaHan97KはOCR領域ではなくも、パターン認識の広い領域でも最大のクラスのデータセットであることは予想される。データセットは、https://github.com/SCUT-DLVCLab/MegaHan97Kから利用できる。",
      "upvotes": 0,
      "discussionId": "6847c0993ec10bdd8ab4df6f",
      "ai_summary": "MegaHan97K, a large-scale dataset for recognizing over 97,000 Chinese characters, addresses the long-tail distribution problem and reveals new challenges in mega-category OCR.",
      "ai_keywords": [
        "Optical Character Recognition (OCR)",
        "mega-category recognition",
        "MegaHan97K",
        "long-tail distribution",
        "zero-shot learning"
      ]
    },
    "publishedAt": "2025-06-05T05:33:06.000Z",
    "title": "MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character\n  Recognition with over 97K Categories",
    "summary": "Foundational to the Chinese language and culture, Chinese characters\nencompass extraordinarily extensive and ever-expanding categories, with the\nlatest Chinese GB18030-2022 standard containing 87,887 categories. The accurate\nrecognition of this vast number of characters, termed mega-category\nrecognition, presents a formidable yet crucial challenge for cultural heritage\npreservation and digital applications. Despite significant advances in Optical\nCharacter Recognition (OCR), mega-category recognition remains unexplored due\nto the absence of comprehensive datasets, with the largest existing dataset\ncontaining merely 16,151 categories. To bridge this critical gap, we introduce\nMegaHan97K, a mega-category, large-scale dataset covering an unprecedented\n97,455 categories of Chinese characters. Our work offers three major\ncontributions: (1) MegaHan97K is the first dataset to fully support the latest\nGB18030-2022 standard, providing at least six times more categories than\nexisting datasets; (2) It effectively addresses the long-tail distribution\nproblem by providing balanced samples across all categories through its three\ndistinct subsets: handwritten, historical and synthetic subsets; (3)\nComprehensive benchmarking experiments reveal new challenges in mega-category\nscenarios, including increased storage demands, morphologically similar\ncharacter recognition, and zero-shot learning difficulties, while also\nunlocking substantial opportunities for future research. To the best of our\nknowledge, the MetaHan97K is likely the dataset with the largest classes not\nonly in the field of OCR but may also in the broader domain of pattern\nrecognition. The dataset is available at\nhttps://github.com/SCUT-DLVCLab/MegaHan97K.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04807.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65fba5700b78c48c9e393a3e",
      "avatarUrl": "/avatars/795cc8167460d8e89ff91d27c5da9fb2.svg",
      "fullname": "Yuyi Zhang",
      "name": "ZZXF",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.23473",
      "authors": [
        {
          "_id": "6847c9693ec10bdd8ab4df91",
          "name": "Xiaorui Wu",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df92",
          "name": "Xiaofeng Mao",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df93",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df94",
          "name": "Fei Li",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df95",
          "name": "Chong Teng",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df96",
          "name": "Yuxiang Peng",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df97",
          "name": "Li Zheng",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df98",
          "name": "Donghong Ji",
          "hidden": false
        },
        {
          "_id": "6847c9693ec10bdd8ab4df99",
          "name": "Zhuang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-29T14:26:46.000Z",
      "submittedOnDailyAt": "2025-06-10T04:28:59.228Z",
      "title": "EVOREFUSE: 進化的プロンプト最適化による評価と対策におけるLLMの過度拒否対策",
      "submittedOnDailyBy": {
        "_id": "63d159132036e44c44f87a91",
        "avatarUrl": "/avatars/6a2c9e5b3b25cf1949277d8c40c0070b.svg",
        "isPro": false,
        "fullname": "Zhuang Li",
        "user": "lizhuang144",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、フェイズマリシャスインストラクションに対して頻繁に拒否することがあります：意味的に無害な入力クエリが、保守的な安全性アラインメントによって不必要なLLM拒否を引き起こすことで、ユーザー経験を大きく損なっています。これらのインストラクションの集約は、過度な拒否を評価し、軽減するために重要ですが、現在のインストラクションカリシャション方法、手動作成やインストラクション改変様だけでは、スケーラビリティがないか、十分な多様性と有効な拒否引き起こしのプロンプトを生成できないことがあります。これらの制限を解決するために、私たちはEVOREFUSEを紹介します。EVOREFUSEは、多様なフェイズマリシャスインストラクションを一貫的に発生させるプロンプト最適化アプローチです。EVOREFUSEは、現在の方法よりもより多様な方向でインストラクションスペースを探索するために、変異戦略と再結合を用いて、LLMsでもっとも信頼性の高い拒否を引き起こすことを目指して、シードインストラクションを進化的に進化させ、LLM拒否確率の証拠下限を最大化します。EVOREFUSEを使用して、私たちは2つの新しいデータセットを作成しました：EVOREFUSE-TESTは、582件のフェイズマリシャスインストラクションのベンチマークで、9つのLLMsでの平均拒否引き起こし率が140.41%高く、辞書的な多様性が34.86%高く、LLMの拒否確信スコアが40.03%上昇し、次のベストのベンチマークを超えます。EVOREFUSE-ALIGNは、3,000件のフェイズマリシャスインストラクションと応答を提供し、サーベィス訓練と好みベースのアラインメント訓練に使用できます。LLAMA3.1-8B-INSTRUCTは、EVOREFUSE-ALIGNによる規制された訓練で、2番目のベストのアラインメントデータセットによるモデルと比べて、14.31%少なくなる過度な拒否を軽減します。EVOREFUSE-TESTによる分析では、モデルは敏感なキーワードを過度に焦点を当て、ブロードなコンテキストを無視して、過度な拒否を引き起こしていることが明らかになりました。",
      "upvotes": 0,
      "discussionId": "6847c9693ec10bdd8ab4df9a",
      "ai_summary": "EVOREFUSE, an evolutionary algorithm, generates diverse pseudo-malicious instructions to optimize LLM refusal training, improving user experience without compromising safety.",
      "ai_keywords": [
        "large language models",
        "pseudo-malicious instructions",
        "safety alignment",
        "instruction optimization",
        "evolutionary algorithm",
        "mutation strategies",
        "recombination",
        "evidence lower bound",
        "refusal probability",
        "lexical diversity",
        "LLM response confidence scores",
        "over-refusals",
        "supervised fine-tuning",
        "preference-based alignment training"
      ]
    },
    "publishedAt": "2025-05-29T10:26:46.000Z",
    "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions",
    "summary": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious\ninstructions: semantically harmless input queries triggering unnecessary LLM\nrefusals due to conservative safety alignment, significantly impairing user\nexperience. Collecting such instructions is crucial for evaluating and\nmitigating over-refusals, but existing instruction curation methods, like\nmanual creation or instruction rewriting, either lack scalability or fail to\nproduce sufficiently diverse and effective refusal-inducing prompts. To address\nthese limitations, we introduce EVOREFUSE, a prompt optimization approach that\ngenerates diverse pseudo-malicious instructions consistently eliciting\nconfident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm\nexploring the instruction space in more diverse directions than existing\nmethods via mutation strategies and recombination, and iteratively evolves seed\ninstructions to maximize evidence lower bound on LLM refusal probability. Using\nEVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582\npseudo-malicious instructions that outperforms the next-best benchmark with\n140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater\nlexical diversity, and 40.03% improved LLM response confidence scores; and\nEVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with\nresponses for supervised and preference-based alignment training.\nLLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to\n14.31% fewer over-refusals than models trained on the second-best alignment\ndataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals\nmodels trigger over-refusals by overly focusing on sensitive keywords while\nignoring broader context.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23473.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d159132036e44c44f87a91",
      "avatarUrl": "/avatars/6a2c9e5b3b25cf1949277d8c40c0070b.svg",
      "fullname": "Zhuang Li",
      "name": "lizhuang144",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  }
]