[
  {
    "paper": {
      "id": "2507.07095",
      "authors": [
        {
          "_id": "686f2579d938c25d68441b43",
          "name": "Ke Fan",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b44",
          "name": "Shunlin Lu",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b45",
          "user": {
            "_id": "6853b71ec1be83a29eb5ba36",
            "avatarUrl": "/avatars/ae205c2ec2c421a0d7851755b4f123a2.svg",
            "isPro": false,
            "fullname": "Minyue Dai",
            "user": "Jixi111",
            "type": "user"
          },
          "name": "Minyue Dai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:19.845Z",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b46",
          "name": "Runyi Yu",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b47",
          "name": "Lixing Xiao",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b48",
          "name": "Zhiyang Dou",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b49",
          "name": "Junting Dong",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b4a",
          "name": "Lizhuang Ma",
          "hidden": false
        },
        {
          "_id": "686f2579d938c25d68441b4b",
          "name": "Jingbo Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T17:52:04.000Z",
      "submittedOnDailyAt": "2025-07-10T01:12:54.854Z",
      "title": "Go to Zero: マイナスシェイクの動作生成に向けて（百万スケールデータを用いる）",
      "submittedOnDailyBy": {
        "_id": "66d59dc9b005ad82ca6fc61d",
        "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
        "isPro": false,
        "fullname": "Runyi YU",
        "user": "IngridYU",
        "type": "user"
      },
      "summary": "テキストに基づいた多様な自然な人間の動きシーケンスを生成することは、コンピュータビジョン、グラフィック、ロボティクスの領域で基本的で難しい研究分野です。この分野では進歩がありますが、現在の手法は、学習データセットのサイズが限られているためのゼロショットの一般化能力に関する課題を多く持っています。また、評価フレームワークの欠如が、改良の方向づけを行うことができなくて、この課題の進歩を妨げています。本稿では、テキストから動きを生成することを新しい時代に引き込み、ゼロショットの一般化能力を達成することを目指します。そのために、最初に、効率的なアノテーションパイプラインを開発し、現在まで最大の人間の動きデータセットであるMotionMillionを紹介します。このデータセットは、2,000時間以上の、200万ページの高品質の動きシーケンスを特徴に持っています。また、MotionMillion-Evalを提案します。これは、ゼロショットの動き生成を評価するための最も幅広いベンチマークです。スケーラブルなアーキテクチャを活用し、モデルを7Bパラメータにスケール化し、MotionMillion-Evalでの性能を検証します。この結果は、領域外と複雑な構成の動きに強い一般化能力を示し、ゼロショットの人間の動き生成に向けて重要なステップを記録します。コードは、https://github.com/VankouF/MotionMillion-Codes にアクセスできます。",
      "upvotes": 32,
      "discussionId": "686f2579d938c25d68441b4c",
      "ai_summary": "A new dataset and evaluation framework improve zero-shot text-to-motion generation through a large-scale, high-quality dataset and a scalable model architecture.",
      "ai_keywords": [
        "MotionMillion",
        "MotionMillion-Eval",
        "zero-shot motion generation",
        "scalable architecture"
      ]
    },
    "publishedAt": "2025-07-09T13:52:04.000Z",
    "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data",
    "summary": "Generating diverse and natural human motion sequences based on textual\ndescriptions constitutes a fundamental and challenging research area within the\ndomains of computer vision, graphics, and robotics. Despite significant\nadvancements in this field, current methodologies often face challenges\nregarding zero-shot generalization capabilities, largely attributable to the\nlimited size of training datasets. Moreover, the lack of a comprehensive\nevaluation framework impedes the advancement of this task by failing to\nidentify directions for improvement. In this work, we aim to push\ntext-to-motion into a new era, that is, to achieve the generalization ability\nof zero-shot. To this end, firstly, we develop an efficient annotation pipeline\nand introduce MotionMillion-the largest human motion dataset to date, featuring\nover 2,000 hours and 2 million high-quality motion sequences. Additionally, we\npropose MotionMillion-Eval, the most comprehensive benchmark for evaluating\nzero-shot motion generation. Leveraging a scalable architecture, we scale our\nmodel to 7B parameters and validate its performance on MotionMillion-Eval. Our\nresults demonstrate strong generalization to out-of-domain and complex\ncompositional motions, marking a significant step toward zero-shot human motion\ngeneration. The code is available at\nhttps://github.com/VankouF/MotionMillion-Codes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07095.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66d59dc9b005ad82ca6fc61d",
      "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
      "fullname": "Runyi YU",
      "name": "IngridYU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06448",
      "authors": [
        {
          "_id": "686f326dd938c25d68441b6a",
          "name": "Zhenhailong Wang",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6b",
          "name": "Xuehang Guo",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6c",
          "name": "Sofia Stoica",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6d",
          "user": {
            "_id": "645b10e80c73ea27d13f7aca",
            "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
            "isPro": false,
            "fullname": "xuhaiyang",
            "user": "xhyandwyy",
            "type": "user"
          },
          "name": "Haiyang Xu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:17.834Z",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6e",
          "name": "Hongru Wang",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b6f",
          "name": "Hyeonjeong Ha",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b70",
          "name": "Xiusi Chen",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b71",
          "name": "Yangyi Chen",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b72",
          "name": "Ming Yan",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b73",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "686f326dd938c25d68441b74",
          "name": "Heng Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T23:22:34.000Z",
      "submittedOnDailyAt": "2025-07-10T01:56:27.555Z",
      "title": "Perception-Aware Policy Optimization for Multimodal Reasoning",
      "submittedOnDailyBy": {
        "_id": "628d7265db4cd1d1717c884f",
        "avatarUrl": "/avatars/dff2a3dd10d84b4a73fa486402de7219.svg",
        "isPro": false,
        "fullname": "Zhenhailong Wang",
        "user": "mikewang",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR)は、Large Language Models (LLMs)に強力的な多ステップ推理能力をもたらすために非常に効果的な戦略であることが証明されています。しかし、その設計と最適化は、テキストドメインに限定されているため、多モデル推理タスクに適用される際には、性能が最適ではありません。特に、現在の多モデル推理での誤りの主な原因は、可視入力の観察であることが見出されています。このブロックを解決するために、Perception-Aware Policy Optimization (PAPO)を提案します。PAPOは、GRPOの簡単で効果的な拡張で、モデルが理由を学ぶことを促し、内部のサブプライズ信号から観察を学ぶことを奨励します。特に、PAPOは追加データカレーレーション、外部報酬モデル、または所有権モデルに依存しません。具体的には、GRPOのオブジェクティブにImplicit Perception LossとしてKL分散項を追加し、それは簡単であるが、多様な多モデルベンチマーク上では显著な全体的な改善（4.4%）を収めます。この改善は、視覚依存性の高いタスクではより明らかに（8.0%）見られます。また、PAPOにより観察誤りが大幅に減少（30.5%）し、観察能力が向上していることがわかります。PAPOについての詳細な分析を行い、Double Entropy Lossを用いて厳密に分析し、軽減しました。まとめて、我々の研究は、RLVRの学習オブジェクティブに観察意識の深い統合を導入し、視覚に基づく理由を促す新しいRLフレームワークの基盤を筑みます。プロジェクトページ：https://mikewangwzhl.github.io/PAPO.",
      "upvotes": 23,
      "discussionId": "686f326dd938c25d68441b75",
      "projectPage": "https://mikewangwzhl.github.io/PAPO",
      "githubRepo": "https://github.com/MikeWangWZHL/PAPO",
      "ai_summary": "Perception-Aware Policy Optimization (PAPO) enhances reinforcement learning with verifiable rewards for multimodal reasoning by integrating implicit perception loss, improving visual perception and reasoning.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "Large Language Models (LLMs)",
        "multimodal reasoning tasks",
        "Perception-Aware Policy Optimization (PAPO)",
        "GRPO",
        "Implicit Perception Loss",
        "KL divergence",
        "Double Entropy Loss",
        "visually grounded reasoning"
      ],
      "githubStars": 10
    },
    "publishedAt": "2025-07-08T19:22:34.000Z",
    "title": "Perception-Aware Policy Optimization for Multimodal Reasoning",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose Perception-Aware Policy\nOptimization (PAPO), a simple yet effective extension of GRPO that encourages\nthe model to learn to perceive while learning to reason, entirely from internal\nsupervision signals. Notably, PAPO does not rely on additional data curation,\nexternal reward models, or proprietary models. Specifically, we introduce the\nImplicit Perception Loss in the form of a KL divergence term to the GRPO\nobjective, which, despite its simplicity, yields significant overall\nimprovements (4.4%) on diverse multimodal benchmarks. The improvements are more\npronounced, approaching 8.0%, on tasks with high vision dependency. We also\nobserve a substantial reduction (30.5%) in perception errors, indicating\nimproved perceptual capabilities with PAPO. We conduct comprehensive analysis\nof PAPO and identify a unique loss hacking issue, which we rigorously analyze\nand mitigate through a Double Entropy Loss. Overall, our work introduces a\ndeeper integration of perception-aware supervision into RLVR learning\nobjectives and lays the groundwork for a new RL framework that encourages\nvisually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06448.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "628d7265db4cd1d1717c884f",
      "avatarUrl": "/avatars/dff2a3dd10d84b4a73fa486402de7219.svg",
      "fullname": "Zhenhailong Wang",
      "name": "mikewang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06920",
      "authors": [
        {
          "_id": "686f1da1d938c25d68441b1b",
          "user": {
            "_id": "677e869467f3bb8d8215eec6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
            "isPro": false,
            "fullname": "Zihan Ma",
            "user": "MichaelErchi",
            "type": "user"
          },
          "name": "Zihan Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:33.413Z",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b1c",
          "name": "Taolin Zhang",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b1d",
          "name": "Maosong Cao",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b1e",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b1f",
          "name": "Minnan Luo",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b20",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "686f1da1d938c25d68441b21",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T14:58:47.000Z",
      "submittedOnDailyAt": "2025-07-10T00:31:10.862Z",
      "title": "リティディング・バリデーションを考え直してLLMコード生成：生成からテストへ",
      "submittedOnDailyBy": {
        "_id": "677e869467f3bb8d8215eec6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
        "isPro": false,
        "fullname": "Zihan Ma",
        "user": "MichaelErchi",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、HumanEvalやLiveCodeBenchなどのコード生成ベンチマークで最近に顕著な成功を収めた。しかし、詳細な調査によると、これらの評価システムは通常、限られた数の同質なテストケースを含んでいて、わずかな誤りが検出されないことが明らかになる。これは、測定された性能を人工的に上昇させ、可視化可能な報酬を利用する強化学習フレームワーク（RLVR）での正確な報酬評価を破壊することも含む。これらの重要な欠点を解決するために、テストケース生成（TCG）タスクを系統的に調査するために、詳細なテストシステムの詳細を厳密に定量化するための多様なメトリックを提案し、人間のプログラミング知識とLLMの理由論能力を組み合わせた人間-LLM協力方法（SAGA）を導入し、生成されるテストケースの覆範と質を大幅に向上させることを目的としている。また、TCGBenchを開発し、TCGタスクの研究を支援する。実験結果によると、SAGAはTCGBenchでの検出率は90.62%、バリデータの正確性は32.58%である。SAGAが合成したコード生成評価ベンチマークのVerifier Accuracy（Verifier Acc）はLiveCodeBench-v6より10.78%高い。これらの結果は、我々の提案方法の効果を示している。我々は、この研究が、信頼性のあるLLMコード評価のスケーラブルな基盤の構築に貢献し、コード生成のRLVRの進展を促進し、自動化された対抗的テスト合成と適応的ベンチマーク統合のための道を開けることを望んでいる。",
      "upvotes": 18,
      "discussionId": "686f1da1d938c25d68441b22",
      "ai_summary": "A human-LLM collaborative method enhances code generation test case generation, improving reliability and detection rates in code evaluation benchmarks.",
      "ai_keywords": [
        "large language models",
        "code-generation",
        "HumanEval",
        "LiveCodeBench",
        "test-case generation",
        "multi-dimensional metrics",
        "human-LLM collaboration",
        "SAGA",
        "TCGBench",
        "reinforcement learning frameworks",
        "verifiable rewards",
        "RLVR",
        "verifier accuracy",
        "adversarial test synthesis",
        "adaptive benchmark integration"
      ]
    },
    "publishedAt": "2025-07-09T10:58:47.000Z",
    "title": "Rethinking Verification for LLM Code Generation: From Generation to\n  Testing",
    "summary": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06920.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "677e869467f3bb8d8215eec6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
      "fullname": "Zihan Ma",
      "name": "MichaelErchi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07105",
      "authors": [
        {
          "_id": "686f5cbad938c25d68441bb2",
          "user": {
            "_id": "643e9efa2263cdc630f88f5c",
            "avatarUrl": "/avatars/96cea51f17e7d41ffb6a4b438e05f5cb.svg",
            "isPro": false,
            "fullname": "Yushen Zuo",
            "user": "YSZuo",
            "type": "user"
          },
          "name": "Yushen Zuo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T09:12:43.813Z",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb3",
          "name": "Qi Zheng",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb4",
          "name": "Mingyang Wu",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb5",
          "name": "Xinrui Jiang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb6",
          "name": "Renjie Li",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb7",
          "name": "Jian Wang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb8",
          "name": "Yide Zhang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bb9",
          "name": "Gengchen Mai",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bba",
          "name": "Lihong V. Wang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bbb",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bbc",
          "name": "Xiaoyu Wang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bbd",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        },
        {
          "_id": "686f5cbad938c25d68441bbe",
          "user": {
            "_id": "62548d5fef3debb2ddf91217",
            "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
            "isPro": false,
            "fullname": "Zhengzhong Tu",
            "user": "vztu",
            "type": "user"
          },
          "name": "Zhengzhong Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T09:12:41.972Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ESnx_PS3_HUQoNDY5cSqI.png"
      ],
      "publishedAt": "2025-07-09T17:59:19.000Z",
      "submittedOnDailyAt": "2025-07-10T04:56:36.746Z",
      "title": "4KAgent: アィグメーションから4K超解像度のアジェント",
      "submittedOnDailyBy": {
        "_id": "62548d5fef3debb2ddf91217",
        "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
        "isPro": false,
        "fullname": "Zhengzhong Tu",
        "user": "vztu",
        "type": "user"
      },
      "summary": "We present 4KAgent, a unified agentic super-resolution generalist system designed to universally upscale any image to 4K resolution (and even higher, if applied iteratively). Our system can transform images from extremely low resolutions with severe degradations, for example, highly distorted inputs at 256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three core components: (1) Profiling, a module that customizes the 4KAgent pipeline based on bespoke use cases; (2) A Perception Agent, which leverages vision-language models alongside image quality assessment experts to analyze the input image and make a tailored restoration plan; and (3) A Restoration Agent, which executes the plan, following a recursive execution-reflection paradigm, guided by a quality-driven mixture-of-expert policy to select the optimal output for each step. Additionally, 4KAgent embeds a specialized face restoration pipeline, significantly enhancing facial details in portrait and selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task categories encompassing a total of 26 diverse benchmarks, setting new state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover natural images, portrait photos, AI-generated content, satellite imagery, fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and X-ray, demonstrating superior performance in terms of both perceptual (e.g., NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic paradigm for low-level vision tasks, we aim to catalyze broader interest and innovation within vision-centric autonomous agents across diverse research communities. We will release all the code, models, and results at: https://4kagent.github.io.",
      "upvotes": 15,
      "discussionId": "686f5cbbd938c25d68441bbf",
      "projectPage": "https://4kagent.github.io/",
      "githubRepo": "https://github.com/taco-group/4KAgent",
      "ai_summary": "4KAgent, a unified agentic super-resolution system, enhances low-resolution images to 4K using profiling, perception, and restoration agents, achieving state-of-the-art performance across various imaging domains.",
      "ai_keywords": [
        "agentic super-resolution",
        "Profiling",
        "Perception Agent",
        "vision-language models",
        "image quality assessment",
        "Restoration Agent",
        "recursive execution-reflection",
        "quality-driven mixture-of-experts",
        "face restoration pipeline",
        "NIQE",
        "MUSIQ",
        "PSNR",
        "low-level vision tasks",
        "autonomous agents"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-07-09T13:59:19.000Z",
    "title": "4KAgent: Agentic Any Image to 4K Super-Resolution",
    "summary": "We present 4KAgent, a unified agentic super-resolution generalist system\ndesigned to universally upscale any image to 4K resolution (and even higher, if\napplied iteratively). Our system can transform images from extremely low\nresolutions with severe degradations, for example, highly distorted inputs at\n256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three\ncore components: (1) Profiling, a module that customizes the 4KAgent pipeline\nbased on bespoke use cases; (2) A Perception Agent, which leverages\nvision-language models alongside image quality assessment experts to analyze\nthe input image and make a tailored restoration plan; and (3) A Restoration\nAgent, which executes the plan, following a recursive execution-reflection\nparadigm, guided by a quality-driven mixture-of-expert policy to select the\noptimal output for each step. Additionally, 4KAgent embeds a specialized face\nrestoration pipeline, significantly enhancing facial details in portrait and\nselfie photos. We rigorously evaluate our 4KAgent across 11 distinct task\ncategories encompassing a total of 26 diverse benchmarks, setting new\nstate-of-the-art on a broad spectrum of imaging domains. Our evaluations cover\nnatural images, portrait photos, AI-generated content, satellite imagery,\nfluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and\nX-ray, demonstrating superior performance in terms of both perceptual (e.g.,\nNIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic\nparadigm for low-level vision tasks, we aim to catalyze broader interest and\ninnovation within vision-centric autonomous agents across diverse research\ncommunities. We will release all the code, models, and results at:\nhttps://4kagent.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ESnx_PS3_HUQoNDY5cSqI.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07105.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62548d5fef3debb2ddf91217",
      "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
      "fullname": "Zhengzhong Tu",
      "name": "vztu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.06457",
      "authors": [
        {
          "_id": "686f2371d938c25d68441b36",
          "name": "Dustin Wang",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b37",
          "user": {
            "_id": "63ff09f24852102d4871c19c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
            "isPro": false,
            "fullname": "Rui-Jie Zhu",
            "user": "ridger",
            "type": "user"
          },
          "name": "Rui-Jie Zhu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:30.737Z",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b38",
          "name": "Steven Abreu",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b39",
          "name": "Yong Shan",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3a",
          "name": "Taylor Kergan",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3b",
          "name": "Yuqi Pan",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3c",
          "name": "Yuhong Chou",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3d",
          "name": "Zheng Li",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3e",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b3f",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "686f2371d938c25d68441b40",
          "name": "Jason Eshraghian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T23:54:11.000Z",
      "submittedOnDailyAt": "2025-07-10T02:04:35.136Z",
      "title": "ハイブリッド線形アテンションのシステム的な分析",
      "submittedOnDailyBy": {
        "_id": "63ff09f24852102d4871c19c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
        "isPro": false,
        "fullname": "Rui-Jie Zhu",
        "user": "ridger",
        "type": "user"
      },
      "summary": "Transformersは長いシーケンスに対して二次的複雑性とメモリ問題があり、固定サイズの隠れ状態を使用して線形アテンション機構を採用することにより解決されます。しかし、線形モデルは通常記憶性能が限られ、線形と全アテンション層を組み合わせたハイブリッドアーキテクチャが導入されます。ハイブリッドアーキテクチャの検討が幅広く行われているにも関わらず、線形アテンション成分の選択には深く調査されていません。我々は、ベクトルの再現から進歩したゲーティング機構までの線形アテンションモデルを構築し、シングルとハイブリッド化されたものを両方とも評価します。この詳細な分析を可能にするために、我々は72モデルを訓練し、開源しました：340Mパラメータ（20Bトークン）のモデル36つと1.3Bパラメータ（100Bトークン）のモデル36つ、5つのハイブリッド比率で6つの線形アテンションの変体を含みます。標準的な言語モデリングと記憶タスクのベンチマークにより、上位のシングル線形モデルはハイブリッドで必ずしも優れていないことが明らかになります。言語モデリングは線形アテンションから全アテンションの比率で安定し、全アテンション層の増加により記憶性能が显著に向上し、特に3:1の比率以下では特に効果的です。我々の研究は、選択的ゲーティング、階層的再現、制御された忘却がハイブリッドモデルにおいて重要であることを明らかにします。HGRN-2やGatedDeltaNetのようなアーキテクチャを推奨し、線形アテンションと全アテンションの比率が3:1から6:1の範囲で効率的にTransformerレベルの記憶性能を達成することを目指します。我々のモデルは、https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e で開源しています。",
      "upvotes": 13,
      "discussionId": "686f2371d938c25d68441b41",
      "projectPage": "https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e",
      "ai_summary": "Research evaluates various linear attention models and their integration with full attention in Transformers, identifying key mechanisms like selective gating and hierarchical recurrence for enhanced recall performance.",
      "ai_keywords": [
        "quadratic complexity",
        "linear attention mechanisms",
        "full attention layers",
        "hybrid architectures",
        "vector recurrences",
        "gating mechanisms",
        "recall performance",
        "language modeling",
        "recall tasks",
        "selective gating",
        "hierarchical recurrence",
        "controlled forgetting",
        "HGRN-2",
        "GatedDeltaNet"
      ]
    },
    "publishedAt": "2025-07-08T19:54:11.000Z",
    "title": "A Systematic Analysis of Hybrid Linear Attention",
    "summary": "Transformers face quadratic complexity and memory issues with long sequences,\nprompting the adoption of linear attention mechanisms using fixed-size hidden\nstates. However, linear models often suffer from limited recall performance,\nleading to hybrid architectures that combine linear and full attention layers.\nDespite extensive hybrid architecture research, the choice of linear attention\ncomponent has not been deeply explored. We systematically evaluate various\nlinear attention models across generations - vector recurrences to advanced\ngating mechanisms - both standalone and hybridized. To enable this\ncomprehensive analysis, we trained and open-sourced 72 models: 36 at 340M\nparameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six\nlinear attention variants across five hybridization ratios. Benchmarking on\nstandard language modeling and recall tasks reveals that superior standalone\nlinear models do not necessarily excel in hybrids. While language modeling\nremains stable across linear-to-full attention ratios, recall significantly\nimproves with increased full attention layers, particularly below a 3:1 ratio.\nOur study highlights selective gating, hierarchical recurrence, and controlled\nforgetting as critical for effective hybrid models. We recommend architectures\nsuch as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1\nto achieve Transformer-level recall efficiently. Our models are open-sourced at\nhttps://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06457.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ff09f24852102d4871c19c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
      "fullname": "Rui-Jie Zhu",
      "name": "ridger",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.07017",
      "authors": [
        {
          "_id": "686f39e8d938c25d68441b89",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tianyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:04.799Z",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8a",
          "user": {
            "_id": "65d2251f98b4a470bf6a26e3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d2251f98b4a470bf6a26e3/C4T0LHYGejrI9mu_k3M8p.jpeg",
            "isPro": false,
            "fullname": "xts",
            "user": "xtsssss",
            "type": "user"
          },
          "name": "Tianshun Xing",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T09:12:45.801Z",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8b",
          "name": "Qingshui Gu",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8c",
          "name": "Taoran Liang",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8d",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8e",
          "name": "Xin Zhou",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b8f",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b90",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b91",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b92",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b93",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b94",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "686f39e8d938c25d68441b95",
          "name": "Zejun Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-09T16:45:48.000Z",
      "submittedOnDailyAt": "2025-07-10T02:29:04.666Z",
      "title": "First Return, Entropy-Eliciting Explore",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "確認できる報酬からの強化学習（RLVR）は、大規模言語モデル（LLMs）の理由能力を向上させますが、不穩定な探索に難しいです。私たちは、理由の軌道での高不確実性の決策点を特定し、セマンティックに基づいた中間的なフィードバックを構築するための目標付きロールアウトを行う構造化された探索フレームワークを提案します。FR3E（First Return, Entropy-Eliciting Explore）は、密なスーパーバイオンに依存しないように、特定の指導を提供します。数学的な理由ベンチマーク（AIME24）における実験結果によると、FR3Eはより穩健な学習を促し、長いかつコラーエルな回答を生成し、完全に正確な軌道の割合を増加させます。これらの結果は、より強固かつ構造化された探索を通じてLLMの理由能力を向上させるフレームワークの効果を明らかにしています。",
      "upvotes": 12,
      "discussionId": "686f39e8d938c25d68441b96",
      "ai_summary": "FR3E enhances LLM reasoning by providing structured exploration through targeted rollouts at high-uncertainty points, leading to more stable training and accurate responses.",
      "ai_keywords": [
        "Reinforcement Learning from Verifiable Rewards",
        "RLVR",
        "Large Language Models",
        "LLMs",
        "FR3E",
        "First Return",
        "Entropy-Eliciting Explore",
        "structured exploration",
        "high-uncertainty decision points",
        "reasoning trajectories",
        "targeted rollouts",
        "semantically grounded intermediate feedback",
        "mathematical reasoning benchmarks",
        "AIME24",
        "stable training",
        "coherent responses",
        "fully correct trajectories"
      ]
    },
    "publishedAt": "2025-07-09T12:45:48.000Z",
    "title": "First Return, Entropy-Eliciting Explore",
    "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07017.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 50
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05687",
      "authors": [
        {
          "_id": "686e2c00a5f0f70d9de40c8c",
          "name": "Shangzhan Li",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c8d",
          "name": "Zefan Wang",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c8e",
          "name": "Ye He",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c8f",
          "name": "Yuxuan Li",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c90",
          "user": {
            "_id": "62ccd26d376917c022420a46",
            "avatarUrl": "/avatars/629858b1c7419dbcdbead3484a36abd1.svg",
            "isPro": false,
            "fullname": "Qi Shi",
            "user": "qshi",
            "type": "user"
          },
          "name": "Qi Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-09T08:49:03.237Z",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c91",
          "name": "Jianling Li",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c92",
          "name": "Yonggang Hu",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c93",
          "name": "Wanxiang Che",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c94",
          "name": "Xu Han",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c95",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "686e2c00a5f0f70d9de40c96",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T05:38:24.000Z",
      "submittedOnDailyAt": "2025-07-10T00:44:20.590Z",
      "title": "AutoTriton: 自動トリトンプログラミングによる強化学習",
      "submittedOnDailyBy": {
        "_id": "62ccd26d376917c022420a46",
        "avatarUrl": "/avatars/629858b1c7419dbcdbead3484a36abd1.svg",
        "isPro": false,
        "fullname": "Qi Shi",
        "user": "qshi",
        "type": "user"
      },
      "summary": "ディープラーニングのカーネル開発は、ハードウェア機能の最適化を行う必要があり、メモリ管理、並列計算、ハードウェア特有の最適化を調整するために実験的な調整を厳密に行う必要がある。ドメイン専門言語のようにTritonは、低レベルの詳細を抽象化してGPUプログラミングを簡単にするが、開発者はタイルサイズやメモリアクセスパターンなどの重要なパラメータを手動で調整する必要があり、最適な性能と広くなる採用に大きな壁がある。この研究では、最初のTritonプログラミングに専門したモデルであるAutoTritonを紹介し、強化学習（RL）をもつ。AutoTritonは高品質なデータガチャパイプラインを用いてTritonプログラミングの基本的な知識を調整するために監督的な微調節（SFT）を行い、Group Relative Policy Optimization（GRPO）アルゴリズムを用いてルールベースの報酬と実行ベースの報酬を組み合わせてTritonプログラミング能力を進める。TritonBenchとKernelBenchの5つの評価チャネルでの実験は、我々の8BモデルAutoTritonが主流の大規模なモデルと同等の性能を達成したことを示し、これらの結果は、RLが自動的に高性能のカーネルを生成する可能性を示し、高性能のカーネルはAIシステムの核心部分であることを示し、この突破は、より効率的なAIシステムの構築に重要な基盤を提供している。モデルとコードは、https://github.com/AI9Stars/AutoTritonから利用可能です。",
      "upvotes": 8,
      "discussionId": "686e2c00a5f0f70d9de40c97"
    },
    "publishedAt": "2025-07-08T01:38:24.000Z",
    "title": "AutoTriton: Automatic Triton Programming with Reinforcement Learning in\n  LLMs",
    "summary": "Kernel development in deep learning requires optimizing computational units\nacross hardware while balancing memory management, parallelism, and\nhardware-specific optimizations through extensive empirical tuning. Although\ndomain-specific languages like Triton simplify GPU programming by abstracting\nlow-level details, developers must still manually tune critical parameters such\nas tile sizes and memory access patterns through iterative experimentation,\ncreating substantial barriers to optimal performance and wider adoption. In\nthis work, we introduce AutoTriton, the first model dedicated to Triton\nprogramming powered by reinforcement learning (RL). AutoTriton performs\nsupervised fine-tuning (SFT) to be equipped with essential Triton programming\nexpertise using a high-quality data gathering pipeline, and conducts RL with\nGroup Relative Policy Optimization (GRPO) algorithm, combining a rule-based\nreward and an execution-based reward to further improve Triton programming\nability, sequentially. Experiments across five evaluation channels of\nTritonBench and KernelBench illustrate that our 8B model AutoTriton achieves\nperformance comparable to mainstream large models, including Claude-4-Sonnet\nand DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial\nrole of each module within AutoTriton, including the SFT stage, the RL stage,\nand the reward design strategy. These findings underscore the promise of RL for\nautomatically generating high-performance kernels, and since high-performance\nkernels are core components of AI systems, this breakthrough establishes an\nimportant foundation for building more efficient AI systems. The model and code\nwill be available at https://github.com/AI9Stars/AutoTriton.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05687.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ccd26d376917c022420a46",
      "avatarUrl": "/avatars/629858b1c7419dbcdbead3484a36abd1.svg",
      "fullname": "Qi Shi",
      "name": "qshi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.06804",
      "authors": [
        {
          "_id": "686f1f4dd938c25d68441b24",
          "name": "Zhenwen Liang",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b25",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b26",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b27",
          "name": "Tao Yang",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b28",
          "name": "Feng Zhang",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b29",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "686f1f4dd938c25d68441b2a",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T22:38:49.000Z",
      "submittedOnDailyAt": "2025-07-10T00:33:32.493Z",
      "title": "解説された複雑なIMO問題を解決するための手法の開発",
      "submittedOnDailyBy": {
        "_id": "62ffa3f8311cad266f9af236",
        "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
        "isPro": false,
        "fullname": "Zhenwen Liang",
        "user": "invokerliang",
        "type": "user"
      },
      "summary": "形式言語での自動定理証明（ATP）はAIの基盤的な課題です。ラージェット・ラングジュージャーモデル（LLMs）は驚異的な進展を達成しましたが、その強力的な非正式的な理由論の能力と正式的な定理証明の性能の間には大きな隙間が残っています。最近の研究によると、非正式的な正確性は80%を超え、正式的な成功率はPutnamBenchのようなベンチマークでは8%未満です。私たちは、この隙間が残っている理由は、現在の最先端の証明器が理由論と定理証明を厳密に結びつけ、深い理由論を優先して浅い、テキストベースの戦略によって訓練されていることにあると主張しています。この基本的な隙間を橋渡すために、私たちは高レベルの理由論と低レベルの定理生成を分離する新しいフレームワークを提案します。私たちのアプローチは、強力な一般的なReasonerと効率的なProverの2つの特殊化されたモデルを使用しています。Reasonerは多様な戦略的な部分目の定理を生成し、Proverはそれらを厳格に証明します。このモジュール化デザインはモデルの全ての理由論の可能性を解放し、端末からの訓練の弊点を回避します。私たちの方法は、2000年以降の難しいIMO問題の難しいセットで評価されました。この問題セットでは、先に開放ソースの証明器が成功を報告していないため。私たちの分離されたフレームワークは、これらの問題のうち5つを成功して、特に難しい数学的な挑戦に対する自動的な理由論に向けて重要なステップを示しました。将来の研究のために、私たちは、広範囲のIMO問題の生成されたおよび証明された定理の完全なデータセットをリリースします。このデータセットは、https://tencent-imo.github.io/ から利用できます。",
      "upvotes": 8,
      "discussionId": "686f1f4ed938c25d68441b2b",
      "ai_summary": "A novel framework decouples reasoning and proving in ATP to improve formal proving performance, achieving success on challenging IMO problems.",
      "ai_keywords": [
        "Automated Theorem Proving",
        "Large Language Models",
        "formal proving",
        "informal reasoning",
        "PutnamBench",
        "subgoal lemmas",
        "Reasoner",
        "Prover",
        "modular design",
        "end-to-end training",
        "IMO problems"
      ]
    },
    "publishedAt": "2025-07-07T18:38:49.000Z",
    "title": "Towards Solving More Challenging IMO Problems via Decoupled Reasoning\n  and Proving",
    "summary": "Automated Theorem Proving (ATP) in formal languages is a foundational\nchallenge for AI. While Large Language Models (LLMs) have driven remarkable\nprogress, a significant gap remains between their powerful informal reasoning\ncapabilities and their weak formal proving performance. Recent studies show\nthat the informal accuracy exceeds 80% while formal success remains below 8% on\nbenchmarks like PutnamBench. We argue this gap persists because current\nstate-of-the-art provers, by tightly coupling reasoning and proving, are\ntrained with paradigms that inadvertently punish deep reasoning in favor of\nshallow, tactic-based strategies. To bridge this fundamental gap, we propose a\nnovel framework that decouples high-level reasoning from low-level proof\ngeneration. Our approach utilizes two distinct, specialized models: a powerful,\ngeneral-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an\nefficient Prover to rigorously verify them. This modular design liberates the\nmodel's full reasoning potential and bypasses the pitfalls of end-to-end\ntraining. We evaluate our method on a challenging set of post-2000 IMO\nproblems, a problem set on which no prior open-source prover has reported\nsuccess. Our decoupled framework successfully solves 5 of these problems,\ndemonstrating a significant step towards automated reasoning on exceptionally\ndifficult mathematical challenges. To foster future research, we release our\nfull dataset of generated and verified lemmas for a wide range of IMO problems,\navailable at https://tencent-imo.github.io/ .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06804.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62ffa3f8311cad266f9af236",
      "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
      "fullname": "Zhenwen Liang",
      "name": "invokerliang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.24044",
      "authors": [
        {
          "_id": "68681d82213f123a1f88b973",
          "user": {
            "_id": "683daabc402acb18654e4674",
            "avatarUrl": "/avatars/9307b4c1c248e45b2daa8ffa1d74d4b4.svg",
            "isPro": false,
            "fullname": "Sicong Jiang",
            "user": "Max2045",
            "type": "user"
          },
          "name": "Sicong Jiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-05T07:52:40.893Z",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b974",
          "name": "Zilin Huang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b975",
          "name": "Kangan Qian",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b976",
          "name": "Ziang Luo",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b977",
          "name": "Tianze Zhu",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b978",
          "name": "Yang Zhong",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b979",
          "name": "Yihong Tang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97a",
          "name": "Menglin Kong",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97b",
          "name": "Yunlong Wang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97c",
          "name": "Siwen Jiao",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97d",
          "name": "Hao Ye",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97e",
          "name": "Zihao Sheng",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b97f",
          "name": "Xin Zhao",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b980",
          "name": "Tuopu Wen",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b981",
          "name": "Zheng Fu",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b982",
          "name": "Sikai Chen",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b983",
          "name": "Kun Jiang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b984",
          "name": "Diange Yang",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b985",
          "name": "Seongjin Choi",
          "hidden": false
        },
        {
          "_id": "68681d82213f123a1f88b986",
          "name": "Lijun Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-30T16:50:02.000Z",
      "submittedOnDailyAt": "2025-07-10T01:10:17.895Z",
      "title": "視覚・言語・アクションモデルについての自動運転に関する調査",
      "submittedOnDailyBy": {
        "_id": "683daabc402acb18654e4674",
        "avatarUrl": "/avatars/9307b4c1c248e45b2daa8ffa1d74d4b4.svg",
        "isPro": false,
        "fullname": "Sicong Jiang",
        "user": "Max2045",
        "type": "user"
      },
      "summary": "多モデル大語言モデル（MLLM）の急速な進歩は、視覚認知、自然言語理解、制御を一つのポリシー内に統合するVision-Language-Action（VLA）パラダイムの実現に道が鋭く開かれた。自動運転領域の研究者は、これらの方法を車両ドメインに適用している。これらのモデルは、高レベルの指示を理解でき、複雑な交通場面を理由的に考え、自らの判断を下す自動運転車に望みを寄せている。しかし、文献は分断していて、急速に拡大している。この調査は、自動運転用のVLA（VLA4AD）の最初の全面的な概要を提供します。私たちは、（i）最近の作品で共有されるアーキテクチャのビルディングブロックを正式化し、（ii）早期の説明者から理由的コンプーツのVLAモデルへの進化を追跡し、（iii）自動運転ドメインでのVLAの進歩に基づいて20より多くの代表的なモデルを比較します。また、既存のデータセットとベンチマークを統合し、駆転安全性、精度、説明の質を共同に測定するプロトコルを主張します。最後に、開放されている挑戦（強固性、実時間効率、正式的な証明）を詳細に説明し、VLA4ADの将来の方向を概観します。この調査は、解釈可能な社会的に一致した自動運転車の進歩について簡潔で完全なリソースとして提供します。GitHubリポジトリは、https://github.com/JohnsonJiang1996/Awesome-VLA4AD{SicongJiang/Awesome-VLA4AD}にあります。",
      "upvotes": 7,
      "discussionId": "68681d82213f123a1f88b987",
      "githubRepo": "https://github.com/JohnsonJiang1996/Awesome-VLA4AD",
      "ai_summary": "This survey provides a comprehensive overview of Vision-Language-Action (VLA) paradigms and their adaptation for autonomous driving, detailing architectural components, evolution of models, datasets, and future challenges.",
      "ai_keywords": [
        "multimodal large language models",
        "Vision-Language-Action",
        "VLA",
        "VLA for Autonomous Driving",
        "VLA4AD",
        "explainer",
        "reasoning-centric models",
        "autonomous driving",
        "driving safety",
        "accuracy",
        "explanation quality",
        "robustness",
        "real-time efficiency",
        "formal verification",
        "interpretable socially aligned autonomous vehicles"
      ],
      "githubStars": 173
    },
    "publishedAt": "2025-06-30T12:50:02.000Z",
    "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
    "summary": "The rapid progress of multimodal large language models (MLLM) has paved the\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\nperception, natural language understanding, and control within a single policy.\nResearchers in autonomous driving are actively adapting these methods to the\nvehicle domain. Such models promise autonomous vehicles that can interpret\nhigh-level instructions, reason about complex traffic scenes, and make their\nown decisions. However, the literature remains fragmented and is rapidly\nexpanding. This survey offers the first comprehensive overview of VLA for\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\nshared across recent work, (ii) trace the evolution from early explainer to\nreasoning-centric VLA models, and (iii) compare over 20 representative models\naccording to VLA's progress in the autonomous driving domain. We also\nconsolidate existing datasets and benchmarks, highlighting protocols that\njointly measure driving safety, accuracy, and explanation quality. Finally, we\ndetail open challenges - robustness, real-time efficiency, and formal\nverification - and outline future directions of VLA4AD. This survey provides a\nconcise yet complete reference for advancing interpretable socially aligned\nautonomous vehicles. Github repo is available at\nhttps://github.com/JohnsonJiang1996/Awesome-VLA4AD{SicongJiang/Awesome-VLA4AD}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.24044.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "683daabc402acb18654e4674",
      "avatarUrl": "/avatars/9307b4c1c248e45b2daa8ffa1d74d4b4.svg",
      "fullname": "Sicong Jiang",
      "name": "Max2045",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.06853",
      "authors": [
        {
          "_id": "686f4142d938c25d68441b98",
          "user": {
            "_id": "64e84ec6d41a68b065bf78a7",
            "avatarUrl": "/avatars/bae3c5e3210b40af6e4f113e85f3e206.svg",
            "isPro": false,
            "fullname": "Liang Wang",
            "user": "AzureLeon1",
            "type": "user"
          },
          "name": "Liang Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T07:09:02.989Z",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b99",
          "name": "Yu Rong",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9a",
          "name": "Tingyang Xu",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9b",
          "name": "Zhenyi Zhong",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9c",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9d",
          "name": "Pengju Wang",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9e",
          "name": "Deli Zhao",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441b9f",
          "name": "Qiang Liu",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441ba0",
          "name": "Shu Wu",
          "hidden": false
        },
        {
          "_id": "686f4142d938c25d68441ba1",
          "name": "Liang Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/zoojDRIzYMaSv2bj_dh7R.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/Lc9mfYTjI12Vz0qsq_JWj.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/brRCb577NUJrWNDqDkcMJ.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/LV-8rRgSCOr0UmateiCPM.jpeg"
      ],
      "publishedAt": "2025-07-09T13:57:20.000Z",
      "submittedOnDailyAt": "2025-07-10T03:05:48.871Z",
      "title": "DiffSpectra: 分子構造の明確化によるスペクトルからのディフュージョンモデル",
      "submittedOnDailyBy": {
        "_id": "64e84ec6d41a68b065bf78a7",
        "avatarUrl": "/avatars/bae3c5e3210b40af6e4f113e85f3e206.svg",
        "isPro": false,
        "fullname": "Liang Wang",
        "user": "AzureLeon1",
        "type": "user"
      },
      "summary": "分子構造の明確化は、化学の基礎的な問題であり、化合物の識別、合成、薬物開発に深い影響を及ぼしています。伝統的な方法は、専門家の解釈により強く依存し、スケーラビリティが低い傾向があります。先駆者の機械学習手法は、検索ベースの戦略を導入したが、有限なリブラリの依存性が新しい分子に対する一般化を制限しています。生成モデルは望ましい代替となりますが、多くは自動回帰的なSMILESベースのアーキテクチャを採用し、3D構造を無視して、多様なスペクトルモデライタを統合することが難しいです。本研究では、DiffSpectras、多モデルスペクトルデータから直接2Dと3Dの分子構造を推論する生成フレームワークを提案します。DiffSpectrasは構造の明確化を条件付き生成プロセスとして構成しています。ノイズ削減ネットワークはDiffusion Molecule Transformerでパラメーター化され、SE(3)同変なアーキテクチャでトポロジカルおよび幾何的な情報を統合しています。条件付きはSpecFormerで提供され、多モデルスペクトルからのスペクトル内と間の依存関係を捉えるディープラーニングツールです。拡張検証は、DiffSpectrasが構造の明確化に高い精度を達成し、サンプリングで16.01%のtop-1精度と96.86%のtop-20精度で正確な構造を復元することを示しています。モデルは3D幾何モデリング、SpecFormerの事前学習、および多モデル条件付きによって大幅に向上しています。これらの結果は、スペクトル条件付きの拡散モデリングが分子構造の明確化の挑戦を解決するに当たる効果性を明らかにしています。私たちの知識によると、DiffSpectrasは、新規分子構造の明確化に向けて多モデルスペクトルの理由と2D/3Dの共通生成モデリングを統合する最初のフレームワークです。",
      "upvotes": 2,
      "discussionId": "686f4142d938c25d68441ba2",
      "ai_summary": "DiffSpectra uses diffusion models with SE(3)-equivariant architecture and SpecFormer spectral encoder to accurately infer both 2D and 3D molecular structures from multi-modal spectral data.",
      "ai_keywords": [
        "diffusion models",
        "SE(3)-equivariant architecture",
        "Diffusion Molecule Transformer",
        "SpecFormer",
        "spectral encoder",
        "multi-modal spectral reasoning",
        "joint 2D/3D generative modeling",
        "de novo molecular structure elucidation"
      ]
    },
    "publishedAt": "2025-07-09T09:57:20.000Z",
    "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using\n  Diffusion Models",
    "summary": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/zoojDRIzYMaSv2bj_dh7R.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/Lc9mfYTjI12Vz0qsq_JWj.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/brRCb577NUJrWNDqDkcMJ.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/LV-8rRgSCOr0UmateiCPM.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06853.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e84ec6d41a68b065bf78a7",
      "avatarUrl": "/avatars/bae3c5e3210b40af6e4f113e85f3e206.svg",
      "fullname": "Liang Wang",
      "name": "AzureLeon1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.05455",
      "authors": [
        {
          "_id": "686f5ec6d938c25d68441bc1",
          "user": {
            "_id": "628c29a54c5a62a1d216c560",
            "avatarUrl": "/avatars/d21b4da766f87f47228112958666643b.svg",
            "isPro": false,
            "fullname": "Ashima Suvarna",
            "user": "Ashima",
            "type": "user"
          },
          "name": "Ashima Suvarna",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-07-10T09:12:39.834Z",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc2",
          "user": {
            "_id": "6543d269326cb9a32bd58e40",
            "avatarUrl": "/avatars/ddabbd80844e3dff676a8c2a182d920c.svg",
            "isPro": false,
            "fullname": "Christina",
            "user": "christinachance",
            "type": "user"
          },
          "name": "Christina Chance",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-10T09:16:25.997Z",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc3",
          "name": "Karolina Naranjo",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc4",
          "user": {
            "_id": "6189ddd0992df2640e3e7d40",
            "avatarUrl": "/avatars/925f2308fc412ae352b57a1b71815028.svg",
            "isPro": false,
            "fullname": "Hamid Palangi",
            "user": "hamidpalangi",
            "type": "user"
          },
          "name": "Hamid Palangi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-10T09:16:36.140Z",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc5",
          "user": {
            "_id": "65d9787bec3dc9ccf7344f6f",
            "avatarUrl": "/avatars/ab361193ed2287158f803fc792bb2df5.svg",
            "isPro": false,
            "fullname": "Sophie Hao",
            "user": "notaphonologist",
            "type": "user"
          },
          "name": "Sophie Hao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-07-10T09:16:42.125Z",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc6",
          "name": "Thomas Hartvigsen",
          "hidden": false
        },
        {
          "_id": "686f5ec6d938c25d68441bc7",
          "name": "Saadia Gabriel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T20:15:18.000Z",
      "submittedOnDailyAt": "2025-07-10T05:04:25.096Z",
      "title": "モデルキャティズンズ：オンライン安全におけるコミュニティの声を代表する",
      "submittedOnDailyBy": {
        "_id": "61c5c25705aa54027c52f7b3",
        "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
        "isPro": false,
        "fullname": "Hritik Bansal",
        "user": "hbXNov",
        "type": "user"
      },
      "summary": "自動有毒言語検出は、安全でインクルージブルなオンラインスペースの作成に重要です。しかし、これは非常に主観的な任務で、有毒言語の見解はコミュニティの規範と現実体験によって形成されています。現在の有毒言語検出モデルは、多様なアノテータの視点を単一の真実として崩し、その重要なコンテキスト特有の有毒言語の概念を消去しています。これに対して、私たちはMODELCITIZENSというデータセットを紹介します。これは6.8Kのソーシャルメディアのポストと40Kの有毒言語のアノテーションを含み、多様なアイデンティティグループにわたります。会話のコンテキストが有毒言語に与える役割を捉えるため、MODELCITIZENSのポストにLLM生成された会話シナリオを追加しています。最先端の有毒言語検出ツール（例：OpenAI Moderation API、GPT-o4-mini）はMODELCITIZENSでより低い性能を示し、コンテキスト追加されたポストではさらに低下しています。最後に、LLAMACITIZEN-8BとGEMMACITIZEN-12Bをリリースします。これらは、MODELCITIZENSで微調節されたLLaMAとGemmaのベースのモデルで、分布内評価でGPT-o4-miniを5.5%より上回ります。我々の見つけは、コミュニティによるアノテーションとモデリングの重要性を強調し、インクルージブルな内容モデレーションにおけるデータ、モデルとコードはhttps://github.com/asuvarna31/modelcitizensに提供されています。",
      "upvotes": 2,
      "discussionId": "686f5ec7d938c25d68441bc8",
      "ai_summary": "A new dataset and models for toxic language detection incorporate diverse community perspectives and conversational context, improving accuracy over existing tools.",
      "ai_keywords": [
        "MODELCITIZENS",
        "LLM-generated conversational scenarios",
        "OpenAI Moderation API",
        "GPT-o4-mini",
        "LLAMACITIZEN-8B",
        "GEMMACITIZEN-12B",
        "LLaMA-based models",
        "Gemma-based models",
        "in-distribution evaluations",
        "community-informed annotation",
        "inclusive content moderation"
      ]
    },
    "publishedAt": "2025-07-07T16:15:18.000Z",
    "title": "ModelCitizens: Representing Community Voices in Online Safety",
    "summary": "Automatic toxic language detection is critical for creating safe, inclusive\nonline spaces. However, it is a highly subjective task, with perceptions of\ntoxic language shaped by community norms and lived experience. Existing\ntoxicity detection models are typically trained on annotations that collapse\ndiverse annotator perspectives into a single ground truth, erasing important\ncontext-specific notions of toxicity such as reclaimed language. To address\nthis, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K\ntoxicity annotations across diverse identity groups. To capture the role of\nconversational context on toxicity, typical of social media posts, we augment\nMODELCITIZENS posts with LLM-generated conversational scenarios.\nState-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,\nGPT-o4-mini) underperform on MODELCITIZENS, with further degradation on\ncontext-augmented posts. Finally, we release LLAMACITIZEN-8B and\nGEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,\nwhich outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our\nfindings highlight the importance of community-informed annotation and modeling\nfor inclusive content moderation. The data, models and code are available at\nhttps://github.com/asuvarna31/modelcitizens.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05455.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61c5c25705aa54027c52f7b3",
      "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
      "fullname": "Hritik Bansal",
      "name": "hbXNov",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06260",
      "authors": [
        {
          "_id": "686f14e3d938c25d68441af8",
          "name": "Satyapriya Krishna",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441af9",
          "name": "Ninareh Mehrabi",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afa",
          "name": "Abhinav Mohanty",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afb",
          "name": "Matteo Memelli",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afc",
          "name": "Vincent Ponzo",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afd",
          "name": "Payal Motwani",
          "hidden": false
        },
        {
          "_id": "686f14e3d938c25d68441afe",
          "name": "Rahul Gupta",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T13:33:35.000Z",
      "submittedOnDailyAt": "2025-07-10T00:10:50.485Z",
      "title": "フロンティアモデルセイビリティフレームワークの基礎上でAmazonのNova Premierの重要なリスクを評価する",
      "submittedOnDailyBy": {
        "_id": "6186fef1b1085ab638324e7f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6186fef1b1085ab638324e7f/BL6_WJCkxB-BatBUBilT8.jpeg",
        "isPro": false,
        "fullname": "Satya",
        "user": "skrishna",
        "type": "user"
      },
      "summary": "ノヴァプリミアはAmazonの最も能力のある多モデル基盤モデルと教師です。ノヴァプリミアは1,000,000トークのコンテキストウィンドウで文章、画像、ビデオを処理し、1つのプロンプトで大規模なコードベース、400ページの文書、90分のビデオの分析を可能にします。私たちはフロンティアモデルセキュリティフレームワークのもとでノヴァプリミアの重要なリスクプロファイルの最初の詳細な評価を提供します。評価は化学、生物学、放射線と原子核（CBRN）、攻撃的サイバーオペレーション、自動化AI開発研究の3つの高リスク領域をターゲットとし、自動ベンチマーク、専門家のテストチームとアップライト研究を組み合わせてモデルがリリースシーカンスを超えているかどうかを決定します。私たちは方法と核心発見を要約します。この評価に基づき、ノヴァプリミアは2025年パリAIセキュリティシンプソンで発表したコミットメントに従って公開できることを確認します。新たなリスクとフロンティアモデルに関連する能力が認識される間、私たちは安全性評価とミューテーションパイプラインを進め続けます。",
      "upvotes": 0,
      "discussionId": "686f14e4d938c25d68441aff"
    },
    "publishedAt": "2025-07-07T09:33:35.000Z",
    "title": "Evaluating the Critical Risks of Amazon's Nova Premier under the\n  Frontier Model Safety Framework",
    "summary": "Nova Premier is Amazon's most capable multimodal foundation model and teacher\nfor model distillation. It processes text, images, and video with a\none-million-token context window, enabling analysis of large codebases,\n400-page documents, and 90-minute videos in a single prompt. We present the\nfirst comprehensive evaluation of Nova Premier's critical risk profile under\nthe Frontier Model Safety Framework. Evaluations target three high-risk domains\n-- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber\nOperations, and Automated AI R&D -- and combine automated benchmarks, expert\nred-teaming, and uplift studies to determine whether the model exceeds release\nthresholds. We summarize our methodology and report core findings. Based on\nthis evaluation, we find that Nova Premier is safe for public release as per\nour commitments made at the 2025 Paris AI Safety Summit. We will continue to\nenhance our safety evaluation and mitigation pipelines as new risks and\ncapabilities associated with frontier models are identified.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06260.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6186fef1b1085ab638324e7f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6186fef1b1085ab638324e7f/BL6_WJCkxB-BatBUBilT8.jpeg",
      "fullname": "Satya",
      "name": "skrishna",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.01702",
      "authors": [
        {
          "_id": "686f7ef5d938c25d6844295f",
          "name": "Zixin Chen",
          "hidden": false
        },
        {
          "_id": "686f7ef5d938c25d68442960",
          "name": "Hongzhan Lin",
          "hidden": false
        },
        {
          "_id": "686f7ef5d938c25d68442961",
          "name": "Kaixin Li",
          "hidden": false
        },
        {
          "_id": "686f7ef5d938c25d68442962",
          "name": "Ziyang Luo",
          "hidden": false
        },
        {
          "_id": "686f7ef5d938c25d68442963",
          "name": "Zhen Ye",
          "hidden": false
        },
        {
          "_id": "686f7ef5d938c25d68442964",
          "name": "Guang Chen",
          "hidden": false
        },
        {
          "_id": "686f7ef5d938c25d68442965",
          "name": "Zhiyong Huang",
          "hidden": false
        },
        {
          "_id": "686f7ef5d938c25d68442966",
          "name": "Jing Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-02T13:32:30.000Z",
      "submittedOnDailyAt": "2025-07-10T07:21:57.693Z",
      "title": "AdamMeme: ハームフルネスに対する多模構造の大規模言語モデルの理由論の機能を適応的に調査する",
      "submittedOnDailyBy": {
        "_id": "6499466c7d1edf7cb612a9a6",
        "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
        "isPro": false,
        "fullname": "Hongzhan Lin",
        "user": "danielhzlin",
        "type": "user"
      },
      "summary": "ソシャルメディア時代における多様なメモイの増殖は、多様なメモイの有害性を理解するために、多様な大語言モデル（mLLMs）が効果的に理解する必要があります。現在のmLLMsの有害メモイ理解を評価するためのベンチマークは、静的なデータセットを用いた精度基準で、モデル無依存評価を行っています。これらのベンチマークは、オンラインメモイが動的に変化しているため、最新的で詳細な評価を提供することができません。これらの問題を解決するために、我々は、mLLMsのメモイの有害性を理解するための理由論的性能を適応的に検証するための柔軟なアガントベースの評価フレームワーク「AdamMeme」を提案します。アガント間の協力により、AdamMemeは、難しいサンプルを含むメモイデータを反復的に更新し、mLLMsが有害性を解釈する方法の特定の制限を明らかにします。拡大的な実験により、我々のフレームワークは、異なるターゲットmLLMsの性能の変化をシステマ的に明らかにし、モデル固有の弱点の詳細的な、フィンエギリスな分析を提供します。我々のコードは、https://github.com/Lbotirx/AdamMeme に公開されています。",
      "upvotes": 0,
      "discussionId": "686f7ef5d938c25d68442967",
      "ai_summary": "AdamMeme, an adaptive agent-based framework, evaluates multimodal Large Language Models' understanding of harmful memes through iterative updates and multi-agent collaboration, revealing model-specific weaknesses.",
      "ai_keywords": [
        "multimodal Large Language Models",
        "meme harmfulness",
        "agent-based evaluation framework",
        "multi-agent collaboration",
        "meme data updates",
        "model-specific weaknesses"
      ]
    },
    "publishedAt": "2025-07-02T09:32:30.000Z",
    "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large\n  Language Models on Harmfulness",
    "summary": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01702.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6499466c7d1edf7cb612a9a6",
      "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
      "fullname": "Hongzhan Lin",
      "name": "danielhzlin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]