[
  {
    "paper": {
      "id": "2505.10554",
      "authors": [
        {
          "_id": "6826a569ea77771e3880f793",
          "user": {
            "_id": "64351475901c5734bcb64248",
            "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
            "isPro": false,
            "fullname": "Zhiyuan Hu",
            "user": "zhiyuanhucs",
            "type": "user"
          },
          "name": "Zhiyuan Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:02:17.850Z",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f794",
          "name": "Yibo Wang",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f795",
          "user": {
            "_id": "63a3ff69f91ad3ea5703841d",
            "avatarUrl": "/avatars/69227c4bce01d33747c1377b6f9672db.svg",
            "isPro": false,
            "fullname": "Hanze Dong",
            "user": "hendrydong",
            "type": "user"
          },
          "name": "Hanze Dong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:08:50.665Z",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f796",
          "user": {
            "_id": "6602869253a0518b2a98cafd",
            "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
            "isPro": false,
            "fullname": "Yuhui Xu",
            "user": "yuhuixu",
            "type": "user"
          },
          "name": "Yuhui Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:09:07.114Z",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f797",
          "user": {
            "_id": "6461c2905dba83471db3be53",
            "avatarUrl": "/avatars/6e36cf86201d590ac729a75d4a439cde.svg",
            "isPro": false,
            "fullname": "Amrita Saha",
            "user": "amritasaha87",
            "type": "user"
          },
          "name": "Amrita Saha",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:09:22.879Z",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f798",
          "user": {
            "_id": "649dbcc4e0fff1ed099dc80a",
            "avatarUrl": "/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg",
            "isPro": false,
            "fullname": "Caiming Xiong",
            "user": "cxiong",
            "type": "user"
          },
          "name": "Caiming Xiong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:09:29.160Z",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f799",
          "user": {
            "_id": "651d8032c50012d33e914f2f",
            "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
            "isPro": false,
            "fullname": "Bryan Hooi",
            "user": "bhooi",
            "type": "user"
          },
          "name": "Bryan Hooi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:09:35.640Z",
          "hidden": false
        },
        {
          "_id": "6826a569ea77771e3880f79a",
          "user": {
            "_id": "61f9d3b54ac99e8a1bae85f4",
            "avatarUrl": "/avatars/ac47d13204dd22452e4bc46e280842d5.svg",
            "isPro": false,
            "fullname": "JunnanLi",
            "user": "JunnanLi",
            "type": "user"
          },
          "name": "Junnan Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:09:57.841Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:58:33.000Z",
      "submittedOnDailyAt": "2025-05-16T01:09:52.437Z",
      "title": "エフェクティブな発見を超えて：大規模な理由論モデルにおけるシステマティックなメタ能力の対位への向け方",
      "submittedOnDailyBy": {
        "_id": "64351475901c5734bcb64248",
        "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
        "isPro": false,
        "fullname": "Zhiyuan Hu",
        "user": "zhiyuanhucs",
        "type": "user"
      },
      "summary": "大論理モデル（LRMs）は、長期のチェーンオフサインの論理に潜在的な能力を持っています。先行の研究は、結果ベースの強化学習（RL）が自発的に進歩的な論理ビジネスその他のようなものを引き出すことができることを示しました。例えば、自己補正、バックトラッキング、バリデーション現象など、モデルの「アハモーニング」と呼ばれるものです。しかし、これらの発生ビジネスの時間と一貫性は予測できず、制御できない状態であり、LRMsの論理能力のスケーラビリティと信頼性に限界があります。これらの制限を解決するために、プロンプトに依存し、自発的な「アハモーニング」を超えて、3つのメタ能力として演繹、推論、逆演繹を明示的にモデルに合わせます。自動的に生成され、自エフェクティブなタスクを使用します。我々の3ステージパイプライン個別の合わせ、パラメータースペースの統合、ドメイン専門的な強化学習を通じて、指示チューニングベースラインに対して10％以上の性能を向上させます。また、メタ能力としての明示的な合わせがドメイン専門的な強化学習からのパーフェクトチェックポイントからの性能の上限において平均で2％の追加効果を示し、数学、コーディング、科学ベンチマークです。これは、明示的なメタ能力の合わせがスケーラビリティと信頼性のある基盤を提供することを示します。コードは以下のURLで利用できます：https://github.com/zhiyuanhubj/Meta-Ability-Alignment",
      "upvotes": 54,
      "discussionId": "6826a56aea77771e3880f7c8",
      "githubRepo": "https://github.com/zhiyuanhubj/Meta-Ability-Alignment",
      "ai_keywords": [
        "large reasoning models",
        "long chain-of-thought reasoning",
        "outcome-based reinforcement learning",
        "RL",
        "self-correction",
        "backtracking",
        "verification phenomena",
        "aha moment",
        "meta-abilities",
        "deduction",
        "induction",
        "abduction",
        "automatically generated tasks",
        "self-verifiable tasks",
        "parameter-space merging",
        "domain-specific reinforcement learning",
        "performance ceiling",
        "math benchmarks",
        "coding benchmarks",
        "science benchmarks",
        "Meta-Ability-Alignment"
      ]
    },
    "publishedAt": "2025-05-15T13:58:33.000Z",
    "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models",
    "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10554.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64351475901c5734bcb64248",
      "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg",
      "fullname": "Zhiyuan Hu",
      "name": "zhiyuanhucs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09666",
      "authors": [
        {
          "_id": "68269a1eaa8aded616d280a0",
          "user": {
            "_id": "64cfa0b9749587dbe01d0079",
            "avatarUrl": "/avatars/93ca0a1d9c5578d052c5af0d4d1a0252.svg",
            "isPro": false,
            "fullname": "Yumin Choi",
            "user": "YuminChoi",
            "type": "user"
          },
          "name": "Yumin Choi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:06.309Z",
          "hidden": false
        },
        {
          "_id": "68269a1eaa8aded616d280a1",
          "user": {
            "_id": "63036b6c5c70c21d0ea79d48",
            "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
            "isPro": false,
            "fullname": "Jinheon Baek",
            "user": "jinheon",
            "type": "user"
          },
          "name": "Jinheon Baek",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:10:10.549Z",
          "hidden": false
        },
        {
          "_id": "68269a1eaa8aded616d280a2",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T16:46:15.000Z",
      "submittedOnDailyAt": "2025-05-16T01:05:44.315Z",
      "title": "マタラーニングによるシステムプライムプロンプト最適化",
      "submittedOnDailyBy": {
        "_id": "63036b6c5c70c21d0ea79d48",
        "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
        "isPro": false,
        "fullname": "Jinheon Baek",
        "user": "jinheon",
        "type": "user"
      },
      "summary": "大語言モデル（LLMs）は、入力プロンプトの最適化が性能を最大化するために重要な役割を果たしています。しかし、LLM のプロンプトは、タスク無関係のシステムプロンプトとタスク特有のユーザープロンプトからなり、現在のプロンプト最適化の研究は、個別のクエリやタスクに特化したユーザープロンプトを中心に、システムプロンプトの重要性を大きく見落としています。これに対して、システムプロンプトの強靭性と、未見のタスクへの転移可能性を目的とした新しい問題を提案します。この問題を解決するために、マタラーニングフレームワークを提案し、データセットの範囲内で多様なユーザープロンプトを最適化しながら、システムプロンプトを元ラーニングし、同時にイテレーション的にユーザープロンプトを更新してそれらの相互作用を確保します。5つの違う領域の14つの未見のデータセットで実験を行い、我々のアプローチは、多様なユーザープロンプトにも適用可能であるシステムプロンプトを生成します。また、我々の発見は、最適化されたシステムプロンプトは、未見のタスクへの急速な適応を可能にし、テスト時のユーザープロンプトの訓練ステップ数を減らしながら、性能を向上させることを示しています。",
      "upvotes": 35,
      "discussionId": "68269a1eaa8aded616d280d1",
      "githubRepo": "https://github.com/Dozi01/MetaSPO",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "bilevel system prompt optimization",
        "meta-learning framework",
        "system prompts",
        "user prompts",
        "unseen datasets",
        "domains",
        "rapid adaptation",
        "test-time user prompts"
      ]
    },
    "publishedAt": "2025-05-14T12:46:15.000Z",
    "title": "System Prompt Optimization with Meta-Learning",
    "summary": "Large Language Models (LLMs) have shown remarkable capabilities, with\noptimizing their input prompts playing a pivotal role in maximizing their\nperformance. However, while LLM prompts consist of both the task-agnostic\nsystem prompts and task-specific user prompts, existing work on prompt\noptimization has focused on user prompts specific to individual queries or\ntasks, and largely overlooked the system prompt that is, once optimized,\napplicable across different tasks and domains. Motivated by this, we introduce\nthe novel problem of bilevel system prompt optimization, whose objective is to\ndesign system prompts that are robust to diverse user prompts and transferable\nto unseen tasks. To tackle this problem, we then propose a meta-learning\nframework, which meta-learns the system prompt by optimizing it over various\nuser prompts across multiple datasets, while simultaneously updating the user\nprompts in an iterative manner to ensure synergy between them. We conduct\nexperiments on 14 unseen datasets spanning 5 different domains, on which we\nshow that our approach produces system prompts that generalize effectively to\ndiverse user prompts. Also, our findings reveal that the optimized system\nprompt enables rapid adaptation even to unseen tasks, requiring fewer\noptimization steps for test-time user prompts while achieving improved\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09666.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63036b6c5c70c21d0ea79d48",
      "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
      "fullname": "Jinheon Baek",
      "name": "jinheon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09723",
      "authors": [
        {
          "_id": "6826b00c251d26fc0cd035cc",
          "user": {
            "_id": "63c20105726f62e411fbe882",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c20105726f62e411fbe882/2UsU9O2psbDjJzz-sAmGH.jpeg",
            "isPro": false,
            "fullname": "Yuxin Jiang",
            "user": "YuxinJiang",
            "type": "user"
          },
          "name": "Yuxin Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:10:30.068Z",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035cd",
          "user": {
            "_id": "6575f9aeca03b6c514fe6e5c",
            "avatarUrl": "/avatars/a6e9d428beaa124ee989d702b9bf4f85.svg",
            "isPro": false,
            "fullname": "Shengcong Chen",
            "user": "Shengcong",
            "type": "user"
          },
          "name": "Shengcong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:10:37.358Z",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035ce",
          "user": {
            "_id": "63c7a33121bd95f80ed74652",
            "avatarUrl": "/avatars/7dd59afea785a2bff0ec2b757abd474e.svg",
            "isPro": false,
            "fullname": "Siyuan Huang",
            "user": "thuhsy",
            "type": "user"
          },
          "name": "Siyuan Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:10:50.217Z",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035cf",
          "user": {
            "_id": "640b00555a9c21b95c6449b3",
            "avatarUrl": "/avatars/5fa43b956f3acc671f033e31b7ca76c5.svg",
            "isPro": false,
            "fullname": "Liliang Chen",
            "user": "pathcn",
            "type": "user"
          },
          "name": "Liliang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:10:55.980Z",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d0",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d1",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d2",
          "name": "Xindong He",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d3",
          "name": "Chiming Liu",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d4",
          "user": {
            "_id": "65c04e9c27a5fdca81abcbd9",
            "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
            "isPro": false,
            "fullname": "Hongsheng LI",
            "user": "hsli-cuhk",
            "type": "user"
          },
          "name": "Hongsheng Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:11:29.857Z",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d5",
          "user": {
            "_id": "67739bfa64e8b7438ae68eb4",
            "avatarUrl": "/avatars/15193bfbce487b2de4ce8c86bd18885a.svg",
            "isPro": false,
            "fullname": "Maoqing Yao",
            "user": "AutobotZero",
            "type": "user"
          },
          "name": "Maoqing Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:11:36.940Z",
          "hidden": false
        },
        {
          "_id": "6826b00c251d26fc0cd035d6",
          "user": {
            "_id": "646ec9b135f55eb49e405faa",
            "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
            "isPro": false,
            "fullname": "Guanghui Ren",
            "user": "sundrops",
            "type": "user"
          },
          "name": "Guanghui Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:11:44.407Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/PBKoxKQrSb2bFzjsx2Dta.gif"
      ],
      "publishedAt": "2025-05-14T18:30:53.000Z",
      "submittedOnDailyAt": "2025-05-16T02:11:01.174Z",
      "title": "EnerVerse-AC: アクション条件付きの具象化環境の見直し",
      "submittedOnDailyBy": {
        "_id": "634e4120038b5879133552f5",
        "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
        "isPro": true,
        "fullname": "Siyuan",
        "user": "SiyuanH",
        "type": "user"
      },
      "summary": "ロボットのイマチャーニング学習は、静的なタスクを解くことから、動的な相互作用シナリオを解くことに進化しましたが、実時間的な動的な環境との相互作用が必要となるため、テストと評価は高額で難しいです。私たちは、ロボットの推論を実際的に制御可能にすることを可能にする行動条件付きの世界モデルであるEnerVerse-AC（EVAC）を提案します。先行のアーキテクチャに基づいて、EVACは動的な多角度画像の生成に向けて、多レベルの行動条件付き機構とライのマップエンコーディングを導入し、多様な失敗トラジェクトを含む訓練データを拡張し、一般化を改善します。データエンジンと評価者の両方ともであるEVACは、人間が集めたトラジェクトを多様なデータセットに変換し、政策テストのために実際的な、行動条件付きのビデオ観測を生成し、物理的なロボットや複雑なシミュレーションの必要性を消滅させます。このアプローチは、ロボット操作評価の高品質を維持するには、コストを大幅に減少することができます。拡張した実験は、私たちの方法の効果性を証明します。コード、チェックポイント、データセットは、<https://annaj2178.github.io/EnerverseAC.github.io>から見つかります。",
      "upvotes": 15,
      "discussionId": "6826b013251d26fc0cd037ba",
      "githubRepo": "https://github.com/AgibotTech/EnerVerse-AC",
      "ai_keywords": [
        "action-conditional world model",
        "future visual observations",
        "multi-level action-conditioning mechanism",
        "ray map encoding",
        "dynamic multi-view image generation",
        "diverse failure trajectories",
        "data engine",
        "evaluator",
        "human-collected trajectories",
        "diverse datasets",
        "action-conditioned video observations",
        "robotic manipulation evaluation"
      ]
    },
    "publishedAt": "2025-05-14T14:30:53.000Z",
    "title": "EnerVerse-AC: Envisioning Embodied Environments with Action Condition",
    "summary": "Robotic imitation learning has advanced from solving static tasks to\naddressing dynamic interaction scenarios, but testing and evaluation remain\ncostly and challenging due to the need for real-time interaction with dynamic\nenvironments. We propose EnerVerse-AC (EVAC), an action-conditional world model\nthat generates future visual observations based on an agent's predicted\nactions, enabling realistic and controllable robotic inference. Building on\nprior architectures, EVAC introduces a multi-level action-conditioning\nmechanism and ray map encoding for dynamic multi-view image generation while\nexpanding training data with diverse failure trajectories to improve\ngeneralization. As both a data engine and evaluator, EVAC augments\nhuman-collected trajectories into diverse datasets and generates realistic,\naction-conditioned video observations for policy testing, eliminating the need\nfor physical robots or complex simulations. This approach significantly reduces\ncosts while maintaining high fidelity in robotic manipulation evaluation.\nExtensive experiments validate the effectiveness of our method. Code,\ncheckpoints, and datasets can be found at\n<https://annaj2178.github.io/EnerverseAC.github.io>.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634e4120038b5879133552f5/PBKoxKQrSb2bFzjsx2Dta.gif"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09723.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4120038b5879133552f5",
      "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
      "fullname": "Siyuan",
      "name": "SiyuanH",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10185",
      "authors": [
        {
          "_id": "68269f67a47cb2b87646b98c",
          "user": {
            "_id": "6550c4f27bbfce1878f5f280",
            "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
            "isPro": false,
            "fullname": "seongyun_lee",
            "user": "Seongyun",
            "type": "user"
          },
          "name": "Seongyun Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:14:03.612Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b98d",
          "user": {
            "_id": "6469949654873f0043b09c22",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6469949654873f0043b09c22/Lk7IJAR16Wa_sGJ2g81AQ.jpeg",
            "isPro": false,
            "fullname": "Seungone Kim",
            "user": "seungone",
            "type": "user"
          },
          "name": "Seungone Kim",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:14:19.025Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b98e",
          "user": {
            "_id": "66f10ac605182775917d8c5a",
            "avatarUrl": "/avatars/21b21284d0a5a95413f91dde9dda346c.svg",
            "isPro": false,
            "fullname": "Minju Seo",
            "user": "Minju2136",
            "type": "user"
          },
          "name": "Minju Seo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:14:25.204Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b98f",
          "user": {
            "_id": "649e06313e5e7504763dfe03",
            "avatarUrl": "/avatars/1c4d19de5f2950d3342480c4b3e01047.svg",
            "isPro": false,
            "fullname": "Yongrae Jo",
            "user": "dreamgonfly",
            "type": "user"
          },
          "name": "Yongrae Jo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:14:30.806Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b990",
          "name": "Dongyoung Go",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b991",
          "user": {
            "_id": "647eaaf61a1fcad2fdc5d1ef",
            "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
            "isPro": false,
            "fullname": "Hyeonbin Hwang ",
            "user": "hbin0701",
            "type": "user"
          },
          "name": "Hyeonbin Hwang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:14:44.935Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b992",
          "user": {
            "_id": "638467ee8283412d401770dd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638467ee8283412d401770dd/UAMwDHhSwf91XSsubfrS_.jpeg",
            "isPro": false,
            "fullname": "Jinho Park",
            "user": "Br3ad",
            "type": "user"
          },
          "name": "Jinho Park",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:14:51.443Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b993",
          "name": "Xiang Yue",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b994",
          "user": {
            "_id": "63e3f3c59db5da2dc1ef6889",
            "avatarUrl": "/avatars/f7546f57a5fd69bc99ff1640cc4a4853.svg",
            "isPro": false,
            "fullname": "Sean Welleck",
            "user": "wellecks",
            "type": "user"
          },
          "name": "Sean Welleck",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:15:17.990Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b995",
          "user": {
            "_id": "60de14638bedd2315529d43f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625166923504-noauth.png",
            "isPro": false,
            "fullname": "Graham Neubig",
            "user": "gneubig",
            "type": "user"
          },
          "name": "Graham Neubig",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:15:24.404Z",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b996",
          "name": "Moontae Lee",
          "hidden": false
        },
        {
          "_id": "68269f67a47cb2b87646b997",
          "user": {
            "_id": "621f05ba970615ad5861ceb1",
            "avatarUrl": "/avatars/7e1902aa71369a524afda9b0a9e88e22.svg",
            "isPro": false,
            "fullname": "Minjoon Seo",
            "user": "minjoon",
            "type": "user"
          },
          "name": "Minjoon Seo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:15:33.521Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T11:31:02.000Z",
      "submittedOnDailyAt": "2025-05-16T00:44:19.223Z",
      "title": "コンティンューション・エンシャンカバリー: 理由モデルの考え方を分析、予測、制御する方法",
      "submittedOnDailyBy": {
        "_id": "6550c4f27bbfce1878f5f280",
        "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
        "isPro": false,
        "fullname": "seongyun_lee",
        "user": "Seongyun",
        "type": "user"
      },
      "summary": "長い連鎖コンシュート（CoT）は、現代の大規模言語モデルの効果的な使用に重要な要素ですが、これらの能力の基盤にある理由論の戦略についての理解は限られています。先行研究では、モデルが生成したCoTを既定の戦略タイプで分類することを試みていますが、これらのアプローチは人間の直感に制限され、モデルの行動の全体の多様性を捉えられません。本研究では、モデルの理由論を分析し、その方向を制御するためのボトムアップ的なフレームワークを導入します。我々の方法は、モデルが生成したCoTから多様な理由論の基準を自動的に抽出し、それらをセマンティックスペースに埋め込み、代表的なカテゴリにクラスタリングし、理由論の行動を解釈するための対照的なルールを得ます。人間の評価により、このフレームワークは現在の方法よりも解釈可能で詳細な分析を提供します。また、この理解により性能の向上が見込みです：モデルが使用する戦略を予測し、より効果的な代替手段へのガイドを提供できます。最後に、実用的なインサイトを提供します：データの形式（例えば、自由形式と複数選択）がデータの領域よりも理由論の行動にさらなる影響を与えることを示し、フォーマットに関するモデル設計の重要性を強調します。",
      "upvotes": 14,
      "discussionId": "68269f68a47cb2b87646b9ed",
      "ai_keywords": [
        "long chain-of-thought (CoT)",
        "large language models",
        "reasoning strategies",
        "predefined strategy types",
        "CoT Encyclopedia",
        "bottom-up framework",
        "reasoning criteria",
        "semantic space",
        "contrastive rubrics",
        "reasoning behavior",
        "interpretability",
        "performance gains"
      ]
    },
    "publishedAt": "2025-05-15T07:31:02.000Z",
    "title": "The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a\n  Reasoning Model will Think",
    "summary": "Long chain-of-thought (CoT) is an essential ingredient in effective usage of\nmodern large language models, but our understanding of the reasoning strategies\nunderlying these capabilities remains limited. While some prior works have\nattempted to categorize CoTs using predefined strategy types, such approaches\nare constrained by human intuition and fail to capture the full diversity of\nmodel behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up\nframework for analyzing and steering model reasoning. Our method automatically\nextracts diverse reasoning criteria from model-generated CoTs, embeds them into\na semantic space, clusters them into representative categories, and derives\ncontrastive rubrics to interpret reasoning behavior. Human evaluations show\nthat this framework produces more interpretable and comprehensive analyses than\nexisting methods. Moreover, we demonstrate that this understanding enables\nperformance gains: we can predict which strategy a model is likely to use and\nguide it toward more effective alternatives. Finally, we provide practical\ninsights, such as that training data format (e.g., free-form vs.\nmultiple-choice) has a far greater impact on reasoning behavior than data\ndomain, underscoring the importance of format-aware model design.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10185.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6550c4f27bbfce1878f5f280",
      "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg",
      "fullname": "seongyun_lee",
      "name": "Seongyun",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09694",
      "authors": [
        {
          "_id": "6826ae4611765454f5757d7c",
          "name": "Hu Yue",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d7d",
          "user": {
            "_id": "63c7a33121bd95f80ed74652",
            "avatarUrl": "/avatars/7dd59afea785a2bff0ec2b757abd474e.svg",
            "isPro": false,
            "fullname": "Siyuan Huang",
            "user": "thuhsy",
            "type": "user"
          },
          "name": "Siyuan Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:16:33.829Z",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d7e",
          "name": "Yue Liao",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d7f",
          "user": {
            "_id": "6575f9aeca03b6c514fe6e5c",
            "avatarUrl": "/avatars/a6e9d428beaa124ee989d702b9bf4f85.svg",
            "isPro": false,
            "fullname": "Shengcong Chen",
            "user": "Shengcong",
            "type": "user"
          },
          "name": "Shengcong Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:16:15.416Z",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d80",
          "name": "Pengfei Zhou",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d81",
          "user": {
            "_id": "640b00555a9c21b95c6449b3",
            "avatarUrl": "/avatars/5fa43b956f3acc671f033e31b7ca76c5.svg",
            "isPro": false,
            "fullname": "Liliang Chen",
            "user": "pathcn",
            "type": "user"
          },
          "name": "Liliang Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:15:58.407Z",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d82",
          "user": {
            "_id": "67739bfa64e8b7438ae68eb4",
            "avatarUrl": "/avatars/15193bfbce487b2de4ce8c86bd18885a.svg",
            "isPro": false,
            "fullname": "Maoqing Yao",
            "user": "AutobotZero",
            "type": "user"
          },
          "name": "Maoqing Yao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:15:51.039Z",
          "hidden": false
        },
        {
          "_id": "6826ae4611765454f5757d83",
          "user": {
            "_id": "646ec9b135f55eb49e405faa",
            "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
            "isPro": false,
            "fullname": "Guanghui Ren",
            "user": "sundrops",
            "type": "user"
          },
          "name": "Guanghui Ren",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:15:44.660Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T18:00:19.000Z",
      "submittedOnDailyAt": "2025-05-16T01:55:39.761Z",
      "title": "EWMBench: 体験世界モデルのスケーン、動き、そして語義質の評価",
      "submittedOnDailyBy": {
        "_id": "634e4120038b5879133552f5",
        "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
        "isPro": true,
        "fullname": "Siyuan",
        "user": "SiyuanH",
        "type": "user"
      },
      "summary": "最近の創造的なAIの進展により、言語指示に基づく高品質の画像とビデオの合成が可能になりました。これらの進歩に基づき、文からビデオの拡散モデルは、物理的に妥当なシーンを言語命令によって生成できる具象化ワールドモデル（EWMs）に進化しました。この研究は、EWMsを一般的な視覚的なメトリクスよりも評価するための重要な課題を解決し、物理的に基づいたおよび行動的に一致する行動を生成することを確認することを目的としています。私たちは、視覚的なシーンの一致性、動きの正確性、セマンティックの一致性の3つのキー的な面で評価するためのEmbodied World Model Benchmark（EWMBench）を提案しました。私たちのアプローチは、多様なシーンと動きパターンを含む細かく製作されたデータセットと、詳細な多次元評価ツールキットを利用して、候補モデルを評価し、比較するための専門的なフレームワークを提供します。提案されたベンチマークは、現在のビデオ生成モデルが具象化タスクの特有の要求に適合することについての制限を明らかにし、将来の進歩についての有價値なエンジニアリングのヒントを提供します。データセットと評価ツールは、https://github.com/AgibotTech/EWMBench で公開しています。",
      "upvotes": 13,
      "discussionId": "6826ae4911765454f5757e32",
      "ai_keywords": [
        "text-to-video diffusion models",
        "embodied world models",
        "physically plausible scenes",
        "language commands",
        "perceptual metrics",
        "visual scene consistency",
        "motion correctness",
        "semantic alignment",
        "Embodied World Model Benchmark (EWMBench)",
        "multi-dimensional evaluation toolkit",
        "video generation models"
      ]
    },
    "publishedAt": "2025-05-14T14:00:19.000Z",
    "title": "EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied\n  World Models",
    "summary": "Recent advances in creative AI have enabled the synthesis of high-fidelity\nimages and videos conditioned on language instructions. Building on these\ndevelopments, text-to-video diffusion models have evolved into embodied world\nmodels (EWMs) capable of generating physically plausible scenes from language\ncommands, effectively bridging vision and action in embodied AI applications.\nThis work addresses the critical challenge of evaluating EWMs beyond general\nperceptual metrics to ensure the generation of physically grounded and\naction-consistent behaviors. We propose the Embodied World Model Benchmark\n(EWMBench), a dedicated framework designed to evaluate EWMs based on three key\naspects: visual scene consistency, motion correctness, and semantic alignment.\nOur approach leverages a meticulously curated dataset encompassing diverse\nscenes and motion patterns, alongside a comprehensive multi-dimensional\nevaluation toolkit, to assess and compare candidate models. The proposed\nbenchmark not only identifies the limitations of existing video generation\nmodels in meeting the unique requirements of embodied tasks but also provides\nvaluable insights to guide future advancements in the field. The dataset and\nevaluation tools are publicly available at\nhttps://github.com/AgibotTech/EWMBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634e4120038b5879133552f5",
      "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
      "fullname": "Siyuan",
      "name": "SiyuanH",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10562",
      "authors": [
        {
          "_id": "6826a5d8154611642ada50da",
          "user": {
            "_id": "64c52b905e5bc55a9201a069",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qAgetu7v3KWc4jOjz85tP.jpeg",
            "isPro": false,
            "fullname": "Wenxuan Wang",
            "user": "gilnore",
            "type": "user"
          },
          "name": "Wenxuan Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:16:46.660Z",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50db",
          "user": {
            "_id": "640ed40dc025ddf618950af7",
            "avatarUrl": "/avatars/a99abd4346fa649ad4144f284ebcc972.svg",
            "isPro": false,
            "fullname": "Fan Zhang",
            "user": "ryanzhangfan",
            "type": "user"
          },
          "name": "Fan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:04.101Z",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50dc",
          "user": {
            "_id": "648683de623b5f050213f2be",
            "avatarUrl": "/avatars/83ecbcf4a21f68d2893de79f0444d6e3.svg",
            "isPro": false,
            "fullname": "Yufeng Cui",
            "user": "YufengCui",
            "type": "user"
          },
          "name": "Yufeng Cui",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:16:58.537Z",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50dd",
          "user": {
            "_id": "64b4a717aa03b6520839e9b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/Rt3ERG-6BVEA4hAwOz0_I.jpeg",
            "isPro": false,
            "fullname": "Haiwen Diao",
            "user": "Paranioar",
            "type": "user"
          },
          "name": "Haiwen Diao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:17:04.879Z",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50de",
          "user": {
            "_id": "657187de7644d1128571495e",
            "avatarUrl": "/avatars/89412c94fd6136f6680055551de3ddc4.svg",
            "isPro": false,
            "fullname": "Zhuoyan Luo",
            "user": "RobertLuo1",
            "type": "user"
          },
          "name": "Zhuoyan Luo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:17:11.336Z",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50df",
          "name": "Huchuan Lu",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50e0",
          "name": "Jing Liu",
          "hidden": false
        },
        {
          "_id": "6826a5d8154611642ada50e1",
          "user": {
            "_id": "63ca558304c979828311c5a5",
            "avatarUrl": "/avatars/2a439d79fba2f987cabe780d10c94d25.svg",
            "isPro": false,
            "fullname": "Xinlong Wang",
            "user": "xinlongwang",
            "type": "user"
          },
          "name": "Xinlong Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:17:32.558Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/640ed40dc025ddf618950af7/c498wQOl3gwkr9MlW8sOO.png"
      ],
      "publishedAt": "2025-05-15T17:59:39.000Z",
      "submittedOnDailyAt": "2025-05-16T01:26:23.739Z",
      "title": "End-to-End Vision Tokenizer Tuning",
      "submittedOnDailyBy": {
        "_id": "640ed40dc025ddf618950af7",
        "avatarUrl": "/avatars/a99abd4346fa649ad4144f284ebcc972.svg",
        "isPro": false,
        "fullname": "Fan Zhang",
        "user": "ryanzhangfan",
        "type": "user"
      },
      "summary": "現在の視覚トークナイザー化は、視覚トークナイザーの最適化と下流トレーニングを分離し、視覚トークンが多様なタスク（例：画像生成と視覚問題回答）でよく一般化できることを単純に仮定しています。低レベルの再構成に最適化された視覚トークナイザーは、下流タスクに必要な多様な表現と語意に関係なくなっています。この分離パラダイムでは、視覚トークナイザーの損失が目標タスクの表現ボトルネックとなることが重要な誤りです。例えば、画像中のテキストのトークナイズエラーは、認識または生成時に不良な結果を出すことになります。これに対して、私たちはETT（End-to-End Vision Tokenizer Tuning）を提案します。ETTは、視覚トークナイザーと目標の自動協調タスクの連携最適化を可能にします。先行の自動協調モデルは、フリーズされた視覚トークナイザーからの離散インデックスのみを使用していましたが、ETTはトークナイザーコードブックの視覚埋め込みを活用し、再構成とキャプションオブジェクティブと共に視覚トークナイザーを終端から最適化します。ETTは、現在のトレーニングパイプラインと無難に統合でき、最小限のアーキテクチャ変更です。私たちのETTは、実装や統合が簡単であり、使用された大規模言語モデルの元のコードブックやアーキテクチャを変更する必要がありません。拡張的な実験は、ETTの提案の終端からの視覚トークナイザー最適化が、フリーズされたトークナイザーベース線形と比較して、多タイプ理解と視覚生成タスクにおいて2-6%の性能向上を示します。私たちは、この非常に簡単で強力な方法が、画像生成と理解の外にも多タイプベースモデルを強化することを望むと考えています。",
      "upvotes": 11,
      "discussionId": "6826a5d9154611642ada5122",
      "ai_keywords": [
        "vision tokenization",
        "end-to-end vision tokenizer tuning (ETT)",
        "autoregressive tasks",
        "visual embeddings",
        "tokenizer codebook",
        "multimodal understanding",
        "visual generation tasks"
      ]
    },
    "publishedAt": "2025-05-15T13:59:39.000Z",
    "title": "End-to-End Vision Tokenizer Tuning",
    "summary": "Existing vision tokenization isolates the optimization of vision tokenizers\nfrom downstream training, implicitly assuming the visual tokens can generalize\nwell across various tasks, e.g., image generation and visual question\nanswering. The vision tokenizer optimized for low-level reconstruction is\nagnostic to downstream tasks requiring varied representations and semantics.\nThis decoupled paradigm introduces a critical misalignment: The loss of the\nvision tokenization can be the representation bottleneck for target tasks. For\nexample, errors in tokenizing text in a given image lead to poor results when\nrecognizing or generating them. To address this, we propose ETT, an end-to-end\nvision tokenizer tuning approach that enables joint optimization between vision\ntokenization and target autoregressive tasks. Unlike prior autoregressive\nmodels that use only discrete indices from a frozen vision tokenizer, ETT\nleverages the visual embeddings of the tokenizer codebook, and optimizes the\nvision tokenizers end-to-end with both reconstruction and caption objectives.\nETT can be seamlessly integrated into existing training pipelines with minimal\narchitecture modifications. Our ETT is simple to implement and integrate,\nwithout the need to adjust the original codebooks or architectures of the\nemployed large language models. Extensive experiments demonstrate that our\nproposed end-to-end vision tokenizer tuning unlocks significant performance\ngains, i.e., 2-6% for multimodal understanding and visual generation tasks\ncompared to frozen tokenizer baselines, while preserving the original\nreconstruction capability. We hope this very simple and strong method can\nempower multimodal foundation models besides image generation and\nunderstanding.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/640ed40dc025ddf618950af7/c498wQOl3gwkr9MlW8sOO.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10562.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640ed40dc025ddf618950af7",
      "avatarUrl": "/avatars/a99abd4346fa649ad4144f284ebcc972.svg",
      "fullname": "Fan Zhang",
      "name": "ryanzhangfan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.07782",
      "authors": [
        {
          "_id": "6822b3c8c10ac9c466c63e01",
          "user": {
            "_id": "6466e31a14e059dde8bbe4be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
            "isPro": true,
            "fullname": "Rushi Qiang",
            "user": "Jerrycool",
            "type": "user"
          },
          "name": "Rushi Qiang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-14T07:35:59.704Z",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e02",
          "user": {
            "_id": "6471bddd609ae9f56368f132",
            "avatarUrl": "/avatars/71a80127a01e662ab2790de0511326b6.svg",
            "isPro": true,
            "fullname": "Yuchen Zhuang",
            "user": "yczhuang",
            "type": "user"
          },
          "name": "Yuchen Zhuang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:17:45.935Z",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e03",
          "user": {
            "_id": "68198e34612ca40b67abbf18",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Gj9eV6k3CcKJinTePC2CV.png",
            "isPro": false,
            "fullname": "Yinghao Li",
            "user": "yinghaoli-yh",
            "type": "user"
          },
          "name": "Yinghao Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:17:54.487Z",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e04",
          "name": "Dingu Sagar V K",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e05",
          "user": {
            "_id": "644a2c4e9a1c5faef7a5dbd8",
            "avatarUrl": "/avatars/fbbbc1347f8e423b2477e2506fdb43d9.svg",
            "isPro": false,
            "fullname": "Rongzhi Zhang",
            "user": "Solute",
            "type": "user"
          },
          "name": "Rongzhi Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:47.628Z",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e06",
          "name": "Changhao Li",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e07",
          "name": "Ian Shu-Hei Wong",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e08",
          "name": "Sherry Yang",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e09",
          "user": {
            "_id": "6409651b9e9f790c905b2335",
            "avatarUrl": "/avatars/1fb8c80b60f21f65a0a027319101f236.svg",
            "isPro": false,
            "fullname": "Percy Liang",
            "user": "percyliang",
            "type": "user"
          },
          "name": "Percy Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:18:20.107Z",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e0a",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "6822b3c8c10ac9c466c63e0b",
          "user": {
            "_id": "635f93577c05eb9f59966209",
            "avatarUrl": "/avatars/add48d7e3790a6b500d6c451ef8b0f75.svg",
            "isPro": false,
            "fullname": "Intelligent Digital Creation",
            "user": "BoDai",
            "type": "user"
          },
          "name": "Bo Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-05-16T08:18:42.045Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-12T17:35:43.000Z",
      "submittedOnDailyAt": "2025-05-16T00:26:20.796Z",
      "title": "MLE-Dojo: 機械学習工学におけるLLMアガントの能力を高めるための相互作用環境",
      "submittedOnDailyBy": {
        "_id": "6466e31a14e059dde8bbe4be",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
        "isPro": true,
        "fullname": "Rushi Qiang",
        "user": "Jerrycool",
        "type": "user"
      },
      "summary": "MLE-Dojoは、システマティックな強化学習、評価、改善のためのGymようなフレームワークです。現存する静的データセットや一回の評価に依存しているベンチマークと異なり、MLE-Dojoはインタラクティブな環境を提供し、アウトプットを通じた構造化されたフラッグバックループを通じてアウトプットが評価され、解決策を進めることを可能にします。200点以上の実世界のKaggleチャレンジを基に、MLE-Dojoはデータ処理、アーキテクチャ検索、ハイパーパラメータ調整、コードデバッグなどの多様な、開放的なMLEタスクを検討し、実際的なエンジニアリングシナリオを反映したものです。完全に実行可能な環境で、サブプールファイナリングと強化学習を通じて、アウトプットの評価を実現し、複雑なエラーの解決を効率的に行うことを可能にします。8つの先進的なLLMの検証により、現在のモデルは意味のある複数回の改善を実現しているが、長期視野の解決策の自動生成と複雑なエラーの効率的な解決においては、显著な制限があることが明らかになりました。また、MLE-Dojoの柔軟かつ拡張可能なアーキテクチャは、多様なデータソース、ツール、評価プロトコルを無難に統合し、モデルベースのアウトプット調整を可能にし、互換性、スケーラビリティ、再現性を促進します。フレームワークとベンチマークをオープンソースし、次世代のMLEアウトプットを目指したコミュニティ駆動のイノベーションを促進することを目的としています。",
      "upvotes": 10,
      "discussionId": "6822b3c9c10ac9c466c63e8a",
      "projectPage": "https://mle-dojo.github.io/MLE-Dojo-page/",
      "githubRepo": "https://github.com/MLE-Dojo/MLE-Dojo",
      "ai_keywords": [
        "reinforcement learning",
        "autonomous large language model (LLM) agents",
        "iterative machine learning engineering (MLE) workflows",
        "Kaggle challenges",
        "MLE tasks",
        "data processing",
        "architecture search",
        "hyperparameter tuning",
        "code debugging",
        "supervised fine-tuning",
        "model-based agent tuning"
      ]
    },
    "publishedAt": "2025-05-12T13:35:43.000Z",
    "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine\n  Learning Engineering",
    "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement\nlearning, evaluating, and improving autonomous large language model (LLM)\nagents in iterative machine learning engineering (MLE) workflows. Unlike\nexisting benchmarks that primarily rely on static datasets or single-attempt\nevaluations, MLE-Dojo provides an interactive environment enabling agents to\niteratively experiment, debug, and refine solutions through structured feedback\nloops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,\nopen-ended MLE tasks carefully curated to reflect realistic engineering\nscenarios such as data processing, architecture search, hyperparameter tuning,\nand code debugging. Its fully executable environment supports comprehensive\nagent training via both supervised fine-tuning and reinforcement learning,\nfacilitating iterative experimentation, realistic data sampling, and real-time\noutcome verification. Extensive evaluations of eight frontier LLMs reveal that\nwhile current models achieve meaningful iterative improvements, they still\nexhibit significant limitations in autonomously generating long-horizon\nsolutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's\nflexible and extensible architecture seamlessly integrates diverse data\nsources, tools, and evaluation protocols, uniquely enabling model-based agent\ntuning and promoting interoperability, scalability, and reproducibility. We\nopen-source our framework and benchmarks to foster community-driven innovation\ntowards next-generation MLE agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.07782.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6466e31a14e059dde8bbe4be",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466e31a14e059dde8bbe4be/D_DWGWEkhOnFdbbdCNP3N.jpeg",
      "fullname": "Rushi Qiang",
      "name": "Jerrycool",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10558",
      "authors": [
        {
          "_id": "6826bc3cf032d8147549ac6b",
          "user": {
            "_id": "635eac5ea81c7f7424a23b8c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635eac5ea81c7f7424a23b8c/Cei8xiGF5jrwscDpMgftt.jpeg",
            "isPro": false,
            "fullname": "intchous",
            "user": "intchous",
            "type": "user"
          },
          "name": "Peiying Zhang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-16T04:17:01.151Z",
          "hidden": false
        },
        {
          "_id": "6826bc3cf032d8147549ac6c",
          "user": {
            "_id": "6412b90e6e51a8e21887ff30",
            "avatarUrl": "/avatars/b0c3a8624a686481b9f609be96b1307c.svg",
            "isPro": false,
            "fullname": "Z",
            "user": "CHERRY-Z",
            "type": "user"
          },
          "name": "Nanxuan Zhao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-16T04:17:01.151Z",
          "hidden": false
        },
        {
          "_id": "6826bc3cf032d8147549ac6d",
          "name": "Jing Liao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:59:21.000Z",
      "submittedOnDailyAt": "2025-05-16T02:49:19.469Z",
      "title": "テキストからベクトルのスタイルカスタマイズメントにおける画像ディフュージョン\n  先駆け先",
      "submittedOnDailyBy": {
        "_id": "635eac5ea81c7f7424a23b8c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635eac5ea81c7f7424a23b8c/Cei8xiGF5jrwscDpMgftt.jpeg",
        "isPro": false,
        "fullname": "intchous",
        "user": "intchous",
        "type": "user"
      },
      "summary": "スケーラブルベクトルグラフィックス（SVGs）は、デザイナーにとって高い評価を受けており、解像度の独立性と整頓されたレイヤ構造によってさらに強力です。 既存のテキストからベクトル（T2V）生成手法は、SVGsをテキストプロンプトから生成することができますが、実用的なアプリケーションでの重要な必要性を飛ばしてしまいます：スタイルのカスタマイズ，これは、一致した可視的な外見とコラフィーな美術性を確保するために必要です。 現在のT2V手法をスタイルカスタマイズに拡張することは、特定の課題を抱えています。\n\n最適化基づきのT2Vモデルは、T2Iモデルの先驅を利用してスタイルカスタマイズを行うことができますが、構造的な正規性を維持することが難しいです。 逆に、前向きT2Vモデルは構造的な正規性を確保できますが、SVGの訓練データの限りによって内容とスタイルを分離することが難しくなります。\n\nこれらの課題に対処するために、私たちは、前向きT2VモデルとT2I画像の先驅の利点を活用する新型の2段階スタイルカスタマイズプインプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプルンプ",
      "upvotes": 8,
      "discussionId": "6826bc3df032d8147549acac",
      "ai_keywords": [
        "T2V (text-to-vector)",
        "SVG (Scalable Vector Graphics)",
        "T2I (text-to-image)",
        "diffusion model",
        "path-level representation",
        "structural regularity",
        "expressive capabilities"
      ]
    },
    "publishedAt": "2025-05-15T13:59:21.000Z",
    "title": "Style Customization of Text-to-Vector Generation with Image Diffusion\n  Priors",
    "summary": "Scalable Vector Graphics (SVGs) are highly favored by designers due to their\nresolution independence and well-organized layer structure. Although existing\ntext-to-vector (T2V) generation methods can create SVGs from text prompts, they\noften overlook an important need in practical applications: style\ncustomization, which is vital for producing a collection of vector graphics\nwith consistent visual appearance and coherent aesthetics. Extending existing\nT2V methods for style customization poses certain challenges.\nOptimization-based T2V models can utilize the priors of text-to-image (T2I)\nmodels for customization, but struggle with maintaining structural regularity.\nOn the other hand, feed-forward T2V models can ensure structural regularity,\nyet they encounter difficulties in disentangling content and style due to\nlimited SVG training data.\n  To address these challenges, we propose a novel two-stage style customization\npipeline for SVG generation, making use of the advantages of both feed-forward\nT2V models and T2I image priors. In the first stage, we train a T2V diffusion\nmodel with a path-level representation to ensure the structural regularity of\nSVGs while preserving diverse expressive capabilities. In the second stage, we\ncustomize the T2V diffusion model to different styles by distilling customized\nT2I models. By integrating these techniques, our pipeline can generate\nhigh-quality and diverse SVGs in custom styles based on text prompts in an\nefficient feed-forward manner. The effectiveness of our method has been\nvalidated through extensive experiments. The project page is\nhttps://customsvg.github.io.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10558.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "635eac5ea81c7f7424a23b8c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635eac5ea81c7f7424a23b8c/Cei8xiGF5jrwscDpMgftt.jpeg",
      "fullname": "intchous",
      "name": "intchous",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10527",
      "authors": [
        {
          "_id": "682699da19c4a596dbcea4f5",
          "name": "Binghai Wang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f6",
          "name": "Runji Lin",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f7",
          "name": "Keming Lu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f8",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4f9",
          "name": "Zhenru Zhang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fa",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fb",
          "name": "Chujie Zheng",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fc",
          "name": "Kai Dang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fd",
          "name": "Yang Fan",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4fe",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea4ff",
          "name": "An Yang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea500",
          "name": "Binyuan Hui",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea501",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea502",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea503",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea504",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea505",
          "name": "Yu-Gang Jiang",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea506",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea507",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "682699da19c4a596dbcea508",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:38:37.000Z",
      "submittedOnDailyAt": "2025-05-16T00:22:55.977Z",
      "title": "WorldPM: 人間の好みモデリングのスケーリング",
      "submittedOnDailyBy": {
        "_id": "63d9d68c1cae35c27bf7a6a7",
        "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
        "isPro": false,
        "fullname": "Bowen Yu",
        "user": "Tigerph",
        "type": "user"
      },
      "summary": "モデルサイズとデータセットサイズによる測定損失のパワーラールのスケーリング法則によって励まされ、これらの法則が好みモデリングにも存在していることを見出しました。私たちは、World Preference Modeling$ (WorldPM) を提案し、このスケーリングポテンシャルを強調しています。World Preferenceは、人類の好みの統一表現です。この論文では、多様なユーザコミュニティをカバーする公開フォーラムからの好みデータを収集し、15Mスケールのデータを使用して様々なモデル（1.5Bから72Bパラメータ）で拡大訓練を実施しました。検証メトリックにおいては、以下の特徴が見出されました： (1) アドバーサリーメトリック（偽装機能を識別する能力）は、学習データと基礎モデルサイズの増加に伴い、一貫してスケーリングします。 (2) 目標メトリック（定義された答えを持つ客観的知識）は、大きな言語モデルでエピーファーバージョンを示し、WorldPMのスケーリングポテンシャルを強調します。 (3) 主観メトリック（有限の人間またはAIからの主観的な好み）は、スケーリングの趨勢を示しません。進めた実験は、WorldPMが好み微調込みの基盤としての効果性を証明しました。7ベンチマークの20サブタスクでの評価を通じて、WorldPMは、7K、100K、800Kサンプルの異なるサイズの人類好みデータセットの拡張性能を広範囲で向上させ、多くのキーサブタスクで5%以上の性能向上を収めました。内部のRLHFパイプラインにWorldPMを統合し、プロフェッショナルな評価セットと公共的な評価セットでも顕著な向上を見出しました。内部評価では、4%〜8%の顕著な向上が見出されました。",
      "upvotes": 8,
      "discussionId": "682699dd19c4a596dbcea604",
      "ai_keywords": [
        "preference modeling",
        "World Preference Modeling (WorldPM)",
        "World Preference",
        "unified representation",
        "human preferences",
        "preference data",
        "public forums",
        "user communities",
        "extensive training",
        "15M-scale data",
        "parameter-efficient fine-tuning",
        "Adversarial metrics",
        "deceptive features",
        "Objective metrics",
        "Subjective metrics",
        "preference fine-tuning",
        "generalization performance",
        "human preference datasets",
        "RLHF (Reinforcement Learning from Human Feedback)",
        "in-house evaluations",
        "public evaluation sets"
      ]
    },
    "publishedAt": "2025-05-15T13:38:37.000Z",
    "title": "WorldPM: Scaling Human Preference Modeling",
    "summary": "Motivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We propose World Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collect preference data\nfrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1) Adversarial metrics (ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)\nSubjective metrics (subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation for preference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves the generalization performance across human preference datasets of\nvarying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%\non many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we\nobserve significant improvements on both in-house and public evaluation sets,\nwith notable gains of 4% to 8% in our in-house evaluations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10527.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63d9d68c1cae35c27bf7a6a7",
      "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
      "fullname": "Bowen Yu",
      "name": "Tigerph",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.10320",
      "authors": [
        {
          "_id": "6826a180cad9000ebc70f038",
          "name": "Chenxi Whitehouse",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f039",
          "name": "Tianlu Wang",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03a",
          "name": "Ping Yu",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03b",
          "name": "Xian Li",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03c",
          "name": "Jason Weston",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03d",
          "name": "Ilia Kulikov",
          "hidden": false
        },
        {
          "_id": "6826a180cad9000ebc70f03e",
          "user": {
            "_id": "64b75f4b037d6452a30f71aa",
            "avatarUrl": "/avatars/5a0322e7ecda05164e45526d605e3619.svg",
            "isPro": false,
            "fullname": "Swarnadeep Saha",
            "user": "swarna92",
            "type": "user"
          },
          "name": "Swarnadeep Saha",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-16T02:22:57.318Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T14:05:15.000Z",
      "submittedOnDailyAt": "2025-05-16T00:58:47.493Z",
      "title": "J1: LLM-as-a-Judgeの考え方を励ますための強化学習",
      "submittedOnDailyBy": {
        "_id": "64b6feee17681d64b19b112b",
        "avatarUrl": "/avatars/afe708392d16af5b55d3bc4b42a585e8.svg",
        "isPro": false,
        "fullname": "Swarnadeep Saha",
        "user": "swarnaNLP",
        "type": "user"
      },
      "summary": "AIの進歩は評価の質によって制限されており、強力なLLM-as-a-Judgeモデルが核心的な解決策として証明されています。強化した判断能力は、より強力なchain-of-thought reasoningによって可能となり、これによりこのようなモデルの最適な訓練マニュアルを探す必要があります。本稿では、J1というリファージンシステムのアプローチを介して、これらのモデルの訓練について説明します。我々の方法は、両方の可証明であるか、不可証明であるプロンプトを判断タスクに変換し、思考を奨励し、判断バイアスを軽減する可証明な報酬を提供します。特に、このアプローチは、8Bまたは70Bモデルの訓練サイズで、すべての現在のモデルよりも優れています。J1は、o1-miniよりも優れていますし、R1もそのようなベンチマークでは優れています。また、J1は、そのサイズが小さいモデルでも優れています。我々は、Pairwise-J1 vs Pointwise-J1モデル、オフラインvsオンライン訓練マニュアル、報酬戦略、シードプロンプト、思考の長さと内容の変化についての分析と比較を提供します。我々のモデルは、評価基準を学習し、自己生成された参照答えと比較し、モデルの回答の正確性を再評価することで、より良い判断を行うことがわかりました。",
      "upvotes": 8,
      "discussionId": "6826a181cad9000ebc70f0a3",
      "ai_keywords": [
        "reinforcement learning",
        "chain-of-thought reasoning",
        "judgment tasks",
        "judgment bias",
        "Pairwise-J1",
        "Pointwise-J1",
        "offline training",
        "online training",
        "reward strategies",
        "seed prompts",
        "evaluation criteria",
        "reference answers",
        "correctness of model responses"
      ]
    },
    "publishedAt": "2025-05-15T10:05:15.000Z",
    "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
    "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10320.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b6feee17681d64b19b112b",
      "avatarUrl": "/avatars/afe708392d16af5b55d3bc4b42a585e8.svg",
      "fullname": "Swarnadeep Saha",
      "name": "swarnaNLP",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09738",
      "authors": [
        {
          "_id": "6826c755f032d814754cadfe",
          "name": "Shaurya Sharthak",
          "hidden": false
        },
        {
          "_id": "6826c755f032d814754cadff",
          "name": "Vinayak Pahalwan",
          "hidden": false
        },
        {
          "_id": "6826c755f032d814754cae00",
          "user": {
            "_id": "6523d85b27d1f3d84ab3a0a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6523d85b27d1f3d84ab3a0a4/GF159gtSvxSWR9TAJIQby.jpeg",
            "isPro": false,
            "fullname": "Adithya Kamath",
            "user": "adi-kmt",
            "type": "user"
          },
          "name": "Adithya Kamath",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:11:49.739Z",
          "hidden": false
        },
        {
          "_id": "6826c755f032d814754cae01",
          "user": {
            "_id": "644bf6ef778ecbfb977e8e84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf6ef778ecbfb977e8e84/x2zFOkSIhDHf0LEZ1wzbU.jpeg",
            "isPro": true,
            "fullname": "Adarsh AS",
            "user": "adarshxs",
            "type": "user"
          },
          "name": "Adarsh Shirawalmath",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-16T05:19:45.785Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/644bf6ef778ecbfb977e8e84/Y-nis0xhyXSFc6TiLirrG.jpeg"
      ],
      "publishedAt": "2025-05-14T19:00:27.000Z",
      "submittedOnDailyAt": "2025-05-16T03:53:49.621Z",
      "title": "テキスト化されたトークナイザーの柔軟性を達成するためのヒューリスティックアダプターフィードバックと超トークン学習",
      "submittedOnDailyBy": {
        "_id": "644bf6ef778ecbfb977e8e84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf6ef778ecbfb977e8e84/x2zFOkSIhDHf0LEZ1wzbU.jpeg",
        "isPro": true,
        "fullname": "Adarsh AS",
        "user": "adarshxs",
        "type": "user"
      },
      "summary": "予測ラベル：「長文翻訳」\n\n予測結果：\n\n予測結果の詳細：「長文翻訳」の結果は、テキストの全文を日本語に翻訳した結果です。翻訳は、テキストの内容を忠実に変換し、日本語の表現に合わせたものです。翻訳には、テキストの構造と意味を理解するために、自然言語処理の技術を使用しました。翻訳結果は、テキストの問題解決や理解のために使用できます。\n\n予測結果のテキスト：予測結果のテキストは、テキストの全文を日本語に翻訳した結果です。翻訳結果は、テキストの問題解決や理解のために使用できます。",
      "upvotes": 8,
      "discussionId": "6826c756f032d814754cae4d",
      "githubRepo": "https://github.com/Tinycompany-AI/tokenadapt",
      "ai_keywords": [
        "Tokenadapt",
        "Supertokens",
        "subword decomposition",
        "semantically similar tokens",
        "zero-shot perplexity",
        "perplexity ratios"
      ]
    },
    "publishedAt": "2025-05-14T15:00:27.000Z",
    "title": "Achieving Tokenizer Flexibility in Language Models through Heuristic\n  Adaptation and Supertoken Learning",
    "summary": "Pretrained language models (LLMs) are often constrained by their fixed\ntokenization schemes, leading to inefficiencies and performance limitations,\nparticularly for multilingual or specialized applications. This tokenizer\nlock-in presents significant challenges. standard methods to overcome this\noften require prohibitive computational resources. Although tokenizer\nreplacement with heuristic initialization aims to reduce this burden, existing\nmethods often require exhaustive residual fine-tuning and still may not fully\npreserve semantic nuances or adequately address the underlying compression\ninefficiencies. Our framework introduces two innovations: first, Tokenadapt, a\nmodel-agnostic tokenizer transplantation method, and second, novel\npre-tokenization learning for multi-word Supertokens to enhance compression and\nreduce fragmentation. Tokenadapt initializes new unique token embeddings via a\nhybrid heuristic that combines two methods: a local estimate based on subword\ndecomposition using the old tokenizer, and a global estimate utilizing the\ntop-k semantically similar tokens from the original vocabulary. This\nmethodology aims to preserve semantics while significantly minimizing\nretraining requirements. Empirical investigations validate both contributions:\nthe transplantation heuristic successfully initializes unique tokens, markedly\noutperforming conventional baselines and sophisticated methods including\nTranstokenizer and ReTok, while our Supertokens achieve notable compression\ngains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid\ninitialization consistently yields lower perplexity ratios compared to both\nReTok and TransTokenizer baselines across different base models and newly\ntrained target tokenizers. TokenAdapt typically reduced the overall perplexity\nratio significantly compared to ReTok, yielding at least a 2-fold improvement\nin these aggregate scores.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/644bf6ef778ecbfb977e8e84/Y-nis0xhyXSFc6TiLirrG.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09738.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644bf6ef778ecbfb977e8e84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf6ef778ecbfb977e8e84/x2zFOkSIhDHf0LEZ1wzbU.jpeg",
      "fullname": "Adarsh AS",
      "name": "adarshxs",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 33
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10565",
      "authors": [
        {
          "_id": "6826bda8caf89edf94b736dc",
          "user": {
            "_id": "6425761a175bd295228311a0",
            "avatarUrl": "/avatars/dcd0d267445563d0616d5a31b5d754b7.svg",
            "isPro": false,
            "fullname": "zehan wang",
            "user": "sleetwang6",
            "type": "user"
          },
          "name": "Zehan Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:01.498Z",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736dd",
          "name": "Siyu Chen",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736de",
          "name": "Lihe Yang",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736df",
          "name": "Jialei Wang",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736e0",
          "name": "Ziang Zhang",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736e1",
          "name": "Hengshuang Zhao",
          "hidden": false
        },
        {
          "_id": "6826bda8caf89edf94b736e2",
          "name": "Zhou Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T17:59:50.000Z",
      "submittedOnDailyAt": "2025-05-16T02:55:45.280Z",
      "title": "Depth Anything with Any Prior",
      "submittedOnDailyBy": {
        "_id": "663b4d6aa55b0634634cd302",
        "avatarUrl": "/avatars/1191982568ad67895225f22844b6da99.svg",
        "isPro": false,
        "fullname": "ZehanWang",
        "user": "ZehanWang",
        "type": "user"
      },
      "summary": "この研究では、「Prior Depth Anything」というフレームワークを提案します。このフレームワークは、測定における不完全であるが正確なメトリック情報と、測定における相対的であるが完全な幾何構造を統合し、どのシーンにおいても正確な、密集的な、詳細なメトリック測定マップを生成することを目指しています。このために、我々は、2つの補間的な測定源を進歩的に統合するための粗略から細かいパイプラインを設計しました。まず、我々は、ピクセルレベルのメトリックアライメントと距離に関する重み付けを介して、明示的に測定を使用して多様なメトリック先驅を予め埋め込む方法を提案します。これは、先驅パターンの領域間違いを効果的に狭め、多様なシナリオでの一般化を向上させることができます。次に、我々は、条件付きモノチューブ測定モデル（MDE）を開発し、測定先驅の内蔵ノイズを精細化します。モデルは、正規化された予め埋め込まれた先驅と予測に基づいて、2つの補間的な測定源を隠れて融合させることができます。我々のモデルは、7つの実世界データセットにおいて、測定の完了、超解像、インパイングに関する卓越したゼロシート一般化を示し、先行のタスク専門的な方法を追い越すこともできます。より重要なことに、これは難しい、見ぬ混合された先驅に対しても優れた動作を示し、予測モデルを切り替えることでテスト時の改善を行うことができ、MDEモデルの進歩とともに柔軟な精度・効率の調整を提供します。",
      "upvotes": 7,
      "discussionId": "6826bdaacaf89edf94b73753",
      "projectPage": "https://prior-depth-anything.github.io/",
      "githubRepo": "https://github.com/SpatialVision/Prior-Depth-Anything",
      "ai_keywords": [
        "metric depth maps",
        "coarse-to-fine pipeline",
        "pixel-level metric alignment",
        "distance-aware weighting",
        "metric priors",
        "domain gap",
        "generalization",
        "conditioned monocular depth estimation (MDE)",
        "zero-shot generalization",
        "depth completion",
        "super-resolution",
        "inpainting",
        "test-time improvements",
        "accuracy-efficiency trade-off"
      ]
    },
    "publishedAt": "2025-05-15T13:59:50.000Z",
    "title": "Depth Anything with Any Prior",
    "summary": "This work presents Prior Depth Anything, a framework that combines incomplete\nbut precise metric information in depth measurement with relative but complete\ngeometric structures in depth prediction, generating accurate, dense, and\ndetailed metric depth maps for any scene. To this end, we design a\ncoarse-to-fine pipeline to progressively integrate the two complementary depth\nsources. First, we introduce pixel-level metric alignment and distance-aware\nweighting to pre-fill diverse metric priors by explicitly using depth\nprediction. It effectively narrows the domain gap between prior patterns,\nenhancing generalization across varying scenarios. Second, we develop a\nconditioned monocular depth estimation (MDE) model to refine the inherent noise\nof depth priors. By conditioning on the normalized pre-filled prior and\nprediction, the model further implicitly merges the two complementary depth\nsources. Our model showcases impressive zero-shot generalization across depth\ncompletion, super-resolution, and inpainting over 7 real-world datasets,\nmatching or even surpassing previous task-specific methods. More importantly,\nit performs well on challenging, unseen mixed priors and enables test-time\nimprovements by switching prediction models, providing a flexible\naccuracy-efficiency trade-off while evolving with advancements in MDE models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10565.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "663b4d6aa55b0634634cd302",
      "avatarUrl": "/avatars/1191982568ad67895225f22844b6da99.svg",
      "fullname": "ZehanWang",
      "name": "ZehanWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09990",
      "authors": [
        {
          "_id": "6826dce4682e62074d0ed686",
          "name": "Long Cheng",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed687",
          "name": "Jiafei Duan",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed688",
          "name": "Yi Ru Wang",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed689",
          "name": "Haoquan Fang",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68a",
          "name": "Boyang Li",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68b",
          "name": "Yushan Huang",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68c",
          "name": "Elvis Wang",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68d",
          "name": "Ainaz Eftekhar",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68e",
          "name": "Jason Lee",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed68f",
          "name": "Wentao Yuan",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed690",
          "name": "Rose Hendrix",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed691",
          "name": "Noah A. Smith",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed692",
          "name": "Fei Xia",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed693",
          "name": "Dieter Fox",
          "hidden": false
        },
        {
          "_id": "6826dce4682e62074d0ed694",
          "name": "Ranjay Krishna",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/IGaHXqPEos-rGXUNMY4mE.png"
      ],
      "publishedAt": "2025-05-15T06:04:42.000Z",
      "submittedOnDailyAt": "2025-05-16T05:07:15.117Z",
      "title": "PointArena: 言語をガイドした指摘による多模構造的基礎的性質の調査",
      "submittedOnDailyBy": {
        "_id": "632b42626110e37dba3d5bcb",
        "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg",
        "isPro": false,
        "fullname": "Duan",
        "user": "Jiafei1224",
        "type": "user"
      },
      "summary": "指し手が視覚コンテキスト内の言語を基礎として、機械人、助言技術、インタラクティブAIシステムなどの様々な領域に応用される基本的で直感的な機能です。最近の多模構モデルは指し手の機能をサポートし始めていますが、現在のベンチマークは通常、参照的な物体位置決定タスクのみを焦点にしています。ポイントアーナ（PointArena）という、多模構指し手の評価を行う一つの詳細なプラットフォームを紹介します。ポイントアーナは3つのコンポーネントから構成されています：（1）Point-Bench、約1,000の指し手タスクを含むカレードされたデータセット、5つの理由論カテゴリを持ち、（2）Point-Battle、相互作用的なWebベースのアリーナ、ブラインドでのペアワイズモデル比較を促進し、現在4,500以上の匿名投票を集めています、（3）Point-Act、実世界的な機械人操作システム、ユーザーが直接多模構モデルの指し手機能を実用的な設定で評価することを可能にします。最先端の開放ソースモデルと所有権モデルの両方を拡張的に評価しました。結果は、Molmo-72Bがその他のモデルを一貫して上回っていることを示し、しかし所有権モデルは性能を比較的に示すことが増えています。また、指し手タスクに特に対するサブジェクト訓練はモデルの性能を大幅に向上させることを見出しました。我々の多段階評価パイプラインの中でも、強い相関を見出し、多模構モデルが抽象的な理由論と具象的な実世界的な行動をエフェクティブにつなげるための指し手機能の精度が重要な役割を果たすことを強調します。プロジェクトページ：https://pointarena.github.io/",
      "upvotes": 7,
      "discussionId": "6826dce8682e62074d0ed7eb",
      "projectPage": "https://pointarena.github.io/",
      "githubRepo": "https://github.com/pointarena/pointarena",
      "ai_keywords": [
        "multimodal models",
        "referential object localization tasks",
        "PointArena",
        "Point-Bench",
        "reasoning categories",
        "Point-Battle",
        "Point-Act",
        "blind, pairwise model comparisons",
        "Molmo-72B",
        "supervised training",
        "precise pointing capabilities"
      ]
    },
    "publishedAt": "2025-05-15T02:04:42.000Z",
    "title": "PointArena: Probing Multimodal Grounding Through Language-Guided\n  Pointing",
    "summary": "Pointing serves as a fundamental and intuitive mechanism for grounding\nlanguage within visual contexts, with applications spanning robotics, assistive\ntechnologies, and interactive AI systems. While recent multimodal models have\nstarted to support pointing capabilities, existing benchmarks typically focus\nonly on referential object localization tasks. We introduce PointArena, a\ncomprehensive platform for evaluating multimodal pointing across diverse\nreasoning scenarios. PointArena comprises three components: (1) Point-Bench, a\ncurated dataset containing approximately 1,000 pointing tasks across five\nreasoning categories; (2) Point-Battle, an interactive, web-based arena\nfacilitating blind, pairwise model comparisons, which has already gathered over\n4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation\nsystem allowing users to directly evaluate multimodal model pointing\ncapabilities in practical settings. We conducted extensive evaluations of both\nstate-of-the-art open-source and proprietary multimodal models. Results\nindicate that Molmo-72B consistently outperforms other models, though\nproprietary models increasingly demonstrate comparable performance.\nAdditionally, we find that supervised training specifically targeting pointing\ntasks significantly enhances model performance. Across our multi-stage\nevaluation pipeline, we also observe strong correlations, underscoring the\ncritical role of precise pointing capabilities in enabling multimodal models to\neffectively bridge abstract reasoning with concrete, real-world actions.\nProject page: https://pointarena.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/IGaHXqPEos-rGXUNMY4mE.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632b42626110e37dba3d5bcb",
      "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg",
      "fullname": "Duan",
      "name": "Jiafei1224",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09926",
      "authors": [
        {
          "_id": "68268c3408f7cb26defd82fc",
          "name": "Bin-Bin Gao",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd82fd",
          "name": "Yue Zhu",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd82fe",
          "name": "Jiangtao Yan",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd82ff",
          "name": "Yuezhi Cai",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8300",
          "name": "Weixi Zhang",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8301",
          "name": "Meng Wang",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8302",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8303",
          "name": "Yong Liu",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8304",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "68268c3408f7cb26defd8305",
          "name": "Chengjie Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T03:24:28.000Z",
      "submittedOnDailyAt": "2025-05-16T00:07:29.541Z",
      "title": "AdaptCLIP: 通用可视异常检测のCLIPの適応",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "Universal visual anomaly detectionの目標は、追加の微調練習を不要にして新しいまたは見ぬ視覚領域からの異常を識別することであり、開放シナリオでは重要である。最近の研究は、CLIPなどの予習された視覚言語モデルが、ゼロまたは少ない通常の画像で強い一般化性能を示していることを明らかにしている。しかし、現在の方法は、プロンプトテンプレートの設計、複雑なトークンの相互作用、または追加の微調練習の必要性を持っているため、柔軟性が限られている。本稿では、2つのキーの見通しに基づいて、簡単で効果的な方法を提案している。1つ目は、適応的な視覚的および語文的な表現は、または共に学習するよりは交替的に学習するべきである。2つ目は、クエリと通常の画像のプロンプトの比較学習は、コンテキスト的と対応付けされた残差特徴を含め、残差特徴のみに依存しないことが重要である。AdaptCLIPは、CLIPモデルを基盤サービスとし、入力または出力端に3つの簡単なアダプターを追加している。AdaptCLIPは、基盤データセットで訓練された後、目標領域でトレーニングを必要としないように、領域間の零/少ないショットの一般化をサポートしている。AdaptCLIPは、12つの工業と医療領域の異常検出ベンチマークで最先端の性能を達成し、現在の動力な方法を大幅に超えている。AdaptCLIPのコードとモデルは、https://github.com/gaobb/AdaptCLIPに公開される。",
      "upvotes": 5,
      "discussionId": "68268c3808f7cb26defd83bf",
      "ai_keywords": [
        "CLIP",
        "pre-trained vision-language models",
        "prompt templates",
        "token interactions",
        "fine-tuning",
        "adaptive visual and textual representations",
        "comparative learning",
        "query and normal image prompt",
        "contextual and aligned residual features",
        "residual features",
        "visual adapter",
        "textual adapter",
        "prompt-query adapter",
        "zero-/few-shot generalization",
        "training-free",
        "anomaly detection benchmarks",
        "industrial domains",
        "medical domains"
      ]
    },
    "publishedAt": "2025-05-14T23:24:28.000Z",
    "title": "AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection",
    "summary": "Universal visual anomaly detection aims to identify anomalies from novel or\nunseen vision domains without additional fine-tuning, which is critical in open\nscenarios. Recent studies have demonstrated that pre-trained vision-language\nmodels like CLIP exhibit strong generalization with just zero or a few normal\nimages. However, existing methods struggle with designing prompt templates,\ncomplex token interactions, or requiring additional fine-tuning, resulting in\nlimited flexibility. In this work, we present a simple yet effective method\ncalled AdaptCLIP based on two key insights. First, adaptive visual and textual\nrepresentations should be learned alternately rather than jointly. Second,\ncomparative learning between query and normal image prompt should incorporate\nboth contextual and aligned residual features, rather than relying solely on\nresidual features. AdaptCLIP treats CLIP models as a foundational service,\nadding only three simple adapters, visual adapter, textual adapter, and\nprompt-query adapter, at its input or output ends. AdaptCLIP supports\nzero-/few-shot generalization across domains and possesses a training-free\nmanner on target domains once trained on a base dataset. AdaptCLIP achieves\nstate-of-the-art performance on 12 anomaly detection benchmarks from industrial\nand medical domains, significantly outperforming existing competitive methods.\nWe will make the code and model of AdaptCLIP available at\nhttps://github.com/gaobb/AdaptCLIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09926.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.08617",
      "authors": [
        {
          "_id": "6826aa068caf98415c50897f",
          "name": "Zhaochen Su",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508980",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508981",
          "name": "Mingyang Song",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508982",
          "name": "Yunzhuo Hao",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508983",
          "name": "Zhengyuan Yang",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508984",
          "name": "Jun Zhang",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508985",
          "name": "Guanjie Chen",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508986",
          "name": "Jiawei Gu",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508987",
          "name": "Juntao Li",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508988",
          "name": "Xiaoye Qu",
          "hidden": false
        },
        {
          "_id": "6826aa068caf98415c508989",
          "name": "Yu Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-13T14:35:51.000Z",
      "submittedOnDailyAt": "2025-05-16T01:29:32.761Z",
      "title": "OpenThinkIMG: 画像をもとに考えることを学ぶための可視化ツールを用いた強化学習",
      "submittedOnDailyBy": {
        "_id": "64264095ba51f8a2136946a0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
        "isPro": false,
        "fullname": "Zhaochen Su",
        "user": "Warrieryes",
        "type": "user"
      },
      "summary": "ヒトは複雑な問題解決においてインタラクティブな視覚的知識を柔軟に活用できますが、LVLM（大視覚言語モデル）が視覚ツールを用いて類似の適応的な行動を学習することは難しい。現在の標準化されたインフラの欠如が、多様なツールの統合、豊富な相互作用データの生成、ロバストなアガントの効果的な学習に負担をかけている。これらの欠点を解決するために、OpenThinkIMGという最初の開放ソースでの機能付きLVLMの一様的な終端から終端までのフレームワークを介しています。標準化された視覚ツールインターフェース、スケーラブルなパラメータ生成、柔軟な学習環境を特徴としています。また、静的な示唆に基づく規範的な微調節（SFT）は動的なツールの呼び出しに対する政策の一般化を限定しているため、新しい強化学習（RL）フレームワークV-ToolRLを提案しています。V-ToolRLはLVLMを動的な政策の学習に向けて外部の視覚ツールを呼び出すことを学習させることを目指しています。V-ToolRLはツールの相互作用からのフィードバックを直接最適化して、LVLMが自動的に最適なツール使用戦略を発見することを可能にします。V-ToolRLは難しいチャート理由論タスクで実験的に検証され、SFTで初期化されたものを+28.83点超え、タコやCogComといった既存の規範的なツール学習ベースラインを平均+12.7点超え、GPT-4.1といった閉じられたソースモデルを+8.68精度点超えました。私たちはOpenThinkIMGが動的な、機能付き視覚的理由論の進歩を促進する基盤的なフレームワークとして役立つことを望んでいます。",
      "upvotes": 5,
      "discussionId": "6826aa078caf98415c5089d6",
      "githubRepo": "https://github.com/zhaochen0110/OpenThinkIMG",
      "ai_keywords": [
        "Large Vision-Language Models (LVLMs)",
        "OpenThinkIMG",
        "vision tool interfaces",
        "scalable trajectory generation",
        "policy initialization",
        "flexible training environment",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "V-ToolRL",
        "adaptive policies",
        "external vision tools",
        "optimal tool-usage strategies",
        "task success",
        "feedback from tool interactions",
        "chart reasoning tasks",
        "Qwen2-VL-2B",
        "Taco",
        "CogCom",
        "GPT-4.1"
      ]
    },
    "publishedAt": "2025-05-13T10:35:51.000Z",
    "title": "OpenThinkIMG: Learning to Think with Images via Visual Tool\n  Reinforcement Learning",
    "summary": "While humans can flexibly leverage interactive visual cognition for complex\nproblem-solving, enabling Large Vision-Language Models (LVLMs) to learn\nsimilarly adaptive behaviors with visual tools remains challenging. A\nsignificant hurdle is the current lack of standardized infrastructure, which\nhinders integrating diverse tools, generating rich interaction data, and\ntraining robust agents effectively. To address these gaps, we introduce\nOpenThinkIMG, the first open-source, comprehensive end-to-end framework for\ntool-augmented LVLMs. It features standardized vision tool interfaces, scalable\ntrajectory generation for policy initialization, and a flexible training\nenvironment. Furthermore, considering supervised fine-tuning (SFT) on static\ndemonstrations offers limited policy generalization for dynamic tool\ninvocation, we propose a novel reinforcement learning (RL) framework V-ToolRL\nto train LVLMs to learn adaptive policies for invoking external vision tools.\nV-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies\nby directly optimizing for task success using feedback from tool interactions.\nWe empirically validate V-ToolRL on challenging chart reasoning tasks. Our\nRL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its\nSFT-initialized counterpart (+28.83 points) and surpasses established\nsupervised tool-learning baselines like Taco and CogCom by an average of +12.7\npoints. Notably, it also surpasses prominent closed-source models like GPT-4.1\nby +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational\nframework for advancing dynamic, tool-augmented visual reasoning, helping the\ncommunity develop AI agents that can genuinely \"think with images\".",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08617.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64264095ba51f8a2136946a0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
      "fullname": "Zhaochen Su",
      "name": "Warrieryes",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.08581",
      "authors": [
        {
          "_id": "6825c47fbe0e3c3bfbf4d285",
          "user": {
            "_id": "66bf75432777c05070bf49dc",
            "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
            "isPro": false,
            "fullname": "Haofeng Liu",
            "user": "HeverLaw",
            "type": "user"
          },
          "name": "Haofeng Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-16T07:12:36.358Z",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d286",
          "name": "Mingqi Gao",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d287",
          "name": "Xuxiao Luo",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d288",
          "name": "Ziyue Wang",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d289",
          "name": "Guanyi Qin",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d28a",
          "name": "Junde Wu",
          "hidden": false
        },
        {
          "_id": "6825c47fbe0e3c3bfbf4d28b",
          "name": "Yueming Jin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66bf75432777c05070bf49dc/-a-XXHMaiah3MqSJzX3FE.mp4"
      ],
      "publishedAt": "2025-05-13T13:56:10.000Z",
      "submittedOnDailyAt": "2025-05-16T02:23:20.718Z",
      "title": "ReSurgSAM2: 手術ビデオで信頼性のある長期タキングによるオブジェクト参照分割",
      "submittedOnDailyBy": {
        "_id": "66bf75432777c05070bf49dc",
        "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
        "isPro": false,
        "fullname": "Haofeng Liu",
        "user": "HeverLaw",
        "type": "user"
      },
      "summary": "手術シーン分割は、計算機協力手術では重要であり、手術の質と患者の結果を向上させるために不可欠です。最近、手術の参照分割が出現し、手術医に目標物の分割をインタラクティブな経験を提供する利点に基づいています。しかし、現在の方法は低い効率性と短期間のトラッキングに制限されていて、複雑な実世界的な手術シナリオでの適用が難しいです。本論文では、Segment Anything Model 2を利用して、文脈による標的物検出を行い、信頼性のある初期フレームの識別と多様性を強化した長期記憶を組み合わせた2ステップの手術参照分割フレームワークReSurgSAM2を介して、検出ステップでは、交叉モーダルスペクトルタイムマンバーを用いて検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、検出ステップでは、検出と分割結果を生成し、これに基づいて信頼性のある初期フレーム選択戦略を提案し、開始フレームを選択した後、トラッキングステップでは、多様性を強化したメモリ機構を組み合わせて信頼性のあるさまざまなメモリバンクを維持し、一致した長期間トラッキングを確保することで、",
      "upvotes": 4,
      "discussionId": "6825c480be0e3c3bfbf4d2c0",
      "githubRepo": "https://github.com/jinlab-imvr/ReSurgSAM2",
      "ai_keywords": [
        "Segment Anything Model 2",
        "text-referred target detection",
        "cross-modal spatial-temporal Mamba",
        "initial frame identification",
        "diversity-driven long-term memory",
        "diversity-driven memory mechanism",
        "memory bank",
        "real-time tracking",
        "real-time operation"
      ]
    },
    "publishedAt": "2025-05-13T09:56:10.000Z",
    "title": "ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible\n  Long-term Tracking",
    "summary": "Surgical scene segmentation is critical in computer-assisted surgery and is\nvital for enhancing surgical quality and patient outcomes. Recently, referring\nsurgical segmentation is emerging, given its advantage of providing surgeons\nwith an interactive experience to segment the target object. However, existing\nmethods are limited by low efficiency and short-term tracking, hindering their\napplicability in complex real-world surgical scenarios. In this paper, we\nintroduce ReSurgSAM2, a two-stage surgical referring segmentation framework\nthat leverages Segment Anything Model 2 to perform text-referred target\ndetection, followed by tracking with reliable initial frame identification and\ndiversity-driven long-term memory. For the detection stage, we propose a\ncross-modal spatial-temporal Mamba to generate precise detection and\nsegmentation results. Based on these results, our credible initial frame\nselection strategy identifies the reliable frame for the subsequent tracking.\nUpon selecting the initial frame, our method transitions to the tracking stage,\nwhere it incorporates a diversity-driven memory mechanism that maintains a\ncredible and diverse memory bank, ensuring consistent long-term tracking.\nExtensive experiments demonstrate that ReSurgSAM2 achieves substantial\nimprovements in accuracy and efficiency compared to existing methods, operating\nin real-time at 61.2 FPS. Our code and datasets will be available at\nhttps://github.com/jinlab-imvr/ReSurgSAM2.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66bf75432777c05070bf49dc/-a-XXHMaiah3MqSJzX3FE.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.08581.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66bf75432777c05070bf49dc",
      "avatarUrl": "/avatars/e18c2f6f5b1c37873dc181de4f255a8b.svg",
      "fullname": "Haofeng Liu",
      "name": "HeverLaw",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.10046",
      "authors": [
        {
          "_id": "6826b00ed4c8b864e5ed1c0f",
          "name": "Bingda Tang",
          "hidden": false
        },
        {
          "_id": "6826b00ed4c8b864e5ed1c10",
          "name": "Boyang Zheng",
          "hidden": false
        },
        {
          "_id": "6826b00ed4c8b864e5ed1c11",
          "name": "Xichen Pan",
          "hidden": false
        },
        {
          "_id": "6826b00ed4c8b864e5ed1c12",
          "name": "Sayak Paul",
          "hidden": false
        },
        {
          "_id": "6826b00ed4c8b864e5ed1c13",
          "name": "Saining Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-15T07:43:23.000Z",
      "submittedOnDailyAt": "2025-05-16T01:55:37.654Z",
      "title": "深層融合の大規模言語モデルとディフュージョンディフュージョンのTransformersを用いた文から画像の合成についての研究",
      "submittedOnDailyBy": {
        "_id": "5f7fbd813e94f16a85448745",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
        "isPro": false,
        "fullname": "Sayak Paul",
        "user": "sayakpaul",
        "type": "user"
      },
      "summary": "この論文は新しい方法を説明しません。代りに、最近の文脈から画像の合成に関する重要なければ研究が不足している設計空間について、詳細な探索を提供します。特に、大規模言語モデル（LLMs）と拡散変換器（DiTs）の深い融合を用いた多タイプ生成に関するものです。先行研究は主に全体的なシステムの性能を焦点にしていましたが、様々な方法との詳細な比較や、重要な設計詳細とトレーニングドキュメントが通常公開されていません。これらの欠陥は、このアプローチの実際の可能性についての不安を生み出しています。これらの欠陥を補うために、文脈から画像の生成について実験的研究を行い、既存の基準との制御された比較を行い、重要な設計選択肢の分析を行い、スケールアップでのトレーニングの確認可能なレシピを提供します。私たちは、この研究が将来の多タイプ生成に関する研究に意味のあるデータ点と実用的なガイドラインを提供していただきます。",
      "upvotes": 3,
      "discussionId": "6826b00ed4c8b864e5ed1c4f",
      "githubRepo": "https://github.com/tang-bd/fuse-dit",
      "ai_keywords": [
        "large language models (LLMs)",
        "diffusion transformers (DiTs)",
        "multi-modal generation",
        "text-to-image synthesis",
        "controlled comparisons",
        "established baselines",
        "important design choices",
        "training recipes",
        "reproducible recipe",
        "training at scale"
      ]
    },
    "publishedAt": "2025-05-15T03:43:23.000Z",
    "title": "Exploring the Deep Fusion of Large Language Models and Diffusion\n  Transformers for Text-to-Image Synthesis",
    "summary": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10046.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f7fbd813e94f16a85448745",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
      "fullname": "Sayak Paul",
      "name": "sayakpaul",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 619
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.09265",
      "authors": [
        {
          "_id": "682547e6501a31b392e78f6a",
          "user": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "isPro": false,
            "fullname": "Bin-Bin Gao",
            "user": "csgaobb",
            "type": "user"
          },
          "name": "Bin-Bin Gao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-15T01:54:50.125Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T10:25:26.000Z",
      "submittedOnDailyAt": "2025-05-16T00:08:33.750Z",
      "title": "MetaUAS: ユニバーサルな異常分割を一プロンプトでのメタラーニングで実現する",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "Zero- and few-shot 視覚異常分割は、手動によって設計された文脈プロンプトを使用して見たことのない異常を検出するために強力な視覚言語モデルを依存しています。しかし、視覚的表現は言語に固有であり、これらが独立しています。本論文では、広く使用されている視覚言語モデルの代わりに、絶対的に視覚的な基盤モデルの可能性を検討し、普遍的な視覚異常分割を行うことを目的としています。新しいパラダイムを提案し、異常分割を変化分割と一つにします。このパラダイムでは、現存する画像データセットから得られる物体レベルと局所領域の変化を特徴とした大規模な合成画像ペアを使用します。これらのデータセットは、目標の異常データセットに依存していません。Meta-learning フレームワークを提案し、Universal Anomaly Segmentation (MetaUAS) を実現します。このフレームワークは、合成データセットで訓練され、実世界の新しいまたは見たことのない視覚異常を効果的に分割できます。プロンプトとクエリ画像の間の幾何学的な変化を処理するために、配对画像の変化認識と単一画像のセマンティックセグメンテーションを結ぶ軟体特徴配列モジュールを提案します。これは、特殊な異常検出データセットとプレーントレーニングされた視覚言語モデルを依存させないまま、絶対的に視覚的なモデルを使用して普遍的な異常分割を実現することができる最初の研究です。我々の方法は、そのひとつの通常画像プロンプトを使用して、言語によるガイドを受けずに効果的に効率的に異常を分割できます。MetaUAS は、先行する zero-shot、few-shot、そして全射異常分割手法に比べて显著に優れています。コードとプレーントレーニングモデルは、https://github.com/gaobb/MetaUAS に提供されています。",
      "upvotes": 3,
      "discussionId": "682547eb501a31b392e79038",
      "githubRepo": "https://github.com/gaobb/MetaUAS",
      "ai_keywords": [
        "Meta-learning",
        "Universal Anomaly Segmentation (MetaUAS)",
        "synthetic image pairs",
        "object-level changes",
        "local region changes",
        "prompt",
        "query images",
        "soft feature alignment module",
        "paired-image change perception",
        "single-image semantic segmentation",
        "universal anomaly segmentation",
        "pure vision model",
        "zero-shot",
        "few-shot",
        "full-shot anomaly segmentation"
      ]
    },
    "publishedAt": "2025-05-14T06:25:26.000Z",
    "title": "MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning",
    "summary": "Zero- and few-shot visual anomaly segmentation relies on powerful\nvision-language models that detect unseen anomalies using manually designed\ntextual prompts. However, visual representations are inherently independent of\nlanguage. In this paper, we explore the potential of a pure visual foundation\nmodel as an alternative to widely used vision-language models for universal\nvisual anomaly segmentation. We present a novel paradigm that unifies anomaly\nsegmentation into change segmentation. This paradigm enables us to leverage\nlarge-scale synthetic image pairs, featuring object-level and local region\nchanges, derived from existing image datasets, which are independent of target\nanomaly datasets. We propose a one-prompt Meta-learning framework for Universal\nAnomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and\nthen generalizes well to segment any novel or unseen visual anomalies in the\nreal world. To handle geometrical variations between prompt and query images,\nwe propose a soft feature alignment module that bridges paired-image change\nperception and single-image semantic segmentation. This is the first work to\nachieve universal anomaly segmentation using a pure vision model without\nrelying on special anomaly detection datasets and pre-trained visual-language\nmodels. Our method effectively and efficiently segments any anomalies with only\none normal image prompt and enjoys training-free without guidance from\nlanguage. Our MetaUAS significantly outperforms previous zero-shot, few-shot,\nand even full-shot anomaly segmentation methods. The code and pre-trained\nmodels are available at https://github.com/gaobb/MetaUAS.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09265.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09264",
      "authors": [
        {
          "_id": "682548bff4997d78fe92cd57",
          "user": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "isPro": false,
            "fullname": "Bin-Bin Gao",
            "user": "csgaobb",
            "type": "user"
          },
          "name": "Bin-Bin Gao",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-15T01:54:51.290Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T10:25:14.000Z",
      "submittedOnDailyAt": "2025-05-16T00:10:20.034Z",
      "title": "学習して、1枚の正常画像だけで多クラスの異常を検出する",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "無ラベル化重建ネットワークを自動注意変換器（self-attention transformers）を使用して、多クラス（統一）異常検出の最先端の性能を達成しました。しかし、これらの自動注意重建モデルは主にターゲット特徴量に対して動作し、これにより、正常と異常の特徴量の両方が上下文と高い一致性を持っているため、それらの完全な重建が可能であることにより、異常検出に失敗します。また、これらのモデルは低スペクトル解析度の潜在空間で重建を行うため、不正確な異常部分分割を生成します。モデルの効率性を高めながら、統一異常検出の拡張性を向上させることを目的として、一つの正常画像プロンプト（OneNIP）で正常特徴量を重建し、異常特徴量を復元する簡単で効果的な方法を提案します。前の研究と異なり、OneNIPは最初に一つの正常画像プロンプトで異常の重建または復元を可能にし、統一異常検出の性能を効果的に向上させます。また、実際の正常画像と合成された異常画像を用いて重建誤差を回帰させるチェックファイナルを提案し、これによりピクセルレベルの異常部分分割を大幅に向上させます。OneNIPは、MVTec、BTAD、VisAの3つの業界異常検出ベンチマークで前の方法を超えます。コードと事前学習モデルは、https://github.com/gaobb/OneNIP に提供されています。",
      "upvotes": 3,
      "discussionId": "682548c1f4997d78fe92cdbc",
      "githubRepo": "https://github.com/gaobb/OneNIP",
      "ai_keywords": [
        "self-attention transformers",
        "multi-class anomaly detection",
        "self-attention reconstruction models",
        "target features",
        "context",
        "latent space",
        "One Normal Image Prompt (OneNIP)",
        "supervised refiner",
        "reconstruction errors",
        "MVTec",
        "BTAD",
        "VisA"
      ]
    },
    "publishedAt": "2025-05-14T06:25:14.000Z",
    "title": "Learning to Detect Multi-class Anomalies with Just One Normal Image\n  Prompt",
    "summary": "Unsupervised reconstruction networks using self-attention transformers have\nachieved state-of-the-art performance for multi-class (unified) anomaly\ndetection with a single model. However, these self-attention reconstruction\nmodels primarily operate on target features, which may result in perfect\nreconstruction for both normal and anomaly features due to high consistency\nwith context, leading to failure in detecting anomalies. Additionally, these\nmodels often produce inaccurate anomaly segmentation due to performing\nreconstruction in a low spatial resolution latent space. To enable\nreconstruction models enjoying high efficiency while enhancing their\ngeneralization for unified anomaly detection, we propose a simple yet effective\nmethod that reconstructs normal features and restores anomaly features with\njust One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP\nallows for the first time to reconstruct or restore anomalies with just one\nnormal image prompt, effectively boosting unified anomaly detection\nperformance. Furthermore, we propose a supervised refiner that regresses\nreconstruction errors by using both real normal and synthesized anomalous\nimages, which significantly improves pixel-level anomaly segmentation. OneNIP\noutperforms previous methods on three industry anomaly detection benchmarks:\nMVTec, BTAD, and VisA. The code and pre-trained models are available at\nhttps://github.com/gaobb/OneNIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09264.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.09263",
      "authors": [
        {
          "_id": "682549a75b5784e023ed7d8a",
          "name": "Guan Gui",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8b",
          "user": {
            "_id": "648972ff99f6c45ff6bbd295",
            "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
            "isPro": false,
            "fullname": "Bin-Bin Gao",
            "user": "csgaobb",
            "type": "user"
          },
          "name": "Bin-Bin Gao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-15T10:31:36.508Z",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8c",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8d",
          "name": "Chengjie Wang",
          "hidden": false
        },
        {
          "_id": "682549a75b5784e023ed7d8e",
          "name": "Yunsheng Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-14T10:25:06.000Z",
      "submittedOnDailyAt": "2025-05-16T00:12:32.405Z",
      "title": "フィードバックアノマリードリブングによるアノマリー分類とセグメンテーション",
      "submittedOnDailyBy": {
        "_id": "648972ff99f6c45ff6bbd295",
        "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
        "isPro": false,
        "fullname": "Bin-Bin Gao",
        "user": "csgaobb",
        "type": "user"
      },
      "summary": "異常検出は工業観測において異常サンプルの不足により実用的かつ艱難な課題です。現在の異常検出手法は、ノイズや外部データを用いて異常を合成することでこの問題を解決していますが、合成された異常と実世界の異常の間には大きな意味的間違いがあり、異常検出の性能が弱くなります。この問題を解決するために、私たちは幾ピックモードの異常駆動生成（AnoGen）メソッドを提案します。これは、幾つかの実際の異常サンプルをみたしたりしてディフュージョンモデルを経由して現実的で多様な異常を生成することで、異常検出モデルの訓練に利益を与えます。特に、私たちの研究は3つのステージに分かれています。最初のステージでは、幾つかの実際の異常サンプルに基づいて異常の分布を学習し、学習した知識を埋め込みに注入します。2番目のステージでは、埋め込みと与えられたボウンディングボックスを用いて特定の物体（またはテクスチャ）に対して現実的で多様な異常を生成することをディフュージョンモデルをガイドします。最終的なステージでは、生成された異常を用いて弱サブジューダリーな異常検出メソッドを提案し、これによりより強力なモデルを訓練します。私たちの方法は、DRAEMとDesTSegを基盤モデルとして、通常の工業用異常検出データセットで実験を行います。実験は、我々が生成した異常が異常分類と分割タスクの両方のモデル性能を効果的に向上させることを示しました。例えば、DRAEMとDesTSegはセグメンテーションタスクのAU-PRメトリックではそれぞれ5.8%と1.5%の向上を収めました。コードと生成された異常データは、https://github.com/gaobb/AnoGenにアクセスできます。",
      "upvotes": 3,
      "discussionId": "682549ac5b5784e023ed7e72",
      "ai_keywords": [
        "Anomaly-driven Generation (AnoGen)",
        "diffusion model",
        "anomaly distribution",
        "embedding",
        "bounding boxes",
        "weakly-supervised anomaly detection",
        "DRAEM",
        "DesTSeg",
        "MVTec",
        "AU-PR metric"
      ]
    },
    "publishedAt": "2025-05-14T06:25:06.000Z",
    "title": "Few-Shot Anomaly-Driven Generation for Anomaly Classification and\n  Segmentation",
    "summary": "Anomaly detection is a practical and challenging task due to the scarcity of\nanomaly samples in industrial inspection. Some existing anomaly detection\nmethods address this issue by synthesizing anomalies with noise or external\ndata. However, there is always a large semantic gap between synthetic and\nreal-world anomalies, resulting in weak performance in anomaly detection. To\nsolve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)\nmethod, which guides the diffusion model to generate realistic and diverse\nanomalies with only a few real anomalies, thereby benefiting training anomaly\ndetection models. Specifically, our work is divided into three stages. In the\nfirst stage, we learn the anomaly distribution based on a few given real\nanomalies and inject the learned knowledge into an embedding. In the second\nstage, we use the embedding and given bounding boxes to guide the diffusion\nmodel to generate realistic and diverse anomalies on specific objects (or\ntextures). In the final stage, we propose a weakly-supervised anomaly detection\nmethod to train a more powerful model with generated anomalies. Our method\nbuilds upon DRAEM and DesTSeg as the foundation model and conducts experiments\non the commonly used industrial anomaly detection dataset, MVTec. The\nexperiments demonstrate that our generated anomalies effectively improve the\nmodel performance of both anomaly classification and segmentation tasks\nsimultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement\nin AU-PR metric on segmentation task, respectively. The code and generated\nanomalous data are available at https://github.com/gaobb/AnoGen.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.09263.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "648972ff99f6c45ff6bbd295",
      "avatarUrl": "/avatars/d1fe9d631a786462d55cfe43c565c88e.svg",
      "fullname": "Bin-Bin Gao",
      "name": "csgaobb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  }
]