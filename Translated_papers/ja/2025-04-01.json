[
  {
    "paper": {
      "id": "2503.23307",
      "authors": [
        {
          "_id": "67eb4bd0eca57c4eebbb343a",
          "user": {
            "_id": "64f8e358766ff9f3d2b0de84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
            "isPro": true,
            "fullname": "Cong Wei",
            "user": "lim142857",
            "type": "user"
          },
          "name": "Cong Wei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:21.554Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343b",
          "name": "Bo Sun",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343c",
          "user": {
            "_id": "650a8979c19e5b4c8a6ff062",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650a8979c19e5b4c8a6ff062/64_JuECX_k_-uK7m7nlua.jpeg",
            "isPro": false,
            "fullname": "Haoyu Ma",
            "user": "haoyum1997",
            "type": "user"
          },
          "name": "Haoyu Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:54:47.847Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343d",
          "name": "Ji Hou",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343e",
          "user": {
            "_id": "6444e8911cfc9ae6bb3ad216",
            "avatarUrl": "/avatars/8c06e064cf24789e4131f7af06dac86b.svg",
            "isPro": false,
            "fullname": "Xu",
            "user": "FelixXu",
            "type": "user"
          },
          "name": "Felix Juefei-Xu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:55:03.326Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb343f",
          "name": "Zecheng He",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3440",
          "user": {
            "_id": "6549417b3ce45eb764faf993",
            "avatarUrl": "/avatars/d310f475d0697f5f13b3d4141ea0ccaf.svg",
            "isPro": false,
            "fullname": "Xiaoliang Dai",
            "user": "daixl1992",
            "type": "user"
          },
          "name": "Xiaoliang Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:54:01.490Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3441",
          "user": {
            "_id": "65a4fa7d2548c41ad9d9b710",
            "avatarUrl": "/avatars/3cace2d2f11f7194d8eca4b95b0b57cc.svg",
            "isPro": false,
            "fullname": "Luxin Zhang",
            "user": "Luczzz",
            "type": "user"
          },
          "name": "Luxin Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:53:55.152Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3442",
          "name": "Kunpeng Li",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3443",
          "user": {
            "_id": "655846d7ed8df83128f5826a",
            "avatarUrl": "/avatars/d7ce174d7d1b8614d5f6f071225c0057.svg",
            "isPro": false,
            "fullname": "Hou",
            "user": "Tingbo",
            "type": "user"
          },
          "name": "Tingbo Hou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:53:10.214Z",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3444",
          "name": "Animesh Sinha",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3445",
          "name": "Peter Vajda",
          "hidden": false
        },
        {
          "_id": "67eb4bd0eca57c4eebbb3446",
          "user": {
            "_id": "6313a86154e6e5d9f0f94e04",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
            "isPro": false,
            "fullname": "Wenhu Chen",
            "user": "wenhu",
            "type": "user"
          },
          "name": "Wenhu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:52:50.248Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T04:22:09.000Z",
      "submittedOnDailyAt": "2025-04-01T00:46:45.446Z",
      "title": "モジャラード：映画レベルの対話キャラクター合成への挑戦",
      "submittedOnDailyBy": {
        "_id": "64f8e358766ff9f3d2b0de84",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
        "isPro": true,
        "fullname": "Cong Wei",
        "user": "lim142857",
        "type": "user"
      },
      "summary": "最近の映像生成の進歩は、印象的な動きの写真感を達成していますが、それは人物を中心とした物語の伝え方に注目を向けていません。これは自動化された映画、アニメーションの生成にとって重要な課題です。我々は、Talking Charactersを紹介します。これは、直接の声と文章から会話的な人物アニメーションを生成するより写実的なタスクです。Talking Headと違い、Talking Charactersは人物全体の肖像を生成することを目指しています。この論文では、MoChaを提案します。これは、この種の最初のもので、会話的な人物アニメーションを生成するものです。映像と声の精密な同期を確保するために、声と映像トークンの対位を効果的に行う声画ウィンドウアテンション機構を提案します。大規模な声標識付き映像データセットの不足を解決するために、声標識付きと文章標識付きの映像データを両方とも利用する両方ともの訓練戦略を提案します。これは、多様な人物の動きにおける汎用化を大幅に向上させます。また、人物タグ付きの構造化プロンプトテンプレートを設計し、これは、まずまず、多様な人物のコースチャレンジを可能にし、フィルム的なコラフィーのコンテキストによる対話をAI生成の人物が行うことを可能にします。極めて詳細な質的的および数量的な評価、これは、人間の好み研究とベンチマーク比較を含むものです。これは、MoChaがAI生成の映画的な物語伝えに新たな標準を設定し、上級の写実感、表現力、制御可能さと汎用化を達成します。",
      "upvotes": 31,
      "discussionId": "67eb4bd3eca57c4eebbb34c7",
      "projectPage": "https://congwei1230.github.io/MoCha/",
      "ai_keywords": [
        "speech-video window attention mechanism",
        "speech-labeled video datasets",
        "text-labeled video data",
        "structured prompt templates",
        "character tags",
        "multi-character conversation",
        "turn-based dialogue",
        "context-aware conversations",
        "cinematic coherence"
      ]
    },
    "publishedAt": "2025-03-30T00:22:09.000Z",
    "title": "MoCha: Towards Movie-Grade Talking Character Synthesis",
    "summary": "Recent advancements in video generation have achieved impressive motion\nrealism, yet they often overlook character-driven storytelling, a crucial task\nfor automated film, animation generation. We introduce Talking Characters, a\nmore realistic task to generate talking character animations directly from\nspeech and text. Unlike talking head, Talking Characters aims at generating the\nfull portrait of one or more characters beyond the facial region. In this\npaper, we propose MoCha, the first of its kind to generate talking characters.\nTo ensure precise synchronization between video and speech, we propose a\nspeech-video window attention mechanism that effectively aligns speech and\nvideo tokens. To address the scarcity of large-scale speech-labeled video\ndatasets, we introduce a joint training strategy that leverages both\nspeech-labeled and text-labeled video data, significantly improving\ngeneralization across diverse character actions. We also design structured\nprompt templates with character tags, enabling, for the first time,\nmulti-character conversation with turn-based dialogue-allowing AI-generated\ncharacters to engage in context-aware conversations with cinematic coherence.\nExtensive qualitative and quantitative evaluations, including human preference\nstudies and benchmark comparisons, demonstrate that MoCha sets a new standard\nfor AI-generated cinematic storytelling, achieving superior realism,\nexpressiveness, controllability and generalization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23307.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64f8e358766ff9f3d2b0de84",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg",
      "fullname": "Cong Wei",
      "name": "lim142857",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23461",
      "authors": [
        {
          "_id": "67eb594988a08fae617242f1",
          "name": "Nikai Du",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f2",
          "user": {
            "_id": "66449e619ff401732687f013",
            "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
            "isPro": false,
            "fullname": "chen",
            "user": "zhen-nan",
            "type": "user"
          },
          "name": "Zhennan Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:46.364Z",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f3",
          "user": {
            "_id": "637c22183d8e2e9c40c09fcf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669079538761-noauth.jpeg",
            "isPro": false,
            "fullname": "Zhizhou Chen",
            "user": "Chenzzzzzz",
            "type": "user"
          },
          "name": "Zhizhou Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:57:44.605Z",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f4",
          "name": "Shan Gao",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f5",
          "name": "Xi Chen",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f6",
          "user": {
            "_id": "67593dd0f522f4409e614ba0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67593dd0f522f4409e614ba0/cvb9w_8seu3Kbjg_XAnNj.jpeg",
            "isPro": false,
            "fullname": "Jiang Zhengkai",
            "user": "jzzzzk",
            "type": "user"
          },
          "name": "Zhengkai Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:58:12.614Z",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f7",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67eb594988a08fae617242f8",
          "user": {
            "_id": "65734004769f3ee9bde1af10",
            "avatarUrl": "/avatars/d6310ed861972fd691687d8f47413f33.svg",
            "isPro": false,
            "fullname": "Ying Tai",
            "user": "yingtai",
            "type": "user"
          },
          "name": "Ying Tai",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:44.350Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T14:36:55.000Z",
      "submittedOnDailyAt": "2025-04-01T01:44:26.275Z",
      "title": "テキストクラフター：複雑な可視スキーム中で複数のテキストを正確に描画する",
      "submittedOnDailyBy": {
        "_id": "66449e619ff401732687f013",
        "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
        "isPro": false,
        "fullname": "chen",
        "user": "zhen-nan",
        "type": "user"
      },
      "summary": "この論文では、複雑な可視的文字生成（CVTG）の任務を検討しています。これは、可視画像内の多様な領域に分布する複雑な文字内容を生成することを中心としています。CVTGでは、画像生成モデルが歪みやブラードな可視文字を生成し、または一部の可視文字を省略することがあります。これらの課題を解決するために、私たちは「TextCrafter」という新しい多可視的文字描画手法を提案しています。TextCrafterは、複雑な可視文字を進歩的なステラジーで分解し、文字内容と可視載体との強固な対位を確保します。また、その過程で可視文字の重要性を強化するためにトークンフォーカス強化機能を採用しています。TextCrafterは、CVTGタスクにおける文字混同、省略、ブラードなどの主要な課題を効果的に解決します。また、CVTGタスクの生成モデルの性能を厳格に評価するために新しいベンチマークデータセット「CVTG-2K」を提出しています。拡張された実験は、私たちの方法が最先端のアプローチを超えることを示しています。",
      "upvotes": 26,
      "discussionId": "67eb594b88a08fae617243ac",
      "projectPage": "https://dnknju.github.io/textcrafter-vue/",
      "githubRepo": "https://github.com/NJU-PCALab/TextCrafter",
      "ai_keywords": [
        "complex visual text",
        "TextCrafter",
        "multi-visual text rendering",
        "progressive strategy",
        "token focus enhancement",
        "CVTG-2K",
        "generative models",
        "CVTG tasks",
        "state-of-the-art approaches"
      ]
    },
    "publishedAt": "2025-03-30T10:36:55.000Z",
    "title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual\n  Scenes",
    "summary": "This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23461.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66449e619ff401732687f013",
      "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
      "fullname": "chen",
      "name": "zhen-nan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24235",
      "authors": [
        {
          "_id": "67eb57023475e7b135788500",
          "user": {
            "_id": "62a42f22c683d02f5b63320c",
            "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
            "isPro": false,
            "fullname": "Qiyuan Zhang",
            "user": "DonJoey",
            "type": "user"
          },
          "name": "Qiyuan Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:57:25.339Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788501",
          "user": {
            "_id": "65d2bb5c6130ef7be012d235",
            "avatarUrl": "/avatars/1c1e3bbb2c683a5c9d1f792a2c13fc4a.svg",
            "isPro": false,
            "fullname": "Fuyuan Lyu",
            "user": "silentspring2",
            "type": "user"
          },
          "name": "Fuyuan Lyu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:56:19.295Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788502",
          "user": {
            "_id": "65d1b42f3da87ce21e33261a",
            "avatarUrl": "/avatars/041cb441fa3871acde4ba565632056bf.svg",
            "isPro": false,
            "fullname": "RubinSun",
            "user": "RubinSun",
            "type": "user"
          },
          "name": "Zexu Sun",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:51.229Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788503",
          "user": {
            "_id": "646def60df618b303b419323",
            "avatarUrl": "/avatars/97aa761d5255abf230304cfeade87835.svg",
            "isPro": false,
            "fullname": "Lei Wang",
            "user": "demolei",
            "type": "user"
          },
          "name": "Lei Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T08:01:21.676Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788504",
          "user": {
            "_id": "63414659c5565a4b8d41bc42",
            "avatarUrl": "/avatars/25b4ca3002edf4c35cded0902c26632a.svg",
            "isPro": false,
            "fullname": "Weixu Zhang",
            "user": "nancy-zwx",
            "type": "user"
          },
          "name": "Weixu Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:56:26.532Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788505",
          "user": {
            "_id": "63c0c2497f52541dfc7d7567",
            "avatarUrl": "/avatars/16c174e2803ef86d09815b36a666ee0e.svg",
            "isPro": false,
            "fullname": "ZhihanGUO",
            "user": "ZhihanGUO",
            "type": "user"
          },
          "name": "Zhihan Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:56:31.979Z",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788506",
          "name": "Yufei Wang",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788507",
          "name": "Irwin King",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788508",
          "name": "Xue Liu",
          "hidden": false
        },
        {
          "_id": "67eb57023475e7b135788509",
          "name": "Chen Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T15:46:15.000Z",
      "submittedOnDailyAt": "2025-04-01T01:37:27.268Z",
      "title": "どう、どのように、どこから、どのぐらい良いか？ 大規模言語モデルのテスト時スケーリングについての調査",
      "submittedOnDailyBy": {
        "_id": "62a42f22c683d02f5b63320c",
        "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
        "isPro": false,
        "fullname": "Qiyuan Zhang",
        "user": "DonJoey",
        "type": "user"
      },
      "summary": "スケーリングのエンタープライズが予習時代におけるデータとパラメータのスケーリングにとりこみ、テスト時スケーリング（TTS）と呼ばれる「テスト時計算」が厳密な研究の焦点として現れた。最近の研究は、TTSが大規模な言語モデル（LLMs）の問題解決能力を進め、数学やコーディングの専門的な論理任務だけでなく、開放的なQ&Aのような一般的な任務においても顕著な進歩を実現できることを示している。しかし、この分野における最近の努力が急激に増えているにも関わらず、システム的な理解を提供する綜合的な調査が急務である。この空間を填えるために、我々は、「何をスケールするか」、「どのようにスケールするか」、「どこでスケールするか」、「どのように良くスケールするか」という4つの核心的な次元に基づいた統一的な、多様的なフレームワークを提案している。このタクロジーに基づいて、我々は手法、応用スケーナー、評価面についての拡大的なレビューを行い、テスト時スケーリングの広いランドスケープでの個々の技術の特徴的な機能的役割を明らかにする有组织な分解を提供している。この分析から、我々は現在までのTTSの主な開発タライスを抽出し、実用的な採用のための手本を提供している。また、我々は数多くの開放的な課題を明らかにし、進展の可能性のある将来の方向を見積もることで、進一にスケールすること、技術の機能的な真実を明確化すること、より多くの任務に一般化すること、さらにその貢献を説明することを提案している。",
      "upvotes": 24,
      "discussionId": "67eb57053475e7b135788624",
      "ai_keywords": [
        "test-time scaling",
        "test-time computing",
        "large language models",
        "specialized reasoning tasks",
        "open-ended Q&A",
        "multidimensional framework",
        "what to scale",
        "how to scale",
        "where to scale",
        "how well to scale",
        "assessment aspects",
        "functional roles",
        "developmental trajectories",
        "practical deployment",
        "open challenges",
        "attributions"
      ]
    },
    "publishedAt": "2025-03-31T11:46:15.000Z",
    "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large\n  Language Models",
    "summary": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24235.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a42f22c683d02f5b63320c",
      "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg",
      "fullname": "Qiyuan Zhang",
      "name": "DonJoey",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24388",
      "authors": [
        {
          "_id": "67eb544113ca8dcb9ccb991b",
          "name": "Zhonghan Zhao",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991c",
          "user": {
            "_id": "64e8505321540e1da3226b54",
            "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
            "isPro": false,
            "fullname": "Wenwei Zhang",
            "user": "ZwwWayne",
            "type": "user"
          },
          "name": "Wenwei Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:59:04.743Z",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991d",
          "name": "Haian Huang",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991e",
          "name": "Kuikun Liu",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb991f",
          "user": {
            "_id": "64070c5c4dc5f2846c925e93",
            "avatarUrl": "/avatars/ac2d7c1cd4ecccd6a88b85767c963ec7.svg",
            "isPro": false,
            "fullname": "Gao Jianfei",
            "user": "pppppM",
            "type": "user"
          },
          "name": "Jianfei Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:59:34.182Z",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb9920",
          "user": {
            "_id": "64c9033c5381684d3eaac7f1",
            "avatarUrl": "/avatars/07d36ca193826044b0df04e3602b9ef8.svg",
            "isPro": false,
            "fullname": "Gaoang Wang",
            "user": "GaoangWang",
            "type": "user"
          },
          "name": "Gaoang Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T07:59:40.156Z",
          "hidden": false
        },
        {
          "_id": "67eb544113ca8dcb9ccb9921",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:59:52.000Z",
      "submittedOnDailyAt": "2025-04-01T01:27:12.837Z",
      "title": "RIG: 終端から終端までの一般的なポリシーでの理由と想像の協調",
      "submittedOnDailyBy": {
        "_id": "6601196cc91ba4c08ad6e270",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
        "isPro": false,
        "fullname": "yuzhe gu",
        "user": "vanilla1116",
        "type": "user"
      },
      "summary": "Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. Thus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments, and thus exhibits more than 17 times sample efficiency improvements and generalization in comparison with previous works. During inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions. Experimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.",
      "upvotes": 20,
      "discussionId": "67eb544213ca8dcb9ccb9963"
    },
    "publishedAt": "2025-03-31T13:59:52.000Z",
    "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist\n  Policy",
    "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than 17times sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24388.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6601196cc91ba4c08ad6e270",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/X2YPNzUOQXBz5Gv-xR9LW.jpeg",
      "fullname": "yuzhe gu",
      "name": "vanilla1116",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.24370",
      "authors": [
        {
          "_id": "67eb4fff13ca8dcb9cca5f9b",
          "user": {
            "_id": "62fae9328e137d7c4b896498",
            "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
            "isPro": false,
            "fullname": "Tong Wu",
            "user": "tongwu2020",
            "type": "user"
          },
          "name": "Tong Wu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:31:28.689Z",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9c",
          "user": {
            "_id": "653319c135ad8b9e58a6b874",
            "avatarUrl": "/avatars/755efb5829f3b6d3cef886fee26e1ba9.svg",
            "isPro": false,
            "fullname": "Chong Xiang",
            "user": "cxiang",
            "type": "user"
          },
          "name": "Chong Xiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:00:19.547Z",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9d",
          "name": "Jiachen T. Wang",
          "hidden": false
        },
        {
          "_id": "67eb4fff13ca8dcb9cca5f9e",
          "name": "Prateek Mittal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:50:13.000Z",
      "submittedOnDailyAt": "2025-04-01T01:02:34.304Z",
      "title": "思考介入による推理モデルの効果的な制御",
      "submittedOnDailyBy": {
        "_id": "62fae9328e137d7c4b896498",
        "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
        "isPro": false,
        "fullname": "Tong Wu",
        "user": "tongwu2020",
        "type": "user"
      },
      "summary": "理由付与された大規模言語モデル（LLMs）は、最終的な答えを生成する前に明記した理由論理ステップを生成し、複雑な問題解決においてモデルの優れた性能を達成します。本論文では、この新興の生成フレームワークがモデルの行動をより細かく制御するための特別な機会を提供していることを示します。私たちは、特定の思考トークンを戦略的に挿入または修正することでLLMsの内部の理由論理プロセスを明記してガイドするための新しいパラダイム「Thinking Intervention」を提案します。IFEvalの指示従い、SEPの指示階層、XSTestとSORRY-Benchの安全性の一致を含む複数のタスクに対して、詳細な評価を実施しました。結果として、Thinking Interventionは基準的なプロンプティングアプローチを大幅に超え、DeepSeek R1オープンソースモデルを使用して指示従いの場合では6.7%の精度の向上、指示階層についての理由論理の改善が15.4%、不安全なプロンプトの拒否率が40.0%の増加を実現しました。続いて、我々の研究は、理由論理LLMsの制御についての新しい研究の可能性を開拓しています。",
      "upvotes": 11,
      "discussionId": "67eb500013ca8dcb9cca5fe0",
      "ai_keywords": [
        "Reasoning-enhanced large language models (LLMs)",
        "intermediate reasoning steps",
        "Thinking Intervention",
        "thinking tokens",
        "instruction following",
        "IFEval",
        "instruction hierarchy",
        "SEP",
        "safety alignment",
        "XSTest",
        "SORRY-Bench",
        "open-source DeepSeek R1 models"
      ]
    },
    "publishedAt": "2025-03-31T13:50:13.000Z",
    "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
    "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62fae9328e137d7c4b896498",
      "avatarUrl": "/avatars/1bda39dec585c099417cc9daa9f53c42.svg",
      "fullname": "Tong Wu",
      "name": "tongwu2020",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24364",
      "authors": [
        {
          "_id": "67eb6e6088a08fae617860f3",
          "user": {
            "_id": "600b381d3cc3b87db94bc0ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
            "isPro": false,
            "fullname": "Łukasz Borchmann",
            "user": "Borchmann",
            "type": "user"
          },
          "name": "Łukasz Borchmann",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-04-01T05:01:23.216Z",
          "hidden": false
        },
        {
          "_id": "67eb6e6088a08fae617860f4",
          "user": {
            "_id": "66c5e93e8f14c260be9d9f63",
            "avatarUrl": "/avatars/f4bf15e23923ef3256d3f01a3278d8bc.svg",
            "isPro": false,
            "fullname": "Marek Wydmuch",
            "user": "sfc-mwydmuch",
            "type": "user"
          },
          "name": "Marek Wydmuch",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:01:57.075Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/600b381d3cc3b87db94bc0ce/ucxS0pF0YXK8TDRhoezTE.qt"
      ],
      "publishedAt": "2025-03-31T17:43:36.000Z",
      "submittedOnDailyAt": "2025-04-01T03:14:34.239Z",
      "title": "Query and Conquer: Execution-Guided SQL Generation",
      "submittedOnDailyBy": {
        "_id": "600b381d3cc3b87db94bc0ce",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
        "isPro": false,
        "fullname": "Łukasz Borchmann",
        "user": "Borchmann",
        "type": "user"
      },
      "summary": "私たちは、テキストからSQLタスクでの精度を大幅に向上させる複雑な出力を生成する新しいアプローチを提案します。私たちの方法は、複数の候補から最も語義的に一致したクエリを選択するために実行結果を活用し、計算量の多い推理方法（例：o1, o3-mini, DeepSeek R1）を超える小さい、コスト効率的なモデルを可能にします。また、推論コストを30倍に減らします。この方法は既存のモデルと容易に統合可能で、実用的でスケーラブルな最先端のSQL生成のパスワードを提供します。",
      "upvotes": 10,
      "discussionId": "67eb6e6188a08fae6178613f",
      "ai_keywords": [
        "text-to-SQL",
        "execution results",
        "semantically consistent",
        "query",
        "candidates",
        "models",
        "reasoning methods",
        "o1",
        "o3-mini",
        "DeepSeek R1",
        "inference cost",
        "SQL generation"
      ]
    },
    "publishedAt": "2025-03-31T13:43:36.000Z",
    "title": "Query and Conquer: Execution-Guided SQL Generation",
    "summary": "We propose a novel approach for generating complex outputs that significantly\nimproves accuracy in text-to-SQL tasks. Our method leverages execution results\nto select the most semantically consistent query from multiple candidates,\nenabling smaller, cost-effective models to surpass computationally intensive\nreasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference\ncost by as much as 30 times. It integrates effortlessly with existing models,\noffering a practical and scalable pathway to state-of-the-art SQL generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/600b381d3cc3b87db94bc0ce/ucxS0pF0YXK8TDRhoezTE.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24364.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "600b381d3cc3b87db94bc0ce",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/600b381d3cc3b87db94bc0ce/I3xpr4gzcG1uXawXBpWpD.jpeg",
      "fullname": "Łukasz Borchmann",
      "name": "Borchmann",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23284",
      "authors": [
        {
          "_id": "67eb5280aeab4ce97de07134",
          "user": {
            "_id": "6424538b9f9e65b42389920e",
            "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
            "isPro": false,
            "fullname": "Feng-Lin Liu",
            "user": "Okrin",
            "type": "user"
          },
          "name": "Feng-Lin Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:05.907Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07135",
          "user": {
            "_id": "662cd8b9322afcbae53fb06e",
            "avatarUrl": "/avatars/9847f5c2282d49e61e76a0a303e0b2b1.svg",
            "isPro": false,
            "fullname": "fuhongbo",
            "user": "fuhongbo",
            "type": "user"
          },
          "name": "Hongbo Fu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:01.616Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07136",
          "user": {
            "_id": "60e272ca6c78a8c122b12127",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
            "isPro": false,
            "fullname": "Xintao Wang",
            "user": "Xintao",
            "type": "user"
          },
          "name": "Xintao Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:09.910Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07137",
          "user": {
            "_id": "6360d9f0472131c3bc4f61df",
            "avatarUrl": "/avatars/c5d884e5ef19b781e3405aba6dd68ca8.svg",
            "isPro": false,
            "fullname": "WeicaiYe",
            "user": "WeicaiYe",
            "type": "user"
          },
          "name": "Weicai Ye",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:16.323Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07138",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de07139",
          "user": {
            "_id": "644c8324f02250233d0d67d9",
            "avatarUrl": "/avatars/feb39d281457c1750f3eada3c060a23e.svg",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "dizhang",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:07:30.236Z",
          "hidden": false
        },
        {
          "_id": "67eb5280aeab4ce97de0713a",
          "name": "Lin Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-30T02:44:09.000Z",
      "submittedOnDailyAt": "2025-04-01T02:19:10.110Z",
      "title": "スケッチビデオ：スケッチベースのビデオ生成と編集",
      "submittedOnDailyBy": {
        "_id": "6424538b9f9e65b42389920e",
        "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
        "isPro": false,
        "fullname": "Feng-Lin Liu",
        "user": "Okrin",
        "type": "user"
      },
      "summary": "テキストプラントまたは画像に基づくビデオ生成と編集において、進歩が達していますが、テキストだけでグローバルなレイアウトとジェネリックの詳細を正確に制御し、動作制御と局所的な編集を画像を通じて行うことが難しいという課題が残っています。本論文では、ビデオ生成におけるスケッチベースの空間的および動作制御を実現し、実的または合成ビデオの細かいグライスレディング編集をサポートすることを目指しています。DiTビデオ生成モデルに基づいて、スケッチ制御ブロックを持つメモリ効率的な制御構造を提案します。スケッチは1つか2つのキーフレーム（任意の時間点で）に描かれ、簡単なインタラクションを可能にします。このような時間的にスパースなスケッチ条件を全フレームに伝播するために、キーフレームと各ビデオフレームの関係を分析する間フレームアテンション機構を提案します。スケッチベースのビデオ編集においては、新しく編集された内容と元ビデオの空間的特徴と動的な動作の一致を維持するために追加したビデオインサートモジュールを設計します。推論時には、未編集フェーズの正確な保存において潜在融合を使用します。拡大的な実験は、我々のスケッチビデオが制御可能なビデオ生成と編集中の上位の性能を達成していることを示しています。",
      "upvotes": 9,
      "discussionId": "67eb5286aeab4ce97de07320",
      "githubRepo": "https://github.com/IGLICT/SketchVideo",
      "ai_keywords": [
        "DiT video generation model",
        "memory-efficient control structure",
        "sketch control blocks",
        "residual features",
        "skipped DiT blocks",
        "temporally sparse sketch conditions",
        "inter-frame attention mechanism",
        "keyframes",
        "video insertion module",
        "spatial feature",
        "dynamic motion",
        "latent fusion",
        "SketchVideo"
      ]
    },
    "publishedAt": "2025-03-29T22:44:09.000Z",
    "title": "SketchVideo: Sketch-based Video Generation and Editing",
    "summary": "Video generation and editing conditioned on text prompts or images have\nundergone significant advancements. However, challenges remain in accurately\ncontrolling global layout and geometry details solely by texts, and supporting\nmotion control and local modification through images. In this paper, we aim to\nachieve sketch-based spatial and motion control for video generation and\nsupport fine-grained editing of real or synthetic videos. Based on the DiT\nvideo generation model, we propose a memory-efficient control structure with\nsketch control blocks that predict residual features of skipped DiT blocks.\nSketches are drawn on one or two keyframes (at arbitrary time points) for easy\ninteraction. To propagate such temporally sparse sketch conditions across all\nframes, we propose an inter-frame attention mechanism to analyze the\nrelationship between the keyframes and each video frame. For sketch-based video\nediting, we design an additional video insertion module that maintains\nconsistency between the newly edited content and the original video's spatial\nfeature and dynamic motion. During inference, we use latent fusion for the\naccurate preservation of unedited regions. Extensive experiments demonstrate\nthat our SketchVideo achieves superior performance in controllable video\ngeneration and editing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23284.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6424538b9f9e65b42389920e",
      "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
      "fullname": "Feng-Lin Liu",
      "name": "Okrin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18809",
      "authors": [
        {
          "_id": "67eaa0f83ace6eb46745a9fe",
          "user": {
            "_id": "674f43d6df6fa102409f6d1a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
            "isPro": false,
            "fullname": "Augusto B. Corrêa",
            "user": "abcorrea",
            "type": "user"
          },
          "name": "Augusto B. Corrêa",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:12.565Z",
          "hidden": false
        },
        {
          "_id": "67eaa0f83ace6eb46745a9ff",
          "user": {
            "_id": "662fb9c891587703a677856e",
            "avatarUrl": "/avatars/9cb7f035a513279532fc205ce9c5902c.svg",
            "isPro": false,
            "fullname": "Andre Grahl Pereira",
            "user": "andregrahl",
            "type": "user"
          },
          "name": "André G. Pereira",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:13.942Z",
          "hidden": false
        },
        {
          "_id": "67eaa0f83ace6eb46745aa00",
          "user": {
            "_id": "66f3dfd4b8703dde248f6d26",
            "avatarUrl": "/avatars/c199c91d422500cc7c7556569291644d.svg",
            "isPro": false,
            "fullname": "Jendrik Seipp",
            "user": "jendrikseipp",
            "type": "user"
          },
          "name": "Jendrik Seipp",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:39.862Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-24T15:50:20.000Z",
      "submittedOnDailyAt": "2025-04-01T00:45:14.321Z",
      "title": "古典計画にLLMジェネレードヒューリスティックを用いる：Pythonコードで最先端の状態を挑戦する",
      "submittedOnDailyBy": {
        "_id": "674f43d6df6fa102409f6d1a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
        "isPro": false,
        "fullname": "Augusto B. Corrêa",
        "user": "abcorrea",
        "type": "user"
      },
      "summary": "近年、大規模言語モデル（LLMs）は様々な人工知能問題において驚異的な能力を示している。しかし、プランニングタスクの詳細な定義を提示されても信頼性のあるプランニングを行うことはできない。LLMsのプランニング能力を向上させるための試み、例えば、chain-of-thought prompting、fine-tuning、明示的な「理由」の設定など、正しいプランを生成することはできなく、より大きなタスクに対しても一般化できないことが多い。本論文では、LLMsを用いて、増加したサイズの外分布タスクでも正しいプランを生成する方法を示している。特定のプランニングドメインに対して、LLMを使ってPythonコードの形でドメイン依存性のあるヒューリスティック関数を生成させ、グリーデスベストファースト探索で訓練タスクで評価し、最も強いものを選択する。その結果、LLMにより生成されたヒューリスティック関数は、古典的なプランニングの最先端的なドメイン非依存性ヒューリスティック関数よりも多くの未見テストタスクを解くことができる。ドメイン依存性の最強な学習アルゴリズムと比較しても、それほど強力であることが示されている。これらの発見は、プロフェッショナルであるが、概念的実装は未最適化されたPythonプランナーに基づいていて、基準となるものは高度に最適化されたC++コードに基づいていることから、特に驚異的である。あるドメインでは、LLMにより生成されたヒューリスティック関数はベースラインよりも少なくなる状態を拡大することが見られ、それらは、状態の拡大を効率的に行うだけでなく、最先端のヒューリスティック関数よりもより情報的であることが示されている。全体として、我々の結果は、プランニングヒューリスティック関数プログラムのサンプリングがLLMsのプランニング能力を大幅に向上させることができることを示している。",
      "upvotes": 8,
      "discussionId": "67eaa0f93ace6eb46745aa3e",
      "ai_keywords": [
        "large language models (LLMs)",
        "chain-of-thought prompting",
        "fine-tuning",
        "reasoning",
        "planning domain",
        "domain-dependent heuristic functions",
        "Python code",
        "greedy best-first search",
        "state-of-the-art domain-independent heuristics",
        "domain-dependent planning",
        "unoptimized Python planner",
        "highly optimized C++ code",
        "planning heuristic function programs"
      ]
    },
    "publishedAt": "2025-03-24T11:50:20.000Z",
    "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State\n  of the Art with Python Code",
    "summary": "In recent years, large language models (LLMs) have shown remarkable\ncapabilities in various artificial intelligence problems. However, they fail to\nplan reliably, even when prompted with a detailed definition of the planning\ntask. Attempts to improve their planning capabilities, such as chain-of-thought\nprompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans\nand usually fail to generalize to larger tasks. In this paper, we show how to\nuse LLMs to generate correct plans, even for out-of-distribution tasks of\nincreasing size. For a given planning domain, we ask an LLM to generate several\ndomain-dependent heuristic functions in the form of Python code, evaluate them\non a set of training tasks within a greedy best-first search, and choose the\nstrongest one. The resulting LLM-generated heuristics solve many more unseen\ntest tasks than state-of-the-art domain-independent heuristics for classical\nplanning. They are even competitive with the strongest learning algorithm for\ndomain-dependent planning. These findings are especially remarkable given that\nour proof-of-concept implementation is based on an unoptimized Python planner\nand the baselines all build upon highly optimized C++ code. In some domains,\nthe LLM-generated heuristics expand fewer states than the baselines, revealing\nthat they are not only efficiently computable, but sometimes even more\ninformative than the state-of-the-art heuristics. Overall, our results show\nthat sampling a set of planning heuristic function programs can significantly\nimprove the planning capabilities of LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18809.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "674f43d6df6fa102409f6d1a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KT_-dmXWshUKvbhtn-LSs.png",
      "fullname": "Augusto B. Corrêa",
      "name": "abcorrea",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24115",
      "authors": [
        {
          "_id": "67eb5116d3a707c0a5b02bd1",
          "user": {
            "_id": "64a0ed5ed5374ca472cfb0ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
            "isPro": false,
            "fullname": "ZhimingMa",
            "user": "JimmyMa99",
            "type": "user"
          },
          "name": "Zhiming Ma",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:36:07.612Z",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd2",
          "user": {
            "_id": "6385f7b969634850f8ddd541",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669723465271-noauth.png",
            "isPro": false,
            "fullname": "Peidong Wang",
            "user": "WDong",
            "type": "user"
          },
          "name": "Peidong Wang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-04-01T02:36:07.612Z",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd3",
          "user": {
            "_id": "6466d57bf3e78d1d6be0505c",
            "avatarUrl": "/avatars/9659b7d0f6fa51efc127afb7a1ba14b1.svg",
            "isPro": false,
            "fullname": "HuangMinhua",
            "user": "HuangMinhua",
            "type": "user"
          },
          "name": "Minhua Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:02:15.224Z",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd4",
          "name": "Jingpeng Wang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd5",
          "name": "Kai Wu",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd6",
          "name": "Xiangzhao Lv",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd7",
          "name": "Yachun Pang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd8",
          "name": "Yin Yang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bd9",
          "name": "Wenjie Tang",
          "hidden": false
        },
        {
          "_id": "67eb5116d3a707c0a5b02bda",
          "name": "Yuchen Kang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/PsAJTW9JTyrHqtjQP0lLJ.png"
      ],
      "publishedAt": "2025-03-31T14:06:17.000Z",
      "submittedOnDailyAt": "2025-04-01T01:08:09.201Z",
      "title": "TeleAntiFraud-28k: 電話違反防止用音声-文字データセット",
      "submittedOnDailyBy": {
        "_id": "64a0ed5ed5374ca472cfb0ac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
        "isPro": false,
        "fullname": "ZhimingMa",
        "user": "JimmyMa99",
        "type": "user"
      },
      "summary": "電話詐欺の検出は、高品質な多モデル訓練データの欠如により重大な課題を抱えています。この空間を填ぐために、私たちは、自動化された電話詐欺分析に特に適した最初の開放ソースアウドィオテキストショートティミングデータセット「TeleAntiFraud-28k」を紹介します。データセットは以下の3つの戦略により構築されています：1) 自動的語音認識（ASR）によるコールレコーディングのテキスト記録（匿名化された元のアウドィオ）を使用したプライバシー保護テキストテロールサンプルの生成、テキストオンソースモデルの再現によるリアルウォールの一致性を確保；2) 真実のASR出力に基づく大規模言語モデル（LLM）による自己インストラクションサンプリングを通じて、シナリオのカバー範囲を拡張；3) 現在の詐欺手法をシミュレートするための多アグリエント相諜合成。生成されたデータセットは、28,511件の厳格に処理されたスピーチテキストペアを含み、詐欺理由の詳細な注釈を付け付けています。データセットは、シナリオ分類、詐欺検出、詐欺種類分類の3つのタスクに分割されています。また、私たちは、データセットから比例的にサンプリングされたインスタンスを含む標準化された評価ベンチマーク「TeleAntiFraud-Bench」を構築し、電話詐欺検出タスクのモデル性能のシステマ的なテストを促進します。また、混合データに基づく産業最適化された訓練（SFT）モデルを提供し、データ処理フレームワークをオープンソース化し、コミュニティ駆動のデータセット拡大を可能にします。この研究は、データプライバシーとシナリオの多様性の重要な課題を解決しながら、多モデルアンチフラウド研究の基盤を築くことを目的としています。このプロジェクトは、https://github.com/JimmyMa99/TeleAntiFraudでリリースされます。",
      "upvotes": 7,
      "discussionId": "67eb5117d3a707c0a5b02c4c",
      "ai_keywords": [
        "automatically speech recognition (ASR)",
        "text-to-speech (TTS)",
        "large language model (LLM)",
        "self-instruction sampling",
        "multi-agent adversarial synthesis",
        "supervised fine-tuning (SFT)",
        "hybrid real/synthetic data"
      ]
    },
    "publishedAt": "2025-03-31T10:06:17.000Z",
    "title": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud\n  Detection",
    "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a0ed5ed5374ca472cfb0ac/PsAJTW9JTyrHqtjQP0lLJ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24115.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a0ed5ed5374ca472cfb0ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a0ed5ed5374ca472cfb0ac/n_wXamXfR_PPn0hRbnR1X.jpeg",
      "fullname": "ZhimingMa",
      "name": "JimmyMa99",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23077",
      "authors": [
        {
          "_id": "67eb58c71e23a7499b683cce",
          "user": {
            "_id": "6650c77a74664a42ddfb9187",
            "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
            "isPro": false,
            "fullname": "yueliu1999",
            "user": "yueliu1999",
            "type": "user"
          },
          "name": "Yue Liu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:11:01.812Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683ccf",
          "name": "Jiaying Wu",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd0",
          "name": "Yufei He",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd1",
          "user": {
            "_id": "62728f4f6253fe2068da1021",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
            "isPro": false,
            "fullname": "Hongcheng Gao",
            "user": "HongchengGao",
            "type": "user"
          },
          "name": "Hongcheng Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:11:54.944Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd2",
          "user": {
            "_id": "67c7a9fb27c2e81cf8660375",
            "avatarUrl": "/avatars/a5129cca93a31d4b730af4c543051d8e.svg",
            "isPro": false,
            "fullname": "Hongyu Chen",
            "user": "HongyuChen",
            "type": "user"
          },
          "name": "Hongyu Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:05.159Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd3",
          "user": {
            "_id": "642577e06d0f0f5f1dc68904",
            "avatarUrl": "/avatars/3df7cfcf9ca6cd9c9e9b10a73f4efc35.svg",
            "isPro": false,
            "fullname": "Bibaolong",
            "user": "Bibaolong",
            "type": "user"
          },
          "name": "Baolong Bi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:18.251Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd4",
          "user": {
            "_id": "669e19e5dac1eb34c0f5f505",
            "avatarUrl": "/avatars/bec7d1d1dac2ad6570844d1f00e7df0a.svg",
            "isPro": false,
            "fullname": "Jiaheng Zhang",
            "user": "jiaheng233",
            "type": "user"
          },
          "name": "Jiaheng Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:23.776Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd5",
          "user": {
            "_id": "66221f1a90f3fd333c4ec52e",
            "avatarUrl": "/avatars/a3173d9603a69020ec24170831c97c2f.svg",
            "isPro": false,
            "fullname": "Zhiqi Huang",
            "user": "Angelalilyer",
            "type": "user"
          },
          "name": "Zhiqi Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:36.398Z",
          "hidden": false
        },
        {
          "_id": "67eb58c71e23a7499b683cd6",
          "user": {
            "_id": "651d8032c50012d33e914f2f",
            "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
            "isPro": false,
            "fullname": "Bryan Hooi",
            "user": "bhooi",
            "type": "user"
          },
          "name": "Bryan Hooi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:12:42.880Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-29T13:27:46.000Z",
      "submittedOnDailyAt": "2025-04-01T01:39:12.154Z",
      "title": "効率的な推論を実現した大規模な理由論モデルの評価：概観",
      "submittedOnDailyBy": {
        "_id": "6650c77a74664a42ddfb9187",
        "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
        "isPro": false,
        "fullname": "yueliu1999",
        "user": "yueliu1999",
        "type": "user"
      },
      "summary": "大論理モデル（LRMs）は、大語言モデル（LLMs）の論理能力を顕著に向上させ、複雑なタスク解決において有望な性能を発挥しています。しかし、論理的な計算プロセスにより、トークンの使用、メモリ消費、推論時間においての不適切性があります。そこで、本調査は、LRMsに特に設計された効率的な推論方法についてのレビューを提供し、トークンの不適切性を抑えながら論理の質を維持することを焦点としています。まず、最近の方法を2つの主なカテゴリに分類するためのタクノロジーを紹介します：（a）明示的な簡約化コインストーク（CoT），トークンを削減しながら明示的な論理構造を維持します，（b）隠れ表現内での論理ステップをエンコードする隠れコインストーク（CoT）。また、彼らの強みと弱点について議論します。次に、既存の方法の性能と効率性についての実験的な分析を行います。また、この分野での開放的な課題について議論します。そして、人間中心的な制御可能な論理、論理の説明性と効率のバランス、効率的な論理の安全性確保、効率的な論理の広範な応用について議論します。また、モデルの統合、新しいアーキテクチャ、アグエントローターのような技術を通じてLRMsの推論効率を向上させるためのキーインサイドを特徴的にします。このワークは、研究者にこの豊かな分野での課題を克服するために有効なガイドとして役立つことを望む。",
      "upvotes": 7,
      "discussionId": "67eb58c81e23a7499b683d12",
      "ai_keywords": [
        "Chain-of-Thought (CoT)",
        "explicit compact Chain-of-Thought (CoT)",
        "implicit latent CoT",
        "model merging",
        "agent routers"
      ]
    },
    "publishedAt": "2025-03-29T09:27:46.000Z",
    "title": "Efficient Inference for Large Reasoning Models: A Survey",
    "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23077.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6650c77a74664a42ddfb9187",
      "avatarUrl": "/avatars/92001bbe0ae9b14309730316b639cede.svg",
      "fullname": "yueliu1999",
      "name": "yueliu1999",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24290",
      "authors": [
        {
          "_id": "67eb762381e530baa56dc830",
          "user": {
            "_id": "625026b7d2d191ac43320c5e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg",
            "isPro": false,
            "fullname": "Jingcheng Hu",
            "user": "reign12",
            "type": "user"
          },
          "name": "Jingcheng Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:09:02.123Z",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc831",
          "user": {
            "_id": "664ae39ab5e5f95dc6209365",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg",
            "isPro": false,
            "fullname": "Yinmin Zhang",
            "user": "YinminZhang",
            "type": "user"
          },
          "name": "Yinmin Zhang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:09:09.884Z",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc832",
          "name": "Qi Han",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc833",
          "user": {
            "_id": "60d4440fe648443279aaffd8",
            "avatarUrl": "/avatars/bf7209c1f14ae120f5bfda5fda1301b7.svg",
            "isPro": false,
            "fullname": "Daxin Jiang",
            "user": "djiang",
            "type": "user"
          },
          "name": "Daxin Jiang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:09:22.914Z",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc834",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "67eb762381e530baa56dc835",
          "name": "Heung-Yeung Shum",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T16:36:05.000Z",
      "submittedOnDailyAt": "2025-04-01T03:44:53.609Z",
      "title": "Open-Reasoner-Zero: 基盤モデル上の強化学習のスケーリングにおけるオープンソースアプローチ",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "オープン・ラジオン・ゼロ、大規模な理由論的なRLトレーニングの最初の開放ソース実装を紹介します。これはスケーラビリティ、簡単性、アクセス性に焦点を当てています。拡大の実験を通じて、最小主義的なアプローチ、ベージャーのPPO（GAE（lambda=1, gamma=1））と直観的なルールベースの報酬を使用し、どのようなKL正規化も含めず、回答の長さとベンチマークの性能を両方スケールアップできることを示します。DeepSeek-R1-Zeroと同じベースモデルを使用して、AIME2024、MATH500、GPQA Diamondベンチマークで上位の性能を達成し、効率的であり、DeepSeek-R1-Zeroパイプラインに比べて十分の一のトレーニングステップを必要とします。開放ソースの精神に基づき、ソースコード、パラメータ設定、トレーニングデータ、モデル重みの各サイズで公開します。",
      "upvotes": 6,
      "discussionId": "67eb762481e530baa56dc872",
      "ai_keywords": [
        "Reinforcement Learning (RL)",
        "vanilla PPO",
        "GAE ($\\lambda=1$, $\\gamma=1$)",
        "rule-based rewards",
        "KL regularization",
        "response length",
        "benchmark performance",
        "AIME2024",
        "MATH500",
        "GPQA Diamond benchmark",
        "training steps"
      ]
    },
    "publishedAt": "2025-03-31T12:36:05.000Z",
    "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model",
    "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of\nlarge-scale reasoning-oriented RL training focusing on scalability, simplicity\nand accessibility. Through extensive experiments, we demonstrate that a\nminimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and\nstraightforward rule-based rewards, without any KL regularization, is\nsufficient to scale up both response length and benchmark performance, similar\nto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as\nDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on\nAIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating\nremarkable efficiency -- requiring only a tenth of the training steps, compared\nto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our\nsource code, parameter settings, training data, and model weights across\nvarious sizes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24290.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6543
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23829",
      "authors": [
        {
          "_id": "67eb759cb9fa8908e1934f21",
          "name": "Yi Su",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f22",
          "user": {
            "_id": "62d58fd53bf5e059f7cc3245",
            "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
            "isPro": false,
            "fullname": "Dian Yu",
            "user": "yudian",
            "type": "user"
          },
          "name": "Dian Yu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:13:47.119Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f23",
          "user": {
            "_id": "64c94eddcb2f1bf0e7db5a4d",
            "avatarUrl": "/avatars/f7e2532d3c85d5e5b5a02c579ea68c3a.svg",
            "isPro": false,
            "fullname": "Linfeng Song",
            "user": "freesunshine0316",
            "type": "user"
          },
          "name": "Linfeng Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:13:54.065Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f24",
          "user": {
            "_id": "6670e285b0c03c4e9d6e0985",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uCZHm4gKSHZ2b0hpHWgZv.jpeg",
            "isPro": false,
            "fullname": "Juntao Li",
            "user": "douvleplus",
            "type": "user"
          },
          "name": "Juntao Li",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:03.120Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f25",
          "user": {
            "_id": "65147a1426fbd558dbd08f1b",
            "avatarUrl": "/avatars/86574ee2d5c22e940be1c4e50be88675.svg",
            "isPro": false,
            "fullname": "Haitao Mi",
            "user": "haitaominlp",
            "type": "user"
          },
          "name": "Haitao Mi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:10.594Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f26",
          "user": {
            "_id": "67485743561b1e6f9579389f",
            "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
            "isPro": false,
            "fullname": "Zhaopeng Tu",
            "user": "zptu",
            "type": "user"
          },
          "name": "Zhaopeng Tu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:16.978Z",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f27",
          "name": "Min Zhang",
          "hidden": false
        },
        {
          "_id": "67eb759cb9fa8908e1934f28",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T08:22:49.000Z",
      "submittedOnDailyAt": "2025-04-01T03:42:19.595Z",
      "title": "多様な領域で確認可能な報酬を持つRLの拡大",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "強化学習（RL）にチェック可能な報酬（RLVR）を導入したものは、数学的な理由とコーディングタスクでの良い構造化された参照解答がある場合には、見込みの成果を示しています。しかし、これらの手法の広域的な適用可能性は、まだ調査が不足しています。本論文では、医学、化学、心理学、経済学などのより多様な領域に対するRLVRの拡張を研究しています。参照解答がある場合に、異なる大規模な言語モデル（LLMs）が二値判断に高い一致率を示し、ドメイン特有の報酬モデルの大規模な注釈の必要性を疑問にすることを観察しています。二値報酬が無構造化された参照解答を処理する際の制限を解決するために、モデルベースのソフトスコアをRLVRに追加し、その柔軟性を向上させています。実験結果によると、経過された生成的な報酬モデルは、ドメイン特有の注釈が不要であることを示し、強化学習に信頼できる報酬信号を提供します。基盤モデルを異なるRLアルゴリズムで最適化し、私たちの報酬モデルを対象として、Qwen2.5-72B-InstructとDeepSeek-R1-Distill-Qwen-32Bなどの最先端の開放ソースの対応LLMsを大幅に超える性能を収めるポリシーを得ました。これは、自由形式の回答のドメインでの優れた性能を示し、RLVRの強固性とスケーラビリティを強化し、実世界の応用において、ノイズや弱いラベルを含む場合にも実用的であることを示しています。",
      "upvotes": 5,
      "discussionId": "67eb759db9fa8908e1934f62",
      "ai_keywords": [
        "reinforcement learning (RL)",
        "verifiable rewards (RLVR)",
        "mathematical reasoning",
        "coding tasks",
        "well-structured reference answers",
        "diverse domains",
        "medicine",
        "chemistry",
        "psychology",
        "economics",
        "large language models (LLMs)",
        "binary judgments",
        "domain-specific reward models",
        "model-based soft scoring",
        "distilled generative reward model",
        "effective cross-domain verifier",
        "reward signals",
        "fine-tuning",
        "base 7B model",
        "RL algorithms",
        "state-of-the-art open-source aligned LLMs",
        "Qwen2.5-72B-Instruct",
        "DeepSeek-R1-Distill-Qwen-32B",
        "free-form answer settings",
        "robustness",
        "scalability",
        "real-world applications",
        "noisy labels",
        "weak labels"
      ]
    },
    "publishedAt": "2025-03-31T04:22:49.000Z",
    "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
    "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement in binary judgments across different large\nlanguage models (LLMs) when objective reference answers exist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporate model-based soft scoring into RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nBy fine-tuning a base 7B model using various RL algorithms against our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23829.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6543
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.21694",
      "authors": [
        {
          "_id": "67eb92defa85fe030e2db9e2",
          "user": {
            "_id": "64295d1f4e073875f6a605ac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
            "isPro": true,
            "fullname": "Zhiyuan Ma",
            "user": "ZhiyuanthePony",
            "type": "user"
          },
          "name": "Zhiyuan Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:51.486Z",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e3",
          "user": {
            "_id": "672111333ced358bdac2925d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qHNDI3zYRkJZmhCHkSZwK.png",
            "isPro": false,
            "fullname": "Xinyue Liang",
            "user": "DarklordLeto",
            "type": "user"
          },
          "name": "Xinyue Liang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:14:58.992Z",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e4",
          "name": "Rongyuan Wu",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e5",
          "name": "Xiangyu Zhu",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e6",
          "name": "Zhen Lei",
          "hidden": false
        },
        {
          "_id": "67eb92defa85fe030e2db9e7",
          "name": "Lei Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64295d1f4e073875f6a605ac/F5lBcbDKq1_K31O-vxWBg.mp4"
      ],
      "publishedAt": "2025-03-27T16:59:15.000Z",
      "submittedOnDailyAt": "2025-04-01T06:05:21.846Z",
      "title": "進歩的渲染提炼：ディープラーニングを用いた3Dデータなしのインスタントテキストからマーシュ生成の適応",
      "submittedOnDailyBy": {
        "_id": "64295d1f4e073875f6a605ac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
        "isPro": true,
        "fullname": "Zhiyuan Ma",
        "user": "ZhiyuanthePony",
        "type": "user"
      },
      "summary": "高度望いられるのは、テキストプラントからショットで高品質の3Dメッシュを生成するモデルを得ることです。最近の試みは、例えばStable Diffusion（SD）のような事前学習されたテキストから画像への拡散モデルを3D表現の生成モデルに適用しようとしています（例えばTriplane）。しかし、これらは、高品質な3Dトレーニングデータの不足により、質が悪くなることが多いです。データ不足を克服するために、私たちは新しいトレーニングスキームを提案しています。これは、Progressive Rendering Distillation（PRD）と呼ばれ、3D真実データの必要を除去し、多点拡散モデルを経験的に学習させ、SDを3Dノートファイルモデルに適用することです。プログレッシブなトレーニングの各イテレーションで、PRDはU-Netを用いて、ラテントをランダムなノイズから進歩的にノイズをディノイズすることを行い、各ステップでディノイズされたラテンを3D出力に解釈します。多点拡散モデル、例えばMVDreamとRichDreamerは、SDと組み合わせて、スコアディスティルレーションを通じて、テキストの一致性のあるテクスチャとジェネリックを3D出力に濃めます。PRDは3D真実データの必要を除去することで、訓練データの拡大や、複雑なテキストプラントに対する生成品質の向上が容易になります。また、PRDは生成モデルの推論速度を数ステップで加速することができます。PRDを用いて、私たちはTriplaneモデルを訓練し、TriplaneTurboという名前をつけました。TriplaneTurboは、SDをTriplane生成に適用するために、学習可能なパラメーターを増やすだけで2.5%しか増えません。TriplaneTurboは、以前のテキストから3Dへの生成モデルと比較して、効率と品質の両方で優れています。特に、高品質の3Dメッシュを1.2秒で生成し、複雑なテキスト入力にも良く拡張できます。コードは、https://github.com/theEricMa/TriplaneTurboに公開されています。",
      "upvotes": 5,
      "discussionId": "67eb92e2fa85fe030e2dbc04",
      "projectPage": "https://theericma.github.io/TriplaneTurbo/",
      "githubRepo": "https://github.com/theEricMa/TriplaneTurbo",
      "ai_keywords": [
        "diffusion models",
        "Stable Diffusion (SD)",
        "3D representations",
        "Progressive Rendering Distillation (PRD)",
        "U-Net",
        "latent from random noise",
        "denoise the latent",
        "3D output",
        "Multi-view diffusion models",
        "MVDream",
        "RichDreamer",
        "score distillation",
        "text-consistent textures",
        "geometries",
        "Triplane generator",
        "TriplaneTurbo",
        "high-quality 3D meshes"
      ]
    },
    "publishedAt": "2025-03-27T12:59:15.000Z",
    "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for\n  Instant Text-to-Mesh Generation without 3D Data",
    "summary": "It is highly desirable to obtain a model that can generate high-quality 3D\nmeshes from text prompts in just seconds. While recent attempts have adapted\npre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into\ngenerators of 3D representations (e.g., Triplane), they often suffer from poor\nquality due to the lack of sufficient high-quality 3D training data. Aiming at\novercoming the data shortage, we propose a novel training scheme, termed as\nProgressive Rendering Distillation (PRD), eliminating the need for 3D\nground-truths by distilling multi-view diffusion models and adapting SD into a\nnative 3D generator. In each iteration of training, PRD uses the U-Net to\nprogressively denoise the latent from random noise for a few steps, and in each\nstep it decodes the denoised latent into 3D output. Multi-view diffusion\nmodels, including MVDream and RichDreamer, are used in joint with SD to distill\ntext-consistent textures and geometries into the 3D outputs through score\ndistillation. Since PRD supports training without 3D ground-truths, we can\neasily scale up the training data and improve generation quality for\nchallenging text prompts with creative concepts. Meanwhile, PRD can accelerate\nthe inference speed of the generation model in just a few steps. With PRD, we\ntrain a Triplane generator, namely TriplaneTurbo, which adds only 2.5%\ntrainable parameters to adapt SD for Triplane generation. TriplaneTurbo\noutperforms previous text-to-3D generators in both efficiency and quality.\nSpecifically, it can produce high-quality 3D meshes in 1.2 seconds and\ngeneralize well for challenging text input. The code is available at\nhttps://github.com/theEricMa/TriplaneTurbo.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64295d1f4e073875f6a605ac/F5lBcbDKq1_K31O-vxWBg.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.21694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64295d1f4e073875f6a605ac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64295d1f4e073875f6a605ac/HEK8dluqPUhiARW-0eBZG.jpeg",
      "fullname": "Zhiyuan Ma",
      "name": "ZhiyuanthePony",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.19901",
      "authors": [
        {
          "_id": "67eac6433755a17e3cbff585",
          "user": {
            "_id": "6630cc7e9ee8861dd0b9bdbd",
            "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
            "isPro": false,
            "fullname": "Liang Pan",
            "user": "lianganimation",
            "type": "user"
          },
          "name": "Liang Pan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:47:34.754Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff586",
          "user": {
            "_id": "649c178950b4be74229d680f",
            "avatarUrl": "/avatars/941dec90fdfc46b9ae23378e3a3113f4.svg",
            "isPro": false,
            "fullname": "Zeshi Yang",
            "user": "Zeshi209",
            "type": "user"
          },
          "name": "Zeshi Yang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:03.851Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff587",
          "user": {
            "_id": "645223fb01d7bd9555ea399a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
            "isPro": false,
            "fullname": "Zhiyang Dou",
            "user": "frankzydou",
            "type": "user"
          },
          "name": "Zhiyang Dou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:09.328Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff588",
          "user": {
            "_id": "6437a813fac5ea753f1c72d2",
            "avatarUrl": "/avatars/69e60e60497e404149a1dad46649dad4.svg",
            "isPro": false,
            "fullname": "wenjia Wang",
            "user": "WenjiaWang",
            "type": "user"
          },
          "name": "Wenjia Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:22.428Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff589",
          "name": "Buzhen Huang",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff58a",
          "user": {
            "_id": "635f93577c05eb9f59966209",
            "avatarUrl": "/avatars/add48d7e3790a6b500d6c451ef8b0f75.svg",
            "isPro": false,
            "fullname": "Intelligent Digital Creation",
            "user": "BoDai",
            "type": "user"
          },
          "name": "Bo Dai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:59.939Z",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff58b",
          "name": "Taku Komura",
          "hidden": false
        },
        {
          "_id": "67eac6433755a17e3cbff58c",
          "user": {
            "_id": "669cb638666901f41dae51bf",
            "avatarUrl": "/avatars/a7cc19e3db84bd86bee3eb6fd4897959.svg",
            "isPro": false,
            "fullname": "Jingbo Wang",
            "user": "jingbocuhk",
            "type": "user"
          },
          "name": "Jingbo Wang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:21:43.455Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6630cc7e9ee8861dd0b9bdbd/2bErZDgOyv5xeo1wDfMz3.mp4"
      ],
      "publishedAt": "2025-03-25T17:57:46.000Z",
      "submittedOnDailyAt": "2025-04-01T06:24:17.315Z",
      "title": "TokenHSI: 物理的ヒト・スケーン相互作用の統合的な合成を通じて\n  タスクトークン化",
      "submittedOnDailyBy": {
        "_id": "6630cc7e9ee8861dd0b9bdbd",
        "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
        "isPro": false,
        "fullname": "Liang Pan",
        "user": "lianganimation",
        "type": "user"
      },
      "summary": "多様な物理的に可能なHuman-Scene Interactions（HSI）の合成は、コンピューターアニメーションと具象化AIの両方にとって重要である。進歩が見覚えられるのに対して、現在の方法は主に別々のコントローラーの開発を焦点としていて、特定のインタラクションタスクに専門化されている。これは、多様な難しいHSIタスクの対処能力を大きく制限している。この問題を解決するために、我々はTokenHSIを紹介します。TokenHSIは、多スキルの統合と柔軟な適応性を可能にするための単一の統合されたTransformerベースのポリシーです。鍵の見解は、人間型の親体認識を別々の共有トークンとしてモデル化し、それをマスク機構を通じて特定のタスクトークンと組み合わせることです。このような統合されたポリシーは、スキル間での知識共有を可能にし、このように多タスクのトレーニングを促進します。また、我々のポリシーアーキテクチャは、変長の入力をサポートし、学習されたスキルの柔軟な適応性を可能にします。さらに、追加のタスクトークナイザーを訓練することで、インタラクションターゲットのジオメトリーを変更することや、複雑なタスクを解決するために複数のスキルを協調することができます。実験は、我々のアプローチが多様性、適応性、拡張性を大幅に向上させることを示しています。ウェブサイト：https://liangpan99.github.io/TokenHSI/",
      "upvotes": 5,
      "discussionId": "67eac6443755a17e3cbff5cf",
      "projectPage": "https://liangpan99.github.io/TokenHSI/",
      "githubRepo": "https://github.com/liangpan99/TokenHSI",
      "ai_keywords": [
        "transformer-based policy",
        "proprioception",
        "shared token",
        "task tokens",
        "masking mechanism",
        "multi-task training",
        "variable length inputs",
        "task tokenizers"
      ]
    },
    "publishedAt": "2025-03-25T13:57:46.000Z",
    "title": "TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through\n  Task Tokenization",
    "summary": "Synthesizing diverse and physically plausible Human-Scene Interactions (HSI)\nis pivotal for both computer animation and embodied AI. Despite encouraging\nprogress, current methods mainly focus on developing separate controllers, each\nspecialized for a specific interaction task. This significantly hinders the\nability to tackle a wide variety of challenging HSI tasks that require the\nintegration of multiple skills, e.g., sitting down while carrying an object. To\naddress this issue, we present TokenHSI, a single, unified transformer-based\npolicy capable of multi-skill unification and flexible adaptation. The key\ninsight is to model the humanoid proprioception as a separate shared token and\ncombine it with distinct task tokens via a masking mechanism. Such a unified\npolicy enables effective knowledge sharing across skills, thereby facilitating\nthe multi-task training. Moreover, our policy architecture supports variable\nlength inputs, enabling flexible adaptation of learned skills to new scenarios.\nBy training additional task tokenizers, we can not only modify the geometries\nof interaction targets but also coordinate multiple skills to address complex\ntasks. The experiments demonstrate that our approach can significantly improve\nversatility, adaptability, and extensibility in various HSI tasks. Website:\nhttps://liangpan99.github.io/TokenHSI/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6630cc7e9ee8861dd0b9bdbd/2bErZDgOyv5xeo1wDfMz3.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.19901.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6630cc7e9ee8861dd0b9bdbd",
      "avatarUrl": "/avatars/01da7d60cbb107d58329bbb80d924eb4.svg",
      "fullname": "Liang Pan",
      "name": "lianganimation",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.14941",
      "authors": [
        {
          "_id": "67eb932522a341478ae86cb6",
          "user": {
            "_id": "67a99d1fef1439e285c4cbec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
            "isPro": false,
            "fullname": "Qihui Zhang",
            "user": "77Hui",
            "type": "user"
          },
          "name": "Qihui Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:00.373Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb7",
          "user": {
            "_id": "65e14c28b1a6de8a71e70172",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e14c28b1a6de8a71e70172/D097SILGsqoufpp3sG8tV.jpeg",
            "isPro": false,
            "fullname": "Munan Ning",
            "user": "MunanNing",
            "type": "user"
          },
          "name": "Munan Ning",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:18:39.336Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb8",
          "name": "Zheyuan Liu",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cb9",
          "name": "Yanbo Wang",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cba",
          "name": "Jiayi Ye",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbb",
          "user": {
            "_id": "6637443ecd9097ac3c996d3c",
            "avatarUrl": "/avatars/d1c38bf03c2517ba0a7004b2f9f9bc96.svg",
            "isPro": false,
            "fullname": "yue",
            "user": "yuehuang",
            "type": "user"
          },
          "name": "Yue Huang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:19:26.627Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbc",
          "name": "Shuo Yang",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbd",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbe",
          "user": {
            "_id": "62c51800cb7033fd49b8efb7",
            "avatarUrl": "/avatars/06c2be0015f8022f9912f2279f2b3597.svg",
            "isPro": false,
            "fullname": "Song",
            "user": "Yibing",
            "type": "user"
          },
          "name": "Yibing Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-04-01T08:19:07.616Z",
          "hidden": false
        },
        {
          "_id": "67eb932522a341478ae86cbf",
          "name": "Li Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T07:15:41.000Z",
      "submittedOnDailyAt": "2025-04-01T05:48:16.581Z",
      "title": "UPME: 多モデル大語言モデル評価の無チェックペアレビューフレームワーク",
      "submittedOnDailyBy": {
        "_id": "67a99d1fef1439e285c4cbec",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
        "isPro": false,
        "fullname": "Qihui Zhang",
        "user": "77Hui",
        "type": "user"
      },
      "summary": "多タイプ大語言モデル（MLLMs）は、視覚問題回答（VQA）の課題を解決するために現れ、これらのモデルの主観的評価を行う新しい研究焦点を引き起こしました。現在の評価方法は、画像に対するQ&Aペアの設計に必要な大きな人力負担により制限され、評価のスケールと範囲が狭まります。自動化されたMLLM-as-judgeアプローチは、自動評価を通じて人力負担を減らすことを試みていますが、それほど偏りを引き起こします。これらの問題を解決するために、私たちは無监督的な同僚評価MLLM評価フレームワークを提案します。これは、画像データのみを利用し、モデルが自動的に質問を生成し、他のモデルからの回答を同僚評価します。これにより、人力負担の依存性を解消します。また、私たちは、視覚理解と理由論、画像と文の関連性、応答の正確性の3つの面でのビジョン言語スコアシステムを紹介し、これにより偏り問題を軽減します。実験結果によると、UPMEはMMstarデータセットでPearson相関係数0.944、ScienceQAデータセットで0.814と、人間の評価に一致し、我々のフレームワークは人間が設計したベンチマークと固有の人間の好みに非常に近いことを示します。",
      "upvotes": 3,
      "discussionId": "67eb932622a341478ae86d15",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Visual Question Answering (VQA)",
        "Q&A pairs",
        "MLLM-as-judge",
        "Unsupervised Peer review MLLM Evaluation (UPME)",
        "vision-language scoring system",
        "response correctness",
        "visual understanding and reasoning",
        "image-text correlation",
        "Pearson correlation",
        "MMstar dataset",
        "ScienceQA dataset"
      ]
    },
    "publishedAt": "2025-03-19T03:15:41.000Z",
    "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation",
    "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.14941.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a99d1fef1439e285c4cbec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VrwUmrY2wsg4sVSIMc--K.png",
      "fullname": "Qihui Zhang",
      "name": "77Hui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.24391",
      "authors": [
        {
          "_id": "67eb72a2291b56e50b66a063",
          "user": {
            "_id": "66606a13fc6c0816442bd161",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
            "isPro": false,
            "fullname": "Xingyu Chen",
            "user": "rover-xingyu",
            "type": "user"
          },
          "name": "Xingyu Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:33.467Z",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a064",
          "user": {
            "_id": "66f80281d88dc2ad510663e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5HV3mTu-cxPCnWlCi_2wB.jpeg",
            "isPro": false,
            "fullname": "Yue Chen",
            "user": "faneggg",
            "type": "user"
          },
          "name": "Yue Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:31.158Z",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a065",
          "name": "Yuliang Xiu",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a066",
          "name": "Andreas Geiger",
          "hidden": false
        },
        {
          "_id": "67eb72a2291b56e50b66a067",
          "name": "Anpei Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T17:59:58.000Z",
      "submittedOnDailyAt": "2025-04-01T07:01:19.970Z",
      "title": "Easi3R: トレーニング不要でのDUSt3Rからの分離された動きの推定",
      "submittedOnDailyBy": {
        "_id": "66606a13fc6c0816442bd161",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
        "isPro": false,
        "fullname": "Xingyu Chen",
        "user": "rover-xingyu",
        "type": "user"
      },
      "summary": "最近のDUSt3Rの進展により、静的スケーンの密度の高い点群とカメラパラメータの強固な推定が可能になりました。これは、Transformerネットワーク構造と大規模な3Dデータセット上の直接的な規範化を利用して実現されました。対比的に、利用可能な4Dデータセットの限界なスケールと多様性は、高度な一般化性能の4Dモデルの訓練において大きなボトルネックとなっています。この制約は、単なる3Dモデルのスケーラブルな動的なビデオデータ上での調整とオプティカルフローや深さなどの追加的な幾何的な先驚を含む従来の4D手法を適用しています。この研究では、対立的な道を歩み、Easi3Rを導入します。Easi3Rは簡単で効率的な4D再構成の訓練無し方法です。我々のアプローチは推論時にアテンションアドゥポートを適用し、スクラッチからの再起動またはネットワークの調整を必要としません。我々はDUSt3Rのアテンション層がカメラと物体の動きに関する豊富な情報を内在的に含んでいることを見出しました。これらのアテンションマップを慎重に分離することで、動的な領域分割、カメラの姿勢推定、4Dの密度の高い点群の再構成を実現しました。実世界的な動的なビデオ上の拡大的な実験は、我々の軽量アテンションアドゥポートが、広範な動的なデータセット上で訓練された前の最先端の方法を大幅に超えることを示しました。我々のコードは、https://easi3r.github.io/ で研究のために公開されています。",
      "upvotes": 2,
      "discussionId": "67eb72a5291b56e50b66a152",
      "ai_keywords": [
        "Transformer network architectures",
        "dense point clouds",
        "camera parameters",
        "direct supervision",
        "3D datasets",
        "4D datasets",
        "4D model",
        "fine-tune",
        "dynamic video data",
        "geometric priors",
        "optical flow",
        "depths",
        "training-free method",
        "4D reconstruction",
        "attention adaptation",
        "inference",
        "attention layers",
        "dynamic region segmentation",
        "camera pose estimation",
        "4D dense point map reconstruction"
      ]
    },
    "publishedAt": "2025-03-31T13:59:58.000Z",
    "title": "Easi3R: Estimating Disentangled Motion from DUSt3R Without Training",
    "summary": "Recent advances in DUSt3R have enabled robust estimation of dense point\nclouds and camera parameters of static scenes, leveraging Transformer network\narchitectures and direct supervision on large-scale 3D datasets. In contrast,\nthe limited scale and diversity of available 4D datasets present a major\nbottleneck for training a highly generalizable 4D model. This constraint has\ndriven conventional 4D methods to fine-tune 3D models on scalable dynamic video\ndata with additional geometric priors such as optical flow and depths. In this\nwork, we take an opposite path and introduce Easi3R, a simple yet efficient\ntraining-free method for 4D reconstruction. Our approach applies attention\nadaptation during inference, eliminating the need for from-scratch pre-training\nor network fine-tuning. We find that the attention layers in DUSt3R inherently\nencode rich information about camera and object motion. By carefully\ndisentangling these attention maps, we achieve accurate dynamic region\nsegmentation, camera pose estimation, and 4D dense point map reconstruction.\nExtensive experiments on real-world dynamic videos demonstrate that our\nlightweight attention adaptation significantly outperforms previous\nstate-of-the-art methods that are trained or finetuned on extensive dynamic\ndatasets. Our code is publicly available for research purpose at\nhttps://easi3r.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.24391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66606a13fc6c0816442bd161",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66606a13fc6c0816442bd161/tS8pBDXEb3QkIjvZao55l.jpeg",
      "fullname": "Xingyu Chen",
      "name": "rover-xingyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23730",
      "authors": [
        {
          "_id": "67eb567141abf40cd86e0e15",
          "user": {
            "_id": "67038a66eb760972bcb62c70",
            "avatarUrl": "/avatars/8cc82af8f11cae994bc83f4bd99b51bc.svg",
            "isPro": false,
            "fullname": "김윤식",
            "user": "yoonshik1205",
            "type": "user"
          },
          "name": "Yoonshik Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:53.729Z",
          "hidden": false
        },
        {
          "_id": "67eb567141abf40cd86e0e16",
          "user": {
            "_id": "646484cfb90150b2706df03b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
            "isPro": true,
            "fullname": "Jaeyoon Jung",
            "user": "lastdefiance20",
            "type": "user"
          },
          "name": "Jaeyoon Jung",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-01T07:46:56.151Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T05:04:25.000Z",
      "submittedOnDailyAt": "2025-04-01T01:43:40.701Z",
      "title": "KOFFVQA: 韓国語での大規模な視覚言語モデルの主観的に評価された自由形式のVQAベンチマーク",
      "submittedOnDailyBy": {
        "_id": "646484cfb90150b2706df03b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
        "isPro": true,
        "fullname": "Jaeyoon Jung",
        "user": "lastdefiance20",
        "type": "user"
      },
      "summary": "最近によって登場したLarge Vision-Language Models（VLMs）は、これらのモデルの評価に関する様々なベンチマークが出現しました。しかし、これらの評価方法では、モデルが事前決められた回答から選択する必要があることや、判定モデルを用いて評価することで、主観的で不信頼な評価が生じることが見られます。また、韓国語のVLMsのベンチマークは、英語のベンチマークと異なる独立な評価基準として必要となりますが、これらが存在しません。これらの理由により、生成モデルの性能は言語によって大幅に異なることがわかります。そこで、KOFFVQAという一般的な韓国語の自由形式の視覚問答ベンチマークを提案します。このベンチマークは、275問の詳細に設計された問題それぞれに画像を付け、VLMの性能に関する10つの異なる面での評価基準を含みます。評価基準は、判定モデルが事前決められたルールに基づいて各回答を評価することで、不信頼性の問題を解決します。評価基準を客観的に定義することで、小さな開放ソースモデルもこのベンチマークにおいて信頼的に評価できるようになります。また、現存するVLMsの多くをベンチマークにおいて評価し、この方法で評価することで、現在の方法よりも非常に信頼的であることを実験的に確認しました。評価コードは、https://github.com/maum-ai/KOFFVQA から利用できます。",
      "upvotes": 2,
      "discussionId": "67eb567341abf40cd86e0e63",
      "githubRepo": "https://github.com/maum-ai/KOFFVQA",
      "ai_keywords": [
        "Large Vision-Language Models (VLMs)",
        "visual question answering benchmark",
        "grading criteria"
      ]
    },
    "publishedAt": "2025-03-31T01:04:25.000Z",
    "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language",
    "summary": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23730.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646484cfb90150b2706df03b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646484cfb90150b2706df03b/8ocSbXBSbrruhlhcxwzEt.png",
      "fullname": "Jaeyoon Jung",
      "name": "lastdefiance20",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.23022",
      "authors": [
        {
          "_id": "67ebadecf9f9390b4cd1c6d9",
          "name": "Xianglong He",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6da",
          "name": "Junyi Chen",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6db",
          "name": "Di Huang",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6dc",
          "name": "Zexiang Liu",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6dd",
          "name": "Xiaoshui Huang",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6de",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6df",
          "name": "Chun Yuan",
          "hidden": false
        },
        {
          "_id": "67ebadecf9f9390b4cd1c6e0",
          "name": "Yangguang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-29T09:21:50.000Z",
      "submittedOnDailyAt": "2025-04-01T07:42:31.137Z",
      "title": "MeshCraft: フローベースのDiTsを用いた効率的かつ制御可能なメッシュ生成の検討",
      "submittedOnDailyBy": {
        "_id": "64d71083a787c9bc7b9f1238",
        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
        "isPro": false,
        "fullname": "Yangguang Li",
        "user": "Lp256",
        "type": "user"
      },
      "summary": "3Dコンテンツ作成の領域では、AIモデルを利用して最適なメッシュトポロジーを達成することは長年3Dアーティストたちの追求でした。以前の方法では、MeshGPTなどがマシン学習を用いて準備された3Dオブジェクトの生成を試みてきました。これらの方法は視覚的に印象的な結果を出しますが、自動復元過程でトークンごとの予測を依存していることから、極端に遅くなる生成速度やメッシュ面の数が制限されないなど、いくつかの大きな制限がありました。本論文では、連続的な空間ディフュージョンを利用して離散な三角形面を生成するための効率的かつ制御可能なメッシュ生成の新しいフレームワークを紹介します。特に、MeshCraftは2つの核心的な構成要素を構成しています：1）変換器ベースのVAEでは、ハイプロンブストラクチャを離散な面レベルトークンに変換し、元のメッシュに戻します。2）面の数に基づくフローベースディフュージョン変換器では、特定の面の数で高品質な3Dメッシュを生成することができます。MeshCraftは、ディフュージョンモデルを利用してメッシュ全体のトポロジーを同時に生成することで、自動復元モデルよりも非常に高速に高品質なメッシュ生成を達成します。特に、MeshCraftは800面のメッシュを3.2秒で生成できます（現在の基準と比較して35倍速くなります）。拡散的な実験は、ShapeNetデータセットでの質的的と量的な評価で最先端の技術を超え、Objaverseデータセットでも優れた性能を示します。また、現在の条件付きガイドニング戦略と無間に統合でき、メッシュ作成における時間がかかる手動ワークからアーティストへと解放することを示しています。",
      "upvotes": 1,
      "discussionId": "67ebadeef9f9390b4cd1c7b3",
      "ai_keywords": [
        "mesh auto-regressive techniques",
        "continuous spatial diffusion",
        "transformer-based VAE",
        "flow-based diffusion transformer",
        "high-fidelity mesh generation",
        "ShapeNet dataset",
        "Objaverse dataset",
        "conditional guidance"
      ]
    },
    "publishedAt": "2025-03-29T05:21:50.000Z",
    "title": "MeshCraft: Exploring Efficient and Controllable Mesh Generation with\n  Flow-based DiTs",
    "summary": "In the domain of 3D content creation, achieving optimal mesh topology through\nAI models has long been a pursuit for 3D artists. Previous methods, such as\nMeshGPT, have explored the generation of ready-to-use 3D objects via mesh\nauto-regressive techniques. While these methods produce visually impressive\nresults, their reliance on token-by-token predictions in the auto-regressive\nprocess leads to several significant limitations. These include extremely slow\ngeneration speeds and an uncontrollable number of mesh faces. In this paper, we\nintroduce MeshCraft, a novel framework for efficient and controllable mesh\ngeneration, which leverages continuous spatial diffusion to generate discrete\ntriangle faces. Specifically, MeshCraft consists of two core components: 1) a\ntransformer-based VAE that encodes raw meshes into continuous face-level tokens\nand decodes them back to the original meshes, and 2) a flow-based diffusion\ntransformer conditioned on the number of faces, enabling the generation of\nhigh-quality 3D meshes with a predefined number of faces. By utilizing the\ndiffusion model for the simultaneous generation of the entire mesh topology,\nMeshCraft achieves high-fidelity mesh generation at significantly faster speeds\ncompared to auto-regressive methods. Specifically, MeshCraft can generate an\n800-face mesh in just 3.2 seconds (35times faster than existing baselines).\nExtensive experiments demonstrate that MeshCraft outperforms state-of-the-art\ntechniques in both qualitative and quantitative evaluations on ShapeNet dataset\nand demonstrates superior performance on Objaverse dataset. Moreover, it\nintegrates seamlessly with existing conditional guidance strategies, showcasing\nits potential to relieve artists from the time-consuming manual work involved\nin mesh creation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d71083a787c9bc7b9f1238",
      "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
      "fullname": "Yangguang Li",
      "name": "Lp256",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.20286",
      "authors": [
        {
          "_id": "67eaa88c40bebc3127ade04c",
          "user": {
            "_id": "67e77099284080c98d8c9bfc",
            "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
            "isPro": false,
            "fullname": "Zhenyu Liang",
            "user": "ZhenyuLiang",
            "type": "user"
          },
          "name": "Zhenyu Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-31T15:15:16.646Z",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04d",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04e",
          "name": "Naiwei Yu",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade04f",
          "name": "Kebin Sun",
          "hidden": false
        },
        {
          "_id": "67eaa88c40bebc3127ade050",
          "name": "Ran Cheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-26T07:30:23.000Z",
      "submittedOnDailyAt": "2025-04-01T00:34:02.763Z",
      "title": "進化多目的最適化とGPU加速を結ぶためのテンソリゼーション手法",
      "submittedOnDailyBy": {
        "_id": "67e77099284080c98d8c9bfc",
        "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
        "isPro": false,
        "fullname": "Zhenyu Liang",
        "user": "ZhenyuLiang",
        "type": "user"
      },
      "summary": "進化多目標最適化（EMO）は、過去20年間において顕著な進展を遂げています。しかし、問題サイズと複雑さが増大するに伴い、伝統的なEMOアルゴリズムは、不足している並列性とスケーラビリティによって性能の限界が見出されています。多くの研究は、これらの課題に対処するためのアルゴリズム設計に焦点を当てているが、ハードウェア加速に対する注意が少なく、EMOアルゴリズムとGPUなどの先進的な計算装置の間に明確なギャップが残されています。このギャップを埋めるために、GPU上でのEMOアルゴリズムの並列化を提案します。テンソリゼーション手法を用いて、EMOアルゴリズムのデータ構造と操作を簡潔なテンソル表現に変換し、これによりGPU計算の自動的な利用が可能になります。我々のアプローチの効果を示すために、NSGA-III、MOEA/D、HypEの3つの代表的なEMOアルゴリズムに対して応用します。我々の方法を全体的に評価するために、GPU加速された物理エンジンを用いた多目標ロボット制御ベンチマークを導入します。実験結果によると、テンソリゼーションされたEMOアルゴリズムは、CPUベースのコンピューターと比べて1113倍のスピードアップを達成し、解の品質を維持しながら、人口サイズを数万に拡大できます。また、テンソリゼーションされたEMOアルゴリズムは、複雑な多目標ロボット制御タスクを効率的に解決し、多様な行動を持つ高品質の解を生成します。ソースコードは、https://github.com/EMI-Group/evomoにアクセス可能です。",
      "upvotes": 1,
      "discussionId": "67eaa88e40bebc3127ade0eb",
      "githubRepo": "https://github.com/EMI-Group/evomo",
      "ai_keywords": [
        "evolutionary multiobjective optimization (EMO)",
        "parallelism",
        "scalability",
        "GPU",
        "tensorization",
        "tensor representations",
        "NSGA-III",
        "MOEA/D",
        "HypE",
        "GPU-accelerated physics engine",
        "multiobjective robot control benchmark",
        "population sizes",
        "high-quality solutions",
        "diverse behaviors"
      ]
    },
    "publishedAt": "2025-03-26T03:30:23.000Z",
    "title": "Bridging Evolutionary Multiobjective Optimization and GPU Acceleration\n  via Tensorization",
    "summary": "Evolutionary multiobjective optimization (EMO) has made significant strides\nover the past two decades. However, as problem scales and complexities\nincrease, traditional EMO algorithms face substantial performance limitations\ndue to insufficient parallelism and scalability. While most work has focused on\nalgorithm design to address these challenges, little attention has been given\nto hardware acceleration, thereby leaving a clear gap between EMO algorithms\nand advanced computing devices, such as GPUs. To bridge the gap, we propose to\nparallelize EMO algorithms on GPUs via the tensorization methodology. By\nemploying tensorization, the data structures and operations of EMO algorithms\nare transformed into concise tensor representations, which seamlessly enables\nautomatic utilization of GPU computing. We demonstrate the effectiveness of our\napproach by applying it to three representative EMO algorithms: NSGA-III,\nMOEA/D, and HypE. To comprehensively assess our methodology, we introduce a\nmultiobjective robot control benchmark using a GPU-accelerated physics engine.\nOur experiments show that the tensorized EMO algorithms achieve speedups of up\nto 1113x compared to their CPU-based counterparts, while maintaining solution\nquality and effectively scaling population sizes to hundreds of thousands.\nFurthermore, the tensorized EMO algorithms efficiently tackle complex\nmultiobjective robot control tasks, producing high-quality solutions with\ndiverse behaviors. Source codes are available at\nhttps://github.com/EMI-Group/evomo.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.20286.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "67e77099284080c98d8c9bfc",
      "avatarUrl": "/avatars/a3120e8d9b1312d8a670161b674f3196.svg",
      "fullname": "Zhenyu Liang",
      "name": "ZhenyuLiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.18225",
      "authors": [
        {
          "_id": "67ebabe3c545cab686735182",
          "name": "Massimo Bini",
          "hidden": false
        },
        {
          "_id": "67ebabe3c545cab686735183",
          "name": "Leander Girrbach",
          "hidden": false
        },
        {
          "_id": "67ebabe3c545cab686735184",
          "name": "Zeynep Akata",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-23T22:00:56.000Z",
      "submittedOnDailyAt": "2025-04-01T07:38:06.739Z",
      "title": "低順位調整での角度と強度の独立化",
      "submittedOnDailyBy": {
        "_id": "63f62ee3b29015adc33aafa0",
        "avatarUrl": "/avatars/b7d7fe1b65333fb698402ed065fc5d13.svg",
        "isPro": false,
        "fullname": "Massimo Bini",
        "user": "mwbini",
        "type": "user"
      },
      "summary": "パラメータ効率的微調（Parameter-Efficient FineTuning, PEFT）の方法は、大規模な事前学習モデルの普及により最近によってより広く気に入ってきました。これらの方法は、最小限の計算コストでダウンストリームタスクに迅速に適応できます。しかし、ローラ（LoRA）などの有名な微調方法は、超パラメータの選択または長期的な学習プランに対しては限定的な強固性を持ち、最適なプロダクト性能を示すことができません。対照的に、エター（ETHER）などの制限付きアプローチは、強固性を高めることができますが、非常に低レンジの適応および固定ストレングスの変換に限られ、適応表現力を低下させます。本稿では、学習可能な低レンジ行列の正規化とスケーリングを行う新しい微調方法である、DeLoRA（Decoupled Low-rank Adaptation）を提案します。DeLoRAは、変換の距離を制限することで、角の学習と適応の強度を離れ、強固性を高めるコストを負担せずに行います。主題駆動画像生成、自然言語理解、および指示チューニングの評価を通じて、DeLoRAは、対戦するPEFT方法の性能を満たしたり、それよりも良く示し、強固性が高いことを示します。コードは、https://github.com/ExplainableML/DeLoRA にあります。",
      "upvotes": 1,
      "discussionId": "67ebabe5c545cab68673521b",
      "githubRepo": "https://github.com/ExplainableML/DeLoRA",
      "ai_keywords": [
        "Parameter-Efficient FineTuning (PEFT)",
        "LoRA",
        "bounded approaches",
        "ETHER",
        "Decoupled Low-rank Adaptation (DeLoRA)",
        "learnable low-rank matrices",
        "angular learning",
        "adaptation strength",
        "subject-driven image generation",
        "natural language understanding",
        "instruction tuning"
      ]
    },
    "publishedAt": "2025-03-23T18:00:56.000Z",
    "title": "Decoupling Angles and Strength in Low-rank Adaptation",
    "summary": "Parameter-Efficient FineTuning (PEFT) methods have recently gained\nsignificant popularity thanks to the widespread availability of large-scale\npretrained models. These methods allow for quick adaptation to downstream tasks\nwith minimal computational cost. However, popular finetuning methods such as\nLoRA exhibit limited robustness when it comes to hyperparameter choices or\nextended training regimes, preventing optimal out-of-the-box performance. In\ncontrast, bounded approaches, such as ETHER, provide greater robustness but are\nlimited to extremely low-rank adaptations and fixed-strength transformations,\nreducing their adaptation expressive power. In this work, we propose Decoupled\nLow-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and\nscales learnable low-rank matrices. By bounding the distance of the\ntransformation, DeLoRA effectively decouples the angular learning from the\nadaptation strength, enhancing robustness without compromising performance.\nThrough evaluations on subject-driven image generation, natural language\nunderstanding, and instruction tuning, we show that DeLoRA matches or surpasses\nperformance of competing PEFT methods, while exhibiting stronger robustness.\nCode is available at https://github.com/ExplainableML/DeLoRA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.18225.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f62ee3b29015adc33aafa0",
      "avatarUrl": "/avatars/b7d7fe1b65333fb698402ed065fc5d13.svg",
      "fullname": "Massimo Bini",
      "name": "mwbini",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.23913",
      "authors": [
        {
          "_id": "67ebaea15baac6e5085afcb9",
          "name": "Xiaoxuan Wang",
          "hidden": false
        },
        {
          "_id": "67ebaea15baac6e5085afcba",
          "name": "Yihe Deng",
          "hidden": false
        },
        {
          "_id": "67ebaea15baac6e5085afcbb",
          "name": "Mingyu Derek Ma",
          "hidden": false
        },
        {
          "_id": "67ebaea15baac6e5085afcbc",
          "name": "Wei Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-31T10:04:35.000Z",
      "submittedOnDailyAt": "2025-04-01T07:47:10.184Z",
      "title": "ヒストロピーベースの自動重み調整による自学習",
      "submittedOnDailyBy": {
        "_id": "64ba5946c0f19c9025665a3c",
        "avatarUrl": "/avatars/bb148094ce52f1f385d30968dc22e0e6.svg",
        "isPro": false,
        "fullname": "Xiaoxuan Wang",
        "user": "xw27",
        "type": "user"
      },
      "summary": "大語言モデルの数学問題解決能力は、研究の焦点となり、自動生成された理由のパスを利用してモデルを改善する可能性に興味が高まっています。これらのパスは、正しい答えのみを必要とした説明のステップごとの論理的過程を捉えます。自動学習法は、外部モデルと手動注釈の必要性を除いた理由のタスクに効果的であり、これを実現します。しかし、自動生成されたデータのモデル訓練の最適化は開放的な課題です。本稿では、自動学習の適応重み付けを実現する「Entropy-Based Adaptive Weighting for Self-Training (EAST)」を提案します。EASTは、自動学習の際にデータの不確実性を優先した適応重み付けを行うための戦略です。特に、EASTは、重みのシャープさを制御する調節パラメータを持つマッピング関数を使用し、モデルがより不確実なデータにおいてより高い重みを割り当てます。このアプローチは、モデルが情報量の多いおよび難しい例をより多くの注目を集めるように、理由の能力を向上させます。GSM8KとMATHベンチマークで、実験結果は、バーチャル方法はMATHでは約0%の改善が見られませんでしたが、EASTはベースモデルより約1%の改善を収めました。GSM8Kでは、バーチャル方法よりさらに1-2%の性能向上を収めました。",
      "upvotes": 0,
      "discussionId": "67ebaea25baac6e5085afcfe",
      "ai_keywords": [
        "Entropy-Based Adaptive Weighting for Self-Training (EAST)",
        "mapping function",
        "tunable parameter",
        "weighting strategy",
        "uncertainty",
        "informative examples",
        "challenging examples"
      ]
    },
    "publishedAt": "2025-03-31T06:04:35.000Z",
    "title": "Entropy-Based Adaptive Weighting for Self-Training",
    "summary": "The mathematical problem-solving capabilities of large language models have\nbecome a focal point of research, with growing interests in leveraging\nself-generated reasoning paths as a promising way to refine and enhance these\nmodels. These paths capture step-by-step logical processes while requiring only\nthe correct answer for supervision. The self-training method has been shown to\nbe effective in reasoning tasks while eliminating the need for external models\nand manual annotations. However, optimizing the use of self-generated data for\nmodel training remains an open challenge. In this work, we propose\nEntropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive\nweighting strategy designed to prioritize uncertain data during self-training.\nSpecifically, EAST employs a mapping function with a tunable parameter that\ncontrols the sharpness of the weighting, assigning higher weights to data where\nthe model exhibits greater uncertainty. This approach guides the model to focus\non more informative and challenging examples, thereby enhancing its reasoning\nability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical\nresults show that, while the vanilla method yields virtually no improvement\n(0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K,\nEAST attains a further 1-2% performance boost compared to the vanilla method.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.23913.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ba5946c0f19c9025665a3c",
      "avatarUrl": "/avatars/bb148094ce52f1f385d30968dc22e0e6.svg",
      "fullname": "Xiaoxuan Wang",
      "name": "xw27",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]